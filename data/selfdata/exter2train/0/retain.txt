A verb paradigm is a set of inflectional categories for a single verb lemma.
We present the dialogue module of the speech-to-speech translation system VERB-MOBIL.
We describe an approach to resolving definite descriptions and pronominal anaphora as subcases of a general strategy for presupposition satisfaction.
We investigate prototype-driven learning for primarily unsupervised grammar induction.
We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines.
We apply interpretive rules to clausal patterns and patterns of modification, and concurrently abstract general 損ossibilistic?propositions from the resulting formulas.
We defined 10 Descriptive Answer Type(DAT)s as answer types for descriptive questions.
We propose a syntax of it-clefts using Tree-Local Multi- Component Tree Adjoining Grammar and a compositional semantics on the proposed syntax using Synchronous Tree Adjoining Grammar.
information retrieval, translation unit discovery, and corpus studies.
This paper describes how to construct a finite-state machine (FSM) approximating a 'unification-based' grammar using a left-corner grammar transform.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
We present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections.
The sys-tem is an application of maximum entropy clas-sification for question/answer type prediction and named entity marking.
We describe sign translation using example based machine translation technology.
This paper presents a sentence- based statistical model for Hangeul-Hanja conversion, with word tokenization included as a hidden process.
information extraction).
In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inflectional languages.
We present a generic template for spoken dialogue systems integrating speech recognition and synthesis with 'higher-level' natural language dialogue modelling components.
classification of unseen verbs.
The paper fo-cuses on phonology.
We present a discriminative, large- margin approach to feature-based matching for word alignment.
We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task.
This paper presents the user-centered interface of a summarization system for physicians in Bone Marrow Transplan-tation (BMT).
the complexity of language domain and concept inventory).
We introduce CST (cross-document structure theory), a paradigm for multi-document analysis.
We present the multilingual sum-marization functionality for VERB-MOBIL, a speech translation system.
In this paper we present Morphy, an integrated tool for German morphology, part-ofspeech tagging and context-sensitive lemmatization.
This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG pars-ing system.
This paper proposes a two-layered model of dialogue structure for task-oriented dialogues that processes contextual information and disambiguates speech acts.
relative clauses.
passivisation.
and questions.
The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences.
We present a method for corpus-based induction of an LFG syntax-semantics interface for frame semantic processing in a computational LFG parsing architecture.
We use LFG抯 functional representations to distinguish local and non-local role assignments.
The domain- specific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication.
grammatical function, phrase type, and other syntactic traits).
a tagset containing information about wordclasses.
This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech.
We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.
subordinators and verbs).
We develop admissible A* search heuristics for synchronous parsing with Inversion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding.
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
the task of WSD of Word- Net glosses and the task of WSD of English lexical sample.
hypernymy glosses, holonymy mixture, descriptive glosses and some hybrid mixtures of these glosses.
We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn `missing' grammar rules from an incomplete grammar.
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
a noun in a noun phrase).
mantics of Combinatory Categorial Grammar, including its handling of coordination constructs.
GermaNet and FrameNet.
This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages (TALs).
We investigated the applicability of probabilistic context-free grammars to syllabification and grapheme-to-phoneme conversion.
LFG and PATR-II.
the language used in official encyclopaedic articles.
We extracted meaningful bigrams using an evaluation function and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers.
We view a modeling problem for n-gram collocations as that for clustering of cohesive words.
directly stored relations.
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).
These kinds of anaphora are pronominal references, surface- count anaphora and one-anaphora.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We describe a method for interpreting abstract flat syntactic representations, LFG f- structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).
QACIAD (Question Answering Challenge for Information Access Dialogue) is an evaluation framework for measuring interactive question answering (QA) technologies.
We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs).
We propose a structured semantic representation, the Lexical Conceptual Paradigm (LOP) which groups nouns into paradigmatic classes exhibiting like behavior.I.
basic segmentation, named entity recognition, error-driven learner and new word detector.
These FSAs are good representations of paraphrases.
We describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues.
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical- Functional Grammar (LFG).
We investigate prototype-driven learning for primarily unsupervised sequence modeling.
This paper describes a mix word-pair mix-WP) identifier to resolve homo-nym/segmentation ambiguities as well as perform STW conversion effec-tively for Chinese input.
We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG.
We then describe a head-driven chart parser for lexicalized SFG.
culminating ideally in soundness J id completeness theorems.
We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences.
We have proposed an incremental trans-lation method in Transfer-Driven Ma-chine Translation (TDMT).
Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects.
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
We introduce feature terms containing sorts, variables, negation and named disjunction for the specification of feature structures.
and lean declarative impIe.mentation.
We propose a syntactic filter for identifying non-coreferential pronoun-NP pairs within a sentence.
LFG f-structures or HPSG feature structures, to dependency triples simple.
In Online Inquiry-Based Learning (OIBL) learners search for information to answer driving questions.
This paper presents our work on real-izing expressions of doubt appropriately in natural language dialogues.
Non-fiction	11.
Fiction	K. General Fiction	I.
(WSJ) speech corpus.
We present a bottom-up approach to arranging sentences extracted for multi-document summarization.
We apply the supervised decision list learn-ing method to Japanese named entity recogni-tion.
Core Roles versus Adjuncts).
We provide an XML serialization for intercomponent communication.
In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank.
We explore unsupervised language model adaptation techniques for Statistical Machine  Translation.
Root based clusters can substitute dictionaries in indexing for IR.
This paper describes classification of typed student utterances within AutoTutor, an intelligent tutoring system.
The classifier presented uses part of speech tagging, cascaded finite state transducers, and simple disambiguation rules.
We present an algorithm that learns the cross-language correspondence between affixes and letter sequences.
We apply default inheritance hierarchies to generating the morphology of Hebrew verbs.
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.
a local area network).
1 Korean syllable structure3.
valid prefixes.
The TIPSTER.
We present ongoing work on prosody predic-tion for speech synthesis.
We retrieve mixed- language web pages based on the expanded queries.
at Grenoble.
We learn models of answer type, query content, and answer extraction from clusters of similar questions.
This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation.
We examine the utility of speech and lexical features for predicting student emotions in computer- human spoken tutoring dialogues.
generate non_grammatical sent_ ences.
This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules.
The phrase-break detector assigns phrase breaks using part-of-speech (POS) information.
We describe an approach to surface generation designed for a "pragmatics-based" dialogue system.
Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification- based grammar (UBG).
1 IntroductionStochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar.
This position paper contrasts rhetorical structuring of propositions with intentional decomposition using communicative acts.
We discuss a grammar development process used to generate the trees of the wide-coverage Lexicalized Tree Adjoining Grammar (LTAG) for English of the XTAG Project.
The discourse entities are used in intra- and extra-sentential pronoun resolution in BBN Janus.
OF COLING-92.
possible structural and lexical attributes.
Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases.
Our algorithm generates lists of non- anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.
in machine translation).
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).
Michael met Maria at the cinema.
This paper explores the segmentation of tutorial dialogue into cohesive topics.
DTG involve two composition operations called subsertion and sister-adjunction.
The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm.
We present an authoring system for logical forms encoded as conceptual graphs (CG).
time, causality.
Structural information is provided by such hierarchical prosodic constituents as Syllable (S), Metrical Foot (MF), Phonological Word (PW), Intonational Group (IG).
The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases.
This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora.
This paper presents a human梞achine dialogue model in the field of task梠riented dialogues.
This paper presents the results of an experiment using machine-readable dictionaries (MRDs) and corpora for building concatenative units for text to speech (ITS) systems.
groups of words).
significant power to an NL system.
We compare vectors containing counts of trigrams of part-of-speech (POS) tags in order to obtain an aggregate measure of syntax difference.
This ex-planation accounts for prefixing, suffix-ing and infixing.
LFG and PATR-II.
We consider the problem of parsing non-recursive context-free grammars, i.e., context-free grammars that generate finite languages.
in terms or facts and rules) .
and 63 lexical entry templates (assigned to parts of speech (POSs) ).
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
OF COLING-92.
information extraction, text sum-marisation, document generation, machine translation, and second language learning).
ARGUMENTATION AND THE SEMANTIC PROGRAM.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
This paper discusses relationships among word pronunciations.
We present a constraint-based case frame lexicon architecture for bi-directional mapping between a syntactic case frame and a semantic frame.
Association for Computational Linguistics.
This paper proposes the 揌ierarchical Directed Acyclic Graph (HDAG) Kernel?for structured natural language data.
XMLencodingNon-XML EncodingFigure 3.
We propose a method of using associative	strength	among	morphemes,morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech.
We make use of a conditional log杔inear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.
We present a novel disambiguation method for unification-based grammars (UBGs).
We extended language modeling approaches in information retrieval (IR) to combine collaborative filtering (CF) and content-based filtering (CBF).
We define POS blocks to be groups of parts of speech.
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
)In the Information Retrieval community.
We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields.
We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment.
We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We look at self-triggerability across hyperlinks on the Web.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
We discuss shift- reduce parsing of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses.
This paper discusses an approach to modeling monolingual and bilingual dictionaries in the description logic species of the OWL Web Ontology Language (OWL DL).
We describe a biographical multi-document summarizer that summarizes information about people described in the news.
We present a joint inference model to improve Chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline: name structure parsing, cross-document coreference, semantic relation extraction and event extraction.
This paper introduces an open computational framework for visual perception and grounded language acquisition called Experience-Based Language Acquisition (EBLA).
We describe experiments learning quasi-synchronous context-free grammars from bitext.
This paper describes an implemented mechanism for handling bound anaphora, disjoint reference, and pronominal reference.
This paper describes text meaning represen-tation for Chinese.
Structural (attachment.)
theme/rheme and background/kontrast.
without inflections or conjugations.
The HMM-LR algorithm uses a generalized LR parser as a language model and hidden Markov models (HMMs) as phoneme models.
We propose a method for semi-automatic classi-fication of verbs to Levin classes via the seman-tic network of WordNet.
We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.
GermaNet, WordNet).
the Italian wordnet in EuroWordNet (ItalWordNet).
lion (11) .
We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags.
This paper presents a disambiguation approach for translating non-segmented-Kana into Kanji.
These rules are noun sequences with part-of-speech tags.
This paper describes methods for finding taxonomy and set-membership relationships, recognizing nouns that ordinarily represent human beings, and identifying active and stative verbs and adjectives.
This paper describes Acorn, a sentence planner and surface realizer for dialogue systems.
A monotone phrasal decoder generates contextual replacements.
This paper presents a maximum entropy-based named entity recognizer (NER).
POS-tagging, lemmatisation and a thesaurus-like thematic dictionary.
the event profile.
Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness.
We describe a corpus-based induction algorithm for probabilistic context-free grammars.
This process is known as 'compiling HPSG to TAG' and derives a Lexicalized Tree-Adjoining Grammar (LTAG).
We classify the problematic behavior into "static disambiguation" and "dynamic disambiguation" tasks.
This paper describes an attribute grammar specification of the Government-binding theory.
easy vs difficult dialogs).
CATEGORY assigns names in hierachies.
In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented.
in a prepositional phrase.
a proper name or noun phrase.
We compose three out-of-vocabulary extraction modules: Character-based tag-ging with different classifiers ?maximum entropy, support vector machines, and con-ditional random fields.
It labels semantic roles of parsed sentences.
We are concerned with dependency- oriented morphosyntactic parsing of running text.
of deep cases relations (or thematic relations).
Schemata are representational structures for stereotypical paragraphs that describe objects.
We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.
In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci.
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank.
This paper investigates semantic and pragmatic presupposition in Discourse Representation Theory (DRT) and enhances the pragmatic perspective of presupposition in DRT.
We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text.
This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs).
We propose a lexical organisation for multilingual lexical databases (MLDB).
-- the problem of diacritics and digraphs.
We present a scheme for efficiently representing a lexicalized tree-adjoining grammar (LTAG).
We report here the contribution of default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, selectional preferences, and topics or subject fields.
In the GPSG framework.
We offer a semantics and pragmatics of the pluperfect in narrative discourse.
This paper presents Archivus, a multi- modal language-enabled meeting browsing and retrieval system.
This paper presents LexPhon, a computational framework for modelling segmental aspects of Lexical Phonology (LP).
This paper focuses on the use of Finite State morphological analysis (FST) resources for Irish and Part of Speech (POS) taggers for German.
We introduce a first-order language for semantic underspecification that we call Constraint Language for Lambda-Structures (CLLS).
CLLS features constraints for dominance, lambda binding, parallelism, and anaphoric links.
We call sentences like (2) semantically disambiguatable garden path sentences (SDGPs).
GEMS is lexically distributed.
We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes.
We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel).
problem oriented systems.
the grammatical morphemes  of the language.
This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs).
This paper extends the base noun phrase(BNP) identification into a research on Chinese base phrase identification.
This paper discusses word choice for natural language generation.
The paper describes a new unification based grammar formalism called Regular Unification Grammar (RUG).
We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG.
PARSE GRAPH STRUCTUREThe Parse Graph Structure (PGS) is built by the parser while applying grammar rules.
Pronunciation-by-analogy (PbA) is an emer-ging technique for text-phoneme conversion based on a psychological model of read-ing aloud.
They are morphological transformation and morpheme Identification.
the primary driving task.
bilingual dictionaries).
Language understanding work at Paramax focuses on applying general-purpose language understanding technology to spoken language understanding, text understanding, and document processing, integrating language understanding with speech recognition, knowledge-based information retrieval and image understanding.
This paper describes an operational se-mantics for DATR theories.
Such speech acts represent dialogue deviations.
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
This paper presents a generalized unknown morpheme handling method with POSTAG(POStech TAGger) which is a statistical/rule based hybrid POS tagging system.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined.
We describe a pilot project in semiautomatically refactoring a biomedical corpus.
phonological rules or metrical systems).
This paper presents work on using Bayesian networks for the dialogue act recognition module of a dialogue sys-tem for Dutch dialogues.
This paper presents a method for auto-matically recognizing local cohesion be-tween utterances, which is one of the dis-course structures in task-oriented spoken dialogues.
Th nlyzr pplis "phrs spotting" mhnism on th output of th sph rognition modul.
the distribution.
distinctive features.
We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-the- art constituency-based parsers.
Unification of disjunctive feature descriptions is important for efficient unification-based parsing.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic para-phrases.
sys-tem.
We associate phrases by frame-based semantics.
We represent questions as frequency weighted vectors of salient terms.
We analyze the association of noun-noun and noun-verb pairs in LOB Corpus.
superordinate substitution, and definite noun phrase reiteration.
We apply SVMs to Japanese dependency structure iden-tification problem.
Confidence measures for MT outputs include the rank-sum-based confidence measure (RSCM) for statistical machine translation (SMT) systems.
(TSR trees).
unscripted) speech.
block bigram features.
SEM encodes logical semantics.
We propose a distribution-based pruning of n-gram backoff language models.
We compute frequencies of unigrams, bigrams, and trigrams of word classes in order to further refine the disambiguation.
This paper presents a word-pair (WP) identifier that can be used to resolve homonym/segmentation ambiguities and perform syllable-to-word (STW) conversion effectively for improving Chinese input systems.
It detects mistypes, Kana-to-Kanji misconversions, and stylistic errors.
We investigate the logical structure of concepts generated by conjunction and disjunction over a monotonic multiple inheritance network where concept nodes represent linguistic categories and links indicate basic inclusion (isA) and disjointness (IsNoTA) relations.
This method focuses on semantic and pragmatic constraints such as semantic constraints on cases, modal expressions, verbal semantic attributes and conjunc-tions to determine the deictic reference of Japanese zero pronouns.
In this paper we describe how to hierarchically organize abstract nouns (adjective hyperonyms) in a semantic map mainly using CSM.
This study addresses pronominal anaphora resolution, including zero pronouns, in Chinese.
The interlingua.
noun plural markers.
statistical lexical head- outward transducers.
We present a corpus-based study of the sequential ordering among premodifiers in noun phrases.
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications.
We coerce the English FrameNet into Chinese using a bilingual lexicon, frame context in FrameNet and taxonomy structure in HowNet.
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
This paper discusses the lexical knowledge of idioms for idiom recognition.
This paper describes the tree-structured maximum mutual information (MMI) encoders used in SSI's Phonetic Engine?to perform large-vocabulary, continuous speech recognition.
These templates represent grammatically correct sentence patterns.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
Further, we have developed MM-DCG translator to transfer rules in MM-DCG into Prolog predicates.
relation based sim-ilarity measure and distribution based similarity measure.
A LOGICAL SEMANTICSFOR NONMONOTONIC SORTSMark A.
the cross-entropy.
Natural language parsing requires ex-tensive lexicons containing subcategori-sation information for specific sublan-guages.
We explore the use of Support Vector Ma-chines (SVMs) for biomedical named en-tity recognition.
Motion Verbs.
Bilingual Corpus-based Analysis.
This paper discusses the partitive-genitive case alternation of Finnish adpositions.
sidesg.
oppositesg.
We represent shallow linguistic information as linguistic features in our ME model.
as a hybrid system.
We review studies of reference resolution, word recognition, and pragmatic effects on syntactic ambiguity resolution.
We present discriminative reordering models for phrase-based statistical machine translation.
We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses.
domain-independent, semantic information for question interpretation.
In this paper I outline Type-inheritance Combinatory Categorial Grammar (TCCG), an implemented feature structure based CCG fragment of English.
The grammars used for speech recognition dictate legal word sequences.
We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text.
We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.
This paper proposes a discriminative HMM (DHMM) with long state dependence (LSDDHMM) to segment and label sequential data.
This part-ofspeech predictor will be used in a part-of-speech tagger to handle out-of-lexicon words.
This paper describes discriminative language modeling for a large vocabulary speech recognition task.
Rohrer.
We describe a bidirectional framework for natural language parsing and genera-tion, using a typed feature formalism arid an HPSG-based grammar with a parser arid generator derived from parallel pro-cessing algorithms.
We present a Korean question answering framework for restricted domains, called K-QARD.
morphological derivation and synonymy expansion) in web search strategies.
This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG).
information technology test reports and medical finding reports.
We investigate independent and relevant event-based extractive mutli-document summarization approaches.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
WordNet) to ambiguous words occurring in a syntactic dependency.
We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors' of noun phrases.
Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases.
This paper describes named entity (NE) extraction based on a max-imum entropy (M.E.)
model and transformation rules.
This paper introduces PhraseNet, a context- sensitive lexical semantic knowledge base system.
The Hidden Markov Model (HMM) for part-of-speech (POS) tagging is typically based on tag trigrams.
The chunker partitions the part-of-speech sequence into segments called chunks.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.
We perform a linguistic analysis of documents during indexing for information retrieval.
This paper describes machine learning based parsing and question classification for question answering.
ethnologue.com).
PROLOG.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
We use linguistic patterns and HTML text structures to extract text fragments contain-ing term descriptions.
We present BAYESUM (for 揃ayesian summarization?, a model for sentence extraction in query-focused summarization.
This paper presents a computational model of incremental utterance produc-tion in task-oriented dialogues.
Tagging compound verb groups in an annotated corpus exploiting the verb rules is described.Keywords: compound verb groups, chunking, morphosyntactic tagging, inductive logic pro-gramming
Text metadata.
We propose a kana-kanji conversion system with input support based on prediction.
We describe a method for retrieving counts for adjective-noun, noun-noun, and verb- object bigrams from the web by querying a search engine.
We describe Talk'n'Travel, a spoken dialogue language system for making air travel plans over the telephone.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
This paper describes DialogueView, a tool for annotating dialogues with utter-ance boundaries, speech repairs, speech act tags, and discourse segments.
Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing.
coreferential) relations.
This paper presents the use of probabilistic class-based lexica for disambiguation in target-word selection.
We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers.
verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction.
This paper presents a method for extracting subcorpora documenting different subcategorization frames for verbs, nouns, and adjectives in the 100 mio.
It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination.
We study unsupervised methods for learning refinements of the nonterminals in a treebank.
Pos811D.
We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system.
We present a machine-learning approach to modeling the distribution of noun phrases (NPs) within clauses with respect to a fine-grained taxonomy of grammatical relations.
representation methods, hypermedia maps.
hypermedia writing.
.
1			Back梤eferencing in text		?.
11			Belief structure	.
handwriting).
in Machine Translation).
Existing speech recognizers work best for "high-quality close-talking speech."
We show how to do this using , -reduction constraints in the constraint language for A-structures (CLLS).
Among the transformational grammarsi.
We present a constraint-based syntax-semantics interface for the construction of RMRS (Robust Minimal Recursion Semantics) representations from shallow grammars.
incomplete syntactic analysis.
(1990) Modelling the perception of concurrent vowels: vowels with different fundamental frequencies.
This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations.
base forms and POS tags.
without enumerating readings.
We describe an algorithm for recovering non-local dependencies in syntactic dependency structures.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
a finite-state transducer.
We automatically classify verbs into lexical se-mantic classes, based on distributions of indica-tors of verb alternations, extracted from a very large annotated corpus.
In this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) tagging framework.
parsing procedures and semantic head-driven generation.
Dialogue system for 3D virtual environ-ments).
General structure of PORTUGA: A - analysis, G - generation.
We propose to score phrase translation pairs for statistical machine translation using term weight based models.
These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs.
We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.
We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms.
This paper describes new default unification, lenient default unification.
noun phrase (NP) syntax.
This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unification- based shallow-level parser using transformational rules over syntactic patterns.
We collected declaratively subjective clues in opinion- expressing sentences from Japanese web pages retrieved with opinion search queries.
This paper compares two approaches to computational semantics, namely semantic unification in Lexicalized Tree Adjoining Grammars (LTAG) and Lexical Resource Semantics (LRS) in HPSG.
This paper presents PROVERB a text planner for argumentative texts.
This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA).
We extend default unification to non-parallel structures, which is important for speech and multimodal dialog systems.
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
variable-matching without variable substitution.
We present in this paper the method of English-to-Korean(E-K) transliteration and back-transliteration.
in morphological analysis).
We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures.
Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.
idf based method.
In this paper, subclasses of monadic context- free tree grammars (CFTGs) are compared.
We have constructed the IPA Lexicon of Basic Japanese Nouns (IPAL-BN), which has a hierarchical structure based on the syntactic and semantic proper-ties of nouns.
We present a discourse process that recognizes characters' thoughts and perceptions in third-person narrative.
SCsem is the semantic weighting.
(Unification Categorial Grammar).
expressions denoting geographical localisations).
We find they can capture effectively allophonic variation, alternative pronunciation, word co-articulation and segmental durations.
Dutch subordinate clauses.
Dutch subordinate clauses for non-transformational theories of grammar.
We then examine the effect of integrating pausal and spontaneous speech phe-nomena into syntactic rules for speech recognition, using118 sentences.
Speaker independent phonetic transcription of fluent speech is performed using an ergodic continuously variable duration hidden Markov model (CVDHMM) to represent the acoustic, phonetic and phonotac tic structure of speech.
We present a comparison of two formalisms for representing natural language utterances, namely deep syntactical Tectogrammatical Layer of Functional Generative Description (FGD) and a semantic formalism, MultiNet.
We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural- language processing.
This paper describes the user expertise model in AthosMail, a mobile, speech-based e-mail system.
trigger words and parsing structures.
The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution.
We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks.
This paper proposes an alignment adaptation	approach	to	improvedomain-specific (in-domain) word alignment.
We present a simple approximation method for turn-ing a Head-Driven Phrase Structure Grammar into a context-free grammar.
We describe an implementation of data- driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
Managing Gigabytes: Compressing and Indexing Documents and Images, Academic Press/Morgan Kaufmann.
WORD- NET DOMAINS).
We consider the translation of European Parliament Speeches.
We investigate the problem of learning a part-of-speech (POS) lexicon for a resource-poor language, dialectal Arabic.
We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees.
Language Understanding (NLU) systems,particularly for data-driven ones.
knowledge and language-specific heuristics.
Figure 1: Dimensions of Language Engineering Complexity.
We use statistical dependency parsers to determine dependency relations between base phrases in a sentence.
We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs.
We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications.
information technology test reports and medical finding reports.
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks).
We apply weighted voting of 8 SVMs-based systems trained with distinct chunk repre-sentations.
We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
part-of-speech tagged corpora).
LFG and GPSG are augmented PS- grammars.
We propose astemming method for Mongolian to extractloanwords correctly.
This paper outlines a formal computational semantics and pragmatics of the major speech act types.
This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame.
We address Question Answering (QA) for biographical questions, i.e., questions asking for biographical facts about persons.
error types).
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language.
CTXMATCH performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.
This paper presents Trace dr Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (UG) and ideas of Government dr Binding Theory (on).
in GPSO and HPSG.
Bayesian, decision tree and neural network classifiers) to discover language model errors.
very large corpora of strings.
We present two methods for unsupervised segmentation of words into morpheme-like units.
This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE).
The method combines morphological and lex-ical processing, bilingual word alignment, and graph-theoretic cluster generation.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
building classifiers using acoustic-prosodic features.
plus petits.
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features.
in content based information retrieval.
This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT).
We present a statistical phrase-based translation model that uses hierarchical phrases?phrases that contain subphrases.
database tuples).
This paper compares a number of generative probability models for a wide- coverage Combinatory Categorial Grammar (CCG) parser.
This paper explores the problem of identifying sen-tence boundaries in the transcriptions produced by automatic speech recognition systems.
Human face-to-face conversation is an ideal model for human-computer dialogue.
We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements.
We define an Earley parsing schema for STG's and characterize the valid parse items.
The model uses syntactic patterns and N-grams reflecting the hierarchical discourse structures of dialogues.
We incorporate some consequence relations into DRT using se- writ calculi.
We present Minimum Bayes-Risk word alignment for machine translation.
This paper describes a media-independent, compositional, plan-based approach to representing attributive descriptions for use in integrated text and graphics generation.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
We investigate the suitability of sub-categorization acquisition for evalu-ation of word sense disambiguation (WSD) systems.
We extend Traum抯 grounding act model by introducing degree of groundedness, and partial and mid-discourse unit grounding.
such as definite noun phrases and pronouns, in a discourse.
base phrases) into the arguments of a predicate.
We propose a context-sensitive method to predict noun- phrases in the next utterance of a telephone inquiry dialogue.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
We therefore use phonetic recognition of utterances and search for salient phonetic sequences within the decodings.
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
We introduce a new method for recovering empty nodes and their antecedents (capturing long distance dependencies) from parser output in CFG trees using LFG f-structure reentrancies.
We here investigate questions with implicatures related to biography facts in a web-based QA system, PowerBio.
A verb paradigm is a set of inflectional categories for a single verb lemma.
We present the dialogue module of the speech-to-speech translation system VERB-MOBIL.
We describe an approach to resolving definite descriptions and pronominal anaphora as subcases of a general strategy for presupposition satisfaction.
We investigate prototype-driven learning for primarily unsupervised grammar induction.
We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines.
We apply interpretive rules to clausal patterns and patterns of modification, and concurrently abstract general 損ossibilistic?propositions from the resulting formulas.
We defined 10 Descriptive Answer Type(DAT)s as answer types for descriptive questions.
We propose a syntax of it-clefts using Tree-Local Multi- Component Tree Adjoining Grammar and a compositional semantics on the proposed syntax using Synchronous Tree Adjoining Grammar.
information retrieval, translation unit discovery, and corpus studies.
This paper describes how to construct a finite-state machine (FSM) approximating a 'unification-based' grammar using a left-corner grammar transform.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
We present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections.
The sys-tem is an application of maximum entropy clas-sification for question/answer type prediction and named entity marking.
We describe sign translation using example based machine translation technology.
This paper presents a sentence- based statistical model for Hangeul-Hanja conversion, with word tokenization included as a hidden process.
information extraction).
In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inflectional languages.
We present a generic template for spoken dialogue systems integrating speech recognition and synthesis with 'higher-level' natural language dialogue modelling components.
classification of unseen verbs.
The paper fo-cuses on phonology.
We present a discriminative, large- margin approach to feature-based matching for word alignment.
We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task.
This paper presents the user-centered interface of a summarization system for physicians in Bone Marrow Transplan-tation (BMT).
the complexity of language domain and concept inventory).
We introduce CST (cross-document structure theory), a paradigm for multi-document analysis.
We present the multilingual sum-marization functionality for VERB-MOBIL, a speech translation system.
In this paper we present Morphy, an integrated tool for German morphology, part-ofspeech tagging and context-sensitive lemmatization.
This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG pars-ing system.
This paper proposes a two-layered model of dialogue structure for task-oriented dialogues that processes contextual information and disambiguates speech acts.
relative clauses.
passivisation.
and questions.
The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences.
We present a method for corpus-based induction of an LFG syntax-semantics interface for frame semantic processing in a computational LFG parsing architecture.
We use LFG抯 functional representations to distinguish local and non-local role assignments.
The domain- specific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication.
grammatical function, phrase type, and other syntactic traits).
a tagset containing information about wordclasses.
This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech.
We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.
subordinators and verbs).
We develop admissible A* search heuristics for synchronous parsing with Inversion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding.
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
the task of WSD of Word- Net glosses and the task of WSD of English lexical sample.
hypernymy glosses, holonymy mixture, descriptive glosses and some hybrid mixtures of these glosses.
We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn `missing' grammar rules from an incomplete grammar.
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
a noun in a noun phrase).
mantics of Combinatory Categorial Grammar, including its handling of coordination constructs.
GermaNet and FrameNet.
This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages (TALs).
We investigated the applicability of probabilistic context-free grammars to syllabification and grapheme-to-phoneme conversion.
LFG and PATR-II.
the language used in official encyclopaedic articles.
We extracted meaningful bigrams using an evaluation function and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers.
We view a modeling problem for n-gram collocations as that for clustering of cohesive words.
directly stored relations.
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).
These kinds of anaphora are pronominal references, surface- count anaphora and one-anaphora.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We describe a method for interpreting abstract flat syntactic representations, LFG f- structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).
QACIAD (Question Answering Challenge for Information Access Dialogue) is an evaluation framework for measuring interactive question answering (QA) technologies.
We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs).
We propose a structured semantic representation, the Lexical Conceptual Paradigm (LOP) which groups nouns into paradigmatic classes exhibiting like behavior.I.
basic segmentation, named entity recognition, error-driven learner and new word detector.
These FSAs are good representations of paraphrases.
We describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues.
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical- Functional Grammar (LFG).
We investigate prototype-driven learning for primarily unsupervised sequence modeling.
This paper describes a mix word-pair mix-WP) identifier to resolve homo-nym/segmentation ambiguities as well as perform STW conversion effec-tively for Chinese input.
We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG.
We then describe a head-driven chart parser for lexicalized SFG.
culminating ideally in soundness J id completeness theorems.
We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences.
We have proposed an incremental trans-lation method in Transfer-Driven Ma-chine Translation (TDMT).
Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects.
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
We introduce feature terms containing sorts, variables, negation and named disjunction for the specification of feature structures.
and lean declarative impIe.mentation.
We propose a syntactic filter for identifying non-coreferential pronoun-NP pairs within a sentence.
LFG f-structures or HPSG feature structures, to dependency triples simple.
In Online Inquiry-Based Learning (OIBL) learners search for information to answer driving questions.
This paper presents our work on real-izing expressions of doubt appropriately in natural language dialogues.
Non-fiction	11.
Fiction	K. General Fiction	I.
(WSJ) speech corpus.
We present a bottom-up approach to arranging sentences extracted for multi-document summarization.
We apply the supervised decision list learn-ing method to Japanese named entity recogni-tion.
Core Roles versus Adjuncts).
We provide an XML serialization for intercomponent communication.
In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank.
We explore unsupervised language model adaptation techniques for Statistical Machine  Translation.
Root based clusters can substitute dictionaries in indexing for IR.
This paper describes classification of typed student utterances within AutoTutor, an intelligent tutoring system.
The classifier presented uses part of speech tagging, cascaded finite state transducers, and simple disambiguation rules.
We present an algorithm that learns the cross-language correspondence between affixes and letter sequences.
We apply default inheritance hierarchies to generating the morphology of Hebrew verbs.
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.
a local area network).
1 Korean syllable structure3.
valid prefixes.
The TIPSTER.
We present ongoing work on prosody predic-tion for speech synthesis.
We retrieve mixed- language web pages based on the expanded queries.
at Grenoble.
We learn models of answer type, query content, and answer extraction from clusters of similar questions.
This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation.
We examine the utility of speech and lexical features for predicting student emotions in computer- human spoken tutoring dialogues.
generate non_grammatical sent_ ences.
This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules.
The phrase-break detector assigns phrase breaks using part-of-speech (POS) information.
We describe an approach to surface generation designed for a "pragmatics-based" dialogue system.
Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification- based grammar (UBG).
1 IntroductionStochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar.
This position paper contrasts rhetorical structuring of propositions with intentional decomposition using communicative acts.
We discuss a grammar development process used to generate the trees of the wide-coverage Lexicalized Tree Adjoining Grammar (LTAG) for English of the XTAG Project.
The discourse entities are used in intra- and extra-sentential pronoun resolution in BBN Janus.
OF COLING-92.
possible structural and lexical attributes.
Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases.
Our algorithm generates lists of non- anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.
in machine translation).
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).
Michael met Maria at the cinema.
This paper explores the segmentation of tutorial dialogue into cohesive topics.
DTG involve two composition operations called subsertion and sister-adjunction.
The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm.
We present an authoring system for logical forms encoded as conceptual graphs (CG).
time, causality.
Structural information is provided by such hierarchical prosodic constituents as Syllable (S), Metrical Foot (MF), Phonological Word (PW), Intonational Group (IG).
The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases.
This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora.
This paper presents a human梞achine dialogue model in the field of task梠riented dialogues.
This paper presents the results of an experiment using machine-readable dictionaries (MRDs) and corpora for building concatenative units for text to speech (ITS) systems.
groups of words).
significant power to an NL system.
We compare vectors containing counts of trigrams of part-of-speech (POS) tags in order to obtain an aggregate measure of syntax difference.
This ex-planation accounts for prefixing, suffix-ing and infixing.
LFG and PATR-II.
We consider the problem of parsing non-recursive context-free grammars, i.e., context-free grammars that generate finite languages.
in terms or facts and rules) .
and 63 lexical entry templates (assigned to parts of speech (POSs) ).
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
OF COLING-92.
information extraction, text sum-marisation, document generation, machine translation, and second language learning).
ARGUMENTATION AND THE SEMANTIC PROGRAM.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
This paper discusses relationships among word pronunciations.
We present a constraint-based case frame lexicon architecture for bi-directional mapping between a syntactic case frame and a semantic frame.
Association for Computational Linguistics.
This paper proposes the 揌ierarchical Directed Acyclic Graph (HDAG) Kernel?for structured natural language data.
XMLencodingNon-XML EncodingFigure 3.
We propose a method of using associative	strength	among	morphemes,morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech.
We make use of a conditional log杔inear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.
We present a novel disambiguation method for unification-based grammars (UBGs).
We extended language modeling approaches in information retrieval (IR) to combine collaborative filtering (CF) and content-based filtering (CBF).
We define POS blocks to be groups of parts of speech.
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
)In the Information Retrieval community.
We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields.
We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment.
We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We look at self-triggerability across hyperlinks on the Web.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
We discuss shift- reduce parsing of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses.
This paper discusses an approach to modeling monolingual and bilingual dictionaries in the description logic species of the OWL Web Ontology Language (OWL DL).
We describe a biographical multi-document summarizer that summarizes information about people described in the news.
We present a joint inference model to improve Chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline: name structure parsing, cross-document coreference, semantic relation extraction and event extraction.
This paper introduces an open computational framework for visual perception and grounded language acquisition called Experience-Based Language Acquisition (EBLA).
We describe experiments learning quasi-synchronous context-free grammars from bitext.
This paper describes an implemented mechanism for handling bound anaphora, disjoint reference, and pronominal reference.
This paper describes text meaning represen-tation for Chinese.
Structural (attachment.)
theme/rheme and background/kontrast.
without inflections or conjugations.
The HMM-LR algorithm uses a generalized LR parser as a language model and hidden Markov models (HMMs) as phoneme models.
We propose a method for semi-automatic classi-fication of verbs to Levin classes via the seman-tic network of WordNet.
We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.
GermaNet, WordNet).
the Italian wordnet in EuroWordNet (ItalWordNet).
lion (11) .
We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags.
This paper presents a disambiguation approach for translating non-segmented-Kana into Kanji.
These rules are noun sequences with part-of-speech tags.
This paper describes methods for finding taxonomy and set-membership relationships, recognizing nouns that ordinarily represent human beings, and identifying active and stative verbs and adjectives.
This paper describes Acorn, a sentence planner and surface realizer for dialogue systems.
A monotone phrasal decoder generates contextual replacements.
This paper presents a maximum entropy-based named entity recognizer (NER).
POS-tagging, lemmatisation and a thesaurus-like thematic dictionary.
the event profile.
Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness.
We describe a corpus-based induction algorithm for probabilistic context-free grammars.
This process is known as 'compiling HPSG to TAG' and derives a Lexicalized Tree-Adjoining Grammar (LTAG).
We classify the problematic behavior into "static disambiguation" and "dynamic disambiguation" tasks.
This paper describes an attribute grammar specification of the Government-binding theory.
easy vs difficult dialogs).
CATEGORY assigns names in hierachies.
In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented.
in a prepositional phrase.
a proper name or noun phrase.
We compose three out-of-vocabulary extraction modules: Character-based tag-ging with different classifiers ?maximum entropy, support vector machines, and con-ditional random fields.
It labels semantic roles of parsed sentences.
We are concerned with dependency- oriented morphosyntactic parsing of running text.
of deep cases relations (or thematic relations).
Schemata are representational structures for stereotypical paragraphs that describe objects.
We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.
In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci.
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank.
This paper investigates semantic and pragmatic presupposition in Discourse Representation Theory (DRT) and enhances the pragmatic perspective of presupposition in DRT.
We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text.
This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs).
We propose a lexical organisation for multilingual lexical databases (MLDB).
-- the problem of diacritics and digraphs.
We present a scheme for efficiently representing a lexicalized tree-adjoining grammar (LTAG).
We report here the contribution of default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, selectional preferences, and topics or subject fields.
In the GPSG framework.
We offer a semantics and pragmatics of the pluperfect in narrative discourse.
This paper presents Archivus, a multi- modal language-enabled meeting browsing and retrieval system.
This paper presents LexPhon, a computational framework for modelling segmental aspects of Lexical Phonology (LP).
This paper focuses on the use of Finite State morphological analysis (FST) resources for Irish and Part of Speech (POS) taggers for German.
We introduce a first-order language for semantic underspecification that we call Constraint Language for Lambda-Structures (CLLS).
CLLS features constraints for dominance, lambda binding, parallelism, and anaphoric links.
We call sentences like (2) semantically disambiguatable garden path sentences (SDGPs).
GEMS is lexically distributed.
We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes.
We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel).
problem oriented systems.
the grammatical morphemes  of the language.
This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs).
This paper extends the base noun phrase(BNP) identification into a research on Chinese base phrase identification.
This paper discusses word choice for natural language generation.
The paper describes a new unification based grammar formalism called Regular Unification Grammar (RUG).
We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG.
PARSE GRAPH STRUCTUREThe Parse Graph Structure (PGS) is built by the parser while applying grammar rules.
Pronunciation-by-analogy (PbA) is an emer-ging technique for text-phoneme conversion based on a psychological model of read-ing aloud.
They are morphological transformation and morpheme Identification.
the primary driving task.
bilingual dictionaries).
Language understanding work at Paramax focuses on applying general-purpose language understanding technology to spoken language understanding, text understanding, and document processing, integrating language understanding with speech recognition, knowledge-based information retrieval and image understanding.
This paper describes an operational se-mantics for DATR theories.
Such speech acts represent dialogue deviations.
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
This paper presents a generalized unknown morpheme handling method with POSTAG(POStech TAGger) which is a statistical/rule based hybrid POS tagging system.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined.
We describe a pilot project in semiautomatically refactoring a biomedical corpus.
phonological rules or metrical systems).
This paper presents work on using Bayesian networks for the dialogue act recognition module of a dialogue sys-tem for Dutch dialogues.
This paper presents a method for auto-matically recognizing local cohesion be-tween utterances, which is one of the dis-course structures in task-oriented spoken dialogues.
Th nlyzr pplis "phrs spotting" mhnism on th output of th sph rognition modul.
the distribution.
distinctive features.
We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-the- art constituency-based parsers.
Unification of disjunctive feature descriptions is important for efficient unification-based parsing.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic para-phrases.
sys-tem.
We associate phrases by frame-based semantics.
We represent questions as frequency weighted vectors of salient terms.
We analyze the association of noun-noun and noun-verb pairs in LOB Corpus.
superordinate substitution, and definite noun phrase reiteration.
We apply SVMs to Japanese dependency structure iden-tification problem.
Confidence measures for MT outputs include the rank-sum-based confidence measure (RSCM) for statistical machine translation (SMT) systems.
(TSR trees).
unscripted) speech.
block bigram features.
SEM encodes logical semantics.
We propose a distribution-based pruning of n-gram backoff language models.
We compute frequencies of unigrams, bigrams, and trigrams of word classes in order to further refine the disambiguation.
This paper presents a word-pair (WP) identifier that can be used to resolve homonym/segmentation ambiguities and perform syllable-to-word (STW) conversion effectively for improving Chinese input systems.
It detects mistypes, Kana-to-Kanji misconversions, and stylistic errors.
We investigate the logical structure of concepts generated by conjunction and disjunction over a monotonic multiple inheritance network where concept nodes represent linguistic categories and links indicate basic inclusion (isA) and disjointness (IsNoTA) relations.
This method focuses on semantic and pragmatic constraints such as semantic constraints on cases, modal expressions, verbal semantic attributes and conjunc-tions to determine the deictic reference of Japanese zero pronouns.
In this paper we describe how to hierarchically organize abstract nouns (adjective hyperonyms) in a semantic map mainly using CSM.
This study addresses pronominal anaphora resolution, including zero pronouns, in Chinese.
The interlingua.
noun plural markers.
statistical lexical head- outward transducers.
We present a corpus-based study of the sequential ordering among premodifiers in noun phrases.
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications.
We coerce the English FrameNet into Chinese using a bilingual lexicon, frame context in FrameNet and taxonomy structure in HowNet.
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
This paper discusses the lexical knowledge of idioms for idiom recognition.
This paper describes the tree-structured maximum mutual information (MMI) encoders used in SSI's Phonetic Engine?to perform large-vocabulary, continuous speech recognition.
These templates represent grammatically correct sentence patterns.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
Further, we have developed MM-DCG translator to transfer rules in MM-DCG into Prolog predicates.
relation based sim-ilarity measure and distribution based similarity measure.
A LOGICAL SEMANTICSFOR NONMONOTONIC SORTSMark A.
the cross-entropy.
Natural language parsing requires ex-tensive lexicons containing subcategori-sation information for specific sublan-guages.
We explore the use of Support Vector Ma-chines (SVMs) for biomedical named en-tity recognition.
Motion Verbs.
Bilingual Corpus-based Analysis.
This paper discusses the partitive-genitive case alternation of Finnish adpositions.
sidesg.
oppositesg.
We represent shallow linguistic information as linguistic features in our ME model.
as a hybrid system.
We review studies of reference resolution, word recognition, and pragmatic effects on syntactic ambiguity resolution.
We present discriminative reordering models for phrase-based statistical machine translation.
We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses.
domain-independent, semantic information for question interpretation.
In this paper I outline Type-inheritance Combinatory Categorial Grammar (TCCG), an implemented feature structure based CCG fragment of English.
The grammars used for speech recognition dictate legal word sequences.
We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text.
We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.
This paper proposes a discriminative HMM (DHMM) with long state dependence (LSDDHMM) to segment and label sequential data.
This part-ofspeech predictor will be used in a part-of-speech tagger to handle out-of-lexicon words.
This paper describes discriminative language modeling for a large vocabulary speech recognition task.
Rohrer.
We describe a bidirectional framework for natural language parsing and genera-tion, using a typed feature formalism arid an HPSG-based grammar with a parser arid generator derived from parallel pro-cessing algorithms.
We present a Korean question answering framework for restricted domains, called K-QARD.
morphological derivation and synonymy expansion) in web search strategies.
This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG).
information technology test reports and medical finding reports.
We investigate independent and relevant event-based extractive mutli-document summarization approaches.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
WordNet) to ambiguous words occurring in a syntactic dependency.
We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors' of noun phrases.
Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases.
This paper describes named entity (NE) extraction based on a max-imum entropy (M.E.)
model and transformation rules.
This paper introduces PhraseNet, a context- sensitive lexical semantic knowledge base system.
The Hidden Markov Model (HMM) for part-of-speech (POS) tagging is typically based on tag trigrams.
The chunker partitions the part-of-speech sequence into segments called chunks.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.
We perform a linguistic analysis of documents during indexing for information retrieval.
This paper describes machine learning based parsing and question classification for question answering.
ethnologue.com).
PROLOG.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
We use linguistic patterns and HTML text structures to extract text fragments contain-ing term descriptions.
We present BAYESUM (for 揃ayesian summarization?, a model for sentence extraction in query-focused summarization.
This paper presents a computational model of incremental utterance produc-tion in task-oriented dialogues.
Tagging compound verb groups in an annotated corpus exploiting the verb rules is described.Keywords: compound verb groups, chunking, morphosyntactic tagging, inductive logic pro-gramming
Text metadata.
We propose a kana-kanji conversion system with input support based on prediction.
We describe a method for retrieving counts for adjective-noun, noun-noun, and verb- object bigrams from the web by querying a search engine.
We describe Talk'n'Travel, a spoken dialogue language system for making air travel plans over the telephone.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
This paper describes DialogueView, a tool for annotating dialogues with utter-ance boundaries, speech repairs, speech act tags, and discourse segments.
Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing.
coreferential) relations.
This paper presents the use of probabilistic class-based lexica for disambiguation in target-word selection.
We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers.
verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction.
This paper presents a method for extracting subcorpora documenting different subcategorization frames for verbs, nouns, and adjectives in the 100 mio.
It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination.
We study unsupervised methods for learning refinements of the nonterminals in a treebank.
Pos811D.
We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system.
We present a machine-learning approach to modeling the distribution of noun phrases (NPs) within clauses with respect to a fine-grained taxonomy of grammatical relations.
representation methods, hypermedia maps.
hypermedia writing.
.
1			Back梤eferencing in text		?.
11			Belief structure	.
handwriting).
in Machine Translation).
Existing speech recognizers work best for "high-quality close-talking speech."
We show how to do this using , -reduction constraints in the constraint language for A-structures (CLLS).
Among the transformational grammarsi.
We present a constraint-based syntax-semantics interface for the construction of RMRS (Robust Minimal Recursion Semantics) representations from shallow grammars.
incomplete syntactic analysis.
(1990) Modelling the perception of concurrent vowels: vowels with different fundamental frequencies.
This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations.
base forms and POS tags.
without enumerating readings.
We describe an algorithm for recovering non-local dependencies in syntactic dependency structures.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
a finite-state transducer.
We automatically classify verbs into lexical se-mantic classes, based on distributions of indica-tors of verb alternations, extracted from a very large annotated corpus.
In this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) tagging framework.
parsing procedures and semantic head-driven generation.
Dialogue system for 3D virtual environ-ments).
General structure of PORTUGA: A - analysis, G - generation.
We propose to score phrase translation pairs for statistical machine translation using term weight based models.
These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs.
We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.
We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms.
This paper describes new default unification, lenient default unification.
noun phrase (NP) syntax.
This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unification- based shallow-level parser using transformational rules over syntactic patterns.
We collected declaratively subjective clues in opinion- expressing sentences from Japanese web pages retrieved with opinion search queries.
This paper compares two approaches to computational semantics, namely semantic unification in Lexicalized Tree Adjoining Grammars (LTAG) and Lexical Resource Semantics (LRS) in HPSG.
This paper presents PROVERB a text planner for argumentative texts.
This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA).
We extend default unification to non-parallel structures, which is important for speech and multimodal dialog systems.
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
variable-matching without variable substitution.
We present in this paper the method of English-to-Korean(E-K) transliteration and back-transliteration.
in morphological analysis).
We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures.
Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.
idf based method.
In this paper, subclasses of monadic context- free tree grammars (CFTGs) are compared.
We have constructed the IPA Lexicon of Basic Japanese Nouns (IPAL-BN), which has a hierarchical structure based on the syntactic and semantic proper-ties of nouns.
We present a discourse process that recognizes characters' thoughts and perceptions in third-person narrative.
SCsem is the semantic weighting.
(Unification Categorial Grammar).
expressions denoting geographical localisations).
We find they can capture effectively allophonic variation, alternative pronunciation, word co-articulation and segmental durations.
Dutch subordinate clauses.
Dutch subordinate clauses for non-transformational theories of grammar.
We then examine the effect of integrating pausal and spontaneous speech phe-nomena into syntactic rules for speech recognition, using118 sentences.
Speaker independent phonetic transcription of fluent speech is performed using an ergodic continuously variable duration hidden Markov model (CVDHMM) to represent the acoustic, phonetic and phonotac tic structure of speech.
We present a comparison of two formalisms for representing natural language utterances, namely deep syntactical Tectogrammatical Layer of Functional Generative Description (FGD) and a semantic formalism, MultiNet.
We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural- language processing.
This paper describes the user expertise model in AthosMail, a mobile, speech-based e-mail system.
trigger words and parsing structures.
The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution.
We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks.
This paper proposes an alignment adaptation	approach	to	improvedomain-specific (in-domain) word alignment.
We present a simple approximation method for turn-ing a Head-Driven Phrase Structure Grammar into a context-free grammar.
We describe an implementation of data- driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
Managing Gigabytes: Compressing and Indexing Documents and Images, Academic Press/Morgan Kaufmann.
WORD- NET DOMAINS).
We consider the translation of European Parliament Speeches.
We investigate the problem of learning a part-of-speech (POS) lexicon for a resource-poor language, dialectal Arabic.
We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees.
Language Understanding (NLU) systems,particularly for data-driven ones.
knowledge and language-specific heuristics.
Figure 1: Dimensions of Language Engineering Complexity.
We use statistical dependency parsers to determine dependency relations between base phrases in a sentence.
We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs.
We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications.
information technology test reports and medical finding reports.
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks).
We apply weighted voting of 8 SVMs-based systems trained with distinct chunk repre-sentations.
We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
part-of-speech tagged corpora).
LFG and GPSG are augmented PS- grammars.
We propose astemming method for Mongolian to extractloanwords correctly.
This paper outlines a formal computational semantics and pragmatics of the major speech act types.
This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame.
We address Question Answering (QA) for biographical questions, i.e., questions asking for biographical facts about persons.
error types).
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language.
CTXMATCH performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.
This paper presents Trace dr Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (UG) and ideas of Government dr Binding Theory (on).
in GPSO and HPSG.
Bayesian, decision tree and neural network classifiers) to discover language model errors.
very large corpora of strings.
We present two methods for unsupervised segmentation of words into morpheme-like units.
This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE).
The method combines morphological and lex-ical processing, bilingual word alignment, and graph-theoretic cluster generation.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
building classifiers using acoustic-prosodic features.
plus petits.
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features.
in content based information retrieval.
This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT).
We present a statistical phrase-based translation model that uses hierarchical phrases?phrases that contain subphrases.
database tuples).
This paper compares a number of generative probability models for a wide- coverage Combinatory Categorial Grammar (CCG) parser.
This paper explores the problem of identifying sen-tence boundaries in the transcriptions produced by automatic speech recognition systems.
Human face-to-face conversation is an ideal model for human-computer dialogue.
We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements.
We define an Earley parsing schema for STG's and characterize the valid parse items.
The model uses syntactic patterns and N-grams reflecting the hierarchical discourse structures of dialogues.
We incorporate some consequence relations into DRT using se- writ calculi.
We present Minimum Bayes-Risk word alignment for machine translation.
This paper describes a media-independent, compositional, plan-based approach to representing attributive descriptions for use in integrated text and graphics generation.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
We investigate the suitability of sub-categorization acquisition for evalu-ation of word sense disambiguation (WSD) systems.
We extend Traum抯 grounding act model by introducing degree of groundedness, and partial and mid-discourse unit grounding.
such as definite noun phrases and pronouns, in a discourse.
base phrases) into the arguments of a predicate.
We propose a context-sensitive method to predict noun- phrases in the next utterance of a telephone inquiry dialogue.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
We therefore use phonetic recognition of utterances and search for salient phonetic sequences within the decodings.
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
We introduce a new method for recovering empty nodes and their antecedents (capturing long distance dependencies) from parser output in CFG trees using LFG f-structure reentrancies.
We here investigate questions with implicatures related to biography facts in a web-based QA system, PowerBio.
The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language.
In this paper, we describe a resource-light system for the automatic morphological analysis and tagging of Russian.
We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges.
'Formerly SDC—A Burroughs Company.
(4) Investigation revealed adequate lube oil.
A modular parallel distributed processing architecture for parsing, representing and paraphrasing sentences with multiple hierarchical relative clauses is presented.
In this work we discuss refinements of the stochastic segment model, an alternative to hidden Markov models for representation of the acoustic variability of phonemes.
Results are presented for speaker-independent phoneme classification in continuous speech based on the TIMIT database.
To obtain verb paradigms we extracted left and right bigrams for the 400 most frequent verbs from over 100 million words of text, calculated the Kullback Leibler distance for each pair of verbs for left and right contexts separately, and ran a hierarchical clustering algorithm for each context.
We present the dialogue module of the speech-to-speech translation system VERB-MOBIL.
This paper proposes a segmentation stan-dard for Chinese natural language processing.
Lexical Functional Grammar (LFG) (Kaplan,Bresnan 82] is a powerful formalism for that purpose.
Prolog provides a good tools for the implementation of LFG.
We investigated of the characteristics of in-text causal relations.
We designed causal relation tags.
This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar.
The strategy includes parsing and phrase prediction algorithms.
After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships.
A fast parsing algorithm using breadth-first search is also proposed.
In this paper, we focus on exploiting both textual and prosodic features for topic segmentation of Mandarin Chinese.
We then contrast these results with a simple text similarity-based classification scheme.
Finally we build a merged classifier, finding the best effectiveness for systems integrating text and prosodic cues.
We describe an approach to resolving definite descriptions and pronominal anaphora as subcases of a general strategy for presupposition satisfaction.
A first model implements a variant of instance-based learning, in which a weighed similarity metric and a database of prototypical exemplars are used to predict new mappings.
In the second model, grapheme-to-phoneme mappings are looked up in a compressed text-to-speech lexicon (table lookup) enriched with default mappings.
We compare performance and accuracy of these approaches to a connectionist (backpropagation) approach and to the linguistic knowledge-based approach.
This paper proposes a new method for learning a context-sensitive conditional probability context-free grammar from an unlabeled bracketed corpus based on clustering analysis and describes a natural language parsing model which uses a probability-based scoring function of the grammar to rank parses of a sentence.
This result supports the assumption that local contextual statistics obtained from an unlabeled bracketed corpus are effective for learning a useful grammar and parsing.
We investigate techniques to support the answering of opinion-based questions.
We first present the OpQA corpus of opinion questions and answers.
As an initial step towards the development of MPQA systems, we investigate the use of machine learning and rule-based subjectivity and opinion source filters and show that they can be used to guide MPQA systems.
An easy way of translating queries in one language to the other for cross-language information retrieval (IR) is to use a simple bilingual dictionary.
This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system: non-monotonic reasoning, implicit reference resolution, and database query paraphrase.
We present a new way to simplify the construction of precise broad-coverage grammars, employing typologically-motivated, customizable extensions to a language-independent core grammar.
We discuss a dual-component linguistic model that attempts to reflect the generation process of the learners.
General Direction for Multilingual Transfers to promote machine translation.
ERRORSAll documents are not suitable for machine translation.
We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order.
In this paper, we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts.
Our method uses dynamic programming with alignment decision based on the local syntactic similarity between two sentences.
The paper presents the Constructive Dialogue Model as a new approach to formulate system goals in intelligent di-alogue systems.
This paper describes a novel framework for using scenario knowledge in open- domain Question Answering (Q/A) applications that uses a state-of-the-art textual entailment system (Hickl et al., 2006b) in order to discover textual information relevant to the set of topics associated with a scenario description.
We describe HARC, a system for speech understanding that integrates speech recognition techniques with natural language processing.
We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors.
This paper describes a. method for positioning un-known words in an existing thesaurus by using word-to-word relationships with relation (case) markers extracted from a large corpus.
In this paper, we introduce TextRank ?a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.
We present a simple method for language independent and task independent text categorization learning, based on character-level n-gram language models.
This paper introduces a graph-based algorithm for sequence data labeling, using random walks on graphs encoding label dependencies.
This paper analyzes various issues in building a HMM based multilingual speech recognizer for Indian languages.
A dialogue based Question Answering (QA) system for Railway information in Telugu has been described.
We investigate prototype-driven learning for primarily unsupervised grammar induction.
This paper proposes a new class-based method to estimate the strength of association in word co-occurrence for the purpose of structural disambiguation.
We have applied our method to determining dependency relations in Japanese and prepositional phrase attachments in English.
This paper describes a new finite-state shallow parser.
This paper investigates the automatic identification of aspects of Information Structure (IS) in texts.
We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines.
We show the performance of C4.5, Bagging, and Ripper classifiers on several classes of instances such as nouns and pronouns, only nouns, only pronouns.
This paper presents the current results of an ongoing research project on corpus distribution of prepositions and pronouns within Polish preposition-pronoun contractions.
The results of corpus-based investigations of the distribution of prepositions within preposition-pronoun contractions can be used for grammar-theoretical and lexicographic purposes.
Computational and Theoretical Background2.1	Computational	background.The	experimental	system	of	automaticquestion-answering TIBAQ (Text-and-Inference Based Answering of Questions,	cf.
also D. Hays' notion of automatic encyclopedia).
For example, head-finding rules are used to augment node labels with lexical heads.
One of the approaches that helped recently is the use of latent semantic analysis to capture the semantic fabric of the document and enhance the n-gram model.
In this paper, we explain a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling.
This approach augments each word with its syntactic descriptor in terms of the part-of-speech tag, phrase type or the supertag.
We also present some observations on the effect of the knowledge of content or function word type in language modeling.
Lexical Chains are powerful representations of documents.
In this paper, we propose a greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts.
For that purpose, we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole-Based Overlapping Clustering Algorithm.
This paper proposes a hybrid of handcrafted rules and a machine learning method for chunking Korean.
We apply interpretive rules to clausal patterns and patterns of modification, and concurrently abstract general 損ossibilistic?propositions from the resulting formulas.
Several attempts have been made to apply statistical machine learning approaches, including Support Vector Machines (SVMs) with sophisticated features and kernels.
Using a novel multi-resolution encoding of the question抯 parse tree, we induce a Conditional Random Field (CRF) to identify informer spans with about 85% accuracy.
Syllable-to-word (STW) conversion is important in Chinese phonetic input methods and speech recognition.
This paper describes a noun-verb event-frame (NVEF) word identifier that can be used to solve these problems effectively.
To further expand its coverage, we shall extend the study of NVEF to that of other co-occurrence restrictions such as noun-noun pairs, noun-adjective pairs and verb-adverb pairs.
We describe the application of the Ling- Pipe toolkit to Chinese word segmentation and named entity recognition for the 3rd SIGHAN bakeoff.
We defined 10 Descriptive Answer Type(DAT)s as answer types for descriptive questions.
Our work is theoretically rooted in previous work on information structuring and word order in the Prague School framework as well as on the systemic-functional notion of Theme.
We report on a project to use the lexical aspect features of (a)telicity re-flected in the Lexical Conceptual Structure of the input text to suggest tense and discourse structure in the English translation of a Chinese newspaper corpus.
This paper discusses the role of morphological and syntactic information in the automatic acquisition of semantic classes for Catalan adjectives, using decision trees as a tool for exploratory data analysis.
Unification-based NL parsers that copy argument graphs to prevent their destruction suffer from inefficiency.
The data structures unified are directed acyclic graphs (DAG's), used to encode grammar rules, lexical entries and intermediate parsing structures.
unification is a destructive operation.
Then, we compare the improvement brought to the engine by the adjunction of two different non-interactive spelling correction strategies: a classical one, based on a string-to-string edit distance calculus, and a contextual one, which adds linguistically- motivated features to the string distance module.
This algorithm enables existing corpus-based estimation techniques for stochastic TAGs to be used for language modelling.
We explore how virtual examples (artificially created examples) improve performance of text classification with Support Vector Machines (SVMs).
The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.
This leads to a new type of semantic lexicon (CoRELEx) that supports underspecified semantic tagging through a design based on systematic polysemous classes and a class-based acquisition of lexical knowledge for specific domains.
recognize]: output in order to handle speech recognition errors.
We define two concept-level CMs, which are on content-words and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars.
Negative life events play an important role in triggering depressive episodes.
This framework integrates a cognitive motivated model, Hyperspace Analog to Language (HAL), to represent words as well as combinations of words.
This paper describes experiments in incremental query processing and indexing with the INQUERY information retrieval system on the TIPSTER queries and document collection.
Our method compares two probability distributions over WordNet by measuring the semantic distance of the component nodes, weighted by their probability.
We present an approach to statistical machine translation that combines ideas from phrase-based SMT and traditional grammar-based MT.
Our system incorporates the concept of multi-word translation units into transfer of dependency structure snippets, and models and trains statistical components according to state- of-the-art SMT systems.
Compliant with classical transfer-based MT, target dependency structure snippets are input to a grammar-based generator.
The domain-specific lexical items were obtained from subsections of a synchronous Chinese corpus, LIVAC.
We present Outilex, a generalist linguistic platform for text processing.
We propose a syntax of it-clefts using Tree-Local Multi- Component Tree Adjoining Grammar and a compositional semantics on the proposed syntax using Synchronous Tree Adjoining Grammar.
To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words.
The pattern-based rules describe how abbreviations are formed from definitions.
This paper describes the Italian all-words sense disambiguation task for Senseval-3.
This paper describes a system which generates animations for cooking actions in recipes, to help people understand recipes written in Japanese.
We designed and compiled the lexicon of cooking actions required for the animation generation system.
We describe a method for automatic word sense disambiguation using a text corpus and a machine-readable dictionary (MRD).
The method is based on word similarity and context similarity measures.
A proposed "Matrix" method for the representation of the inflectional paradigms of Arabic words is presented.
This paper presents a maximum entropy word alignment algorithm for Arabic- English based on supervised training data.
The main focus of our work is to provide an efficient parser that is practical to use with combining only lexical and part-ofspeech features toward language independent parsing.
Text prediction is a form of interactive machine translation that is well suited to skilled translators.
Besides the widely used lexical-level features, such as word formation pattern, morphological pattern, out-domain POS and semantic trigger, we also explore the name alias phenomenon, the cascaded entity name phenomenon, the use of both a closed dictionary from the training corpus and an open dictionary from the database term list SwissProt and the alias list LocusLink, the abbreviation resolution and in- domain POS using the GENIA corpus.
This paper describes a multi-lingual phrase-based Statistical Machine Translation system accessible by means of a Web page.
This paper deals with translation ambiguity and target polysemy problems together.
We present a novel context pattern induction method for information extraction, specifically named entity extraction.
Using token membership in these extended lists as additional features, we improved the accuracy of a conditional random field-based named entity tagger.
We present CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Ros磂 et al., 2002a).
We introduce a new technique for using the speech of multiple reference speakers as a basis for speaker adaptation in large vocabulary continuous speech recognition.
Term translation probabilities proved an effective method of semantic smoothing in the language modelling approach to information retrieval tasks.
In this paper, we use Generalized Latent Semantic Analysis to compute semantically motivated term and document vectors.
Our experiments demonstrate that GLSAbased term translation probabilities capture semantic relations between terms and improve performance on document classification.
Johnston 1998 presents an approach in which strategies for mul-timodal integration are stated declaratively using a unification-based grammar that is used by a multi-dimensional chart parser to compose inputs.
In this paper, we present an alternative approach in which multi modal parsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and out-puts their joint interpretation.
Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.
Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.
Each source of information is represented by kernel functions.
We also compare the SVM with KNN on different kernels.
We model the likely contexts of all words in an ASR system vocabulary by performing a lexical co-occurrence analysis using a large corpus of output from the speech system.
We present a divide-and-conquer strategy based on finite state technology for shallow parsing of real-world German texts.
Shallow parsing is supported by suitably configured prepro-cessing, including: morphological and on-line com-pound analysis, efficient POS-filtering, and named entity recognition.
This paper describes a heuristic for morpheme- and morphology-learning based on string edit distance.
A new method of autcmatic lexical disambiguation of big texts is described, using recent proof-theoretical results from the theory of categorial grainier.O.
One of the tasks of the Thesaurus, one of the departments, is to build a database for lexicological research.
As was the case for the morphological analyzer, the syntactic parser is an implementation of a categorial calculus.
ite construction of and philosophy behind the LaMbak categorial parser we use for the disambiguation and syntactic analysis is the topic of this paperl.
This paper describes SMES, an informa-tion extraction core system for real world German text processing.
A parsing architecture is described which uses a permanent store of context-free rule patterns encoded as split composite vectors, and two interacting working memory units.
For example, Waltz and Pollack (1984) have devised a model of word-sense and syntactic disambiguation, and Cottrell (1985) has proposed a neural network style model of parsing.
Conjunctions are particularly difficult to parse in traditional, phrase-based grammars.
This paper explores the use of support Vector Machines (sVMs) for anextended named entity task.
Furthermore we compare the performance of the sVM model to a standard HMM bigram model.
SRV can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training.
We describe an approach to interpreting LFG f-structures (Kaplan & Bresnan, 1982) truth-conditionally as underspeci-tied quasi-logical forms.
We provide a reverse mapping from QLFs to f-structures and establish isomorphic subsets of the QLF and LFG formalism.
In particular I will discuss approaches using stochastic grammars analogous to those used in computational linguistics, both for gene finding and protein family classification.
This paper describes an initial evaluation of systems that answer questions seeking definitions.
In this paper, we propose an implementable characterization of genre suitable for automatic genre identification of web pages.
This paper describes an analysis method which uses heuristic knowledge to find local syntactic structures of Chinese sentences.
We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.
Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.
We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a 揵ag-of-words?kernel.
The aim of this paper is to present a language-neutral, theory-neutral method for annotating sentence- internal temporal relations.
Asearchable	multi-languagedatabase has already been created.
We present an efficient procedure for cost-based abduction, which is based on the idea of using chart parsers as proof procedures.
We define a data model for storing geographic information from multiple sources that enables the efficient production of customizable gazetteers.
This paper presents a method of resolv-ing ambiguity by using a variant of cir-cumscription, prioritized circumscrip-tion.
We also discuss an im-plementation of prioritized circumscrip-tion by a hierarchical logic program-ming (HCIA.))
In this paper, I present a model of the local organization of extended text.
This paper describes an approach for modeling events in text as event intervals and for generating linear orders of event intervals, useful for the summarization of events or as the basis for question answering systems.
There is a type of nominal ellipsis that has been neglected in the study of ellipsis resolution.
We have drawn primarily on explicit and implicit information from machine-readable dictionaries (MRD's) to create a broad coverage lexicon.
Categorial grammar has traditionally used the X-calculus to represent meaning.
We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking.
Cmcsim-Tutor version 2, a dialogue-based intelligent tutoring system (ITS), is nearly five years old.
We propose to restrain VP-ellipsis resolution by presupposition neutralization.
One is a statistical dependency transduc-tion model using head transducers, the other a case-based transduction model in-volving a lexical similarity measure.
We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese.
We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger.
Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters.
We present preliminary results concerning robust techniques for resolving bridging definite descriptions.
The described tagger is based on a hidden Markov model and uses tags composed of features such as partof-speech, gentler, etc.
In this paper, we focus on prosody-based topic segmentation of Mandarin Chinese.
Serial verb constructions (SVCs) in Chinese are popular structural ambiguities which make parsing difficult.
This paper describes a novel system for acquiring adjectival subcategorization frames (SCFs) and associated frequency information from English corpus data.
The system incorporates a decision-tree classifier for 30 SCF types which tests for the presence of grammatical relations (GRs) in the output of a robust statistical parser.
It uses a powerful pattern- matching language to classify GRs into frames hierarchically in a way that mirrors inheritance-based lexica.
information retrieval, translation unit discovery, and corpus studies.
This paper describes how to construct a finite-state machine (FSM) approximating a 'unification-based' grammar using a left-corner grammar transform.
This paper presents a new approach to automated table extraction that exploits formatting cues in semi-structured HTML tables, learns lexical variants from training examples and uses a vector space model to deal with non-exact matches among labels.
We propose a novel Co-Training method for statistical parsing.
We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their performance by means oftwo experiments: coarse-level clustering and simple information retrieval.
We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Tree- bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.
This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.
Case-based machine translation is a promising approach to resolving problems in rule-based machine translation systems, such as difficulties in control of rules and low adaptability to specific domains.
We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency.
Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance.
The	system's maincomponents are:	a Generalized Phrase StructureGrammar (GPSG); a top-down parser; a logic transducer that outputs a first-order logical representation; and a "disambiguator" that uses sortal information to convert "normal-form" first-order logical expressions into the query language for HIRE, a relational database hosted in the SPHERE system.
This paper describes an unsupervised method for noun compound bracketing which extracts statistics from Web search engines using a x2 measure, a new set of surface features, and paraphrases.
In this paper we propose a generalization of the Stack-based decoding paradigm for Statistical Machine Translation.
Experimental results are also reported for a search algorithm for phrase-based statistical translation models.
Ergodic IIMMs have been successfully used for modeling sentence production.
This paper introduces the N-th order Ergodic Multigram TIMM for language modeling of such languages.
This work represents a first attempt at modeling morphological-syntactic interaction in a generative probabilistic framework to allow for MH parsing.
We show that morphological information selected in tandem with syntactic categories is instrumental for parsing Semitic languages.
We further show that redundant morphological information helps syntactic disambiguation.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
We apply the LOP-CRF to two sequencing tasks.
We describe a probabilistic extraction model that provides mutual benefits to both 搕op-down?relational pattern discovery and 揵ottom-up?relation extraction.
We describe the LDV-COMBO system presented at the Shared Task.
News on electrical bulletin boards con-sist of high density expressions.
In this paper we describe our experiences with a tool for the development and testing of natural language grammars called GTU (German: GrammatikTestumgebumg; grammar test environment).
GTU supports four grammar formalisms under a window-oriented user interface.
This paper deals with multilingual database generation from parallel corpora.
This paper discusses the establishment and implementation of a curriculum for teach-ing NLP.
We propose a novel approach to the identification of biomedical terms in research publications using the Perceptron HMM algorithm.
We present an implemented machine learning system for the automatic detection of nonreferential it in spoken dialog.
The system builds on shallow features extracted from dialog transcripts.
The detection of prosodic characteristics is an important aspect of both speech synthesis and speech recognition.
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
The lexical structures proposed are derived from the analysis of semantic collocations over large text corpora.
This paper describes a method of incremental natural language generation using a parallel marker-passing algorithm for modeling simultaneous interpretation.
real- time speech-to-speech dialog translation system developed at the Center for Machine Translation at Carnegie Mellon University, and publicly demonstrated since March 1989.
We then report on experiments which go a step further, using four-class classifiers based on automated scoring of texts for each dimension of the typology.
We present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections.
Our method combines and extends two previous techniques that were based mostly on manually crafted lexical patterns and WordNet hypernyms.
Then, a method to combine the different models for bilingual lexicon extraction is presented.
Lastly, methods for bilingual terminology extraction and thesaurus enrichment are discussed.
We present the results from a series of experiments aimed at uncovering the discourse structure of man-machine communication in natural language (Wizard of Oz experiments).
We also show how this predictability can be used to aid non-native speakers to determine the countability of English nouns when building a bilingual machine translation lexicon.
This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture.
The goal is to classify the emotional affinity of sentences in the narrative domain of children抯 fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis.
We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations.
This paper describes a supervised learn-ing method to automatically select froma set of noun phrases, embedding propernames of different semantic classes, theirmost distinctive features.
This classifier is used to esti-mate the probability distribution of anout of vocabulary proper name over atagset.
This probability distribution isitself used to estimate the parameters ofa stochastic part of speech tagger.
This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.
In our new system, three types of agents: a) domain agents, 13) strat-egy agents, and c) context agents were realized.
Vieira and Poesio (2000) proposed an algorithm for definite description (DD) resolution that incorporates a number of heuristics for detecting discourse- new descriptions.
In this paper, a large class of English declarative sentences, including post-noun-modification by relative clauses, is formalized using a two-level grammar.
In this paper we present an application fostering the integration and interoperability of computational lexicons, focusing on the particular case of mutual linking and cross-lingual enrichment of two wordnets, the ItalWordNet and Sinica BOW lexicons.
We present a novel, data-driven method for integrated shallow and deep parsing.
Mediated by an XML-based multi-layer annotation architecture, we interleave a robust, but accurate stochastic topological field parser of German with a constraint- based HPSG parser.
We here adapt a result on the complexity of ID/LP-grammars to the dependency framework.
We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses.
There exists strong word association in natural language.
Both the distance-independent(DI) and distancedependent(DD) MI-Trigger-based models are constructed within a window.
This paper describes results from several dozen experimental systems, and draws conclusions about the ability of speech recognition models to represent the relationship among syntax, prosody, and segmental acoustics.
Careful analysis of the relationship between prosody and syntax indicates that syntactic phrase boundaries are the most important cue for prosodic phrase boundary recognition, while part of speech is the most important cue for locating pitch accents, but that neither of these cues is entirely sufficient for either classification task.
We use machine learners trained on a combination of acoustic confidence and pragmatic plausibility features computed from dialogue context to predict the accuracy of incoming n-best recognition hypotheses to a spoken dialogue system.
Our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data.
The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT).
We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training.
We describe a computational formalism that captures structural relationships among participants in a dynamic scenario.
In particular, we investigate feature sets derived from graph representations of sentences and sets of sentences.
We show that a highly connected graph produced by using sentence-level term distances and pointwise mutual information can serve as a source to extract features for novelty detection.
These feature sets allow us to increase the accuracy of an initial novelty classifier which is based on a bagof-word representation and KL divergence.
This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.
In this paper, we present a semiautomatic approach for annotating semantic information in biomedical texts.
To construct BioProp, a semantic role labeling (SRL) system trained on PropBank is used to annotate BioProp.
This paper proposes an English adverb ordering method based on adverb gram-matical functions (subjuncts, adjuncts, dis-juncts and conjuncts) and meanings (pro-cess, space, time etc.
We have recently begun a project on summarizing simple narrative stories.
We present a named entity recognition and classification system that uses only probabilistic character-level features.
We present an integrated approach to speech and natural language processing which uses a single parser to create training for a statistical speech recognition component and for interpreting recognized text.
On the speech recognition side, our innovation is the use of a statistical model combining N-gram and context-free grammars.
This paper presents a unified solution, which is based on the idea of 搑oles tagging?
to the complicated problems of Chinese unknown words recognition.
The result and experiments in our system ICTCLAS shows that our approach based on roles tagging is simple yet effective.Keywords: Chinese unknown words recognition, roles tagging, word segmentation, Viterbi algorithm.
We use average mutual informaLion as global similarity metric to do classification.
Also, we use the resulting classes to do experiments on word class-based language model.
This paper describes a novel method of compiling ranked tagging rules into a deterministic finite-state device called a bimachine.
This paper presents a syntactical method of interpreting pronouns in Polish.
We describe our work in progress on natural language analysis in medical question- answering in the context of a broader medical text-retrieval project.
Spoken dialogue managers have benefited from using stochastic planners such as Markov Decision Processes (MDPs).
We use a Partially Observable Markov Decision Pro-cess (POMDP)-style approach to generate dialogue strategies by inverting the notion of dialogue state; the state represents the user抯 intentions, rather than the system state.
Today, several part-of-speech tagged corpora are readily available for research use.
We present a new approach to stochastic modeling of constraint-based grammars that is based on log-linear models and uses EM for esti-mation from unannotated data.
The techniques are applied to an LFG grammar for German.
Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models.
We present a statistical question answering sys-tem developed for TREC-9 in detail.
The sys-tem is an application of maximum entropy clas-sification for question/answer type prediction and named entity marking.
We explore a hybrid approach for Chinese definitional question answering by combining deep linguistic analysis with surface pattern learning.
This paper presents a word segmenta-tion system in France Telecom R&D Beijing, which uses a unified approach to word breaking and OOV identifica-tion.
We describe sign translation using example based machine translation technology.
We use a user- centered approach in developing an automatic sign translation system.
We have developed a prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream.KeywordsSign, sign detection, sign recognition, sign translation.
This paper describes a comparative applica-tion of Grammar Learning by Partition Searchto four different learning tasks: deep parsing,NP identification, flat phrase chunking and NPchunking.
It handles irregular, regular and semi-regular morphology through a ro-bust generative model using weighted Levenshtein alignments.
Unsupervised induction of grammatical gender is performed via global modeling of context-window feature agreement.
This paper proposes an automatic method of detecting grammar elements that decrease readability in a Japanese sentence.
In this paper, we describe ontology-based text categorization in which the domain ontologies are automatically acquired through morphological rules and statistical methods.
We present a system that incorporates agent- based technology and natural language generation to address the problem of natural language summarization of live sources of data.
We propose stochas-tic finite-state models for these two subproblems in this paper.
We present a method for learning stochastic finite-state models for lexical choice and lexical reordering that are trained automatically from pairs of source and target utterances.
Punjabi Machine Transliteration (PMT) is a special case of machine transliteration and is a process of converting a word from Shahmukhi (based on Arabic script) to Gurmukhi (derivation of Landa, Shardha and Takri, old scripts of Indian subcontinent), two scripts of Punjabi, irrespective of the type of word.The Punjabi Machine Transliteration System uses transliteration rules (character mappings and dependency rules) for transliteration of Shahmukhi words into Gurmukhi.
We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited.
We first discuss several challenges to error-driven discriminative approaches.
Content based navigation is realized by use of automatic speech recognition, information retrieval, information extraction and human computer interaction technology.
In addition to the browsing and querying functionalities, acoustics-based caller ID technology is used to proposes caller names from existing caller acoustic models trained from user feedback.
Specifically, we experiment with three lexical semantic classes of English verbs.
We provide a logical definition of Min-imalist grammars, that are Stabler抯 formalization of Chomsky抯 minimal-ist program.
This paper presents a sentence- based statistical model for Hangeul-Hanja conversion, with word tokenization included as a hidden process.
We present a new architecture for reversible NLP.
Separate parsing and generation grammars are constructed from the underlying application's semantic model and knowledge base.
In this paper we describe the algorithms used in the BBN BYBLOS Continuous Speech Recognition system.
The BYB LOS system uses context-dependent hidden Markov models of phonemes to provide a robust model of phonetic coarticulation.
The two features that have made the largest improvements in recognition accuracy since 1982 were the use of robust context-dependent phonetic models, and the addition of derivative spectral parameters in multiple codebooks.
The BYBLOS systemThe BYBLOS system uses context-dependent hidden Markov models (HMM) of phonemes to provide a robust model of coarticulation [1, 21.
A corpus-based statistical Generalization Tree model is described to achieve rule optimization for the information extraction task.
We present a pattern matching method for compiling a bilingual lexicon of nouns and proper nouns from unaligned, noisy parallel texts of Asian/Indo-European language pairs.
We also show how the results can be used in the compilation of domain-specific noun phrases.
semantic features, selectional restrictions) for complex terms and /or unknown words.
information extraction).
In this paper, we propose to introduce syntactic classes in a lexicalized dependency formalism.
We describe and experimentally evaluate a complete method for the automatic ac-quisition of two-level rules for morphologi-cal analyzers/generators.
In phase one, a mini-mal acyclic finite state automaton (AFSA) is constructed from string edit sequences of the input pairs.
Segmentation of the words into morphemes is achieved through view-ing the AFSA as a directed acyclic graph (DAG) and applying heuristics using prop-erties of the DAG as well as the elemen-tary edit operations.
We introduce the notion of a delimiter edge.
The paper addresses the issue of how to increase adaptivity in response generation for a spoken dialogue system.
We first describe a Java/XML-based generator which produces different realizations of system responses based on agendas specified by the dialogue manager.
This paper describes a Chinese word segmentation system that is based on majority voting among three models: a forward maximum matching model, a conditional random field (CRF) model using maximum subword-based tagging, and a CRF model using minimum subwordbased tagging.
This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).
This paper presents a new open text word sense disambiguation method that combines the use of logical inferences with PageRank-style algorithms applied on graphs extracted from natural language documents.
We also explore and evaluate methods that combine several open-text word sense disambiguation algorithms.
A theory of interlanguage (IL) lexicons is outlined, with emphasis on IL lexical entries, based on the I-IPSG notion of lexical sign.
In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inflectional languages.
SEAFACT (Semantic Analysis For the Animation of Cooking Tasks) is a natural language interface to a computer-generated animation system operating in the domain of cooking tasks.
This paper describes the semantic analysis of verbal modifiers on which the SEAFACT implementation is based.
We present a generic template for spoken dialogue systems integrating speech recognition and synthesis with 'higher-level' natural language dialogue modelling components.
This paper focuses on the typology of CJK orthographic variation, provides a brief analysis of the linguistic issues, and discusses why lexical databases should play a central role in the disambiguation process.
In this paper we discuss technical issues arising from the interdependence between tokenisation and XML-based annotation tools, in particular those which use standoff annotation in the form of pointers to word tokens.
We also describe experiments which explore the effects of different granularities of tokenisation on NER tagger performance.
We use patterns of linguistic features (e.g.
These are filtered using plan-based conversational implicatures to eliminate inappropriate ones.
Computational neurolinguistics (CN) is an approach to computational linguistics which includes neurally-motivated constraints in the design of models of natural language processing.
Based on Support Vector Machines (SVMs), a basic segmenter is designed regarding Chinese word segmentation as a problem of character-based tagging.
By applying latent semantic analysis (LSA) to filter extracted hyponymy relations we reduce the rate of error of our initial pattern-based hyponymy extraction by 30%, achieving precision of 58%.
Applying a graph-based model of noun-noun similarity learned automatically from coordination patterns to previously extracted correct hyponymy relations, we achieve roughly a fivefold increase in the number of correct hyponymy relations extracted.
In particular, tree kernels have been used to represent verbal subcategorization frame (SCF) information for predicate argument classification.
classification of unseen verbs.
A proposed "Matrix" method for the representation of the inflectional paradigms of Arabic words is presented.
This paper describes a framework for multilingual inheritance-based lexical representation which al-lows sharing of information across languages at all levels of linguistic description.
The paper fo-cuses on phonology.
This article is concerned with determining the constraints on the selection of appropriate intonation in speech generation in human- machine information seeking dialogues.
We take into consideration factors such as dialogue history, speaker's attitudes, hearer's expectations, and semantic speech functions.
We describe the Spanish-to-English LDVCOMBO system for the Shared Task 2: 揈xploiting Parallel Texts for Statistical Machine Translation?of the ACL-2005 Workshop on 揃uilding and Using Parallel Texts: Data-Driven Machine Translation and Beyond?
Several phrase-based translation models are built out from these alignments.
In this paper we describe the analytic question answering system HITIQA (High-Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts.
Parsing in type logical grammars amounts to theorem proving in a substructural logic.
This paper takes the proof net presentation of Lambek抯 associative calculus as a case study.
We compare the effect of lexical, temporal, syntactic, semantic, and prosodic features on system performance.
This paper proposes the use of uncertainty reduction in machine learning methods such as co-training and bilingual bootstrapping, which are referred to, in a general term, as 慶ollaborative bootstrapping?
It proposes a new measure for representing the degree of uncertainty correlation of the two classifiers in collaborative bootstrapping and uses the measure in analysis of collaborative bootstrapping.
Furthermore, it proposes a new algorithm of collaborative bootstrapping on the basis of uncertainty reduction.
This paper analyzes the requirements for adding a temporal reasoning component to a natural language database query system, and proposes a computational model that satisfies those requirements.
The paper describes a similarity-based model to present the morphological rules for Chinese com-pound nouns.
A technique for reducing a tagset used for n-gram part-of-speech disambiguation is introduced and evaluated in an experiment.
A scalable natural language generation (NLG) system called HYPERBUG 1 embedded in an agent-based, multimodal dialog system is presented.
The few existing approaches have focused on the development of algorithms specific to word domain disambiguation.
In this paper we explore an alternative approach where word domain disambiguation is achieved via word sense disambiguation.
The paper presents a computational theory for resolving Japanese zero anaphora, based on the notion of discourse segment.
We see that the discourse segment reduces the domain of antecedents for zero anaphora and thus leads to their efficient resolution.Also we make crucial use of functional notions such as empathy hierarchy and minimal semantics thesis to resolve reference for zero anaphora [Kuno, 1987].
Multimodal dialogue systems allow users to input information in multiple modalities.
This paper discusses human semantic knowledge and processing in terms of the SCHOLAR system.
The strategy for natural language interpretation presented in this paper implements the dynamics of context change by translating natural language texts into a meaning representation language consisting of (descriptions of) programs, in the spirit of dynamic predicate logic (DPL) [5].
We present a discriminative, large- margin approach to feature-based matching for word alignment.
The structural features provide the information for the correspondences of parts-ofspeech (POS) sequences which are useful in translation.
Discourse chunking is a simple way to segment dialogues according to how dialogue participants raise topics and negotiate them.
The CIAOSENSO WSD system is based on Conceptual Density, WordNet Domains and frequences of WordNet senses.
This paper describes the upvunige-CIAOSENSO WSD system, we participated in the english all-word task with, and its versions used for the english lexical sample and the Word- Net gloss disambiguation tasks.
We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus.
We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task.
Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state.
This article presents the Structural Correspondence Specification Environment (SCSE) being implemented at GETA.The SCSE is designed to help linguists to develop, consult and verify the SCS Grammars (SCSG) which specify linguistic models.
This paper addresses two previously unresolved issues in the automatic evaluation of Text Structuring (TS) in Natural Language Generation (NLG).
This paper investigates the use of a language inde-pendent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as inde-pendent evidence sources.
We propose a new approach under the example-based machine translation paradigm.
This paper investigates the feasibility of automated scoring of spoken English proficiency of non-native speakers.
(2) We use classification and regression trees to understand the role of different features and feature classes in the characterization of speaking proficiency by human scorers.
In this article, we apply to natural language parsing and tagging the device of trigger- pair predictors, previously employed exclusively within the field of language modelling for speech recognition.
We describe MSR-MT, a large-scale hybrid machine translation system under development for several language pairs.
Trained on English and Spanish technical prose, a blind evaluation shows that MSR-MT抯 integration of rule-based parsers, example based processing, and statistical techniques produces translations whose quality exceeds that of uncustomized commercial MT systems in this domain.
The phrase acquisition is based on entropy minimization and the feature selection is driven by the entropy reduction principle.
We further demonstrate that the phrase- grammar based n-gram language model significantly outperforms a phrase-based n-gram language model in an end-to-end evaluation of a spoken language application.
The method is based on graph rewriting using memory- based learning, applied to dependency structures.
It also facilitates dependency-based evaluation of phrase structure parsers.
TEXAN is a system of transfers-oriented text analysis.
The transition of this concept of linguistic actions (text acts) to the model of computer analysis is performed by a context-free illocution grammar processing categories of actions and a propositional structure of states of affairs.
This is accomplished by formulating the semantic role labeling as a classification problem of dependency relations into one of several semantic roles.
A dependency tree is created from a constituency parse of an input sentence.
The dependency tree is then linearized into a sequence of dependency relations.
Finally, the features are input to a set of one-versus-all support vector machine (SVM) classifiers to determine the corresponding semantic role label.
We present in this paper the Language Computer抯 system for WMT06 that employs LM- powered reranking on hypotheses generated by phrase-based SMT systems
As part of a cooperative project, we present an innovative approach to auxiliaries and multiple genitive NPs in German.
The approach developed for mul-tiple genitive NPs provides a more ab-stract, language independent representa-tion of genitives associated with noun-nalized verbs.
This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces.
We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives.
Temporal information analysis is very important for Chinese Information Process.
We demonstrate how this grammar may be used for efficient incremental parsing, by employing normalisation techniques.
We address the problem of using partially la-belled data, eg large collections were only little data is annotated, for extracting biological en-tities.
We describe and experimentally evalu-ate a system, FeasPar, that learns pars-ing spontaneous speech.
This paper describes the semantic diversity of A no B and a method of semantic analysis for such phrases based on feature unification.
The parsers combine lexical indices such as discourse markers with formatting instruc-tions (HTML tags) for analyzing enumera-tions and associated initializers.
Corpus-based grammar induction relies on us-ing many hand-parsed sentences as training examples.
This paper presents the user-centered interface of a summarization system for physicians in Bone Marrow Transplan-tation (BMT).
It is based on pragmatic proper-ties of Japanese conditionals.
This paper first presents a review of the options available for conveying maps and graphics to visually impaired and blind people.
Tactile perception is serial rather than synoptic.
We also outline an approach for automatically revising attribute value grammars using counter-examples.
This paper examines a particular PROLOG implementation of Discourse Representation theory (DR theory) constructed at the University of Texas.
The implementation also contains a Lexical Functional Grammar parser that provides f-structures; these 1-structures are then translated into the semantic representations posited by DR theory, structures which are known as Discourse Representation Structures (DRSs).
Srini vas	(97)	enriches	traditionalmorpho-syntactic POS tagging withsyntactic	information	by	introducingSupertags.
We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts.
In this paper, we discuss the performance of cross- lingual information extraction systems employing an automatic pattern acquisition module.
Our system employs a set of semantic similarity metrics using the noun portion of WordNet as a knowledge source.
We describe a system that models all levels of the spoken language system using stochastic language models and present experimental results.
This paper describes the Corinna system which integrates a theoretical approach to dialogue modeling with text generation techniques to conduct cooperative dialogues in natural language.
We propose an organization of the lexicon and its interaction with grammar and knowledge that makes extensive use of lexical functions from the Meaning-Text-Theory of Mel'euk.
the complexity of language domain and concept inventory).
The featural model provides statistical evidence for the claim that speakers use non-canonicals to communicate information about discourse structure.
This paper provides an account of the role that the interaction between nominal and temporal reference plays in resolving temporal reference.
We present an empirically grounded method for evaluating content selection in summarization.
We introduce CST (cross-document structure theory), a paradigm for multi-document analysis.
CST takes into account the rhetorical structure of clusters of related textual documents.
This paper describes MIMIC, an adaptive mixed initia-tive spoken dialogue system that provides movie show-time information.
First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumu-lative effect of information dynamically extracted from user utterances during the dialogue.
Event clustering on streaming news aims to group documents by events automatically.
A dynamicthreshold using time decay function and spanning window is proposed.
The experimental results show that both event words and co-reference chains are useful on event clustering.
In this paper we describe a new technique for parsing free text: a transformational grammar' is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.
CDG represents a sentence抯 grammatical structure as assignments of dependency relations to functional variables associated with each word in the sentence.
In this paper, we describe a statistical CDG (SCDG) parser that performs parsing incrementally and evaluate it on the Wall Street Journal Penn Treebank.
Factors contributing to the SCDG parser抯 performance are analyzed.
We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions.
The concern of this paper is the signalling of segments and relations in written texts.
We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases.
Speech repairs occur often in spontaneous spo-ken dialogues.
We present a framework to detect and correct speech repairs where all rel-evant levels of information, i.e., acoustics, lexis, syntax and semantics can be integrated.
Second a stochastic model.
Finally a lattice parser decides on accepting the repair.
PeriPhrase is a high-level computer language developed by A.L.P.
Systems to facilitate parsing and structural transfer.
The following simple rule forms a noun phrase (NP).!
Suppose that we declared a category NV for marking noun-verb homographs.
We present the multilingual sum-marization functionality for VERB-MOBIL, a speech translation system.
Observed counts vary more than simple models predict, prompting the use of overdispersed models like Gamma-Poisson or Beta-binomial mixtures as robust alternatives.
We propose using zero- inflated models for dealing with this, and evaluate competing models on a Naive Bayes text classification task.
We have proposed a method of ma-chine translation, which acquires trans-lation rules from translation examples using inductive learning, and have eval-uated the method.
In this paper we present Morphy, an integrated tool for German morphology, part-ofspeech tagging and context-sensitive lemmatization.
Syntactic ambiguities can be resolved with a standard statistical part-of-speech tagger.
The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verb- final clauses.
Successive models are evaluated on precision and recall of phrase markup.
This paper proposes a multi-dimensional framework for classifying text documents.
Using k-NN, na飗e Bayes and centroidbased classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications.
We also introduce 106 novel diathesis alternations, created as a side product of constructing the new classes.
We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance.
This paper describes a system for rapidly retargetable interactive translingual retrieval.
We call this the process of GRAMMATICAL KrourzAmoN2 forced by grammar sharing.
Then what we do is identify sharable packages of common string-types and information structures among independently motivated language-specific grammatical assertaions.
This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments.
Recent work in Question Answering has focused on web-based systems that extract answers using simple lexicosyntactic patterns.
Among these are parameters based on the variance of the word-frequency distribution (NOCC/EK), and on information theoretical (signal- noise S/N) premises.
We describe a parser used in the CoNLL 2006 Shared Task, 揗ultingual Dependency Parsing.
?The parser first identifies syntactic dependencies and then labels those dependencies using a maximum entropy classifier.
THALES is a software package for plane geometry constructions, supplied with a natural language interface.
A simple class of Context-free Queue Grammars is introduced and discussed.
We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features.
We find, amongst other things, a strong relationship between citation function and sentiment classification.
Experimental results and a comparison to other state—of—theart taggers are reported.Keywords: POS Tagging, Corpus—based modeling, Decision Trees, Ensembles of Classifiers.
The Sydney Language Independent Named En-tity Recogniser and Classifier (SLINERC) is a multi-stage system for the recognition and clas-sification of named entities.
This paper presents cohesion trees (CT) as a data structure for the perspective, hierarchical organization of text corpora.
CTs operate on alternative text representation models taking lexical organization, quantitative text characteristics, and text structure into account.
This paper describes a method to acquire hyponyms for given hypernyms from HTML documents on the WWW.
We present a system for analyzing conversational data.
We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead of context-free rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models.
This paper describes the initial results of an experiment in integrating knowledge-based text processing with real-world reasoning in a question answering system.
We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns.
SRI's approach is to integrate speech and linguistic knowledge into the HMM framework.
This paper describes performance improvements arising from detailed phonological modeling and from the incorporation of cross-word coarticulatory constraints.
We show that it is possible to learn the contexts for linguistic operations which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy.
We cast the problem of learning the contexts for the linguistic operations asclassification tasks, and applystraightforward machine learning techniques, such as decision tree learning.
The training data consist of linguistic features extracted from syntactic and semantic representations produced by a linguistic analysis system.
The target features are extracted from links to surface syntax trees.
Many corpus-based machine translation systems require parallel corpora.
In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus.
A decision tree is used for organizing contextual descriptions of phonological variation.
In this paper, we present and com-pare various single-word based align-ment models for statistical machinetranslation.
We discuss the fiveIBM alignment models, the Hidden-Markov alignment model, smooth-ing techniques and various modifica-tions.
We present different methodsto combine alignments.
This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG pars-ing system.
In this paper we describe results from using a maximum entropy (ME) classifier to identify metaphors.
This paper introduces new definitions of Chinese base phrases and presents a hybrid model to combine Memory-Based Learning method and disambiguation proposal based on lexical information and grammar rules populated from a large corpus for 9 types of Chinese base phrases chunking.
We select the final translation set using the mutual information score and the TF譏DF score.
We address the problem of structural disambiguation in syntactic parsing.
We define a 'three-word probability' for implementing LPR, and a 'length probability' for implementing RAP and ALPP.
Large-scale knowledge-based machine translation requires significant amounts of lexical knowledge in order to map syntactic structures to conceptual structures.
This paper presents a new approach based on Equivalent Pseudowords (EPs) to tackle Word Sense Disambiguation (WSD) in Chinese language.
The experiment verifies the value of EP for unsupervised WSD.
In this paper, we present a method for determining the animacy of English nouns using WordNet and machine learning techniques.
The comma plays an important role in long Chinese sentence segmentation.
This paper proposes a method for classifying commas in Chinese sentences by their context, then segments a long sentence according to the classification results.
We also carried out a pilot study on using this automatically generated bilingual word sense dictionary to choose the best translation candidates and show the first significant evidence that frame semantics are useful for translation disambiguation.
These results demonstrate the great potential for future application of bilingual frame semantics to machine translation tasks.
We present two methods for semiautomatic detection and correction of errors in textual databases.
This paper proposes a two-layered model of dialogue structure for task-oriented dialogues that processes contextual information and disambiguates speech acts.
The final goal is to improve translation quality in a speech-to-speech translation system.
LP grammar formalism separates constraints on immediate dominance front those on linear order.
First we map verb tokens in sentential contexts to a fixed set of seed verbs using WordNet :: Similarity and Moby抯 Thesaurus.
We then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modifier.
Most machine transliteration systems transliterate out of vocabulary (OOV) words through intermediate phonemic mapping.
It uses the linguistic knowledge of possible conjuncts and diphthongs in Bengali and their equivalents in English.
It uses n-gram mutual information, relative frequency count and parts of speech as the features for compound extraction.
The problem is modeled as a two-class classification problem based on the distributional characteristics of n-gram tokens in the compound and the non-compound clusters.
The SAUMER system uses specifications of natural language grammars.
which consist of rules and metarules.
like the correspondence between syntactic and semantic rules, with definite clause grammars (DCGs) (Pereira and Warren.
1980) to create an executable grammar specification.
relative clauses.
passivisation.
and questions.
This paper describes the characteristic features of dependency structures of Japanese spoken language by investigating a spoken dialogue corpus, and proposes a stochastic approach to dependency parsing.
The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences.
Included here are all closed-class morphemes and words--inflections, par-ticles, adpositons, conjunctions, demonstratives, etc.--as well as syntactic constructions, grammatical relations; categorial identities, word order, and intonation.
A scheme for syntax-directed translation that mirrors compositional model-theoretic semantics is discussed.
We present two methods to select the most significant single word trigger pairs.
This paper reports on the SYN-RA (SYNtax-based Reference Annotation) project, an on-going project of annotating German newspaper texts with referential relations.
The project has developed an inventory of anaphoric and coreference relations for German in the context of a unified, XML-based annotation scheme for combining morphological, syntactic, semantic, and anaphoric information.
We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.
We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints.
Finally, we proposed a way to construct a noun case frame dictionary by using examples of "X no Y ."
Our algorithm builds on previous research on corpus-based methods for acquiring extraction patterns and semantic lexicons.
In this paper we sketch an approach for Natural Language parsing.
These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure - a top down approach.Keywords: Example-based parsing, SSTC .
To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them.
We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other.
The basic QA system relies on complete sentence parsing, inferences, and semantic representation matching.
The paper deals with the application of NLP technology in e-learning.
We report our research on intelligent platforms for computer- mediated education.
We present a method for corpus-based induction of an LFG syntax-semantics interface for frame semantic processing in a computational LFG parsing architecture.
We show how to model frame semantic annotations in an LFG projection architecture, including special phenomena that involve non-isomorphic mappings between levels.
We evaluate the results by applying the inducedframe assignment rules to LFG parser output.'
We follow this line and explore the potential of deep syntactic analysis for role labelling, choosing Lexical Functional Grammar as underlying syntactic framework.
We aim at a computational interface for frame semantics processing that can be used to (semi-)automatically extend the size of current training corpora for learning stochastic models for role labelling, and ?ultimately ?as a basis for automatic frame assignment in NLP tasks, based on the acquired stochastic models.We discuss advantages of semantic role assignment on the basis of functional syntactic analyses as provided by LFG parsing, and present an LFG syntax-semantics interface for frame semantics, building on a first study in Frank and Erk (2004).
Finally, we apply the acquired frame assignment rules in a computational LFG parsing architecture.The paper is structured as follows.
In Section 3 we discuss advantages of deeper syntactic analysis for a principle-based syntax- semantics interface for semantic role labelling.
Section 4 describes the method we apply to derive frame assignment rules from corpus annotations: we port the frame annotations to a 損arallel?LFG corpus and induce general LFG frame assignment rules, by extracting syntactic descriptions for the frame constituting elements.
We use LFG抯 functional representations to distinguish local and non-local role assignments.
We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords.
We define a set of deterministic bottom-up left to right parsers which analyze a subset of Tree Adjoining Languages.
The LR parsing strategy for Context Free Grammars is extended to Tree Adjoining Grammars (TAGs).
We use a machine, called Bottom-up Embedded Push Down Automaton (BEPDA), that recognizes in a bottom-up fashion the set of Tree Adjoining Languages (and exactly this set).
Each parser consists of a finite state control that drives the moves of a Bottom-up Embedded Pushdown Automaton.
The parsers handle deterministically some context-sensitive Tree Adjoining Languages.In this paper, we informally describe the BEPDA then given a parsing table, we explain the LR parsing algorithm.
We then show how to construct an LR(0) parsing table (no lookahead).
Then, we explain informally the construction of SLR(1) parsing tables for BEPDA.
In this paper Brills rule-based PoS tagger is tested and adapted for Hungarian.
Reordering is currently one of the most important problems in statistical machine translation systems.
This paper presents a novel strategy for dealing with it: statistical machine reordering (SMR).
This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation.
In this interactive presentation, a Chinese named entity and relation identification system is demonstrated.
The domain- specific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication.
A generative statistical model of dependency syntax is proposed based on Tesniere's classical theory.
We investigate lexical paraphrasing in the context of two distinct applications: document retrieval and node identification.
Node identification ?performed in the context of a Bayesian argumentation system ?matches users?Natural Language sentences to nodes in a Bayesian network.
Lexical paraphrases are generated using syntactic, semantic and corpus-based information.
Our evaluation shows that lexical paraphrasing improves retrieval performance for both applications.
This work applies boosted wrapper induction (BWI), a machine learning algorithm for information extraction from semi-structured documents, to the problem of named entity recognition.
The default feature set of BWI is augmented with features based on distributional term clusters induced from a large unlabeled text corpus.
The treebank is built from dictionary definition sentences, and uses an HPSG based Japanese grammar to encode the syntactic and semantic information.
We show how this treebank can be used to extract thesaurus information from definition sentences in a language-neutral way using minimal recursion semantics.
grammatical function, phrase type, and other syntactic traits).
This paper describes the design and application of time-enhanced, finite state models of discourse cues to the automated segmentation of broadcast news.
We describe our analysis of a broadcast news corpus, the design of a discourse cue based story segmentor that builds upon information extraction techniques, and finally its computational implementation and evaluation in the Broadcast News Navigator (BNN) to support video news browsing, retrieval, and summarization.
In previous work, supertag disambiguation has been presented as a robust partial parsing technique.
CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements).
This system identifies features of sentences based on semantic similarity measures and discourse structure.
Intra-sentential quality is evaluated with rule-based heuristics.
In this paper, we present a statistical approach for dialogue act processing in the dialogue component of the speech-to-speech translation system VERBMOBIL.
Statistics in dialogue processing is used to predict follow-up dialogue acts.
We study parsing of tree adjoining grammars with particular emphasis on the use of shared forests to represent all the parse trees deriving a well-formed string.
The schemes using hg and cfg to represent parses can be seen to underly most of the existing tag parsing algorithms.
We discuss the advantages of lexical-ized tree-adjoining grammar as an al-ternative to lexicalized PCFG for sta-tistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its pars-ing performance.
This paper presents an efficient algorithm for the incremental construction of a minimal acyclic sequential transducer (ST) for a dictionary consisting of a list of input and output strings.
Our method of capturing ungrammaticalities involves using malrules (also called 'error productions/.
It is popular in WSD to use contextual information in training sense tagged data.
This paper reports on word sense disambiguation of English words using static and dynamic sense vectors.
Finally, a word sense of a target word is determined using static and dynamic sense vectors.
In this paper, we propose an Inductive Logic Programming learning method which aims at automatically extracting special Noun-Verb (N-V) pairs from a corpus in order to build up semantic lexicons based on Pustejovsky's Gen-erative Lexicon (GL) principles (Pustejovsky, 1995).
a tagset containing information about wordclasses.
A semantic lexicon based on conceptual graph structures is used to guide text understanding.
We examine their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization.
This paper presents some of the first data visualizations and analysis of distributions for a lexicalized statistical parsing model, in order to better understand their nature.
We explore several distance-based combining methods, as well as a number of distance metrics involving both word and character ngrams.
A maximum entropy classifier is used in our semantic role labeling system, which takes syntactic constituents as the labeling units.
The maximum entropy classifier is trained to identify and classify the predicates?semantic arguments together.
The word sequences are predicted from the attribute using word n-gram model.
The spell of Unknow word is predicted using character n-gram model.
We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicate- argument output by solving an optimization problem.
We present a new approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models.
In a test of a VMM based tagger on the Brown corpus, 95.81% of tokens are correctly classified.
We explore the application of memory- based learning to morphological analysis and part-of-speech tagging of written Arabic, based on data from the Arabic Treebank.
We report on the performance of the morphological analyzer and part-of-speech tagger.
As part of the CUBRICON project, we are developing NL processing technology that incorporates deictic and graphic gestures with simultaneous coordinated NL for both user inputs and system-generated outputs.
Lastly we show that patterns of information exchanges in speaker alternation and initiative-taking can be used to characterise three-party dialogues.
To acquire these noun-verb pairs, we use ASARES, a machine learning technique that automatically infers extraction patterns from examples and counter-examples of realization noun-verb pairs.
We present a neural-network based self-organizing approach that enables vi-sualization of the information retrieval while at the same time improving its precision.
ALLiS 2.0 is a theory refinement system using hierarchical data.
This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech.
In this paper, we describe the pronominal anaphora resolution module of Lucy, a portable English understanding system.
We also propose keyword pair model for the synchronisation between text and speech.
This paper discusses the translation of temporal expressions, in the framework of the machine translation system Rosetta.
In section 3will sketch isomorphic grammars for temporal expressions and illustrate them by some examples.
This paper looks at representing paraphrases using the formalism of Synchronous TAGs; it looks particularly at comparisons with machine translation and the modifications it is necessary to make to Synchronous TAGs for paraphrasing.
We have analyzed definitions from Webster's Seventh New Collegiate Dictionary using Sager's Linguistic String Parser and again using basic UNIX text processing utilities such as grep and awk.
This paper describes a fully implemented system for fusing related news stories into a single comprehensive description of an event.
We describe our experience in developing a discourse-annotated corpus for community-wide use.
We show that this methodology can be used for automatic domain template creation.
We present a supervised machine learning algorithm for metonymy resolution, which exploits the similarity between examples of conventional metonymy.
We show that syntactic head-modifier relations are a high precision feature for metonymy recognition but suffer from data sparseness.
We also compare automatic and manual methods for syntactic feature extraction.
HMM-based models are developed for the alignment of words and phrases in bitext.
There has recently been a revival of interest in Categorial Grammars (CG) among computational linguists.
This algorithm offers a uniform treatment for "spurious" syntactic ambiguities and the "genuine" structural ambiguities which any processor must cope with, by exploiting the associativity of functional composition and the procedural neutrality of the combinatory rules of grammar in a bottom-up, left-to-right parser which delivers all semantically distinct analyses via a novel unification-based extension of chart-parsing.L Combinatory Categorial Grammars"Pure" categorial grammar (CC) is a grammatical notation, equivalent in power to context-free grammars, which puts all syntactic information in the lexicon, via the specification of all grammatical entities as either functions or arguments.
We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases.
The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations.
We also present a model and algorithm for machine translation involving optimal "tiling" of a dependency tree with entries of a costed bilingual lexicon.
We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation.
This paper discusses complex structural changes during transfer within a non-destructive transfer framework.
a small hand-annotated corpus of news messages.
We are carrying out investigations into the application of biophysical and computational models to speech processing.
Described here are studies of the robustness of a speech representation using a biophysical model of the cochlea; experimental results on the representation of speech and complex sounds in the mammalian auditory cortex; and descriptions of computational sequential processing networks capable of recognizing sequences of phonemes.
This paper investigates the problems facing modelling agents?beliefs in Discourse Representation Theory (DRT) and presents a viable solution in the form of a dialogue-based DRT representation of beliefs.
Integrating modelling dialogue interaction into DRT allows modelling agents?beliefs, intentions and mutual beliefs.
We describe the development environment available to linguistic developers in our lab in writing large-scale grammars for multiple languages.
Our discourse-level annotation scheme covers properties of discourse referents (e.g., semantic sort, delimitation, quantification, familiarity status) and anaphoric links (coreference and bridging).
In this paper, we consider exploiting both prosodic and text-based features for topic segmentation of Mandarin Chinese.
We contrast these results with classification using text- based features, exploiting both text similarity and n-gram cues, to achieve accuracies between 77- 95.6%, if silence features are used.
Finally we integrate prosody, text, and silence features using a voting strategy to combine decision tree classifiers for each feature subset individually and all subsets jointly.
We propose a statistical dialogue analysis model to determine discourse structures as well as speech acts using maximum entropy model.
We propose the idea of tagging discourse segment boundaries to represent the structural information of discourse.
Using this representation we can effectively combine speech act analysis and discourse structure analysis in one framework.
We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.
Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text.
For example, the "lexical association" strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth.
We developed a fully automated Information Retrieval System which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval.
We investigate the use of a statistical sentence gener-ation technique that recombines words probabilistically in order to create new sentences.
Given a set of event-related sentences, we use an extended version of the Viterbi algorithm which employs dependency relation and bigram proba-bilities to find the most probable sum-mary sentence.
This pa-per presents a reinforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying re-inforcement learning to a working dialogue system with human users.
We first dis-cuss a problem of developing a knowl-edge base by using natural language doc-uments: wrong information in natural language documents.
This paper reports sentence alignment results from 2 corpora, in a 2- pass dictionary based alignment system.
We also propose metrics for measuring directed lexical influence and compare performances.
Keywords: contextual post-processing, defining context, lexical influence, directionality of context
This paper proposes to apply machine learning techniques to the task of combining outputs of multiple LVCSR models.
We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal.
Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure.
The design and implementation of an XML-based corpus environment for multilevel anno-tated multimodal (language) data is described.
subordinators and verbs).
This paper presents a semi-automatic technique for developing broad-coverage finite-state morphological analyzers for any language.
This elicitbuild-test technique compiles lexical and inflectional information elicited from a human into a finite state transducer lexicon and combines this with a sequence of morphographemic rewrite rules that is induced using transformation-based learning from the elicited examples.
Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches.
We also present an approximate string searching method to alleviate the latter problem.
This paper presents a methodology for using the argument structure of sentences, as encoded by the PropBank project, to develop clusters of verbs with similar meaning and usage.
We develop admissible A* search heuristics for synchronous parsing with Inversion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding.
We also combine the dynamic programming hook trick with A* search for decoding.
This paper presents a new language independent model for analysis and generation of word forms based on Finite State Transducers (FSTs).
This processor uses syntactic and semantic informations about words in order to construct a semantic net representing the meaning of the sentences.
This work describes our next step to improve the usability of speechrecognizers---the use of vocabulary-independent (VI) models.
He combines a two-level approach to morpho-graphemics with a unification grammar approach (a modified PATR rule interpreter) to morpho-syntax.
The approach uses topic identification algorithm named FIFA to text classification.
This paper proposes a new approach for Multi-word Expression (MWE)extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis.
Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics.
We perform this developed LCS technique combined with linguistic criteria in MWE extraction.
We propose a statistical method that finds the maximum-probability seg-mentation of a given text.
We describe and experimentally evaluate an efficient method for automatically de-termining small clause boundaries in spon-taneous speech.
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection.
So the process consists of finding word stems and than correcting word endings .
This paper describes a technique for parsing dependency grammars using a bottom-up chart parser originally designed for phrase-structure grammars, using typed feature structures as the only data structure.
The first method employs HPSG'S BACK-GROUND feature and a constraint-satisfaction component pipe-lined after the parser.
The second method uses subsorts of referential in-dices, and blocks readings that violate selec-tional restrictions during parsing.
In this paper we present a system for automatically producing multimedia pages of information that draws both from results in data-driven aggregation in information visualization and from results in communicative-goal oriented natural language generation.
We introduce an original data structure and efficient algorithms that learn some families of transformations that are relevant for part-of-speech tagging and phonological rule systems.
We compare five generative graphical models and a neural network, using lexical, syntactic, and semantic features, finding that the latter help achieve high classification accuracy.
Statistical language modeling requires a large corpus for the application domain.
the task of WSD of Word- Net glosses and the task of WSD of English lexical sample.
In this paper, an augmented chart data structure with efficient word lattice parsing scheme in speech recognition applications is proposed.
This paper presents SPQR (Selectional Pat-tern Queries and Responses), a module of the PUNDIT text-processing system designed to facili-tate the acquisition of domain-specific semantic information, and to improve the accuracy and efficiency of the parser.
This paper presents the English valency lexicon EngValLex, built within the Functional Generative Description framework.
In this paper a novel solution to automatic and unsupervised word sense induction (WSI) is introduced.
Like most existing approaches it utilizes clustering of word co-occurrences.
We present an unsupervised approach to Word Sense Disambiguation (WSD).
We automatically acquire English sense examples using an English-Chinese bilingual dictionary, Chinese monolingual corpora and Chinese-English machine translation software.
We then train machine learning classifiers on these sense examples and test them on two gold standard English WSD datasets, one for binary and the other for fine-grained sense identification.
We extend the centering model for the resolution of intra-sentential anaphora and specify how to handle complex sentences.
In SPHINX-II, we incorporated additional dynamic and speaker-normalized features, replaced discrete models with sex-dependent semi-continuous hidden Markov models, augmented within-word triphones with between-word triphones, and extended generalized triphone models to shared- distribution models.
The configuration of SPHINX-II being used for this task includes sex-dependent, semi-continuous, shared- distribution hidden Markov models and left context dependent between-word triphones.
The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer.
A previous paper described a near-optimal admissible Viterbi A* search algorithm for use with noncross-word acoustic models and no-grammar language models [16].
This paper extends this algorithm to include unigram language models and describes a modified version of the algorithm which includes the full (forward) decoder, cross-word acoustic models and longer-span language models.
This paper introduces a system for categorizing un-known words.
This paper presents a method for combining different expressions of the re-sampling approach in a mixture of experts framework.
This paper studies the enrichment of Spanish WordNet with synset glosses automatically obtained from the English Word- Net glosses using a phrase-based Statistical Machine Translation system.
This paper presents a framework for the definition of monotonic repair rules on chart items and Lexicalized Tree Grammars.
These local rules cover ellipsis and common extra-grammatical phenomena such as self-repairs.
We present the psycholinguistically motivated task of predicting human plausibility judgements for verb-role-argument triples and introduce a probabilistic model that solves it.
We report work on effectively incorporating lin-guistic knowledge into grammar induction.
We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn `missing' grammar rules from an incomplete grammar.
The task is to generate sentences with hotel-information from a structured database.
The system uses core technologies such as speaker segmentation, automatic speech recognition, transcription alignment, keyword extraction and speech indexing and retrieval to make spoken communications easy to navigate.
This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.
by the use of graph-structured stacks and packed parse forests (Tomita 1985)).
iii contrast, statistical techniques using lexeme co-occurrence provide a relatively simple mechanism which can imitate semantic filtering in many cases.
Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson抯 product moment correlation coefficient or Spearman抯 rank order correlation coefficient between human scores and automatic scores.
General characteristics of a pragmatic metric for the production evaluation of speech-to-speech translations are dis-cussed.
This work attempts to provide a computational solution, called Word Filtering, to handle those three points prior to parsing.The proposed model of Thai morphological analysis consists of three steps: sentence segmenting, spell checking and word filtering.
We are studying how to extract hierarchical relation on verbs from definition sentences in a Japanese dictionary.
In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text.
In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers.
This paper proposes a knowledge representation model and a logic proving setting with axioms on demand successfully used for recognizing textual entailments.
We describe an approach to Machine Transla-tion of transcribed speech, as found in closed captions.
In particular, we describe components for proper name recognition and input segmentation.
In this paper we present a corpus-based study ofNSUs.
This paper introduces a context-sensitive electronic dictionary that provides translations for any piece of text displayed on a computer screen, without requiring user interaction.
A language independent model for recognition and production of word forms is presented.
In this paper we describe the web and mobile-phone interfaces to our multi- language factoid question answering (QA) system together with a prototype speech interface to our English-language QA system.
Using a statistical, data-driven approach to factoid question answering has allowed us to develop QA systems in five languages in a matter of months.
We have developed a discourse level tagging tool for spoken dialogue corpus using machine learning methods.
In dialogue act tagging, we have implemented a transformation-based learning procedure and resulted in 70% accuracy in open test.
A language-independent method of finite- state surface syntactic parsing and word-disambiguation is discussed.
The role of the morphological analysis in accurate name recognition is discussed.
We also provide evaluations of both morphological analysis and name recognition.
In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.
This presentation describes an example- based English-Japanese machine trans- lation system in which an abstract linguistic representation layer is used to extract and store bilingual translation knowledge, transfer patterns between languages, and generate output strings.
The project deals with the evaluation of term and semantic relation extraction from corpora in French.
This expression covers respectively, term extractors, classifiers and semantic relation extractors.
Functional Unification Grammars (FUGs) are popular for natural language applications because the formalism uses very few primitives and is uniform and expressive.
We have implemented an extension of traditional functional unification.
It is based on the notions of typed features and typed constituents.
We show the advantages of this extension in the context of a grammar used for text generation.
Both anaphora resolution and prepositional phrase (PP) attachment are the most frequent ambiguities in natural language processing.
This paper describes the CTB Coreference An-notation Guidelines for annotating pronominal anaphoric expressions in the Penn Chinese Tree-bank.
There are mainly two kinds of statistic- based measures for word extraction: the internal measure and the contextual measure.
This paper suggests refinements for the Distributional Similarity Hypothesis.
Our proposed hypotheses relate the distributional behavior of pairs of words to lexical entailment ?a tighter notion of semantic similarity that is required by many NLP applications.
We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing.
To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel抯 implementation of Collins?lexicalized PCFG model, and on Chiang抯 CFG-based decoder for hierarchical phrase-based translation.
This paper describes a simple pattern- matching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.
This paper presents a technique for discover-ing translationally equivalent texts.
This paper outlines the experimental development of the SWESIL system: a structured lexicon-based word expert system designed to play a pivotal role in the process of Distributed  Language Translation (DLT) which is being developed in the Netherlands.
We describe the design of an MT system that em-ploys transfer rules induced from parsed bitexts and present evaluation results.
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
We propose in this paper a maximum entropy approach for restoring diacritics in a document.
The approach can easily integrate and make effective use of diverse types of information; the model we propose integrates a wide array of lexical, segment- based and part-of-speech tag features.
We develop an efficient way of ex-tracting the compound noun index-ing rules automatically and perform extensive experiments to evaluate our indexing rules.
We report on a study examining the generation of noun phrases within a spoken dialog agent for a navigation domain.
We present MAGEAD, a morphological analyzer and generator for the Arabic language family.
We present a detailed evaluation of MAGEAD.
Four kinds of classifiers were used in our experiments: Na飗e Bayes, Rocchio, k-NN, and SVM.
We exploit the event/result reading distinction, combined with other aspectual information.
We describe a parser for robust and flexible interpretation of user utterances in a multi-modal system for web search in newspaper databases.
Our parser integrates shallow parsing techniques with knowledge-based text retrieval to allow for robust processing and coordination of input modes.
These co-occurrence statistics concern typical noun phrases as they appear in newspaper texts.
Anaphoric reference is an important linguistic phenomenon to understand the discourse structure and content.
The regularity acquired from training was then tested and compared with other approaches in both choosing and resolving anaphora.Keywords: anaphoric reference, semantic roles(case), natural language acquisition, case- based learning.
The reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem.
In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism.
The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
a noun in a noun phrase).
This paper describes an automatic method for extracting systematic polysemy from a hierar-chically organized semantic lexicon (WordNet).
We compare the systematic relations extracted by our auto-matic method to manually extracted WordNet cousins.
We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference.
We evaluate three state- of-the-art re-ranking algorithms (MaxEntRank, SVMRank, and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and cross- document inference.
These problems include Topic Classification, Information Retrieval, Extracting Named Entities, and Extracting Relations.128
This paper reports on both context-independent and context-dependent strategies for utter-ance verification that show that the use of dialog context is crucial for intelligent se-lection of which utterances to verify.
This paper proposes a learning method of translation rules from parallel corpora.
This method applies the maximum entropy principle to a probabilistic model of translation rules.
First, we define feature functions which express statistical properties of this model.
mantics of Combinatory Categorial Grammar, including its handling of coordination constructs.
We demonstrate a problem with the stan-dard technique for learning probabilistic decision lists.
We describe an incremental parser which annotates grammatical functions in German on top of a shallow annotation structure consisting of chunks, topological fields and clauses.
We apply our paraphrasing method in the context of machine translation evaluation.
We show how techniques known from generalized LR parsing can be applied to left- corner parsing.
A strong advantage of our presentation is that it makes explicit the role of left-corner parsing in these algorithms.Keywords: Generalized LR parsing, left- corner parsing, chart parsing, hidden left recursion.
As a case study, we show how a prob-abilistic approach can be used to model anaphora resolution in dialogue1.
This paper proposes the automatic generation of Fill-in-the-Blank Questions (FBQs) together with testing based on Item Response Theory (IRT) to measure English proficiency.
This paper describes automatic treatment of multi-word expressions in a morphologically complex flective language ?Estonian.
It focuses on a special type of multi-word expressions ?the verbal multi-word expressions that can function as predicates.
This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs.
This paper proposes a method for packing feature structures, which automatically collapses equivalent parts of lexical/phrasal feature structures of HPSG into a single packed feature structure.
Preliminary experiments show that this method can significantly improve a unification speed in parsing.
We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords.
Our work views sentence compression as an optimisation problem.
We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints.
This is based on our experiences with 2 TPI systems, one for generating asthma- information booklets and one for generating smoking-cessation letters.
GermaNet and FrameNet.
We present an efficient multi-level chart parser that was designed for syntactic analysis of closed captions (subtitles) in a real-time Machine Translation (MT) system.
We report progress on adding affect- detection to a program for virtual dramatic improvisation, monitored by a human director.
The project contributes to the application of sentiment and subjectivity analysis to the creation of emotionally believable synthetic agents for interactive narrative environments.
This paper presents an approach of extracting Pinyin names from English text, suggesting translations to these Pinyin using a database of names and their characters with usage probabilities, followed with IR techniques with a corpus as a disambiguation tool to resolve the translation candidates.
In this paper, word sense disambiguation (WSD) ac-curacy achievable by a probabilistic classifier, using very minimal training sets, is investigated.
Our system, named Bayesian Hierarchical Disambiguator (BHD), uses the Internet, arguably the largest corpus in existence, to address the sparse data problem, and uses WordNet's hierarchy for se-mantic contextual features.
We present results on the recognition accuracy of a continuous speech, speaker independent HMM recognition system that incorporates a novel noise reduction algorithm.
In this paper, we describe a machine translation system called PalmTree which uses the "pattern- based" approach as a fundamental framework.
The pure pattern-based translation framework has several issues.
This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages (TALs).
In this paper, a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing.
We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far.
The formalism, which we refer to as Partially Linear PATR (PLPATR) manipulates feature structures rather than stacks.
This paper describes a lexicon organized around sys-tematic polysemy: a set of word senses that are related in systematic and predictable ways.
We systematically compare two inductive learning approaches to tagging: MX-POST (based on maximum entropy modeling) and MBT (based on memory-based learning).
In this paper, we propose a method for resolving the syntactic ambiguities of translation examples of bilingual corpora and a method for acquiring lexical knowledge, such as case frames of verbs and attribute sets of nouns.
We discuss maximum a posteriori estimation of continuous density hidden Markov models (CDHMM).
The classical MI,Ereestimation algorithms, namely the forward-backward algorithm and the segmental k-means algorithm, are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities.
We also propose a new probabilistic sentence reduction method based on support vector machine learning.
A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper.
In recent years, statistical approaches on ATR (Automatic Term Recognition) have achieved good results.
We present an unsupervised system that exploits linguistic knowledge resources, namely English and German lexical databases and the World Wide Web, to identify English inclusions in German text.
We then apply our method to two complementary tasks: information ordering and extractive summarization.
Our approach to the problem uses textual similarity and context from other clauses.
We explore the possibility that syntactic information can be used to improve the performance of an HMM-based system for restoring punctuation (specifically, commas) in text.
We explore the use of morphological analysis as preprocessing for protein name tagging.
Our method finds protein names by chunking based on a morpheme, the smallest unit determined by the morphological analysis.
In addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verbs and prepositions, aspectual markers, copular verbs, null arguments, and slot fillers.
Retrieval effectiveness is examined utilizing query weight ratio of these three categories of phrasal terms.
Answer Extraction is performed by recognizing event inter-relationships and by inferring over multiple sentences and texts, using background knowledge.
This paper describes a reestimation method for stochastic language models such as the N-gram model and the Hidden Markov Model(HMM) from ambiguous observations.
We investigated the applicability of probabilistic context-free grammars to syllabification and grapheme-to-phoneme conversion.
The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries.
This paper discusses the supervised learning of morphology using stochastic transducers, trained using the Expectation- Maximization (EM) algorithm.
This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations.
The algorithm creates verb argument structures using VerbNet syntactic patterns.
We present two measures for compar-ing corpora based on information the-ory statistics such as gain ratio as well as simple term-class frequency counts.
The classic one is the construction of thesaurus.
Lexical features are key to many approaches to sentiment analysis and opinion detection.
A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexicosyntactic patterns.
A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation).
Some practical uses for semantic concordances are proposed.
Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages.
This paper addresses a dialogue strategy to clarify and constrain the queries for speech-driven document retrieval systems.
This paper addresses the documentation oflarge-scale grammars.'
Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone, e.g.
LFG and PATR-II.
This paper presents a parser for French developed on an unification based categorial grammar (FG) which avoids these problems.
This parser is a bottom-up chart parser augmented with a heuristic eliminating spurious parses.
Automatic extraction of multiword expressions (MWE) presents a tough challenge for the NLP community and corpus linguistics.
We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.
Specifically, we report on combining the BU system based on stochastic segment models and the BBN system based on hidden Markov models.
Previous research has shown that the plausibility of an adjective-noun com-bination is correlated with its corpus co-occurrence frequency.
In this paper, we estimate the co-occurrence frequen-cies of adjective-noun pairs that fail to occur in a 100 million word corpus using smoothing techniques and com-pare them to human plausibility rat-ings.
Both class-based smoothing and distance-weighted averaging yield fre-quency estimates that are significant predictors of rated plausibility, which provides independent evidence for the validity of these smoothing techniques.
Word segmentation in MSR-NLP is an integral part of a sentence analyzer which includes basic segmentation, derivational morphology, named entity recognition, new word identification, word lattice pruning and parsing.
In this paper, we show that time- synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.
We propose a Question-answering (QA) system in Korean that uses a predictive answer indexer.
The predictive answer indexer, first, extracts all answer candidates in a document in indexing time.
One way is to load the system with lexical semantics for nouns or adjectives.
The paper describes a system which uses packed parser output directly to build semantic representations.
This paper is a presentation of a doctoral research in progress focused on a new genre: online encyclopaedias.
the language used in official encyclopaedic articles.
This paper describes the creation of a script in the framework of ontological semantics as the formal representation of the complex event BANKRUPTCY.
We report an empirical study on the role of syntactic features in building a semi- supervised named entity (NE) tagger.
Our study shows that constituency and dependency parsing constraints are both suitable features to extract NEs and train the classifier.
This paper argues that stylistically and pragmatically high-quality spoken language translation requires the transfer of pragmatic information at an abstract level of "utterance strategies".
Previous approach to this problem involves measuring context similarity only based on co-occurring words.
This paper presents a new algorithm using information extraction support in addition to co-occurring words.
Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features.
Statistical annealing is applied to derive the final entity coreference chains by globally fitting the pairwise context similarities.
This paper shows that quantifier scope phenomena can be precisely characterized by a semantic representation constrained by surface constituency, if the distinction between referential and quantificational NPs is properly observed.
This paper describes a statistical methodology for automatically retrieving collocations from POS tagged Korean text using interrupted bigrams.
We devised four statistics, 'frequency', 'randomness', 'condensation', and 'correlation' to account for the more flexible word order properties of Korean collocations.
We extracted meaningful bigrams using an evaluation function and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers.
We view a modeling problem for n-gram collocations as that for clustering of cohesive words.
This paper reports results on automatically training a Problematic Dialogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain.
This paper reports on experiments in classifying the semantic role annotations assigned to prepositional phrases in both the PENN TREEBANK and FRAMENET.
In addition to using traditional word collocations, the experiments incorporate class-based collocations in the form of WordNet hypernyms.
directly stored relations.
A semantic net needs proper representation of lexical gaps.
This article discusses the detection of discourse markers (DM) in dialog transcriptions, by human annotators and by automated means.
Then, several types of features are defined for automatic disambiguation of like: collocations, part-of-speech tags and duration-based features.
This paper addresses the issue of "algorithm vs. representation" for case-based learning of linguistic knowledge.
Next, we present a technique for automating feature set selection for case-based learning of linguistic knowledge.
We apply the linguistic bias approach to feature set selection to the problem of relative pronoun disambiguation and show that the case- based learning algorithm improves as relevant biases are incorporated into the underlying instance representation.
This paper proposes a methodology for the customisation of natural language interfaces to information retrieval applications.
This article presents a method for aligning words between translations, that imposes a compositionality constraint on alignments produced with statistical translation models.
This paper presents a multi-modal feature- based system for extracting salient keywords from transcripts of instructional videos.
Specifically, we propose to extract domain-specific keywords for videos by integrating various cues from linguistic and statistical knowledge, as well as derived sound classes and characteristic visual content types.
We present the approach to generation used in our implemented system for generating and interpreting indirect answers to Yes-No questions in English.
Generation of a discourse plan is performed in two phases: content planning and plan pruning.
In this paper, we propose a statistical approach for clustering of articles us-ing on-line dictionary definitions.
Columbia抯 Newsblaster tracking and summarization system is a robust system that clusters news into events, categorizes events into broad topics and summarizes multiple articles on each event.
In this paper we describe an approach to coherent sentence ordering for summarizing newspaper articles.
Combining the refinement algorithm with topical segmentation and chronological ordering, we address our experiment to test the effectiveness of the proposed method.
The results reveal that the proposed method improves chronological sentence ordering.
We conduct a set of experiments to investigate how noun phrase identification and feature selection can contribute to coreference resolution performance.
We believe that this is the first attempt at an unsupervised approach to Chinese noun phrase coreference resol ution.
We present a loss-based decoding framework for coreference resolution and a greedy algorithm for approximate coreference decoding, in conjunction with Perceptron and logistic regression learning algorithms.
We develop the notion of a word order domain structure, which is linked but structurally dissimilar to the syntactic dependency tree.
A purely functional implementation of LR-parsers is given, together with a simple correctness proof.
It is presented as a generalization of the recursive descent parser.
We propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver, and using Singular Value Decomposition (SVD) to identify the main terms.
This paper reports a pilot study, in which Constraint Grammar inspired rules were learnt using the Progol machine-learning system.
This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system.
Each rule consists of a rule number, a phrase structure rule, and a semantic (logical translation) rule.
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).
The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted.
The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger.
Under the probabilistic WFST framework, the use of N-best hypotheses from the speech recognizer instead of the 1- best can further improve performance requiring only a small additional processing time.
We describe our ongoing work on "Polibox", a web-based text generator producing adap-tive hypertext from a product database, currently one of computational linguistics textbooks.
In this work we propose a translation model for monolingual sentence retrieval.
These kinds of anaphora are pronominal references, surface- count anaphora and one-anaphora.
We only use the following kinds of information: lexical (the lemma of each word), morphologic (person, number, gender) and syntactic.
A recognized effective approach to word segmentation is Longest Matching, a method based on dictionary.
We describe an approach to connected word recognition that allows the use of segmental information through an explicit decomposition of the recognition criterion into classification and segmentation scoring.
This paper describes improved HMM-based word level alignment models for statistical machine translation.
We present a method for using part of speech tag information to improve alignment accu-racy, and an approach to modeling fertility and cor-respondence to the empty word in an HMM align-ment model.
We address the problem of acquiring entire phrases, specifically figurative phrases, through augmenting a phrasal lexicon.
We have designed and implemented a program called RINA which uses demons to implement functional-grammar principles.
In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.
In this project note, we present the main features of lexicalisation strategies deployed by humans in question- answering (QA) tasks.
When a machine learning-based named entity recognition system is employed in a new domain, its performance usually degrades.
This paper examines the benefits of system combination for unsupervised WSD.
We investigate several voting- and arbiter- based combination strategies over a diverse pool of unsupervised WSD systems.
In this paper, we compare the rela-tive effects of segment order, segmen-tation and segment contiguity on the retrieval performance of a translation memory system.
We take a selec-tion of both bag-of-words and segment order-sensitive string comparison meth-ods, and run each over both character-and word-segmented data, in combina-tion with a range of local segment con-tiguity models (in the form of N-grams).
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006).
We describe sources of cross- lingual error and ways to ameliorate them.
We present an annotation scheme for student emotions in tutoring dialogues.
We discuss issues concerning the implementation of emotion prediction and adaptation in the computer tutoring dialogue system we are developing.
We use two well- known techniques, partitioning clustering, k- means and a loss function to create category hierarchy.
k-means is to cluster the given categories in a hierarchy.
This paper presents empirical results in cross-lingual information retrieval using English queries to access Chinese documents (TREC-5 and TREC-6) and Spanish documents (TREC-4).
We explore how a combined probability model of term translation and retrieval can reduce the effect of translation ambiguity.
This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources.
We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities.
This paper describes a multi-word expression processor for preprocessing Turkish text for various language engineering applications.
We then summarize the computational setting in which we employ a series of components for tokenization, morphological analysis, and multi-word expression extraction.
In this paper, parsing-as-deduction and constraint pro-gramming are brought together to outline a procedure for the specification of constraint-based chart parsers.
This paper describes a novel undertaking: comparing the relationship between grammatical ambiguity (syncretism) in nouns, as represented in a default inheritance hierarchy, with textual frequency distributions.
Typically, statistical alignment models are based on single-word dependencies.
This approach extends the chunk tags for every problem by a tag-extension function.
In this paper we will present our ongoing work on a plan-based discourse processor developed in the context of the Enthusiast Spanish to English translation system as part of the JANUS multi-lingual speech-tospeech translation system.
Some statistical learning systems are evaluated using measures of distributional similarity.
Here, we investigate the sensi-tivity of entropy-based similarity measures to noise from uninformative smoothing.
The ESPRIT project POLYGLOT aims at developing multi-lingual Speech-to-Text and Text-to-Speech conversion and to integrate this technology in a number of commercially viable prototype applications.
Speech-to-Text conversion is mainly concerned with very large vocabulary isolated word recognition.
The goal of this paper is to integrate the Conceptual Mapping Model with an ontology-based knowledge representation (i.e.
This paper will examine 2000 random examples of 慹conomy?
Finally, the facts are encoded as predicate calculus axioms.
This paper considers several important issues for monolingual and multilingual link detection.
This paper proposes a methodology for building application-specific lexica by using WordNet.
In the past decade, several researchers have started reinvestigating the use of sub-phonetic models for lexical representations within automatic speech recognition systems.
We propose a unified framework in which to treat semantic underspecification and parallelism phenomena in discourse.
The framework employs a constraint language that can express equality and subtree relations between finite trees.
The constraints are solved by context unification.
In this paper we revisit Puste-jovsky's proposal to treat ontologi-cally complex word meaning by so-called dotted pairs.
The Clarity project is devoted to automatic detection and classification of discourse structures in casual, non-task-oriented conversation using shallow, corpus-based methods of analysis.
For the Clarity project, we have tagged speech acts and dialogue games in the Call Home Spanish corpus.
We have done preliminary cross-level experiments on the relationship of word and speech act n-grams to dialogue games.
Knowledge structures called Concept Clustering Knowledge Graphs (CCKGs) are introduced along with a process for their construction from a machine read-able dictionary.
Often in these texts complex linguistic constructions like conjunctions, negations, ellipsis and anaphorae are involved.
The paper shows that incorporating several latent features into the tense classifier boosts the tense classifier抯 performance, and a tense classifier using only the latent features outperforms one using only the surface features.
This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms.
We propose to address these ambiguities with predicate argument structures and semantic co-occurrence similarity information, and present encouraging results.
This paper presents a method that as-sists in maintaining a rule-based named-entity recognition and classifi-cation system.
The findings suggest some agendas for computational linguistics.
In this paper, we present and compare various align-ment models for statistical machine translation.
We also compare the im-pact of different alignment models on the translation quality of a statistical machine translation system.
This paper presents a dependency language model (DLM) that captures linguistic constraints via a dependency structure, i.e., a set of probabilistic dependencies that express the relations between headwords of each phrase in a sentence by an acyclic, planar, undirected graph.
First, we incorporate the dependency structure into an n-gram language model to capture long distance word dependency.
Finally, we evaluate the proposed models on a realistic application (Japanese Kana-Kanji conversion).
This paper explores techniques to take advantage of the fundamental difference in structure between hidden Markov models (HMM) and hierarchical hidden Markov models (HHMM).
We present a new approach to HPSG processing: compiling HPSG grammars expressed as type constraints into definite clause programs.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We introduce a character-based chunking for unknown word identification in Japanese text.
The method is built upon SVM-based chunking, by use of character n-gram and surrounding context of n-best word segmentation candidates from statistical morphological analysis as features.
The approach presented bases associations upon thesaurus categories.
This paper describes a corpus-based investigation of anaphora in dialogues, using data from English and Portuguese face- to-face conversations.
Possible applications comprise machine translation, computer-aided language learning, and dialogue systems in general.
The central concept worked out here in some detail is the concept of 'partial isomorphy' between subgrammars.
We also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain.
We present an algorithm, NOMEN, for learning generalized names in text.
An open, extendible multi-dictionary system is introduced in the paper.
This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using natural language learning tech-niques: looking for characteristic statistical "language-signatures" in test corpora.
In this paper, we describe how the LIDAS System (Linguistic Discourse Analysis System), a discourse parser built as an implementation of the Unified Linguistic Discourse Model (U-LDM) uses information from sentential syntax and semantics along with lexical semantic information to build the Open Right Discourse Parse Tree (DPT) that serves as a representation of the structure of the discourse (Polanyi et al., 2004; Thione 2004a,b).
More specifically, we discuss how discourse segmentation, sentence-level discourse parsing, and text-level discourse parsing depend on the relationship between sentential syntax and discourse.
Specific discourse rules that use syntactic information are used to identify possible attachment points and attachment relations for each Basic Discourse Unit to the DPT.
We consider several empirical estimators for probabilistic context-free grammars, and show that the estimated grammars have the so-called consistency property, under the most general conditions.
Our estimators include the widely applied expectation maximization method, used to estimate probabilistic context-free grammars on the basis of unannotated corpora.
In this paper, we will present an efficient method to compute the co-occurrence counts of any pair of substring in a parallel corpus, and an algorithm that make use of these counts to create sub- sentential alignments on such a corpus.
In this paper we present the syntax and inference mechanisms for the language.
We describe a trainable and scalable summarization system which utilizes features derived from information retrieval, information extraction, and NLP techniques and on-line resources.
We also present preliminary results from a task-based evaluation on summarization output usability.
GETARUNS is a symbolic linguistically-based parser written in Prolog Horn clauses which uses a strong deterministic policy by means of a lookahead mechanism and a WFST.
We investigate various contextual effects on text interpretation, and account for them by providing contextual constraints in a logical theory of text interpretation.
This paper proposes a hybrid Chinese named entity recognition model based on multiple features.
Firstly, the proposed Hybrid Model integrates coarse particle feature (POS Model) with fine particle feature (Word Model), so that it can overcome the disadvantages of each other.
In this paper we present methods for reducing the computation time of joint segmentation and recognition of phones using the Stochastic Segment Model (SSM).
The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities.
A hybrid approach to the extraction of habitual collocations and idioms is presented, aiming at a detailed description of collocations and their morphosyntax for natural language generation systems as well as to support learner lexicography.
The paper deals with generation of natural language text in a dialog system.
We present new statistical models for jointly labeling multiple sequences and apply them to the combined task of partof-speech tagging and noun phrase chunking.
The model is based on the Factorial Hidden Markov Model (FHMM) with distributed hidden states representing partof-speech and noun phrase sequences.
Further, we extend this into a novel model, Switching FHMM, to allow for explicit modeling of cross-sequence dependencies based on linguistic knowledge.
This paper briefly describes our system in the third SIGHAN bakeoff on Chinese word segmentation and named entity recognition.
This is done via a word chunking strategy using a context-dependent MutualInformation Independence Model.
Various methods have been proposed for automatic synonym acquisition, as synonyms are one of the most fundamental lexical knowledge.
This study has experimentally investigated the impact of contextual information selection, by extracting three kinds of word relationships from corpora: dependency, sentence co-occurrence, and proximity.
In this paper I report on an investigation into the problem of assigning tones to pitch contours.
We report on a method for compiling decision trees into weighted finite-state transducers.
These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996).
This paper presents an unsupervised method for assembling semantic knowledge from a part-ofspeech tagged corpus using graph algorithms.
We describe a method for interpreting abstract flat syntactic representations, LFG f- structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).
It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f- structures through the translation images of f-structures as UDRSs.
We propose a conditional random field- based method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of English and Japanese.
A `lexicalized' grammar naturally follows from the extended domain of locality of TAGs.A general parsing strategy for `lexicalized' grammars is discussed.
Subsets of these lexicons are being incrementally interfaced to the parser.We finally show how idioms are represented in lexicalized TAGs.
This paper argues for a two-level theory of semantics as opposed to a one-level theory, based on the example of the system of temporal and durational connectives.
QACIAD (Question Answering Challenge for Information Access Dialogue) is an evaluation framework for measuring interactive question answering (QA) technologies.
Modern Question/Answering systems rely on expected answer types for processing questions.
This paper describes a study in which a corpus of spoken Danish annotated with focus and topic tags was used to investigate the relation between information structure and pauses.
We present results of experiments with Elman recurrent neural networks (Elman, 1990) trained on a natural language processing task.
The task was to learn sequences of word categories in a text derived from a primary school reader.
The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.
We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs).
are pervasive in dialogue.
We present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue.
We propose a structured semantic representation, the Lexical Conceptual Paradigm (LOP) which groups nouns into paradigmatic classes exhibiting like behavior.I.
We use this as a principled distinguishing characteristic of polysemous types.
We distinguish two systems that together comprise the meaning of lexical items, the lexical system and the conceptual system.
We limit ourselves in this paper to cases of polysemy involving norninals.We will proceed as follows.
basic segmentation, named entity recognition, error-driven learner and new word detector.
The basic segmentation and named entity recognition, implemented based on conditional random fields, are used to generate initial segmentation results.
Markov logic is a highly expressive language recently introduced to specify the connectivity of a Markov network using first-order logic.
We validate our algorithms on the tasks of citation matching and author disambiguation.
This paper overviews the overall architecture of the project and provides briefs on the three components of this project, namely Urdu Lexicon, English to Urdu Machine Translation System and Urdu Text to Speech System.
This paper studies issues on compiling a bilingual lexicon for technical terms.
As a method of translation estimation for technical terms, we pro-pose a compositional translation esti-mation technique.
Within the context of MUC-7, the SIFT system for extraction of template entities (TE) and template relations (TR) used a novel, integrated syntactic/semantic language model to extract sentence level information, and then synthesized information across sentences using in part a trained model for cross-sentence relations.
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.
These FSAs are good representations of paraphrases.
The algorithm generates appropriate interpretations for cases of VP ellipsis, pseudo-gapping, bare ellipsis (stripping), and gapping.
A FSA (finite state automaton) based on the discourse grammar determines the possible moves which the dialogue might take.
An extension to classical unification, called graded unification is presented.
We describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues.
We represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques.
Automatic book indexing systems are based on the generation of phrase structures capable of reflecting text content.
First, we segment the text with a character-level CRF model.
Then we apply three word-level CRF models to the labeling person names, location names and organization names in the segmentation results, respectively.
Corpus-based sense disambiguation methods, like most other statistical NLP approaches, suffer from the problem of data sparseness.
Using the definition- based conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information.
We report on our experiments to automatically identify links between the senses in a machine- readable dictionary.
In particular, we automatically identify instances of zero-affix morphology, and use that information to find specific linkages between senses.
This work has provided insight into the performance of a stochastic tagger.
These new ideas have been preliminarily tested on named entity recognition and PP attachment disambiguation.
This paper presents a connectionist syntactic parser which uses Structure Unification Grammar as its grammatical framework.
To achieve translation technology that is adequate for speech-to-speech translation (S2S), this paper introduces a new attempt named Corpus-Centered Computation, (abbreviated to C3 and pronounced c-cube).
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
In this paper, we study the incorporation of MT models and ASR models using finite-state automata.
We also propose some transducers based on MT models for rescoring the ASR word graphs.
This paper presents a method for the automatic classification of semantic relations in nominalized noun phrases.
The paper presents preliminary results for the semantic classification of the most representative NP patterns using four distinct learning models.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
To demonstrate the benefits of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories.
We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical- Functional Grammar (LFG).
The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components.
In this paper, we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture.
It investigates more than 10 classifier combination methods, including second order classifier stacking, over 6 major structurally different base classifiers (enhanced Na飗e Bayes, cosine, Bayes Ratio, decision lists, transformation-based learning and maximum variance boosted mix-ture models).
We describe a statistical approach for modelling and detecting dialogue acts in Instant Messaging dialogue.
The model we developed combines naive Bayes and dialogue-act n-grams to obtain better than 80% accuracy in our tagging experiment.
This paper presents the results of using Roget's International Thesaurus as the taxonomy in a semantic similarity measurement task.
We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages.
This paper presents a hybrid approach for named entity (NE) tagging which combines Maximum Entropy Model (MaxEnt), Hidden Markov Model (HMM) and handcrafted grammatical rules.
MaxEnt includes external gazetteers in the system.
Sub-category generation is also discussed.
With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone.
One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996).
Sentence ranking is a crucial part of generating text summaries.
We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence- based approaches.
The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton & Buckley (1988)).
We present MAGEAn, a morphological analyzer and generator for the Arabic language family.
Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as Co- Occurrence Double Check (CODC), are presented.
The experiments on link detection of named entities using the strategies of direct association, association matrix and scalar association matrix verify that the double-check frequencies are reliable.
The paper presents a new segmentation-free approach to the Arabic optical character recognition.
We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine- grained classes.
off-line) material in foreign languages.
Our experiments have shown that this method is effective for inferring the POS of unknown words.
Processing in GENIE is category-driven, i.e., grammatical rules are distributed over a part-of-speech hierarchy and, using an inheritance mechanism, are invoked only if appropriate for the category being processed.1- IntroductionThis paper discusses relational-grammar-based generation in the context of JETS, a Japanese-English machine translation (MT) system that is being developed at the IBM Research Tokyo Research Laboratory.To put our work in perspective, we first explain the motivation for basing JETS on relational grammar (RG) and then sketch the processing flow in translation.
Three machine learning methods are compared for this task: decision tree induction, a genetic algorithm, and log-linear regression.
This paper proposes a general probabilistic setting that formalizes a probabilistic notion of textual entailment.
We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting.
We explore here the relationship between protein/gene names and expressions used to characterize protein/gene function.
We achieve 81% accuracy on the task of discourse relation type classification and 70% accuracy on relation identification.
This structure ( named network structure ) involves all possible combinations of syntactically collect phrases.
RASTA (Rhetorical Structure Theory Analyzer), a system for automatic discourse analysis, reliably identifies rhetorical relations present in written discourse by examining information available in syntactic and logical form analyses.
Whereas introspection is a viable strategy for human analysts, a computational discourse analysis system likeRASTA requires explicit methods for identifying discourse relations.
Identifying rhetorical relationsIn the computational discourse analysis literature, there are three strands concerning the identification of rhetorical relations.
Typically simple pattern matching is used to identify cue phrases.
Hence, true spontaneous speech is generated.
Caseframe parsers employ both semantic and syntactic knowledge.
The key problem to be faced when building a HMM-based continuous speech recogniser is maintaining the balance between model complexity and available training data.
This paper describes a method of creating a tied-state continuous speech recognition system using a phonetic decision tree.
We describe a corpus-based investigation of proposals in dialogue.
We will also introduce a web-based interface to MindNet lexicons (MNEX) that is intended to make the data contained within MindNets more accessible for exploration.
The basic model is statistical, but we use broad-coverage rule- based parsers in two ways ?during training for learning rewrite patterns, and at runtime for reordering the source sentences.
We investigate prototype-driven learning for primarily unsupervised sequence modeling.
This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.
Two WSD classifiers are trained from a word sense-tagged corpus: one is a classifier obtained by supervised learning, the other is a classifier using hypernyms extracted from definition sentences in a dictionary.
The algorithm combines four original alignment models based on relative corpus frequency, con-textual similarity, weighted string simi-larity and incrementally retrained inflec-tional transduction probabilities.
This paper describes a method for estimat-ing conditional probability distributions over the parses of "unification-based" grammars which can utilize auxiliary distributions that are estimated by other means.
We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic "Unification-based" Grammars (SUBGs).
We show how the recognition performance of a speech recognition component in a speech retrieval system affects the retrieval effectiveness.
A speech retrieval system facilitates content-based retrieval of speech documents, i.e.
audio recordings containing spoken text.
ATE GrammarsAn ATM grammar is a set of networks formed by labelled states and directed arcs connecting them.
We describe the development of PASTAWeb, a WWW-based interface to the extraction output of PASTA, an IE system that extracts protein structure information from MEDLINE ab-stracts.
Lastly, an ongoing project that integrates POS tagging, parsing, and sense disambiguation in one system is presented.
A series of experiments on speaker-independent phone recognition of continuous speech have been carried out using the recently recorded BREF corpus.
This paper describes a method for asking statistical questions about a large text corpus.
In this paper I propose a Binding rule for the identification of pronoun and anaphor referents in phrase-structure trees, assuming the general framework of the Government-binding theory outlined by Chomsky (1981).
The fragment of the attribute grammar shown here is part of an English grammar and parser being developed in the Prolog and PLNLP languages.
Furthermore; we consider weakly supervised learning technique; CoTraining; to combine labeled data and unlabeled data.Keywords : Korean Named Entity; HMM; Co-Training
We also describe the results of the experiments on learning probabilistic sub-categorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference.
The first feature is the partial disambiguation function of the Bi-directional Retriever, which can be used for search request translation in cross-language IR.
The Papillon project is a collaborative project to establish a multilingual dictionary on the Web.
It aims to apply the LINUX cooperative construction paradigm to establish a broad- coverage multilingual dictionary.
This paper discusses the challenges and proposes a solution to performing information retrieval on the Web using Chinese natural language speech query.
It uses the query model to facilitate the extraction of main core semantic string (CSS) from the Chinese natural language speech query.
This paper describes a mix word-pair mix-WP) identifier to resolve homo-nym/segmentation ambiguities as well as perform STW conversion effec-tively for Chinese input.
Collocational knowledge is necessary for language generation.
(ii) High quality translation via word sense disambiguation and accurate word order generation of the target language.
We describe a mechanism which receives as input a segmented argument composed of NL sentences, and generates an inter-pretation.
In this paper, we present a prosody generation architecture based on K-ToBI (Korean Tone and Break Index) representation.ToBI is a multi-tier representation system based on linguistic knowledge to transcribe events in an utterance.
We developed automatic corpus-based K-TOBI labeling tools and prediction methods based on several lexicosyntactic linguistic features for decision-tree induction.
We propose a formal characterization of variation in the syntactic realization of semantic arguments, using hierarchies of syntactic relations and thematic roles, and a mechanism of lexical inheritance to obtain valency frames from individual linking types.
We embed the formalization in the new lexicalized, dependency-based grammar formalism of Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001).
1 IntroductionThis paper deals with the mapping (or linking) of semantic predicate-argument structure to surface syntactic realizations.
We present a formal architecture in the framework of a multi-dimensional, heavily lexicalized, efficiently parsable dependency formalism (Duchier and Debusmann, 2001), which uses lexical inheritance as a means to explicitly model syntactic variation.
We concentrate on variation between prepositional phrases and nominal phrases which realize verbal arguments, and remedy problems that occur with this kind of variation in recent approaches like the HPSG linking architecture proposed by (Davis, 1998).Section 2 presents and analyses some of the problematic data we can model, English dative shift, optional complements and thematic role alternations.
cal Dependency Grammar (TDG) by adding a new representational level (thematic structure additionally to ID structure) to the framework in Section 4.1 and introducing the concept of a valency frame in the TDG inheritance lexicon (Sections 4.2 and 4.3).
Section 5 contrasts our analysis of the dative shift construction with the analysis of thematic role alternations.2 Linguistic DataInsights from corpus studies (e.g.
The type of local dependencies considered are sequences of part of speech categories for words.
We propose a distinction between two kinds of metonymy: "referential" metonymy, in which the referent of an NP is shifted, and "predicative" metonymy, in which the referent of the NP is unchanged and the argument place of the predicate is shifted instead.
We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank.
We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG.
Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser.
This paper presents a disambiguation method in which word senses are determined using a dictionary.
A model-based spectral estimation algorithm is derived that improves the robustness of speech recognition systems to additive noise.
We name the algorithm minimummean-log-spectral-distance (MMLSD).
In this paper, we introduce LiveTree, a core component of LIDAS, the Linguistic Discourse Analysis System for automatic discourse parsing with the Unified Linguistic Discourse Model (U-LDM) (X et al, 2004).
Elementary logic (i.e.
Traditional intensional logics (i.e.
We present an approach to the incremental ac-crual of lexical information for unknown words that is constraint, based and compatible with standard unification-based grammars.
We show how morphological information, especially in-flectional class, is successfully acquired using a type-based HPSG-like analysis.
We present an implemented unification-based parser for relational grammars developed within the stratified feature grammar (SFG) framework, which generalizes Kasper-Rounds logic to handle relational grammar analyses.
We first introduce the key aspects of SFG and a lexicalized, graph-based variant of the framework suitable for implementing relational grammars.
We then describe a head-driven chart parser for lexicalized SFG.
The basic parsing operation is essentially ordinary feature-structure unification augmented with an operation of label unification to build the stratified features characteristic of SFG.
Named entity (NE) recognition is a task in which proper nouns and nu-merical information in a document are detected and classified into categories such as person, organization, location, and date.
NE recognition plays an es-sential role in information extraction systems and question answering sys-tems.
This paper describes recent improvements in the SPHINX Speech Recognition System.
These enhancements include function-phrase modeling, between-word coarticulation modeling, and corrective training.
We provide a constraint based computational model of linear precedence as employed in the HPSG grammar formalism.
In this paper, we present a logic-based computational model for movement theory in Government and Binding Theory.
A news article is first segmented into clauses, then into words by a Viterbi-based word identification system.
We also adopt Kupiec's concept of word equivalence classes in the tagger.
The method exploits a definition of N2 in a dictionary.
Parts of speech are found through a tagger, and related neighboring words are identified by a phrase extractor operating on the tagged text.
To suggest possible senses, each heuristic draws on semantic relations extracted from a Webster's dictionary and the semantic thesaurus WordNet.
We propose a general model for joint inference in correlated natural language processing tasks when fully annotated training data is not available, and apply this model to the dual tasks of word sense disambiguation and verb subcategorization frame determination.
This paper presents the XTAG system, a grammar development tool based on the Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic grammar for English.
As part of the DARPA Spoken Language System program, we recently initiated an effort in spoken language understanding.
This paper describes our early experience with the development of the MIT VOYAGER spoken language system.
The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features.
In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax.
We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics.
We created a data collection for research, development and evaluation of a method for automatically answering why-questions (why-QA) The resulting collection comprises 395 why-questions.
We developed a question analysis method for why-questions, based on syntactic categorization and answer type determination.
This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora.
We combine vari-ous clues such as cognates, similar context, preservation of word similar-ity, and word frequency.
We describe two new strategies to automatic bracketing of parallel corpora, with particular application to languages where prior grammar resources are scarce: (1) coarse bilingual grammars, and (2) unsupervised training of such grammars via EM (expectation-maximization).
This paper deals with the reference choices involved in the generation of argumentative text.
We describe GLP, a chart parser that will be used as a SYNTAX module of the Erlangen Speech Understanding System.
words.
Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, our algorithm obtains additional semantic knowledge from the Web.
We present an integrated strategy for ordering information, combining constraints from chronological order of events and cohesion.
We describe an implemented system for robust domain-independent syntactic parsing of English, using a unification-based grammar of part-ofspeech and punctuation labels coupled with a probabilistic LR parser.
Here, we discuss the development of RACE ?Retrospective Analysis of Communications Events ?a test-bed prototype to investigate issues relating to multi-modal multi-party discourse.
We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning.
Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.
Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing.
In this paper, we propose a new method, combining the transliterated string segmentation module with phoneme- based and grapheme-based transliteration modules in order to enhance the back?transliterations of Japanese words.
The paper describes two equivalent grammatical formalisms.
In this paper are described experiments on un-supervised learning of the domain lexicon and relevant phrase fragments from a dialog cor-pus.
Suggested approach is based on using do-main independent words for chunking and us-ing semantical predictional power of such words for clustering and automatic extraction phrase fragments relevant to dialog topics.
We propose re-segmenting the ASR hypotheses as well as applying post- classification to improve the performance.
We use acoustic, phonetic, language model, NER and other scores as confidence measure.
We present here a simple two-stage method for extracting complex relations between named entities in text.
We participated in the SENSEVAL-3 English lexical sample task and multilingual lexical sample task.
The knowledge sources used were partof-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
Lexicalized Tree Adjoining Grammars have proved useful for NLP.
This paper describes a non-statistical approach for semantic annotation of documents by analysing their syntax and by using semantic/syntactic behav-iour patterns described in VerbNet.
We describe an automatic process for learning word units in Japanese.
Our method applies a compound-finding algorithm, previously used to find word sequences in English, to learning syllable sequences in Japanese.
We used SemCat as training data to investigate name classification techniques.
We generated a statistical language model and probabilistic context- free grammars for gene and protein name classification, and compared the results with the new model.
This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also called n-gram transliteration model (TM).
With the n-gram TM model, we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary.
An inheritance network.Bohrow and Winograd's KR1.
In this paper, I outline several fully intensional semantics for intensional semantic networks by discussing the relations between a semanttc-network "language- I. and several candidates for I. .
We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).
This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.
We describe an approach to improve the bilingual cooccurrence dictionary that is used for word alignment, and evaluate the improved dictionary using a version of the Competitive Linking algorithm.
We have experimented with using a statistical word-alignment algorithm to derive word association pairs (French-English) that complement an existing multipurpose bilingual dictionary.
EBMT (Example-Based Machine Translation) is proposed.
This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the bread applicability of EBMT to several difficult translation problems for RBMT and discusses the advantages of integrating EBMT with RBMT.
This paper mainly presents a Turkish sentence generator for producing the actual text from its semantic description.
We use a functional linguistic theory called Systemic-Functional Grammar (SFG) to represent the linguistic resources, and FUF text generation system as a software tool to perform them.
In this paper, we present the systemic-functional representation and realization of simple Turkish sentences.
In this paper we present an algorithm for automated inversion of a PROLOG-coded unification parser into an efficient unification generator, using the collections of minimal sets of essential arguments (MSEA) for predicates.
Our approach distinguishes two orthogonal yet mutu-ally constraining structures: a syntactic dependency tree and a topological de-pendency tree.
We present a classifier-based parser that produces constituent trees in linear time.
The parser uses a basic bottom-up shift- reduce algorithm, but employs a classifier to determine parser actions instead of a grammar.
SenseClusters is a freely available system that clusters similar contexts.
There are three measures based on clustering criterion functions, and another on the Gap Statistic.
We present an environment for multimodal visualization of geometrical constructions, including both graphical and textual realizations.
We describe a segmentation component that utilizes minimal syntactic knowledge to produce a lattice of word candidates for a broad coverage Japanese NL parser.
This paper describes an implementation to compute positional ngram statistics (i.e.
Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays.
Positional ngrams are ordered sequences of words that represent continuous or discontinuous substrings of a corpus.
In particular, the positional ngram model has shown successful results for the extraction of discontinuous collocations from large corpora.
Features include: morphological variants; WordNet synonyms and hypernyms; log-likelihood-based word pairings dy-namically obtained from baseline sen-tence alignments; and formal string features such as word-based edit dis-tance.
The model is an m-component mixture of Ingram models.
Next, we analyze common approaches to two subproblems: term overlap for answer sentence identification, and answer typing for short answer extraction.
We present a syntax-based constraint for word alignment, known as the cohesion constraint.
We describe SmartMail, a prototype system for automatically identifying action items (tasks) in email messages.
We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others).
Spelling out means making explicit.
First, it discusses how to perform comprehension under Optimality Theory grammars consisting of finite-state constraints.
Information Extraction (IE) is the task of extracting knowledge from unstructured text.
Hidden Markov models (HMMs) are powerful statistical models that have found successful applications in Information Extraction (IE).
In order to retrieve extraction- relevant segments from documents, we introduce a method to use HMMs to model and retrieve segments.
This paper presents our method of incorporating character clustering based on mutual information into Decision- Tree Dictionary-less morphological analysis.
Today there is a relatively large body of work on automatic acquisition of lexicosyntactical preferences (subcategorization) from corpora.
One experiment is described in (Carroll et al., 1998) ?they use subcategorization probabilities for ranking trees generated by unification-based phrasal grammar.
This paper reported our work on annotating Chinese texts with information structures derived from HowNet.
An information structure consists of two components: HowNet definitions and dependency relations.
This work is part of a multi-sentential approach to Chinese text understanding.
Efficient algorithms for generation in this framework take a semantics-driven strategy.
Therefore Lambek Theorem Proving is a natural candidate for a 'uniform' architecture for natural language parsing and generation.Keywords: generation algorithm; natural language generation; theorem proving; bidirectionality; categorial grammar.
It has been hypothesized that Tree Adjoining Grammar (TAG) is particularly well suited for sentence generation.
We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences.
The algorithm exploits robust lexical, syntactic, and semantic knowledge sources.
Two different classes of metonymies are inferred by using (1) lexico-semantic connections between concepts or (2) morphological cues and logical formulae defining lexical concepts.
In both cases the derivation of metonymic paths is based on approximations of sortal constraints retrieved from Word- Net.This novel method of inferring coercions validates the related knowledge through coreference links.
We have proposed an incremental trans-lation method in Transfer-Driven Ma-chine Translation (TDMT).
The paper compares use of presentation features to word features, and the combination thereof, using Na飗e Bayes, C4.5 and SVM classifiers.
Example-based parsing has already been proposed in literature.
Link Grammar based parsing has been considered as the underlying parsing scheme for this work.
This paper assesses two new approaches to deterministic parsing with respect to the analysis of unbounded dependencies (UDs).
We examine the predictions made by a LR(1) deterministic parser and the Lexicat deterministic parser concerning the analysis of UDs.
We show how the language, composed of orthographic mks, word formation rules, and paradigm inheritance, can be compiled into a run-time data stricture for efficient morphological analysis and generation with a dynamic secondary storage lexicon.
We introduce inversion-invariant transduction grammars which serve as generative models for parallel bilingual sentences with weak order constraints.
Focusing on transduction grammars for bracketing, we formulate a normal form, and a stochastic version amenable to a maximum-likelihood bracketing algorithm.
Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects.
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
all bills only.
We introduce feature terms containing sorts, variables, negation and named disjunction for the specification of feature structures.
We define context-unique feature descriptions, a relational, constraint-based representation language and give a normalization procedure that allows to test consistency of feature terms.
In its most robust configuration, the system uses only a general lexicon, a local morphosyntactic parser and a dictionary of synonyms.
We show that treating ANNs as a post-process to partially recognized speech from an HMM pre-processor is superior to using ANNs alone or to hybrid systems applying ANNs before HMM processing.
We will use task domain constraints provided by particular application packages on personal computers to create constrained natural language understanding.
Furthermore we will implement interactive voice and text response mechanisms such as dialog boxes and speech synthesis to respond to the users input.
We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
We use a language-universal rule-based algorithm to find a good set of parameters, and then train the parameter weights using EM.
In this paper, we present our work on the detection of question- answer pairs in an email conversation for the task of email summarization.
We show that various features based on the structure of email- threads can be used to improve upon lexical similarity of discourse segments for question- answer pairing.
LEXTER is a software package for extracting terminology.
This paper describes the mechanisms used by the UNITRAN machine translation system for mapping an underlying lexical- conceptual structure to a syntactic structure (and vice versa), and it shows how these mechanisms coupled with a set of general linking routines solve the problem of thematic divergence in machine translation.
A Natural Language Generation system produces text using as input semantic data.
and lean declarative impIe.mentation.
This paper presents a method for improving the performance of a link detection system by using a variety of similarity measures and using source-pair specific statistical information.
The techniques investigated were SVMs, voting, and decision trees, each of which makes use of similarity and statistical information differently.
Decoding algorithm is a crucial part in statistical machine translation.
To deal with organization names, keywords, prefix, word association and parts-of-speech are applied.
Shalt2 is a knowledge-based machine translation system with a symmetric architecture.
The grammar rules, mapping rules between syntactic and conceptual (semantic) representations, and transfer rules for conceptual paraphrasing are all bi-directional knowledge sources used by both a parser and a generator.
We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3).
An NE transliteration model is presented and iteratively trained using named entity pairs extracted from a bilingual dictionary.
We propose a syntactic filter for identifying non-coreferential pronoun-NP pairs within a sentence.
In particular, the fitter formulates constraints on pronominal anaphora in terms of the head-argument structures provided by Slot Grammar syntactic representations rather than the configurational tree relations, particularly c-command, on which the binding theory relies.
We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles.
The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR.
This paper describes the RenTAL system, which enables sharing resources in LTAG and HPSG formalisms by a method of grammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar.
We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steed- man by varying the set of reduction rules.
We characterize the reduction rules capable of generating context-sensitive languages as those having a partial combination rule and a combination rule in the reverse direction.
We describe here a method for hierarchically organ-ising discourse markers.
of simple translations.
Most current definitional question answering systems apply one-size-fits-all lexicosyntactic patterns to identify definitions.
By analyzing a large set of online definitions, this study shows that the semantic types of definienda constrain both lexical semantics and lexicosyntactic patterns of the definientia.
incorporates semantic-typedependent lexicosyntactic patterns (e.g., 揟ERM locates ...?
We will demonstrate SconeEdit, a new tool for exploring and editing knowledge bases (KBs) that leverages interaction with domain texts.
This paper presents an analysis of purpose clauses in the context of instruction understanding.
We compare the asymptotic time complexity of left-to-right and bidirectional parsing techniques for bilexical context-free grammars, a grammar formal-ism that is an abstraction of language models used in several state-of-the-art real-world parsers.
The TiGer DB is a dependency bank derived from the TiGer Treebank containing predicate- argument relations and several grammatical features which can be considered as semantically meaningful.
LFG f-structures or HPSG feature structures, to dependency triples simple.
This position paper outlines approaches to diagram summarization that can augment the many well-developed techniques of text summarization.
We describe the advances in raster image vectorization and parsing needed to produce corpora for diagram summarization.
In Online Inquiry-Based Learning (OIBL) learners search for information to answer driving questions.
This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.
Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.
We present two efficient search algorithms for real-time spoken language systems.
The first called the Word-Dependent N-Best algorithm is an improved algorithm for finding the top N sentence hypotheses.
The second algorithm 4 a fast match scheme for continuous speech recognition called the Forward-Backward Search.
The main objective of this paper is to examine various N- gram models of generating translation units for bilingual lexicon extraction.
Three N-gram models, a baseline model (Bound-length N-gram) and two new models (Chunk-bound N- gram and Dependency-linked N-gram) are compared.
This paper describes how to automatically extract grounding features and segment a dialogue into discourse units, once the dialogue has been annotated with the DRI backward- and forward-looking tags.
Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%.
This paper presents our work on real-izing expressions of doubt appropriately in natural language dialogues.
This paper describes our ongoing research project on text simplification for congenitally deaf people.
In this paper, we discuss the issues we should address to realize text simplification and report on the present results in three different aspects of this task: readability assessment, paraphrase representation and post-transfer error detection.
A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus.
Press: reportage	13.
Non-fiction	11.
Learned3.
Fiction	K. General Fiction	I.
Mystery	M. Science	N. Adv.
then use discriminant analysis, a. technique from descriptive statistics.
(WSJ) speech corpus.
All Gaussians have diagonal covariance matrices.2.
We discuss such 搒trapping?methods in general, and exhibit a particular method for strapping word- sense classifiers for ambiguous words.
We report work1 in progress on adding affect-detection to an existing program for virtual dramatic improvisation, monitored by a human director.
in users?text input, by means of pattern-matching, robust parsing and some semantic analysis.
In this paper we propose and evaluate a technique to perform semi-supervised learning for Text Categorization.
In this paper, a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a Chinese multiple-clause sentence.
It takes into account the effects of linguistic features such as tense/aspect, temporal connectives, and discourse structures.
A set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution.
We present a bottom-up approach to arranging sentences extracted for multi-document summarization.
To capture the association and order of two textual segments (eg, sentences), we define four criteria, chronology, topical-closeness, precedence, and succession.
This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts.
The paper proposes an architecture for advanced NLG systems that handle narratives.
Domain modelling and meta-knowledge modelling for a narratological structurer are exemplified.
This paper proposes a model using associative processors (APs) for real-time spoken language translation.
We have already proposed a model, TDMT (Transfer-Driven Machine Trans-lation), that translates a sentence utilizing ex-amples effectively and performs accurate struc-tural disambiguation and target word selection.
This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical co- occurrence and error-driven filtering.
In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.
The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.
Two experiments on opinion/modality classification confirm that subtree features are important.
causal connectives, may be categorised into different discourse domains.
The tool is designed to be used in three applications: generating training data for machine learning of co-reference relations, evaluating theories of referring expression generation and resolution in texts, and developing theories for understanding reference in dialogs.
We address the issue of judging the significance of rare events as it typically arises in statistical natural- language processing.
We first define a general approach to the problem, and we empirically compare results obtained using log-likelihood-ratios and Fisher抯 exact test, applied to measuring strength of bilingual word associations.
Tins paper focuses on the issue of named entity chunking in Japanese named entity recognition.
We apply the supervised decision list learn-ing method to Japanese named entity recogni-tion.
We also investigate and incorporate sev-eral pained-entity noun phrase chunking tech-niques and experimentally evaluate and com-pare their performance.
The three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polysemous word by using knowledge about patterns of word co- occurrences.
The techniques were based on Bayesian decision theory, neural networks, and content vectors as used in information retrieval.
This paper presents an algorithm for learning the probabilities of optional phonological rules from corpora.
This paper presents comparative experimental results on four techniques of language model adaptation, including a maximum a posteriori (MAP) method and three discriminative training methods, the boosting algorithm, the average perceptron and the minimum sample risk method, on the task of Japanese Kana-Kanji conversion.
In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.
This paper describes the Lycos Retriever system, a deployed system for automatically generating coherent topical summaries of popular web query topics.
The METAL machine translation project incorporates two methods of structural transfer - direct transfer and transfer by grammar.
This paper describes our initial efforts at porting the VOYAGER spoken language system to Japanese.
The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus.
The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages.
Our experiments were conducted using three original translation models and two types of neural nets (single-layer and multi- layer perceptrons) for the confidence estimation task.
We use machine learning techniques to find the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references.
We find that using first mention, utterance distance, and lexical distance computed using either Google or WordNet results in an accuracy significantly higher than obtained in previous experiments.
The whole process is performed in a unification-based framework.
Using alignment techniques from phrase- based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot.
We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.
We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.
We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification).
Core Roles versus Adjuncts).
This paper presents a novel approach to combining different word alignments.
We view word alignment as a pattern classification problem, where alignment combination is treated as a classifier ensemble, and alignment links are adorned with linguistic features.
This paper presents the strategy and design of a highly efficient semiautomatic method for labelling the semantic features of common nouns, using semantic relationships between words, and based on the information extracted from an electronic monolingual dictionary.
ILLICO combines two principles: modularity in the representation of knowledge defined at the different levels of language processing, and sentence composition using partial synthesis and guided composition.
Our approach was to develop a model of natural language generation from semantics, and train the model using maximum likelihood and smoothing.
To address this issue we have created a robust, semantic parser as a single finite-state machine (FSM).
We describe a method for evaluating a grammar checking application with hand–bracketed parses.
We introduce two measures to make dialogs for fixing recognition errors.
With performance above 97% accuracy for newspaper text, part of speech (POS) tagging might be considered a solved problem.
Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance.
However, for grammar formalisms which use more fine-grained grammatical categories, for example TAG and CCG, tagging accuracy is much lower.
We extend this multi- tagging approach to the POS level to overcome errors introduced by automatically assigned POS tags.
Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging.
This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators.
Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation.
In this paper, we introduce a new parser, called SXLFG, based on the Lexical- Functional Grammars formalism (LFG).
Words unknown to the lexicon present a substantial problem to part-of-speech tagging.
In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible partsof-speech for unknown words.
Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus: prefix morphological rules, suffix morphological rules and ending-guessing rules.
We provide an XML serialization for intercomponent communication.
A the-saurus navigator having novel functions such as term clustering, thesaurus overview, and zooming-in is proposed.
We propose a computational model of visually-grounded spatial language understanding, based on a study of how people verbally describe objects in visual scenes.
We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework.
superordinate -hyponym relation, synonym relation) is one of the most important problems for thesaurus construction.
This paper presents an active-learning word selection strategy that is mindful of human limitations.
In this paper we describe a coreference resolution method that employs a classification and a clusterization phase.
This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice, thereby improving the naturalness of synthetic speech in a spoken language dialogue system.
The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized.
This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.
A number of operational models are introduced, with web interfaces for lexical databases, DFSA matrices, finite- state phonotactics development, and DATR lexica.
We instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment, a technique commonly used in bioinformatics.
In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank.
This report summarizes Me system and its performance on the MUG-4 task.
In this paper, we evaluate a vari-ety of knowledge sources and super-vised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
The learning al-gorithms evaluated include Support Vec-tor Machines (SVM), Naive Bayes, Ad-aBoost, and decision tree algorithms.
We describe a three-tiered approach for evaluation of spoken dialogue systems.
I estimate a confidence value of each SCF using corpus-based statistics, and then perform clustering of SCF confidence- value vectors for words to capture co- occurrence tendency among SCFs in the lexicon.
I apply my method to SCFs acquired from corpora using lexicons of two large-scale lexicalized grammars.
Corpus-based Natural Language Processing (NLP) tasks for such popular languages as English, French, etc.
This POS-tagger made use of the Transformation-Based Learning (or TBL) method to bootstrap the POS-annotation results of the English POS-tagger by exploiting the POS-information of the corresponding Vietnamese words via their word- alignments in EVC.
Then, we directly project POSannotations from English side to Vietnamese via available word alignments.
single event, multiple events and declared event(s).
Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in- sequence n-grams automatically.
The second method relaxes strict n-gram matching to skipbigram matching.
Skip-bigram is any pair of words in their sentence order.
Skip-bigram co- occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
This paper introduces a method for the semi-automatic generation of grammar test items by applying Natural Language Processing (NLP) techniques.
We describe the possible integration of topic signatures with ontologies and its evaluatonon an automated text summarization system.
This paper describes a rule-learning approach towards Chinese prosodic phrase prediction for TTS systems.
We explore unsupervised language model adaptation techniques for Statistical Machine  Translation.
In this paper we approach word sense disambiguation and information extraction as a unified tagging problem.
We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model.
This paper advocates an account of the different imperfective readings in terms of pragmatic principles and inferential heuristics based on, and supplied by, a semantic skeleton consisting of a 憇electional theory?of aspect.
Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English.
We propose an approach of combining lexical information, web statistics, and inverse search based on Google to backward translate a Chinese named entity (NE) into English.
The paper discusses how compositional semantics is implemented in the Verb-mobil speech-to-speech translation sys-tem using LUD, a description language for underspecified discourse representa-tion structures.
In this paper, we explore the use of structured content as semantic constraints for enhancing the performance of traditional term-based document retrieval in special domains.
First, we describe a method for automatic extraction of semantic content in the form of attribute-value (AV) pairs from natural language texts based on domain models constructed from a semi- structured web resource.
Sentence retrieval plays a very important role in question answering system.
In this paper, we present a novel cluster-based language model for sentence retrieval in Chinese question answering which is motivated in part by sentence clustering and language model.
Sentence clustering is used to group sentences into clusters.
The POSBIOTM/W1 is a workbench for machine-learning oriented biomedical text mining system.
These paraphrases are generated using WordNet and part-of-speech information to propose synonyms for the content words in the queries.
Statistical information, obtained from a corpus, is then used to rank the paraphrases.
This paper describes a system for constructing conceptual graph representation of text by using a combination of existing linguistic resources (VerbNet and WordNet).
We use a two- step approach, by firstly identifying the semantic roles in a sentence, and then using these roles, together with semi-automatically compiled domain-specific knowledge to construct the conceptual graph representation.
We describe the outline of Text Summarization Challenge 2 (TSC2 hereafter), a sequel text summarization evaluation conducted as one of the tasks at the NTCIR Workshop 3.
One long-term goal of this research is the development of predictive models of natural modality integration to guide the design of emerging multimodal architectures.Keywordsmultimodal interaction, integration and synchronization, speech and pen input, dynamic interactive maps, spatial location information, predictive modeling
Based on this analy-sis, we propose a variety of new morphological un-known-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German.
In this paper we describe an approach to constraint based syntactic theories in terms of finite tree automata.
We achieve this by using the intertranslatability of formulae of MSO logic and tree automata and the embedding of MSO logic into a constraint logic programming scheme.
In	this paper,	the	addition of	part-of-speech	ambiguity to	adeterministic parser written in Prolog is described.
Second, acquire a phonetic transcription of the new word.
In this paper we present the results of a preliminary study that employs a novel approach to the problem of acquiring the orthographic transcription through the use of an n-gram language model of english spelling and a quad-letter labeling of acoustic models that when taken together potentially produce an acoustic to spelling transcription of any spoken input.
Word posterior probabilities are a common approach for confidence estimation in automatic speech recognition and machine translation.
We will generalize this idea and introduce n-gram posterior probabilities and show how these can be used to improve translation quality.
This article describes the construction of a morphological, syntactic and semantic analyzer to operate a high-grade search engine for Hebrew texts.
This paper discusses an innovative approach to the computer assisted scoring of student responses in WebLAS (web-based language assessment system)- a language assessment system delivered entirely over the web.
In this paper, we explain a rapid development method of multimodal dialogue sys-tem using MIML (Multimodal Interaction Markup Language), which defines dialogue patterns between human and various types of interactive agents.
This paper discusses how a two-level knowledge representation model for machine translation integrates aspectual information with lexical-semantic information by means of parameterization.
Verbal and compositional lexical aspect provide the underlying temporal structure of events.
We show that it is possible to represent lexical aspect—both verbal and compositional—on a large scale, using Lexical Conceptual Structure (LCS) representations of verbs in the classes cataloged by Levin (1993).
In this paper we describe a Disambiguation Module which analyzes the content of dictionary definitions, in particular, definitions of the form "to VERB with NP".
The paper describes a language-independent approach for semiautomatic extension of lexical-semantic word nets and evaluates the method on CoreNet, the Korean version of word net.
This paper proposes a novel method to extract paraphrases of Japanese noun phrases from a set of documents.
In this paper we examine some issues pertaining to the task of selection in text planning.
Root based clusters can substitute dictionaries in indexing for IR.
This paper extends our previous work on committee-based sample selection for probabilistic classifiers.
We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging.
To realize our goal, we focus on expressions associated with time-slot (time-associated words).
We therefore use a semi-supervised learning method, the Na飗e Bayes classifier backed up with the Expectation Maximization algorithm, in order to iteratively extract time-associated words while improving the classifier.
This paper describes classification of typed student utterances within AutoTutor, an intelligent tutoring system.
The classifier presented uses part of speech tagging, cascaded finite state transducers, and simple disambiguation rules.
The Probabilistic Context-Free Grammar (PCFG) model is widely used for parsing natural languages, including Modern Chinese.
Now in this paper, we move on to the PCFG parsing of Classical Chinese texts.
We present an algorithm that learns the cross-language correspondence between affixes and letter sequences.
This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation (MT) based Cross-Language Information Retrieval (CLIR).
We apply default inheritance hierarchies to generating the morphology of Hebrew verbs.
in speech recognition, POS tagging and machine translation, but its justification is rarely questioned.
In this paper we investigate the applicability of Co-training to train classifiers that predict emotions in spoken dialogues.
- actionorganizationcivil fan namenationality mil itary	?assassin ccomando		terroris	...extremist			murderer narcoterror 1st rebel			robber			... subversive			1 la	attr桬	terrorist	guerri		Tat				... terrorist			.... thief		(KB EdANALYZEXTEND(Quit)Figure 3: Hierarchy EditorFlUSER: (none)癕AIN: (none)120000ES Find: 癰iKnowledge Base Graph Tool(Grow)(Done)(Prune)(Options)(Grow All)(Prune All)(Redisplay)(Ent ire Graph)ID: C offammo(Add Vocabulary)Children: Coff(Recalculate Graph)INLET: Interactive Natural Language Engineering ToolMOI111?11111111(Quit)RULE EDITOR: professional (1 of 1)IMMO ISEIMB MUD	Elli013 0 0 =II IMRule modes: CatonNode(s): .
Functional unification (FU) grammar is a general linguistic formalism based on the merging of feature-sets.
The left-corner transform removes left-recursion from (probabilistic) context-free grammars and uni-fication grammars, permitting simple top-down parsing techniques to be used.
An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of 揳tomic abbreviation pairs?from a large text corpus.
This paper proposes an approach to improve statistical word alignment with a rule-based translation system.
This approach first uses IBM statistical translation model to perform alignment in both directions (source to target and target to source), and then uses the translation information in the rule-based machine translation system to improve the statistical word alignment.
We propose that logic (enhanced to encode probability information) is a good way of characterizing semantic interpretation.
In support of this we give a fragment of an axiomatization for word-sense disambiguation, noun- phrase (and verb) reference, and case disambiguation.
We describe an inference engine (Frail3) which actually takes this axiomatization and uses it to drive the semantic interpretation process.
Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model.
It is implemented using oneversus-all support vector machine (SVM) classifiers and a number of feature extractors at several linguistic levels.
Two kinds of semantic variants can be found in traditional terminologies : strict synonymy links and fuzzier relations like see-also.
He validated an important part of the detected links as synonymy.
We present two different approaches to the modeling of extraposition, both based on machine learned decision tree classifiers.
The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics.
In this paper, we present a fully automated extraction system, named IntEx, to identify gene and protein interactions in biomedical text.
Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles.
Then, tagging biological entities with the help of biomedical and linguistic ontologies.
To disambiguate homograph, several efficient approaches have been proposed such as part-of-speech (POS) n-gram, Bayesian classifier, decision tree, and Bayesian-hybrid approaches.
This paper discusses the application of a previously reported theory of explanation rhetoric (Maybury, 1988b) to the task of explaining constraint violations in a hybrid rule/frame based system for resource allocation (Dawson et al, 1987).
We begin by exploring theoretical and practical issues with phrasal SMT, several of which are addressed by syntax-based SMT.
The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning.
We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone.
Among the types of ambiguities handled are prepositional phrases, very compound nominals, adverbials, relative clauses, and preposed prepositional phrases.
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.
We thus propose a Senseval-3 lexical sam-ple activity where the training data is collected via Open Mind Word Expert.
We propose a range of deep lexical acquisition methods which make use of morphological, syntactic and ontological language resources to model word similarity and bootstrap from a seed lexicon.
In this paper, we present a machine learning system for identifying non-referential it.
We identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model.
These considerations make syntactic focusing a more accurate predictor of the interpretation of one-anaphoric noun phrases without decreasing the accuracy for definite pronouns.
This paper presents a framework and a system for implementing, comparing and analyzing parsers for some classes of Constraint-Based Grammars.
a set of morphemes with associated features.
a local area network).
The regularity of named entities is used to learn names and to extract named entities.
Novel aspects of our system include multimodal ambiguity resolution, modular ontology- driven artifact manipulation, and a meeting browser for use during and after meetings.
This paper presents an approach for processing incomplete and inconsistent knowledge.
To help developing a localization oriented EBMT system, an automatic machine translation	evaluation	method	isimplemented which adopts edit distance, cosine correlation and Dice coefficient as criteria.
In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers.
This work introduces a methodology for designing dialogue managers in spoken dialogue systems for restricted domains.
This paper describes the framework of a Korean phonological knowledge base system using the unification- based grammar formalism : Korean Phonology Structure Grammar (KPSG).
The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system.
The Korean syllable structure has two types: one is the type of consonant and vowel goup(CV type : : ga), and the other is the type of consonant, vowel and consonant group(CVC type 4 : gak).
1 Korean syllable structure3.
Korean Phonology Structure GrammarAs mentioned above, KLSG[5] is a new grammar theory for the Korean language and follows a unification-based grammar such as GPSG[6] and HPSG[7].
The phonological feature system in ICPSGAll the Korean phonological categories of KPSG are presented by the sets of feature and they consist of feature and their values.
This paper presents preliminary experiments in the use of translation equivalences to disambiguate prepositions or case suffixes.
The context in which a target word occurs is represented by surface lexical features such as unigrams, bigrams, and second order co-occurrences.
We apply the log-linear model to automatically restore missing articles based on features of the noun phrase.
We first show that the model yields competitive results in article generation.
We use HBMs as word models conditioned on both DAs and (hidden) DA- segments.
These we will call IC Classes.
(1) a. Configurations: sisterhood, c-command, m-command, ±maximal projection ...b. Lexical features: ±N, ±V, ±Funct, ±c-selected, ±Strong Agrc.
These features are compiled into context-free rules in our parser.
valid prefixes.
We present an approach to parallel natural language parsing which is based on a con-current, object-oriented model of computa-tion.
This paper presents our experiments in applying Latent Semantic Analysis (LSA) to dialogue act classification.
We employ both LSA proper and LSA augmented in two ways.
Unlexicalized probabilistic context-free parsing is a general and flexible approach that sometimes reaches competitive results in multilingual dependency parsing even if a minimum of language-specific information is supplied.
We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.
The paper presents a new model for context- dependent interpretation of linguistic expressions about spatial proximity between objects in a natural scene.
This paper proposes an approach to automated ontology alignment and domain ontology extraction from two knowledge bases.
First, WordNet and HowNet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus.
This paper presents the results of applying transformation-based learning (TBL) to the problem of semantic role labeling.
Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain.
In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary sub- trees of dependency trees.
Speech-to-speech translation can be ap-proached using finite state models and several ideas borrowed from automatic speech recognition.
A 搒erial architecture?would use the Hidden Markov and the language models for recognizing input utterance and the transducer for finding the transla-tion.
In this paper, a hybrid disambiguation method for the prepositional phrase (PP) attachment and interpretation problem is presented.1 The data needed, semantic PP interpretation rules and an annotated corpus, is described first.
We consider the problem of question- focused sentence retrieval from complex news articles describing multi-event stories published over time.
To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.
This paper addresses an important problem in Example-Based Machine Translation (EBMT), namely how to measure similarity between a sentence fragment and a set of stored examples.
Training classifiers to perform text categorization on abstracts is one way to accomplish this task.
We present a method for improving text classification for biological journal abstracts by generating additional text features using the knowledge represented in a biological concept hierarchy (the Gene Ontology).
This paper describes the input specification language of the WAG Sentence Generation system.
We present two methods for improving performance of person name recognizers for email: email- specific structural features and a recall- enhancing method which exploits name repetition across multiple documents.
We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton.
We create a word-trie, transform it into a minimal DFA, then identify hubs.
The surface forms of applicable morphophonemic transformations are then derived using finite state machines.
This investigation proposes an approach to modeling the discourse of spoken dialogue using semantic dependency graphs.
Text Summarization Evaluation (SUMMAC) has established definitively that automatic text summarization is very effective in relevance assessment tasks.
The normalized representation can be used to detect paraphrases in texts.
Normalization and paraphrase detection tasks are built on top of a robust analyzer for English and are exclusively achieved using symbolic methods.
This paper presents an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts.
We present a new composite similarity metricthat combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units.
Several grammars have been proposed for modeling RNA pseudoknotted structure.
In this paper, we focus on multiple context-free grammars (MCFGs), which are natural extension of context-free grammars and can represent pseudoknots, and extend a specific subclass of MCFGs to a probabilistic model called SMCFG.
We present ongoing work on prosody predic-tion for speech synthesis.
The predic-tion is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neigh-bour algorithm or an analogy-based approach.
This paper presents a new method for translating a term-list by using a corpus in the target language.
We discuss a computer implementation of these representations using the Semantic Network Processing System (SNePS) and an ATN parser-generator with a question-answering capability.
We first define a word similarity measure based on the distributional pattern of words.
The similarity measure allows us to construct a thesaurus using a parsed corpus.
We propose a new framework to mine key phrase translations from web corpora.
We retrieve mixed- language web pages based on the expanded queries.
Finally, we extract the key phrase translation from the second- round returned web page snippets with phonetic, semantic and frequency- distance features.
This paper introduces a grammar formalism specifically designed for syntax-based statistical machine translation.
A Dependency Insertion Grammars (DIG) is a generative grammar formalism that captures word order phenomena within the dependency representation.
Synchronous Dependency Insertion Grammars (SDIG) is the synchronous version of DIG which aims at capturing structural divergences across the languages.
We then introduce a probabilistic extension of SDIG.
We finally evaluated our current implementation of a simplified version of SDIG for syntax based statistical machine translation.
For example, with a gene name dictionary, wecan identify the gene names contained in an article.
We propose a corpus-based method (Biber,1993; Nagao,1993; Smadja,1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase.
The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences.Keywords: Thai language, classifier, corpus-based method, Noun Classifier Associations (NCA)
This paper presents a Unicode based Chinese word segmentor.
Analysis grammar of Japanese in the Mu-project Is presented.
The proposed tool supports semi-automatic tagging.
This paper sketches some basic features of the SYNPHONICS account of the computational modelling of incremental language production with the example of the generation of passive sentences.
We differentiate between two possible kinds of stimuli within the generation process that trigger the formation of passive sentences: a Formulator-external stimulus and a Formulator-internal one.
This paper presents a multi-layered Question Answering (Q.A.)
capabilities with the possibility of processing complex questions.
This paper discusses aspects of the planning of explanatory texts for logic based systems.
It presents a method for deriving Natural Language text plans from Natural Deduction-based structures.
at Grenoble.
We propose a paradigm for concurrent natural language generation.
In order to represent grammar rules distributively, we adopt categorial unification grammar (CUG) where each category owns its functional typo.
We augment typed lambda calculus with several new combinators, to make the order of A-conversions free for partial / local processing.
Th.e concurrent calculus is modeled with Chemical Abstract Machine.
This paper describes an extension to the hidden Markov model for part-of-speech tagging using second-order approximations for both contextual and lexical probabilities.
Achieving this goal requires identification of not only the different topics in the documents but also of the particular flow of these topics.Our approach to content similarity evaluation employs n-grams of lexical chains and measures similarity using the cosine of vectors of n-grams of lexical chains, vectors of tf*idf-weighted keywords, and vectors of unweighted lexical chains (unigrams of lexical chains).
Our results show that n-grams of unordered lexical chains of length four or more are particularly useful for the recognition of content similarity.
This paper addresses the task of extract-ing opinions from a given document collection.
We also find that partitioning the data may help memory-based learning.
In this research, naive, syntax-based disambiguation is attempted by assigning each word a part-of-speech tag and by enriching the 慴ag-ofwords?data representation often used for document clustering with synonyms and hypernyms from WordNet.
In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences.
In this paper, a language tagging template named POC-NLW (position of a character within an n-length word) is presented.
In this method, the basic word segmentation is based on n-gram language model, and a Hidden Markov tagger based on the POC-NLW template is used to implement the out-of-vocabulary (OOV) word identification.
We present a variant of 'FAGG, called synchronous TAGs, which characterize correspondences between languages.
Various lexical and syntactic fea-tures are derived from parse trees and used to derive statistical clas-sifiers from hand-annotated training data.
A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task.
We exhibit a linear-time chart parsing algorithm with a low grammar constant.
Generation procedure in SEMSYNThis section summarizes the SEMSYN genration procedure.
We propose a knowledge-poor method that exploits the sentencial context of words for extracting similarity relations between them as well as semantic in nature word clusters.
Distributional similarity is a useful notion in estimating the probabilities of rare joint events.
Here, we examine the tradeoffs between model size and prediction accuracy for cluster-based and nearest neighbors distributional models of unseen events.
The paper describes a method to process recursive noun phrases with finite-state cascades.
A coarse match between voiced segments of speech and voiced segments of the phonetic spelling of the utterance is executed by dynamic programming as for approximate string matching.
Issues to be considered include comparative inflections, left recursion and other forms of nesting, extraposition of comparative complements, ellipsis, the wh element "how", and the translation of normalized parse trees into logical form.
An experimental system for dialogue structure analysis based on a new type plan recognition model for spoken dialogues has been implemented.
Anticipating the availability of large question- answer datasets, we propose a principled, data- driven Instance-Based approach to Question Answering.
We learn models of answer type, query content, and answer extraction from clusters of similar questions.
In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model.
Over the past few years, HNC has developed a neural network based, vector space approach to text retrieval.
In this paper, we argue how the richer vocabulary for lexical semantics proposed in Pustejovsky's "Generative Lexicon" theory allows one to explore the role of lexical information in such cases, and therefore sheds more light on the distinction between lexical inferences, which follow from defaults associated with lexical items and rules of composition, and pragmatic inferences, which depend on reasoning with respect to the context of the utterance.
GOD (General Ontology Discovery) is an unsupervised system to extract semantic relations among domain specific entities and concepts from texts.
This paper describes acquisition of English surface case frames from it corpus, based on a gradual knowledge acquisition approach.
In this paper I outline a normal form system for a sequent formulation of the product-free associative Lambek Calculus.
We propose an apparently minor extension to Kay's (1985) notation for describing directed acyclic graphs (DAGs).
This paper introduces a methodology to analyse and resolve cases of coreference in dialogues in English and Portuguese.
Traditional grammars classify words according to generic syntactic functions or morphological characteristics.
Correlational Grammar is an attempt in that direction.
It exemplifies this approach in two ares of grammar: predicative adjoctives and transitive verbs.
Speech recognition errors are inevitable in a speech dialog system.
We present an overview of the underlying dimensions that were used in describing the semantics and pragmatics of the Dutch subordinating conjunctions.
Traditionally, word sense disambiguation (WSD) involves a different context model for each individual word.
A context clustering scheme is developed within the Bayesian framework.
To achieve this task, we advocate that Segmented Discourse Representation Theory (SDRT) is a most expressive discourse framework.
This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation.
MAP adaptation can also be based on either supervised or unsupervised adaptation data.
This paper deals with aspects of the resolution of deictic and elliptic expressions that are related to gestures.
The first approach uses click free mouse gestures for deictic pointing, while manipulative gestures are performed by using mouse button events as is usual in graphic interfaces.
Syntactic knowledge is important for pronoun resolution.
In this paper, we present a method that automatically constructs a Named Entity (NE) tagged corpus from the web to be used for learning of Named Entity Recognition systems.
We use an NE list and an web search engine to collect web documents which contain the NE instances.
This paper reports on work carried out to de-velop a spelling and grammar corrector for Dan-ish, addressing in particular the issue of how a form of shallow parsing is combined with er-ror detection and correction for the treatment of context-dependent spelling errors.
This has particularly been a problem with WordNet which is widely used for word sense disambiguation (WSD).
We examine the utility of speech and lexical features for predicting student emotions in computer- human spoken tutoring dialogues.
We then extract acoustic-prosodic features from the speech signal, and lexical items from the transcribed or recognized speech.
We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions.
The central questions are: How useful is information about part-of-speech frequency for text categorisation?
In this paper we introduce an empirical approach to the semantic interpretation of superlative adjectives.
We present a corpus annotated for superlatives and propose an interpretation algorithm that uses a wide-coverage parser and produces semantic representations.
Va...rious devices for the improvement of phrase structure grammars (PSG) have been suggested recently.
generate non_grammatical sent_ ences.
In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations.
We present the computational, linguistic and ergonomic aspects of the mock-up, and discuss them in the perspective of building an operational prototype in the Inture.KeywordsInteractive MT, DBMT for monolingual author, Interactive disambiguation, Production of disambiguation dialogues, Distributed architecture, Whiteboard approach
We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters.
We present the technique of Virtual Annotation as a specialization of Predictive Annotation for answering definitional What is questions.
Virtual Annotation uses a combination of knowledge-based techniques using an ontology, and statistical techniques using a large corpus to achieve high precision.KeywordsQuestion-Answering, Information Retrieval, Ontologies
The method is used within a text-to-speech system to help generate pronunciations of unknown words.
Based on this approach, we develop a client-side personalized web search agent PAIR (Personalized Assistant for Information Retrieval), which supports both English and Chinese.
This empirical study attempts to find answers to the question of how a natural language (henceforth NL) system could resolve attachment of prepositional phrases (henceforth PPs) by examining naturally occurring PP attachments in typed dialogue.
We then proceed to show how to define a SSTC with a Structural Correspondence Static Grammar (SCSC), and which constraints to put on the rules of the SCSG to get a "natural" SSTC.linguistic	descriptors,	discontinuousconstituents, discontinuous phrase structure grammars, structured string-tree correspondences, structural correspondence static grammarsAbrevia,..ioes: DPSG, MT, NI., SSTC, STOP.Ordered trees, annotated with simple labels or complex "cecorations" (property lists), are widely used for representing natural language (NL) utterances.
In the last part, we show how to define a SSTC with a Structural Correspondence Static Grammar (SCSS), and which constraints to put on the rules of the SCSG to get a "natural" SSTC.I.
Two new parsing algorithms for context-free phrase structure grammars are presented which perform a bounded amount of processing per word per analysis path, independently of sentence length.
This paper designs a novel lexical hub to disambiguate word sense, using both syntagmatic and paradigmatic relations of words.
It only employs the semantic network of WordNet to calculate word similarity, and the Edinburgh Association Thesaurus (EAT) to transform contextual space for computing syntagmatic and other domain relations with the target word.
Frequency-based approaches with and without dictionary are proposed to extract formulation rules of named entities for individual languages, and transformation rules for mapping among languages.
An application of the results on cross language information retrieval is also shown.
In Chinese, zero anaphors occur frequently.
Syntactic tagging of words in a running text for detection of anaphora requires specification of argument structure for verbal elements.
We describe the use of energy function op-timisation in very shallow syntactic pars-ing.
The rules are con-textual constraints for resolving syntactic ambiguities expressed as alternative tags, and the statistical language model consists of corpus-based n-grams of syntactic tags.
In this paper a sentence compression tool is described.
We propose a path-based transfer model for machine translation.
A semi-automatic procedure of linguistic knowledge acquisition is proposed, which combines corpus-based techniques with the conventional rule-based approach.
We show that for context-sensitive spelling correction the Web Corpus results are better than using a search engine.
This paper presents an evaluation of indirect anaphor resolution which considers as lexical resource the semantic tagging provided by the PALAVRAS parser.
This paper proposes an unsupervised learning model for classifying named entities.
This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classifica-tion tasks.
This paper explores the determinants of adjective-noun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpus-based variables: co-occurrence frequency of the adjective-noun pair, noun frequency, conditional probability of the noun given the adjective, the log-likelihood ratio, and Resnik's (1993) selectional association measure.
Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.
In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.
Our experiments show that log-linear models significantly outperform IBM translation models.
The last stage constructs the surface string using knowledge about syntax, morphology, and style.
A simple approach to address the semantic class of the particle verb is introduced.
Three models (and several variants) are trained and tested: an n-gram language model, a discriminative maximum entropy model using structural information (and incorporating the language model as a separate feature), and finally an SVM ranker trained on the same feature set.
In this paper we report on several issues arising out of a first attempt to annotate task-oriented spo-ken dialog for rhetorical structure using Rhetorical Structure Theory.
The purpose of this paper is to compare different ways of adopting reason-maintenance techniques in incremental parsing (and interpretation).
A reason- maintenance system supports incremental formation and revision of beliefs.
Moreover, an assumption-based reason-maintenance system (ATMS) can be used to support efficient comparisons of (competing) interpretations.
Automatic recognition of Arabic dialectal speech is a challenging task because Arabic dialects are essentially spoken varieties.
At first glance, generalized phrase structure grammar (GPSG) appears to be a blessing on two counts.
metarules and the theory of syntactic features) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG.
This paper examines efficient predictive broad- coverage parsing without dynamic programming.
In contrast to bottom-up methods, depth-first top-down parsing produces partial parses that are fully connected trees spanning the entire left context, from which any kind of non-local dependency or partial semantic interpretation can in principle be read.
We contrast two predictive parsing approaches, top- down and left-corner parsing, and find both to be viable.
the construction of domain- independent lexica.
We compute the consensus alignment using a multi-sequence alignment algorithm used for DNA sequence alignment.
We present an application of this technique to bootstrap bilingual data for the general domain of instant messaging.
We train hierarchical statistical translation models on the bootstrapped bilingual data and show that the resulting statistical translation model outperforms each individual off-the-shelf translation system.
We introduce total rank distance and scaled total rank distance, we prove that they are metrics and investigate their max and expected values.
We will concentrate on multiword expressions (MWE), particularly on multiword nouns, (i) illustrating their most relevant morphological features, and (ii) pointing out the methods and techniques adopted to generate the inflected forms from lemmas.
This paper describes a fully-automated real- time broadcast news video and audio processing system.
The system combines speech recognition, machine translation, and cross- lingual information retrieval components to enable real-time alerting from live English and Arabic news sources.
The computational model, called Augmented Dependency Grammar (ADG), formulates not only the linguistic dependency structure of sentences but also the semantic dependency structure using the extended deep case grammar and field-oriented fact-knowledge based inferences.
Legato based on the ADG framework, constructs semantic dependency structure of Japanese input sentences by feature-oriented dependency grammar rules as main control information for syntactic analysis, and by semantic inference mechanism on a object fields' fact knowledge base.
The second component, Crescendo, extracts a conceptual structure about facts from the semantic dependency structure through logicalinterpretation on the language-particular semantic dependency using knowledge based inferences.Input SentenceIPMorphological AnalysisWord List IP Dependency structure AnalysisSemantic Dependency / 11, Structure 	)Inference EngineThesaurus Knowledge BaseLegato:Crescendo: 'Conceptual StructureAnalysisConceptual DependencyStructure Fig.
1	VENUS Analysis Module198
This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules.
The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check.The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes.
The phrase-break detector assigns phrase breaks using part-of-speech (POS) information.
In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes.
In this paper we tackle this problem by learning rules that generalize the state-based strategy.
This paper describes a practical method of automatic simultaneous interpretation utilizing an example-based incremental transfer mechanism.
We primarily show how incremental translation is achieved in the context of an example-based framework.
Finally, we propose a scheme for automatic simultaneous interpretation exploiting this example-based incremental translation mechanism.
This paper examines language similarity in messages over time in an online community of adolescents from around the world using three computational measures: Spearman抯 Correlation Coefficient, Zipping and Latent Semantic Analysis.
This paper describes to what extent deep processing may benefit from shallow processing techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad梒overage unification梑ased grammar of Spanish.
We have collected, transcribed and analyzed over 8 hours of human-human interactive problem solving dialogue in the air travel planning domain, including traveler-agent dialogues and the more constrained agent-airline dialogues.
In this paper, we present a learning approach for coreference resolution of noun phrases in unrestricted text.
Listen to Ramesh.
In this paper we consider the problem of assigning a probability distribution to alternative sets of coreference relationships among entity descriptions.
We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions.
In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs.
In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs.
This paper presents two word segmentation (WS) systems and a named entity recognition (NER) system in France Telecom R&D Beijing.
The one system of WS is for open tracks based on n- gram language model and another one is for closed tracks based on maximum entropy approach.
The NER system uses a hybrid algorithm based on Class-based language model and rule-based knowledge.
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classification (maximum entropy) with linguistic information.
Minimal Recursion Semantics (MRS) is the standard formalism used in large-scale HPSG grammars to model underspecified semantics.
We have established a phonotactic language model as the solution to spoken language identification (LID).
A voice tokenizer converts a spoken document into a text-like document of acoustic tokens.
We apply latent semantic analysis to the vectors, in the same way that it is applied in information retrieval, in order to capture salient phonotactics present in spoken documents.
This work explores computing distributional similarity between sub-parses, i.e., fragments of a parse tree, as an extension to general lexical distributional similarity techniques.
In the same way that lexical distributional similarity is used to estimate lexical semantic similarity, we propose using distributional similarity between sub- parses to estimate the semantic similarity of phrases.
We present some novel machine learning techniques for the identification of subcategorization informa-tion for verbs in Czech.
We show how the learning algorithm can be used to dis-cover previously unknown subcategorization frames from the Czech Prague Dependency Treebank.
In this paper, we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation.
To cope with sparseness, we use N-best phrase alignments and bilingual phrase clustering, and investigate a variety of combinations of conditioning factors.
The two current approaches to language generation, template-based and rule-based (linguistic) NLG, have limitations when applied to spoken dialogue systems, in part because they were developed for text generation.
In this paper, we propose a new corpus-based approach to natural language generation, specifically designed for spoken dialogue systems.
We describe an approach to surface generation designed for a "pragmatics-based" dialogue system.
We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features.
The paper discusses three different kinds of syntactic ill-formedness: ellipsis, conjunctions, and actual syntactic errors.
In this paper we propose a trainable method for extracting Chinese entity names and their relations.
We view the entire problem as series of classification problems and employ memory-based learning (MBL) to resolve them.
Rather than using length-based or translation-based criterion, a part-of-speech-based criterion is proposed.
We investigate the utility of an algo-rithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexi-cons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.
We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihood- based adaptation scheme for combining a trigger model with an -gram model.
We describe the application of such language models for automatic speech recognition.
We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via cross- lingual information retrieval and machine translation, proposed elsewhere.
We propose a method for dealing with semantic complexities occurring in information retrieval systems on the basis of linguistic observations.
In this paper we present LX-Suite, a set of tools for the shallow processing of Portuguese.
This suite comprises several modules, namely: a sentence chunker, a tokenizer, a POS tagger, featurizers and lemmatizers.
We investigate the impact of the precision/recall trade-off of information extraction on the performance of an offline corpus-based question answering (QA) system.
We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al.
We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies.
Speech recognition problems are a reality in current spoken dialogue systems.
We apply Chi Square (x2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns.
This paper studies the computational complexity of disambiguation under probabilistic tree-grammars as in (Bod, 1992; Schabes and Waters, 1993).
We present an approach to named entity recognition that uses support vector machines to capture transition probabilities in a lattice.
Proposed approaches can be classified into table lookup, linguistic, combinatorial and rule-based techniques.This paper proposes a new approach to enhance a rule-based Arabic stemmer.
In this paper, we propose a novel Cooperative Model for natural language understanding in a dialogue system.
We build this based on both Finite State Model (FSM) and Statistical Learning Model (SLM).
This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.
A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model.
We identify lexical semantic information auto-matically by exploiting the consistent correspondences between surface syntactic cues and lexical meaning.
The algorithm successfully combines temporal occurrence similarity across dates in news cor-pora, wide and local cross-language context similarity, weighted Levenshtein distance, relative frequency and burstiness similarity measures.
These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyper- links.
We further show that topic translation with online machine translation resources yields effective CL-SR.
In this paper, we tackle the problem of vocabulary selection, language modeling and pruning for inflective languages.
In this paper, we propose a method for constructing bigram LR tables by way of incorporating bigram constraints into an LR table.
Using a bigram LR table, it is possible for a GLR parser to make use of both bigram and CFG constraints in natural language processing.Applying bigram LR tables to our GLR method has the following advantages:(1) Language models utilizing bigram LR tables have lower perplexity than simple bigram language models, since local constraints (bigram) and global constraints (CFG) are combined in a single bigram LR table.
In this paper, we present an evaluation of the Link Grammar parser on a corpus consisting of sentences describing protein-protein interactions.
We introduce the notion of an interaction subgraph, which is the subgraph of a dependency graph expressing a protein-protein interaction.
In this paper, we propose an approach for content determination and surface generation of answers in a question-answering system on the web.
The DLSI-UA team is currently working on several word sense disambiguation approaches, both supervised and unsupervised.
This paper presents a view of different system results for Word Sense Disambiguation in different tasks of SENSEVAL-3.
In this paper we present a corpus-based study of NSUs.
We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only.
Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor.
Within the complex domain of HPSG parse selection, we show that ideas from ensemble learning can help further reduce the cost of annotation.
We describe how diathesis alternation patterns can be used to make coarse sense distinctions for Chinese verbs as a necessary step in annotating the predicate-structure of Chinese verbs.
We then discuss the representation scheme we use to label the semantic arguments and adjuncts of the predicates.
From 1000 newly treebanked Korean sentences we generate a deterministic shift-reduce parser.
We present a strictly lexical parsing model where all the parameters are based on the words.
Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification- based grammar (UBG).
1 IntroductionStochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar.
These grammars can incorporate virtually all kinds of linguistically important constraints (including non-local and non-context-free constraints), and are equipped with a statistically sound framework for estimation and learning.Abney (1997) pointed out that the non-contextfree dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of log- linear models for defining probability distributions over the parses of a unification grammar.
This paper presents an unsupervised batch learner for the quantity-insensitive stress systems described in Gordon (2002).
This position paper contrasts rhetorical structuring of propositions with intentional decomposition using communicative acts.
We det.cribe a set of modules that together make up a grapheme-to-phoneme conversion system for Dutch.
Modules include a syllabification program, a fast tnorphological parser, a lexical database, a phonological knowledge base, transliteration rules, and phonological rules.
In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FERGUS, a stochastic surface realizer.
We present an integrated probabilistic model for Japanese syntactic and case structure analysis.
This model selects the syntactic and case structure that has the highest generative probability.
We evaluate both syntactic structure and case structure.
We discuss a grammar development process used to generate the trees of the wide-coverage Lexicalized Tree Adjoining Grammar (LTAG) for English of the XTAG Project.
This paper shows how lexical choice during text generation depends on linguistic context.
We also discuss the limits of bigram statistical knowledge.
This paper investigates two approaches to speech segmentation based on different heuristics: the utterance-boundary strategy, and the predictability strategy.
This paper is concerned with the summarization of spontaneous conversations.
Previous work has focused on textual features extracted from transcripts.
This paper describes Vi-xfst, a visual interface and a development environment, for developing finite state language processing applications using the Xerox Finite State Tool, xfst.
The verb subcategorization frame information plays a major role of disambiguations in many NLP applications.
The discourse entities are used in intra- and extra-sentential pronoun resolution in BBN Janus.
The MTTK alignment toolkit for statistical machine translation can be used for word, phrase, and sentence alignment of parallel documents.
This paper examines the use of an unsupervised statistical model for determining the attachment of ambiguous coordinate phrases (CP) of the form n1 p n2 cc n3.
Attention on constraint-based grammar formalisms such as Head-driven Phrase Structure Grammar (liPsG) has focussed on syntax and semantics to the exclusion of phonology.
This paper investigates the incorporation of a non-procedural theory of phonology into HPSG, based on the 'one-lever model of Bird & Ellison (1992).
Prosodic domains, which limit the applicability of phonological constraints, are expressed in a prosodic type hierarchy modelled on imso's lexical type hierarchy.
We explore the differences in verb subcategorization frequencies across several corpora in an effort to obtain stable cross corpus subcategorization probabilities for use in norming psychological experiments.
In this paper an artificial life approach to the explanation of the shape of vowel systems is presented.
mildly context-sensitive grammars while keeping a polynomial parse time behavior.
Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues.
We describe our use of an existing re-source, the Mouse Anatomical Nomen-clature, to improve a symbolic interface to anatomically-indexed gene expression data.
We address the problem of automatically constructing a thesaurus by clustering words based on corpus data.
We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs, and propose a learning algorithm based on the Min-imum Description Length (MDL) Prin-ciple for such estimation.
We also evaluated the method by conduct-ing pp-attachment disambiguation ex-periments using an automatically con-structed thesaurus.
We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality?
This results in best-first processing of the idiomatic analysis.
Two models are discussed fot the lexical representation of idioms.
The connectionist model has an important advantage over the continuation class model: the conventionality principle follows naturally from the architecture of the connectionist model.Keywords: idiom processing, ambiguity resulution, two- level morphology, connectionism.
In the view presented here, lexical ambiguity resolution is an integral part of the same procedure that creates the semantic interpretation of a sentence itself.Keywords: lexical ambiguity, lexical semantics, cativositionality, lexical organization.
Collocation-based tagging and bracketing pro- grains have attained promising results.
OF COLING-92.
We present a new chart parsing method for Lambek grammars, inspired by a method for D- Tree grammar parsing.
for text classification problems in order to apply it to word sense disambiguation (WSD) problems.
In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2.
We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for large- vocabulary speech recognition.
The parser uses structural and lexical dependencies not considered by n- gram models, conditioning recognition on more linguistically-grounded relationships.
This approach to text planning can be conveniently implemented as a Functional Unification Grammar.
We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English, French, German, Japanese, Norwegian, and Urdu.1
We then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels.
The results also showed that the proposed method is effective in understanding misrecognition speech sentences and in improving speech translation results.
Answer Extraction (AE) over texts from highly restricted domains.
We focus on the problem of building large repositories of lexical conceptual structure (LCS) representations for verbs in multi-ple languages.
We are currently using these lexicons in an operational foreign language tutoring and machine translation.
We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible.
We derive the rhetorical structures of texts by means of two new, surface-form-based algorithms: one that identifies discourse usages of cue phrases and breaks sentences into clauses, and one that produces valid rhetorical structure trees for unrestricted natural language texts.
Finally an LFGbased multilingual parser simulating parsing strategies with ambiguous sentences.
In this paper, we address the issue of generating multilingual computational semantic lexicons from analysis lexicons, showing the necessity of relying on a conceptual lexicon.
The goal in this work is to develop a new language modeling approach based on randomly grown Decision Trees (DTs) and apply it to automatic speech recognition.
We study our RF approach in the context of -gram type language modeling.
This paper proposes a new paradigm for sentiment analysis: translation from text documents to a set of sentiment units.
We developed a high-precision sentiment analysis system at a low development cost, by making use of an existing transfer-based machine translation engine.
We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems.
We develop a model of preposition definitions in a machine-readable dictionary using the theory of labeled directed graphs and analyze the resulting digraphs to determine a primitive set of preposition senses.
We propose a generalization of Categorial Grammar in which lexical categories are defined by means of recursive constraints.
possible structural and lexical attributes.
Our approach is language-, structure-, and domain-independent because it only requires some shallow syntactic analysis (e.g., a POS-tagger and a phrase chunker).
This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases database.
Using Dana Scott's domain theory, we elucidate the nature of the feature systems used in augmented phrase-structure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar, lexical functional grammar and PATRII, and provide a denotational semantics for a simple grammar formalism.
This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using Natural Language Learning techniques: looking for characteristic statistical "language-signatures" in test corpora.
some kind of character-stream.
Our language-detection algorithm for symbolic input uses a number of statistical clues: data compression ratio, "chunking" to find character bit-length and boundaries, and matching against a Zipfian type-token distribution for "letters" and "words".
Our current research goal is to apply Natural Language Learning techniques to the identification of "higher-level" grammatical and semantic structure in a linguistic signal.
We apply a complexity theoretic notion of feasible learnability called "polynomial learnability" to the evaluation of grammatical formalisms for linguistic description.
This process created a data set that could be used to train a simple Hidden Markov Model (HMM) entity tagger.
In this paper we describe a new technique for parsing free text: a transformational grammar' is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.
This paper describes LINGUA - an architecture for text processing in Bulgarian.
First, the pre-processing modules for tokenisation, sentence splitting, paragraph segmentation, partof-speech tagging, clause chunking and noun phrase extraction are outlined.
1 IntroductionPrevious CoNLL shared tasks focused on NP chunking (1999), general chunking (2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005).
This shared task on full (dependency) parsing is the logical next step.
This paper presents a novel algorithm for efficiently generating paraphrases from disjunctive logical forms.
The algorithm is couched in the framework of Combinatory Categorial Grammar (CCG) and has been implemented as an extension to the OpenCCG surface realizer.
This paper describes a system for seg-menting Chinese text into words us-ing the MBDP-1 algorithm.
Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases.
We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems.
Our algorithm generates lists of non- anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.
We explored supervised machine-learning systems using Support Vector Machines to automatically classify images into six representative categories based on text, image, and the fusion of both.
In this paper, we present a method to han-dle complex sentences with the centering theory and describe our framework that, identifies the antecedents of zero pro-nouns in naturally occurring Japanese discourses.
The lightweight procedures in IdentiFinder are SGML recognition, hidden Markov models, finite state pattern recognition, and SGML output.By heavyweight processing, we mean procedures that depend on global evidence and involve deeper understanding.
We present a trainable model for identify-ing sentence boundaries in raw text.
in machine translation).
This paper describes an example-based correction component for Japanese word segmentation and part of speech labelling (AMED), and a way of combining it with a pre-existing rule-based Japanese morphological analyzer and a probabilistic part of speech tagger.Statistical algorithms rely on frequency of phenomena or events in corpora; however, low frequency events are often inadequately represented.
Here we report on an example- based technique used in finding word segments and their part of speech in Japanese text.
Probabilistic models have been effective in resolving prepositional phrase attachment ambiguity, but sparse data remains a significant problem.
Three thesauruses are compared on this task: two existing generic thesauruses and a new specialist PP thesaurus tailored for this problem.
We also compare three smoothing techniques for prepositional phrases.
We investigated both English and Chinese ad-hoc information retrieval (IR).
We also investigated clustering of output documents from term level retrieval.
Best results were obtained by combining retrievals of bigram and short-word with character representation.
Browman and Goldstein have developed a general model of the timing of articulatory gestures.
This paper describes a transformation-based learning approach to disfluency detection in speech transcripts using primarily lexical features.
This paper proposes a system of mapping classes of syntactic structures as instruments for automatic text understanding.
The system illustrated in Japanese consists of a set of verb classes and information on mapping them together with noun phrases, tense and aspect.
This paper describes a bootstrapping algorithm called Basilisk that learns high- quality semantic lexicons for multiple categories.
Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
We evaluate Basilisk on six semantic categories.
The purpose of the study is to develop an integrated knowledge management system for the domains of genome and nano-technology, in which terminology-based literature mining, knowledge acquisition, knowledge structuring, and knowledge retrieval are combined.
In order to treat lexical issues systematically in transfer-based MT systems, we introduce the concept of bilingual-sings which are defined by pairs of equivalent monolingual signs.
In this paper, we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems.
We first apply the training strategy of combining active learning and self-training (Tur et al., 2005) for topic classifier.
Also, we propose a practical method for bootstrapping the topic-dependent semantic classifiers from a small amount of labeled sentences.
This paper investigates the correlation between acoustic confidence scores as returned by speech recognizers with recognition quality.
In this paper we propose an integration of a selforganizing map and semantic networks from WordNet for a text classification task using the new Reuters news corpus.
The Hypernym relation in WordNet supplements the neural model in classification.
We present an overview of TARSQI, a modular system for automatic temporal annotation that adds time expressions, events and temporal relations to news texts.
The paper describes how the information about the semantic interpretation of PPs is represented in the lexicon and in PP interpretation rules and how this information is used during semantic analysis.
Moreover, we report on experiments that evaluate the impact of using this information about PP interpretation on the CLEF question answering task.
We present an approach to natural language understanding based on a computable grammar of constructions.
A grammar is a set of constructions.
We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology.
To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others.
To determine proper suffixes for a given adjective-verb pair, we have examined the verbal features involved in the theory of Lexical Conceptual Structure.
We make preliminary comparisons with an analytical constraint-based approach to modeling loanword formation.An intended application for the NN parser, was to devise an English-Japanese proper or place-name translator, which would map an English phoneme sequence into Katakana(e.g.
We investigated automatic action item detection from transcripts of multi-party meetings.
We provide two different methods for bounding search when parsing with freer word-order languages.
In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods.
Multimodal grammars provide an expressive formalism for multimodal integration and understanding.
This paper gives an overview of our work on statistical machine translation of spoken dialogues, in particular in the framework of the V~RBMoBiL project.
AN APPLICATION OF COMPUTER TECHNIQUES TO ANALYSISOF THE VERB PHRASE IN HINDI AND ENGLISH:A Preliminary ReportDr.
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).
An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.
We present a novel controlled natural language interface to temporal databases, based on translating nat-ural language questions into SQL/Temporal, a temporal database query language.
The semantic analysis is done using a novel theory of the semantics of tempo-ral questions, focusing on the role of temporal preposition phrases rather than the more tradi-tional focus on tense and aspect.
The method allows a user to explore a model of syntax-based statistical machine translation (MT), to understand the model抯 strengths and weaknesses, and to compare it to other MT systems.
The model is improved further by the use of class-based modifier- head bigrams constructed using semantic classes automatically extracted from a corpus.
We also present preliminary results obtained with a word prediction model integrating compound and simple word prediction.
Fourteen indicators that measure the frequency of lexico-syntactic phenomena linguistically related to aspectual class are applied to aspectual classification.
We automatically annotate training text with positive and negative examples of fact extractions and train Rote, Naive Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages.
A novel cross-field bootstrapping method leverages data interdependencies to yield improved performance.
Theories of discourse structure hypothesize a hierarchical structure of discourse segments, typically tree-structured.
In this paper, we explore prosodic cues to discourse segmentation in human- computer dialogue.
Using data drawn from 60 hours of interactions with a voice-only conversational spoken language system, we identify pitch and intensity features that signal segment boundaries.
We introduce a new model of selectional preference induction.
CABs pose conceptual constraints on the formation of complex RefOs in general.
A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guess- Mg heuristic, a most-frequent heuristic, and a co-occurrence heuristic.
This technique generalizes and exemplifies a new and original use of an existing concept of "proper guides" recently proposed in literature for controlling top-down left-to-right (TDLR) execution in logic programs.
Perhaps the best known example of this is using the IDLIZ derivation for left-recursive rules.
This paper describes a hybrid model that combines a rule-based model with two statistical models for the task of POS guessing of Chinese unknown words.
In this paper I elaborate a model of competence for corpus-based machine translation (CBMT) along the lines of the representations used in the transla-tion system.
This paper explores the segmentation of tutorial dialogue into cohesive topics.
A latent semantic space was created using conversations from human to human tutoring transcripts, allowing cohesion between utterances to be measured using vector similarity.
A novel moving window technique using orthonormal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task.
DTG involve two composition operations called subsertion and sister-adjunction.
This paper is concerned with the identification of two semantically close categories ?temporal locating adverbials and time-denoting expressions.
This paper presents experiments performed on lexical knowledge acquisition in the form of verbal argumental information.
The system obtains the data from raw corpora after the application of a partial parser and statistical filters.
This work presents a model for learning inference	procedures	for	storycomprehension	through	inductivegeneralization	and	reinforcementlearning, based on classified examples.
In this paper, we outline a theory of referential accessibility called Veins Theory (VT).
To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.
In this paper, we describe our hybrid approach to two key NLP technologies: biomedical named entity recognition (Bio-NER) and (Bio-SRL).
In Bio-NER, our system successfully integrates linguistic features into the CRF framework.
In addition, we employ web lexicons and template-based post-processing to further boost its performance.
We only annotate the predicate-argument structures (PAS抯) of thirty frequently used biomedical verbs (predicates) and their corresponding arguments.
Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) machine- learning model.
Grammaticality is a continuum phenomenon with many dimensions.
We refer the anomalous terms used in such context as network informal language (NIL) expres-sions.
We propose to study NIL expressions with a NIL corpus and investigate techniques in processing NIL ex-pressions.
Two methods for Chinese NIL ex-pression recognition are designed in NILER system.
We define a new learning task, minimum average lookahead grammar induction, with strong potential implications for incremental parsing in NLP and cognitive models.
Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours.
In this paper, we consider a number of algo-rithms for estimating the parameters of ME mod-els, including iterative scaling, gradient ascent, con-jugate gradient, and variable metric methods.
Application-specific data are collected with the help of Wizard-of-Oz techniques.
This paper proposes a new error-driven HM3/1- based text chunk tagger with context-dependent lexicon.
Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry.
This implements an approach to portable grammar-based language modelling in which all models are derived from a single linguistically motivated unification grammar.
In this article we report on a double-blind experiment with a surface- oriented morphosyntactic grammatical representation used in a large-scale English parser.
ATRS1 is an automatic transcription reconstruction system that can produce near-literal transcriptions with almost no human labor.
Processing stages include lexical lookup, syntactic parsing, semantic analysis, and pragmatic analysis.
In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
In oursystem, the resolution of anaphoric ambiguities is done uniquely by the semantic analyzer.
We have proposed a technique for labelling ambiguities in texts and in dialogue transcriptions, and experimented it on multilingual data.
This paper shows that inference rules with temporal constraints can be acquired by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun co- occurrences.
Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.
We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views.
In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.
This paper accompanies a demo of the GoDiS sys-tem.
An assumption shared by many theories of discourse is that discourse structure constrains anaphora resolution (cf.
The aim of this paper is (i) to show that this assumption also applies to multiple VP ellipsis (VPE), (ii) to argue that other levels of linguistic information (such as syntax and semantics) interact with discourse structure in determining multiple VPE acceptability and (iii) to make these intuitions precise by providing a unification-based account of multiple VPE resolution.
A deductive approach is used to predict vowel and consonant places of articulation.
(PDG), and proposes the 揋raph Branch Algorithm?for computing the optimum dependency tree from a DF.
In this paper we report results of a supervised machine-learning approach to Chinese word segmentation.
This paper describes Multi-Modal-Method, a design method for building grammar-based multi modal systems.
We introduce a first-order version of Categorial Grammar, based on the idea of encoding syntactic types as definite clauses.
Our approach to encoding types-as definite clauses presupposes a modification of standard Horn logic syntax to allow internal implications in definite clauses.
This paper describes a hybrid proposal to combine n-grams and Stochastic Context-Free Grammars (SCFGs) for language modeling.
A classical n-gram model is used to capture the local relations between words, while a stochas-tic grammatical model is considered to repre-sent the long-term relations between syntactical structures.
In order to define this grammatical model, winch will be used on large-vocabulary complex tasks, a category-based SCFG and a probabilistic model of word distribution in the categories have been proposed.
This paper describes a heuristic algorithm capable of automatically assigning a label to each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a computational-semantic lexicon for treatment of lexical ambiguity.
We also describe an implementation of the algorithm for labeling definition sentences in Longman Dictionary of Contemporary English (LDOCE).
We introduce a purely applicative language (PAL) as an intermediate representation and an object-oriented computation mechanism for its interpretation.
It exploits a vector-space model developed in information retrieval research.
The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm.
We present an authoring system for logical forms encoded as conceptual graphs (CG).
This paper discusses approaches for visualizing the affective content of documents and describes an interactive capability for exploring emotion in a large document collection.
The UNL module of ETAP-3 naturally combine's the two major approaches accepted in machine translation: the transfer-based approach and the iiiterlingua approach.
The identification of genes in biomedical text typically consists of two stages: identifying gene mentions and normalization of gene names.
The system identifies human gene synonyms from online databases to generate an extensive synonym lexicon.
SenseClusters is a freely available word sense discrimination system that takes a purely unsupervised clustering approach.
This paper explores the relationships between a computational theory of temporal representation (as developed by James Allen) and a formal linguistic theory of tense (as developed by Norbert Hornstein) and aspect.
time, causality.
Assuming an ordered representation of the predicate-argument structure, this work proposes a Combinatory Categorial Grammar formulation of relating surface case cues to categories and types for correctly placing the arguments in the predicate- argument structure.
This is achieved by treating case markers as type shifters.
The classification method used is linear discriminatory analysis, based on a learning sample.
This paper describes the system MC-WSD presented for the English Lexical Sample task.
We present two translation systems experimented for the shared-task of 揥orkshop on Statistical Machine Translation,?a phrase-based model and a hierarchical phrase-based model.
Experiments showed that the hierarchical phrase- based model performed very comparable to the phrase-based model.
We also report a phrase/rule extraction technique differentiating tokenization of corpora.
We describe a basic Geo-coding service encompassing a geo-parsing tool and integrated digital gazetteer service.
Figure 1: The geo-coding process
This paper presents a class of dependency-based formal grammars (FODG) which can be parametrized by two different but similar measures of nonprojectivity.
Various feature descriptions are being employed in constrained-based grammar formalisms.
We incorporate context into the search for an optimal alignment in two complementary ways: learning rules for matching paragraphs using topic structure and further refining the matching through local alignment to find good sentence pairs.
We will elaborate on our architecture and the experimental results.Keywords: answer set driven IR, attribute- based classification, automatic knowledge base construction,.
Structural information is provided by such hierarchical prosodic constituents as Syllable (S), Metrical Foot (MF), Phonological Word (PW), Intonational Group (IG).
Onto these structures, phonological rules are applied such as the "letter梩osound" rules, automatic word stress rules,internal stress hierarchy rules indicating secondary stress,external sandhi rules, phonological focus assignment rules, logical focus assignment rules.
We present a supervised learning approach to identification of anaphoric and non-anaphoric noun phrases and show how such information can be incorporated into a coreference resolution system.
We describe an approach to semiautomatic lexicon development from machine readable dictionaries with specific reference to verbal diatheses, envisaging ways in which the results obtained can be used to guide word classification in the construction of dictionary databases.
We describe the MFCDCN algorithm, an environment-independent extension of the efficient SDCN and FCDCN algorithms developed previously.
lb deal with long-distance dependencies, Applicative Universal Grammar (AUG) proposes a new type of categorial rules, called superposition rules.
We are developing an Intelligent Network News Reader which extracts news articles for users.
This paper proposes a method to extract rules for anaphora resolution of Japanese zero pronouns from aligned sentence pairs.
Then resolution rules for Japanese zero pronouns are automatically extracted using the pairs of Japanese zero pronouns and translation equivalents of their antecedents in English and equivalent word/phrase pairs which were extracted from the aligned sentence pairs, based on the syntactic and semantic structure of the Japanese sentence.
In this paper, we will introduce the anaphoric component of the Mimo formalism.
In Mimo, the translation of anaphoric relations is compositional.
We present two methods for learning the struc-ture of personal names from unlabeled data.
This paper introduces a novel Support Vector Machines (SVMs) based voting algorithm for reranking, which provides a way to solve the sequential models indirectly.
In this paper' I describe the use of Danish pronouns and deictics in dialogues.
This paper describes an approach to using se-mantic representations for learning information extraction (IE) rules by a type-oriented induc-tive logic programming (11,1)) system.
Knowledge-based interlingual machine translation systems produce semantically accurate translations, but typically require massive knowledge acquisition.
We describe our experiences building spo-ken language interfaces to four demon-stration applications all involving 2- or 3-D spatial displays or gestural interac-tions: an air combat command and control simulation, an immersive VR tactical sce-nario viewer, a map-based air strike sim-ulation tool with cartographic database, and a speech/gesture controller for mobile robots.
We present a linear-time, bidirectional subsump-tion test for typed feature structures and demonstrate that (a) subsumption- and equivalence-based packing is applicable to large HPSG grammars and (b) average parse complexity can be greatly reduced in bottom-up chart parsing with comprehensive HPSG implementations.
In this paper we present an algorithm to anchor floating quantifiers in Japanese, a language in which quantificational nouns and numeral- classifier combinations can appear separated from the noun phrase they quantify.
The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases.
In this paper we present a novel feature- enriched approach that learns to detect the conversation focus of threaded discussions by combining NLP analysis and IR techniques.
Using the graph-based algorithm HITS, we integrate different features such as lexical similarity, poster trustworthiness, and speech act analysis of human conversations with feature- oriented link generation functions.
This paper describes the framework for a new abstraction method that utilizes event-units written in sentences.
This report outlines the principles of autodirective beamforming for acoustic arrays, and it describes two experimental implementations.
gz.
Relationships are formulated as discourse plans.
We present a method for automatically identifying verbal participation in diathesis alternations.
This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora.
While classical hybrid systems manually define local part-ofspeech patterns that lead to the identification of well-known multiword units (mainly compound nouns), our solution automatically identifies relevant syntactical patterns from the corpus.
We present a view of DATR as a language for defining certain kinds of partial functions by cases.
We show that this event identification task and a related task, identifying the semantic class of these events, can both be formulated as classification problems in a word-chunking paradigm.
Information extraction systems incorporate multiple stages of linguistic analysis.
We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger.
They are expressed in terms of special binary relations on trees called command relations.
Pre-processing includes garbage text removal and question segmentation.
We describe the grammar formalism used and report a parsing experiment which compared eight parsing strategies within the framework of chart parsing.
We take advantage of the intrinsic graphical structure of an ontology for representing a context.
This paper investigates the use of sentential pronouns in English and Norwegian.
In this paper I describe research devoted to developing techniques for building sublanguage lexicons via syntactic and statistical corpus analysis coupled with analytic techniques based on the tenets of a generative lexicon.
In this paper, we show that naive Bayes classification can be used to iden-tify non-native utterances of English.
We also char-acterize part-of-speech sequences that play a role in detecting non-native speech.
This paper introduces a new statistical approach to partitioning text automatically into coherent segments.
We present a method that takes as input a syntactic parse forest with associated constraint- based semantic construction rules and directly builds a packed semantic structure.
Inverse Document Frequency (IDF) is a popular measure of a word's importance.
In order to facilitate the use of a dictionary for language production and language learning we propose the construction of a new network-based electronic dictionary along the lines of Zock (2002).
This paper presents a novel method for unsupervised word sense disam-biguation, which combines multiple in-formation sources, including seman-tic relations, large unlabeled corpora, and cross-lingual distributional statis-tics.
A system for the automatic segmentation of German words into morphs was developed.
The main linguistic knowledge sources used by the system are a word syntax and a morph dictionary.
A means for converting the feature dependencies into a unification grammar is described wherein feature structures are projected on to unlabeled words.
an early corpus from the Sundial project in spoken language dialogue systems development.
At our institute a speech understanding and dialog system is developed.
This paper presents a human梞achine dialogue model in the field of task梠riented dialogues.
The paper proposes a multimodal interface for a real sales database application.
This paper presents the results of an experiment using machine-readable dictionaries (MRDs) and corpora for building concatenative units for text to speech (ITS) systems.
Theoretical questions concerning the nature of phonemic data in dictionaries are raised; phonemic dictionary data is viewed as a representative corpus over which to extract n- gram phonemic frequencies in the language.
A methodology is defined to compute phonemic n-grams for incorporation into a TTS system.
This paper explores the contribution of a broad range of syntactic features to WSD: grammatical relations coded as the presence of adjuncts/arguments in isolation or as subcategorization frames, and instantiated grammatical relations between words.
We present a novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a unification- based parser.
In this paper we discuss a mechanism for modifying context in a tutorial dialogue.
In the paper, we outline several types of PMMs and detail a particular PMM in a sample dialogue situation.
groups of words).
In this paper we study different improvements to the standard phrase-based translation system.
(ACL Workshop on Parallel Texts 2005).
We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue.
We focus on the issue of analyzing and responding to multi- sentential explanations.
Different from previous approaches, LEILA uses a deep syntactic analysis.
The system currently translates air travel planning queries from English to Swedish.
The methods include Linear Discriminant Analysis, Supervised Vector Quantization, Shared Mixture VQ, Deleted Estimation of Context Weights, NMI Estimation Using "N-Best" Alternatives, Cross- Word Triphone Models.
In this paper, we investigate cross language information retrieval (CUR) for Chinese and Japanese texts utilizing the Han characters ?common ideographs used in writing Chinese, Japanese and Korean (CJK) languages.
We discuss the importance of Han character semantics in document indexing and retrieval of the ideographic languages.
This paper presents a plan-based model of dialogue that combines world, linguistic, and contextual knowledge in order to recognize complex communicative actions such as expressing doubt.
We present a novel new word extraction method from Japanese texts based on expected word frequencies.
First, we compute expected word frequencies from Japanese texts using a robust stochastic N-best word segmenter.
One of the aims of automatic ex-- traction is to produce a thesaurus.
Developed extraction programs analyze the definition sentence in LDOCE with a pattern matching based algorithm.
shows suds a system for retrieving a Japanese dictionary.
We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
This paper describes a system for managing dialogue in a natural language interface.
The dialogue manager integrates information about segment types and moves into a hierarchical dialogue tree.
In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a language tutoring application for deaf learners of written English.
We describe a generation-oriented workbench for the Performance Grammar (PG) formalism, highlighting the treatment of certain word order and movement constraints in Dutch and German.
significant power to an NL system.
This paper presents an approach to pragmatic modeling in which metaplans are used to model that level of discourse structure for problem-solving discourse of the sort arising in NL interfaces to expert systems or databases.The discourse setting modeled by metaplans in this work is expert-assisted problem-solving.
In this paper, we outline the development of a system that automatically constructs ontologies by extracting knowledge from dictionary definition sentences using Robust Minimal Recursion Semantics (RMRS).
In this paper we present a family of kernel functions, named Syntagmatic Kernels, which can be used to model syntagmatic relations.
We evaluated the syntagmatic kernel on two standard Word Sense Disambiguation tasks (i.e.
In this paper we describe how infor-mation extraction technology has been used to build a summarisation system in the domain of occupational health and safety.
The core of the applica-tion is based on named entity recog-nition using pattern-action semantic grammar rules.
This paper describes applications of stochastic and symbolic NLP methods to treebank annotation.
Both the English and French parts of the corpus are analysed with a POS tagger and a robust parser.
We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g.
We compare vectors containing counts of trigrams of part-of-speech (POS) tags in order to obtain an aggregate measure of syntax difference.
In all cases, the problems referred to a binary document classification.
The Segmentation Problem.
We show how it is possible to perform the analogical anal-ysis and generation of sentences, using a tree-bank and approximate pattern-matching.
In this paper, we introduce a generative probabilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transformation into the noisy output of an OCR system.
This paper proposes an Optimality Theory (Prince & Smolensky, 1993) [OT]-based generator of the Interlanguage [1111 syllabification of Korean speakers of English.
However, in order to treat some features of Korean accented English such as vowel epenthesis, segment modification (stop voicing, devoicing, nasalization, etc.
I will also use the ALIGN family of constraints to treat the Korean coda neutralization phenomena effectively.
We advance previous work by adopting a Maximum Entropy approach and by using Viterbi search to find the highest probability tag sequence for a given sentence.
Further we examine the use of syntactic pattern based re-ranking to further increase performance.
The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classi-fier to select likely sentence-level para-phrases from a large corpus of topic-clustered news data.
Our system combines two existing robust components: the WU S-II natural language understanding system and the SPOKESMAN generation system.
Finally, we look at the role of paraphrasing in a cooperative dialog system.
We consider here the task of linear thematic segmentation of text documents, by using features based on word distributions in the text.
Central to this WSD framework is the sense division and semantic relations based on topical analysis of dictionary sense definitions.
The process begins with an initial disambiguation step using an MRDderived knowledge base.
We also evaluated the new light-stemming algorithm within the context of information retrieval, comparing its performance with other stemming algorithms.
This paper deals with the automatic translation of route descriptions into graphic sketches.
This paper describes an application of active learning methods to the classification of phone strings recognized using unsupervised phonotactic models.
Topic-driven Discourse Structure is formalized which identifies mainly non-human zero pronouns as a by-product.
This paper presents a study aimed at evaluating the feasibility of an NLP-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neo-nates.
In this paper, we prove the decidability of the generation problem for those unification grammars which are based on context- free phrase structure rule skeletons, like e.g.
LFG and PATR-II.
Using encyclopedia resources and text information resources on the Web, we focus on the method of constructing domain knowledge base through technologies in natural language text analysis and machine learning.
This paper describes the Sentence Planner (se) in the HealthDoc project, which is concerned with the production of customized patient- education material from a source encoded in terms of plans.
We present an engine for text adventures ?computer games with which the player interacts using natural language.
Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit.
We propose a new method of classifying documents into categories.
We define for each category a finite mixture model based on soft clustering of words.
We consider the problem of parsing non-recursive context-free grammars, i.e., context-free grammars that generate finite languages.
Personal MT (PMT) is a new concept in dialogue based MT (DBMT) , which we are currently studying and prototyping in the LIMA project Ideally, a PMT system should run on PCs and be usable by everybody.
The paper briefly presents some of them (HyperText, distributed architecture, guided language, hybrid transfer/interlingua, the goes on to study in more detail the structure of the dialogue with the writer and the place of speech synthesis [1].KeyworPersonal Machine Translation, dialogue-based Machine Translation, iVian-Machine Dialogue:, Ambiguity Resolution, Speech Synthesis.lt rod twtionA first classificatio of MAT (Machine Aided Translation) systems is by user.
This paper presents the WordFrame model, a noise- robust supervised algorithm capable of inducing morphological analyses for languages which exhibit prefixation, suffixation, and internal vowel shifts.
They can access Page-X via modem over a standard ASCII terminal.
Data-driven grammatical function tag assignment has been studied for English using the Penn-II Treebank data.
We use three machine-learning methods to assign Cast3LB function tags to sentences parsed with Bikel抯 parser trained on the Cast3LB treebank.
In atask-based evaluation we generate LFG functional-structures from the functiontag-enriched trees.
Most question answering (QA) systems rely on both keyword index and Named Entity (NE) tagging.
speech recognition results.
We present the distilling method and guidelines for distillation.
This paper reports on an ongoing project that combines NLP with semantic web technologies to support a content-based storage and retrieval of medical pathology reports.
In this paper we discuss issues related to speeding up parsing with wide-coverage unification grammars.
As an alternative, we describe an optimisation technique that combines ambiguity packing at the constituent structure level with pruning based on local features.
We describe how a surface generator can produce complex sentences when given these features in input.
We use state-of-the-art NLP techniques to perform the linguistic annotation using xML-based tools and a combination of rule- based and statistical methods.
We focus here on the predictive capacity of tense and aspect features for a classifier.
We propose an algorithm to resolve anaphors, tackling mainly the problem of intrasentential antecedents.
Data-Oriented Translation (DOT), based on Data- Oriented Parsing (DOP), is a language-independent MT engine which exploits parsed, aligned bitexts to produce very high quality translations.
This paper proposes a method for assigning gestures to text based on lexical and syntactic information.
Two well-known phenomena in the area of pronoun binding are considered: Indirect binding of pronouns by indefinite NPs ("donkey sentences") and surface- syntactic constraints on binding ("weak cross-over").
We apply this method to English partof-speech tagging and Japanese morphological analysis, and show that the method performs well.
This paper describes a method for lan-guage independent extractive summariza-tion that relies on iterative graph-based ranking algorithms.
Moreover, we show how a meta-summarizer relying on a layered appli-cation of techniques for single-document summarization can be turned into an ef-fective method for multi-document sum-marization.
This paper compares two systems for computational morphological analysis of Dutch.
This paper describes a Chinese word segmentation system based on unigram language model for resolving segmen-tation ambiguities.
and 63 lexical entry templates (assigned to parts of speech (POSs) ).
A finite-state method, based on leftmost longest-match replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes.
The system consists of three specialized pattern-based tagging modules, a high-precision co-reference resolution module, and a configurable template generation module.
In this paper we propose and investigate Ontology Population from Textual Mentions (OPTM), a sub-task of Ontology Population from text where we assume that mentions for several kinds of entities (e.g.
This paper shows how a hypernym-hyponym based lexi-con for Swedish can be created directly from a news paper corpus.
An algorithm is presented for building partial hierarchi-cal structures from non domain-specific texts.
A new, flexible inference method for Horn logic program is proposed.
Chart- like parsing and semantic-head-driven generation emerge from this method.
This paper present a new method for parsing English sentences.
The parser called LUTE-EJ parser is combined with case analysis and ATNG-based analysis.
LUTE-EJ parser has two interesting mechanical characteristics.
This parser's features are (1)extracting a case filler, basically as a noun phrase, by ATNGbased analysis, including recursive case analysis, and (2)mixing syntactic and semantic analysis by using case frames in case analysis.
This paper presents results from experiments in automatic classification of animacy for Norwegian nouns using decision-tree classifiers.
The method makes use of relative frequency measures for linguistically motivated morphosyntactic features extracted from an automatically annotated corpus of Norwegian.
This paper presents a linguistic model for language understanding and describes its application to an experimental machine translation system called LUTE.
The language understanding model is an interactive model between the memory structure and a text.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
Simulations using artificial language stimuli demonstrate the capacity of the model to learn suffixation, prefixation, infixation, circumfixation, mutation, template, and deletion rules.
This paper describes recent MADCOW activities.
In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language.
We demonstrate the open-source LKB system which has been used to teach the fundamentals of constraint-based grammar development to several groups of students.
We present a semantic and pragmatic account of the anaphoric properties of past and perfect that improves on previous work by integrating discourse structure, aspectual type, surface structure and commonsense knowledge.
A novel aspect of our account is that we distinguish between two kinds of temporal intervals in the interpretation of temporal operators — discourse reference intervals and event intervals.
This distinction makes it possible to develop an analogy between centering and temporal centering, which operates on discourse reference intervals.
Our temporal property- sharing principle is a defeasible inference rule on the logical form.
This paper addresses the question whether metaphors can be represented in Word- Nets.
We present the lexical-semantic net for German "GermaNet" which integrates conceptual ontological information with lexical semantics, within and across word classes.
GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs.
An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented.
The model is a piecewise nonlinear transformation applied to the noisy speech feature.
The transformation is a set of multidimensional linear least-squares filters whose outputs are combined using a conditional Gaussian model.
This paper represents a status report on the MIT ATIS system.
This paper presents a lexicon-based approach to syntactic analysis, Lexicase, and applies it to a lexicon-driven computational parsing system.
The basic descriptive mechanism in a Lexicase grammar is lexical features.
Syntactic tree structures are representaed as networks of pairwise dependency relationships among the words in a sentence.
Section 2 describes the way in which grammatical information can be presented as s set of generalizations about classes of lexical items represented in a dependency-type tree format.
The rules of lexica se grammar proper are lexical rules, rules that express relations among lexical items and among features within lexical entries.
)Figure 1 lists the rule types in a lexicase grammar and their interrelationships.
We present a method for induction of concise and accurate probabilistic context- free grammars for efficient use in early stages of a multi-stage parsing technique.
The results provide direct evidence demonstrating that both grapheme frequency and grapheme entropy influence performance on pseudoword naming.
We discuss the implications of those findings for current models of phonological coding in visual word recognition.
This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the con-struction of large-scale knowledge sources.
We describe two experiments: one which ignored word-sense distinctions, re-sulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy.
Unification-based theories of grammar al-low to integrate different levels of linguis-tic descriptions in the common framework of typed feature structures.
We present an approach to a modular use of codescrip-tions on the syntactic and semantic level.
In the paper we describe the partitioning of grammatical information for the parsers and present results about the performance.
This paper proposes architecture of multilingual news summarizer, including monolingual and multilingual clustering, similarity measure among meaningful units, and presentation of summarization results.
We explore the relationship between question answering and constraint relaxation in spoken dialog systems.
In particular, we describe methods for dealing with the results of database queries in information- seeking dialogs.
We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.
We propose a support vector learning-based method employing target language corpus and bilingual dictionary data, and evaluate it over a English Japanese machine translation task.
We use IE patterns learned from the MUC-4 training set as anchors to identify domain-specific web pages and then learn new IE patterns from them.
A specialized transition networkmechanism, the interruptable transition network (ITN) is used to perform the last of three stages in a multiprocessor syntactic parser.
The most common statistic is n-grams measuring word cooccurrences in texts.
Collocation map is a sigmoid belief network that can be constructed from bigrams.
We compared the conditional probabilities and mutual information computed from bigrams and Collocation map.
Hearst [1991] suggests using syntactic information and partof-speech tagging to aid in the disambiguation.
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.
We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extra- sentential context.
More and more researchers have recognized the potential value of the parallel corpus in the research on Machine Translation and Machine Aided Translation.
An iterative algorithm based on degree of word association is proposed to identify the multiword units for Chinese and English.
This paper presents a computational model of how conversational participants collaborate in making referring expressions.
We use the primitive actions s-refer and s-attr.
In this part, we use linguistic and statistical methods to produce keywords from a stream of data.
We use the same extraction module based on linguistic and add a knowledge based system to deduce implicit keywords.
OF COLING-92.
In this paper, we present a parallel context sensitive graph rewriting formalism for a dependency-oriented generation grammar.
information extraction, text sum-marisation, document generation, machine translation, and second language learning).
We put forward a statistical language model that resolves these problems, does POS tagging, and can be used as the language model of a speech recognizer.
This paper proposes a new dialogue control method for spoken dialogue systems.
For speaker-independent speech recognition, speaker variation is one of the major error sources.
In this paper, a speaker-independent normalization network is constructed such that speaker variation effects can be minimized.
A codeword-dependent neural network is associated with each speaker cluster.
We will report on one of the two tasks in the IREX (Information Retrieval and Extraction Exercise) project, an evaluation-based project for Information Retrieval and Information Ex-traction in Japanese (Sekine and Isahara, 2000) (IREX Committee, 1999).
In this paper, the Named Entity (NE) task is reported.
The present study deals with conflict resolution process in metaphorical interpretation for the noun phrase.
By using production system couped with contex free parser (ELINGOL), theworking system called META桽IM is constructed to analyze the noun phrase metaphor.
We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees.
, This paper proposes a new disambiguation method for Japanese text input.
This method evaluates candidate sentences by measuring the number of Word Co-occurrence Patterns (WCP) included in the candidate sentences.
An automatic WCP extraction method is also developed.
A co- occurrence pattern matrix with semantic categories is built based on these WCP.
Using this matrix, the mean number of candidate sentences in Kana-to-Kanji translation is reduced to about 1/10 of those from existing morphological methods.1桰ntroductionFor keyboard input of Japanese, Kana-to-kanji translation method [Kawada,79] [Makino80] [A be861 is the most popular technique.
For this purpose, Dependency Localization Analysis (DLA) is used.
This identifies the word pairs having a definite dependency relationship using syntactic analysis and some heuristic rules.This paper will first describe collocational analysis, a new concept in Ka,na-to-Kanji translation, then the compilation of WCP dictionary, next the translation algorithm and finally translation experimental results.2.
are examples of WCP.
1 Concept of collocational analysis770
We report a comparative study of two methods for estimating word co- occurrence frequencies required by word similarity measures.
A lexicon-grammar is constituted of the elementary sentences ofa language.
Instead of considering words as basic syntactic unitsto which grammatical information is attached, we use simplesentences (subject-verb-objects) as dictionary entries.
N raises a question,- the lexicon-grammar of support verbs.
We present the structure of the lexicon-grammar built for French and we discuss its algorithmic implications for parsing.275
We end by proposing extensions to lexical correction and to some syntactic errors.
We present a new approach to topological parsing of German which is corpus-based and built on a simple model of probabilistic CFG parsing.
Besides the practical aspect of developing a robust and accurate topological parser for hybrid shallow and deep NLP, we investigate to what extent topological structures can be handled by context-free probabilistic models.
We discuss experiments with systematic variants of a topological treebank grammar, which yield competitive results.1
This paper outlines our strategy for dealing with spontaneous spoken input in a speech recognition system.
ARGUMENTATION AND THE SEMANTIC PROGRAM.
In this paper, we propose to model access to heterogeneous databases, by interpreting natural language queries into queries in formal languages such as SQL, OQL, and CPL by accounting for various language-specific constructions including join relations, path expressions, and object bindings with domain resources and a common lexicon, in a combinatory categorial grammar framework. '
Research in discourse processing has identified two representational requirements for discourse planning systems.
This paper describes a novel framework for interactive question-answering (Q/A) based on predictive questioning.
In this paper we discuss the use of multi- layered tagsets for dialogue acts, in the context of dialogue understanding for multiparty meeting recording and retrieval applications.
We then define MALTUS, a new tagset based on the ICSI-MR and Switchboard tagsets, which satisfies these requirements.
We present some experiments using MALTUS which attempt to compare the merits of integrated versus multi-level classifiers for the detection of dialogue acts.
In this paper, we introduce a new data representation format for language processing, the syntactic and semantic graphs (SSGs), and show its use for call classification in spoken dialog systems.
For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels).
At the core of our new structured language model is a fast context-sensitive and lexicalized PLCG parsing algorithm that uses dynamic programming.
In contrast to knowledge- rich sentence aggregation approaches explored in the past, this approach exploits statistical parsing and robust coreference detection.
We introduce the bilingual dual-coding theory as a model for bilingual mental representation.
Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation.
The Lambek categorial grammar is one representative of the grammar family under consideration.
Word sense disambiguation (WSD) is a difficult problem in natural language processing.
In this paper, a sememe co-occurrence frequency based WSD method was introduced.
In this method, Hownet was used as our information source , and a co-occurrence frequency database of sememes was constructed and then used for WSD.
The experimental result showed that this method is successful.Keywordsword sense disambiguation, Hownet, sememe, co-occurrence
SenseClusters is a freely available system that identifies similar contexts in text.
In this paper, we compare and contrast IDF with inverse sentence frequency (ISF) and inverse term frequency (ITF).
We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse.
In particular, we are investigating the problem of intonational assignment in synthetic speech.
This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken rlialogue agents.
Grammar Association is a technique for Machine Translation and Language Understanding introduced in 1993 by Vidal, Pieraccini and Levin.
This paper deals with the automatic translation of prepositions, which are highly polysemous.
Following cognitive principles of spatial conceptualization, we design, a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy.
IntroductionThis paper deals with a general phenomenon of (machine) translation.
We design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from the semantic sort hierarchy.
This paper presents a multi-neuro tagger that uses variable lengths of contexts and weighted inputs (with information gains) for part of speech tagging.
We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge.
It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process, producing a rule -based WSD model.
We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
In translation, we apply source sentence reordering on word level and use a reordering automaton as input.
We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints.
We further add weights to the reordering automata.
We propose an algorithm which ranks MWEs by their compositionality relative to a semantic field taxonomy based on the Lancaster English semantic lexicon (Piao et al., 2005a).
The semantic information provided by the lexicon is used for measuring the semantic distance between a MWE and its constituent words.
We compared the output of our tool with human judgments using Spearman抯 rank-order correlation coefficient.
This experiment demonstrates that a semantic lexicon can assist in MWE compositionality measurement in addition to statistical algorithms.
We propose an ontology-based framework for linguistic annotation of written texts.
In this paper we present hidden Markov models for Korean part-of-speech tagging, which consider Ko-rean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity.
We describe a hybrid SNN/HMM system that combines the speed and performance of our HMM system with the segmental modeling capabilities of SNNs.
The system is built around two separate neural network methodologies: context vectors and self organizing maps.
We present first results using paraphrase as well as textual entailment data to test the language universal constraint posited by Wu抯 (1995, 1997) Inversion Transduction Grammar (ITG) hypothesis.
In machine translation, parsing of long English sentences still causes some problems, whereas for short sentences a good machine translation system usually can generate readable translations.
We describe work in progress on a corpus-based tutoring system for education in traditional and formal grammar.
This paper concerns the discourse understanding process in spoken dialogue systems.
The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus.
We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure.
Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement.
In this paper, we address the issue of syntagmatic expressions from a computational lexical semantic perspective.
In particular, we focus on the place of lexical and semantic restricted co-occurrences.
From a processing viewpoint, we show how to generate/analyze syntagmatic expressions by using an efficient constraint- based processor, well fitted for a knowledge-driven approach.
This paper presents a constraint logic programming language cu-Prolog and introduces a simple Japanese parser based on Japanese Phrase Structure Grammar (JPSG) as a suitable application of cu-Prolog.cu-Prolog adopts constraint unification instead of the normal Prolog unification.
Such a clause is called Constraint Added Horn Clause (CAHC).
It enables a natural implementation of JPSG and other unification based grammar formalisms.From this April, Fujitsu Corporation 1 IntroductionProlog is frequently used in implementing natural language parsers or generators based on unification based grammars.
The models we use are structures built from intervals of time, events and individuals.
We present a classification-based word prediction model based on IGTREE, a decision-tree induction algorithm with favorable scaling abilities and a functional equivalence to n-gram models with back- off smoothing.
This paper addresses the problem of acquiring lexical semantic relationships, applied to the lexical entailment relation.
Our main contribution is a novel conceptual integration between the two distinct acquisition paradigms for lexical relations ?the pattern- based and the distributional similarity approaches.
Supervised machine learning was applied to monitor the performance of the rule-based method, using Memory Based Learning (MBL).
We have coupled the parser LCFLEX with a lexicon and ontology derived from two lexical resources, Verb- Net for verbs and CoreLex for nouns.
This suggests that current statistical parsing methods are sufficiently general to produce accurate shallow semantic annotation.
We describe our latest attempt at adaptive language modeling.
The other components are a selective unigram cache, a conditional bigram cache, and a conventional static trigram.
We introduce a novel search algorithm for statistical machine translation based on dynamic programming (DP).
During the search process two statistical knowledge sources are combined: a translation model and a bigram language model.
We present experimental results on the Verbmobil task.
Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words.
This paper discusses relationships among word pronunciations.
The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words.
Lexifanis is the first working tool for Modern Greek Language.
The goal of this work is recognizing opinionated and evaluative (subjective) language in text.
This paper focuses on disambiguating potentially subjective expressions in context, based on the density of other clues in the surrounding text.
We present a machine learning based method for identifying patients diagnosed with congestive heart failure and other related conditions by automatically classifying clinical notes.
The aim of this paper is to investigate how much the effectiveness of a Question Answering (QA) system was affected by the performance of Machine Translation (MT) based question translation.
We investigate the effects of lexicon size and stopwords on Chinese information retrieval using our method of short-word segmentation based on simple language usage rules and statistics.
We present a constraint-based case frame lexicon architecture for bi-directional mapping between a syntactic case frame and a semantic frame.
In this paper, we evaluate the results of the Antwerp University word sense dis-ambiguation system in the English all words task of SENSEVAL-2.
Association for Computational Linguistics.
We compare and contrast two different models for detecting sentence-like units in continuous speech.
The first approach uses hidden Markov sequence models based on N-grams and maximum likelihood estimation, and employs model interpolation to combine different representations of the data.
The second approach models the posterior probabilities of the target classes; it is discriminative and integrates multiple knowledge sources in the maximum entropy (maxent) framework.
Both models combine lexical, syntactic, and prosodic information.
In this paper we compare two interlin-gua representations for speech transla-tion.
In this paper, we present an unsupervised methodology for propagating lexical co- occurrence vectors into an ontology such as WordNet.
We use to specify schemata annotated rules and the LFG uniqueness, completeness and coherence principles.
This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars.
The principles of Dependency Unification Grammar (DUG) are discussed.
A unification-based parsing procedure is part of the formalism.
Correspondingly, three types of attributes are grouped together in each DRL-term: a lexeme, a syntagmatic role and a complex morpho-syntactic category.
In this paper, we describe our efforts to create a robust, large-scale lexical-semantic resource for the recognition and classification of expressions of commonsense psychology in English Text.
The algorithm which is used for deciding satisliability of a feature description is based on a restricted deductive closure construction for sets of literals (atomic formulas and negated atomic formulas).
The deductive closure construction is the direct proof-theoretic correlate of the congruence closure algorithm (cf.
The verb-noun sequence in Chinese often creates ambiguities in parsing.
Finally, we show how to translate Chinese norninals within a knowledge-based framework.
SYSTRAN抯 Chinese word segmentation is one important component of its Chinese-English machine translation system.
The Chinese word segmentation module uses a rule-based approach, based on a large dictionary and fine-grained linguistic rules.
Considering some differences between PPAs and other kinds of anaphors, such as personal or demonstrative pronouns, we define three knowledge sources (KSs) for PPA resolution: surface patterns (taking in account factors such as syntactic parallelism), possessive relationship rules and sentence centering.
We describe two "semantically-oriented" dependency-structure formalisms, 11-forms and S-forms.
Stochastic categorial grammars (SCGs) are introduced as a more appropriate formalism for statistical language learners to estimate than stochastic context free grammars.
This paper presents recent natural language work on HARC, the BBN Spoken Language System.
The HARC system incorporates the Byblos system [6] as its speech recognition component and the natural language system Delphi, which consists of a bottom-up parser paired with an integrated syntax/semantics unification grammar, a discourse module, and a database question-answering backend.
This paper presents a general computational method for automated inversion of a unification-based parser for natural language into an efficient generator.
In this paper the functional uncertainty machinery in LFG is compared with the treatment of long distance dependencies in TAG.
Lexico-semantic pattern matching, with rules that combine lexical analysis with ordering and semantic categories, is a good method for this form of analysis.
The explored approaches include using Model-1 conditional probability, a boosting strategy for lexicon probabilities based on importance sampling, applying Parts of Speech to discriminate English words and incorporating information of English base noun phrase.
This paper proposes the 揌ierarchical Directed Acyclic Graph (HDAG) Kernel?for structured natural language data.
We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function.
This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring.
This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning.
This paper describes a highly-portable multilingual question answering system on multiple relational databases.
We apply semantic category and pattern-based grammars, into natural language interfaces to relational databases.
Lexico-semantic pattern (LSP) and multi-level grammars achieve portability of languages, domains, and DBMSs.
MATES/EK is a transfer-based system and it has several subsystems that can be used to support other MT-developments.
(Kim, 1992)?Augmented Context Free Grammars for English Syntactic Analysis : We developed a set of augmented context free grammar rules for general English syntactic analysis and the analyzer is implemented using Tomita Lit parsing algorithm (Tomita, 1987).
Applications that can benefit from such an annotated corpus include information extraction (e.g., normalizing temporal references for database entry), question answering (answering 搘hen?questions), summarization (temporally ordering information), machine translation (translating and normalizing temporal references), and information visualization (viewing event chronologies).KeywordsAnnotation, temporal information, semantics, ISO-8601.
XMLencodingNon-XML EncodingFigure 3.
Using standard methods and formats established at LADL, and adopted by several European research teams to construct large- coverage electronic dictionaries and grammars, we elaborated for Portuguese a set of lexical resources, that were implemented in INTEX We descnbe the main features of such linguistic data, refer to their maintenance and extension, and give different examples of automatic text parsing based on those dictionaries and grammarsKeywords Text parsing, large-coverage dictionaries, computational lexicons; word tagging, information retneval.
This paper describes how a language-planning system can produce natural-language referring expressions that satisfy multiple goals.
Most statistical machine translation systems employ a word-based alignment model.
Linear algebraic technique called LSA/SVD is used to find co-relationships of sparse words.
Three variant estimation methods are sug-gested and they are evaluated for estimating unseen noun-verb co-occurrence probability.
Tied-mixture (or semi-continuous) distributions are an important tool for acoustic modeling, used in many high- performance speech recognition systems today.
Additionally, we describe an extension of tied mixtures to segment-level distributions.
The designed Chinese morphological analyzer contains three major functions, 1) to segment a word into a sequence of morphemes, 2) to tag the part-of-speech of those morphemes, and 3) to identify the morpho-syntactic relation between morphemes.
We propose a method of using associative	strength	among	morphemes,morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech.
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.
In this paper we define two intermediate models of textual entailment, which correspond to lexical and lexical-syntactic levels of representation.
We present three ways in which a natural language generator that produces textual descriptions of objects from symbolic information can exploit OWL ontologies, using M-PIRO抯 multilingual generation system as a concrete example.
We propose a novel method to predict the inter- paragraph discourse structure of text, i.e.
Our method combines a clustering algorithm with a model of segment 搑elatedness?acquired in a machine learning step.
The model integrates information from a variety of sources, such as word co-occurrence, lexical chains, cue phrases, punctuation, and tense.
Our method outperforms an approach that relies on word co-occurrence alone.
Improvements included use of a Hidden Markov Model (HMM) statistical classifier to identify the likely linguistic provenance of a surname, and application of language-specific rules to generate plausible spelling variations of names.
We focus here on the results obtained by starting with morphological analysis and proceeding to a grammatical (part-of-speech) tagging.In the proposed system, the vocalic ambiguity is detected by means of a double dictionary of voweled and non-voweled forms.
Currently, information architects create meta- data category hierarchies manually.
Among them are parallel multiple context-free grammars (pmcfg's) and lexical-functional grammars (lfg's).
Finite state translation systems (fts') were introduced as a computational model of transformational grammars.
In this paper, three subclasses of lfg's called nc-lfg's, dc-lfg's and fc-lfg's are introduced and the generative capacities of the above mentioned grammatical formalisms are investigated.
This algorithm relies on a combination of smoothing and linear segmentation together with the notion of word start groups.
For example, using the contextual words, instead of contextual parts of speech, enhances the prediction power for tagging parts of speech.
The generation of words in speech involves a number of processing stages.
In the computational model, lexical concepts figure in a semantic, spreading-activation network.The lexical concept is input to a process called lexical selection.
Lemmas are syntactic words.
In this paper, we will discuss one aspect of this type of work: designing the so-called Vietools to detect and correct spelling of Vietnamese texts by using a spelling database based on TELEX code.
We describe a new method for the representation of NLP structures within reranking approaches.
We make use of a conditional log杔inear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.
Next, a method for natural language transfer is presented that integrates translation examples (represented as typed feature structures with source-target indices) with linguistic rules and constraints.
We present a novel disambiguation method for unification-based grammars (UBGs).
In this article, we present a statistical approach to machine translation that is based on Data-Oriented Parsing: Data-Oriented Translation (DOT).
Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units.
This paper describes an algorithm for the resolution of discourse deictic anaphors, which constitute a large percentage of anaphors in spoken dialogues.
In this paper, we address the word alignment problem for statistical machine translation.
This paper describes a morphological analyzer which, when parsing a word, uses two sets of rules: rules describing the syntax of words, and rules describing facts about orthography.
This paper describes a general approach to the design of natural language interfaces that has evolved during the development of DATALOG, an English database query system based on Cascaded ATN grammar.
In this paper we present the results of the combination of stochastic and rule-based disambiguation methods applied to Basque languagel .
The methods we have used in disambiguation are Constraint Grammar formalism and an H1VIM based tagger developed within the MULTEXT project.
We show that the phrase-based translation engine we implemented benefits from Tree-Phrases.
We extended language modeling approaches in information retrieval (IR) to combine collaborative filtering (CF) and content-based filtering (CBF).
We first introduce a multinomial model that combines CF and CBF in a language modeling framework.
We then generalize the model to another multinomial model that approximates the Polya distribution.
We present a constraint-based morphological disambiguation system in which individual constraints vote on matching morphological parses, and disambiguation of all the tokens in a sentence is performed at the end by selecting parses that receive the highest votes.
We address these claims empirically in an important application domain for machine learning — text categorization.
The past decade has witnessed exciting work in the field of Statistical Machine Translation (SMT).
We propose Hidden Markov models with unsupervised training for extractive summarization.
Extractive summarization selects salient sentences from documents to be included in a summary.
Our method incorporates unsupervised training with clustering, into a probabilistic framework.
Text cohesion is modeled by the transition probabilities of an HMM, and term distribution is modeled by the emission probabilities.
Parameter training is carried out by the segmental K-means (SKM) algorithm.
We ex-tract a set of lexical, syntactic and onto-logical features and the corresponding noun-classifier pairs from a corpus and then train SVMs to assign classifers to nouns.
The cluster n-gram model is a variant of the n-gram model in which similar words are classified in the same cluster.
The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion.
The approach is based on the "distributional hypothesis".
We investigate the connection between part of speech (POS) distribution and content in language.
We define POS blocks to be groups of parts of speech.
For many years, statistical machine translation relied on generative models to provide bilingual word alignments.
The purpose of this paper is to automatically create multilingual translation lexicons with regional variations.
We focus on the production of efficient descriptions of objects, actions and events.
We present in this paper a series of induced methods to assign domain tags to WordNet entries.
Next we further examine the similarity between common lexical taxonomy and the semantic hierarchy of WordNet.
We propose this as the first step of wordnet expansion into a bona fide semantic network linked to real-world knowledge.0.
Introduction1WordNet is a lexicon comprising of nouns, verbs, adjectives and adverbs.
We describe noun phrases composed of a proper noun and/or a description of a human occupation.
We take into account synonymy and hyperonymy.
We are pursuing lexical acquisition through the syntactic relationships of words in medical corpora.
We have designed a dependency grammar parser that learns through a transformational-based algorithm.
Further work will evaluate the usefulness of this parse for lexical acquisition.
This paper proposes a new method for word translation disambiguation using a machine learning technique called 態ilingual Bootstrapping?
An efficient context-free parsing algorithm is presented that can parse sentences with unknown parts of unknown length.
This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics.
We propose three schemes, that is, utterance unit, discourse segment and discourse markers.
The extracted relations are used for query expansion in IR.
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
In this paper, we evaluate an approach to automatically acquire sense- tagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.
Our investigation reveals that this method of acquiring sense-tagged data is promising.
We consider here the problem of Base Noun Phrase translation.
In one method, we employ an ensemble of Na飗e Bayesian Classifiers constructed with the EM Algorithm.
In the other method, we use TF-IDF vectors also constructed with the EM Algorithm.
We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional.
We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3- Partition problem.Key words: computational complexity, Lambek Categorial Grammar
We describe a classifier which determines the rhetorical status of sentences in texts from a corpus of judgments of the UK House of Lords.
The basic compo-nents include basic segmentation, factoid recognition, and named entity recognition.
The postprocessors include merging of ad-joining words, morphologically derived word recognition, and new word identi-fication.
This paper presents an unsupervised topic identification method integrating linguistic and visual information based on Hidden Markov Models (HMMs).
For an input sentence, syntactic constituent structure parses are generated by a Charniak parser and a Collins parser.
Semantic role labels are assigned to the constituents of each parse using Support Vector Machine classifiers.
It demonstrates the need, under particular assumptions, for more access to full text articles and for the use of Part-of-Speech tagging.
An investigation into the use of Bayesian learning of the parameters of a multivariate Gaussian mixture density has been carried out.
In a continuous density hidden Markov model (CDHMM) framework, Bayesian learning serves as a unified approach for parameter smoothing, speaker adaptation, speaker clustering, and corrective training.
Our approach is to use Bayesian learning to incorporate prior knowledge into the CDHMM training process in the form of prior densities of the HMM parameters.
This paper describes the results of some experiments using a new approach to information access that combines techniques from natural language process-ing and knowledge representation with a penalty-based technique for relevance estimation and passage retrieval.
We introduce a new pointwise-prediction single-classifier method that predicts trigrams of class labels on the basis of windowed input sequences, and uses a simple voting mechanism to decide on the labels in the final output sequence.
We apply the method to maximum-entropy, sparse- winnow, and memory-based classifiers using three different sentence-level chunking tasks, and show that the method is able to boost generalization performance in most experiments, attaining error reductions of up to 51 %.
We compare and combine the method with two known alternative methods to combat near-sightedness, viz.
a feedback-loop method and a stacking method, using the memory-based classifier.
We report results for this small test corpus on a variety of experiments involving automatic speech recognition and named entity tagging.
The lexicons for Knowledge-Based Machine Translation systems require knowledge intensive morphological, syntactic and semantic information.
This system is currently being used in the ESTRATO machine translation project at the Center for Machine Translation.
We propose that while better document matching leads to better parallel sentence extraction, better sentence matching also leads to better document matching.
Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words.
Part of speech taggers based on Hidden Markov Models rely on a series of hypotheses which make certain errors inevitable.
Their lexical and syntactic counts are computed on the basis of the lexical and syntactic counts of their constituents, using impurity functions.
The first approach uses a Finite State Gra.mmar (i SC) to pro-duce noun and verb groups while the second uses a Supertagging model to produce de-pendency linkages.
We discuss the impact of these two input representations on the shn-plification process.
We describe the metaphoneme inventory defined for Dutch, English and German, comparing the results for vowels and consonants.
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.
To support summarization of automatically transcribed meetings, we introduce a classifier to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues.
We present a general model for PP attachment resolution and NP analysis in French.
Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit.
)In the Information Retrieval community.
1991) and (Church and Hanks 1990) have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subject-verb, verb-object pairs.
(Hearst 1992) has shown that certain lexical-syntactic templates can reliably extract hyponym relations froM text.
Components of LEI include a language analyzer, a geograph-ical reasoner, an object-oriented geographic knowledge base derived from US Geological Survey digital maps with user input, and a graphical user interface.
We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial contributions.
We present an experiment for designing a logic based QA system, WEBCOOP, that integrates knowledge representation and advanced reasoning procedures to generate cooperative responses to natural language queries on the web.
We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields.
We extend existing methods for automatic sentence boundary detection by leveraging multiple recognizer hypotheses in order to provide robustness to speech recognition errors.
We present in this paper a method for achieving in an integrated way two tasks of topic analysis: segmentation and link detection.
This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches.
This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change.
We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment.
This paper proposes an approach to full parsing suitable for Information Extraction from texts.
It was implemented in the IE module of FACILE, a EU project for multilingual text classification and IE.
Word dependency is important in parsing technology.
This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver.
We expect this method is also applicable to phrase structure parsers.
We describe a history-based generative parsing model which uses a k-nearest neighbour (k-NN) technique to estimate the model抯 parameters.
An algorithm is described that computes finite-state approximations for context-free grammars and equivalent augmented phrase-structure grammar formalisms.
We refer to such interfaces as Speech-In List-Out or SILO interfaces.
We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches.
This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines.
We call the system Parallel Substrate for TFS (PSTFS).
The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers.
In this paper we describe the analytic question answering system HITIQA (High- Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts.
We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG).
We extend XDG to lexicalize dependency subgraphs, and show how to compile them into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver.
This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy (CME) modeling.
This paper discusses the extension of ViewGen, a program for belief ascription, to the area of intensional object identification with applications to battle environments, and its combination in a overall system with MGR, a Model-Generative Reasoning system, and PREMO a semantics-based parser for robust parsing of noisy message data.ViewGen represents the beliefs of agents as explicit, partitioned proposition-sets known as environments.
We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues.
In this paper, we propose a new context- dependent SMT model that is tightly coupled with a language model.
We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.
These models enable word- alignment process to leverage topical contents of document-pairs.
This paper discusses the consequences of allowing discontinuous constituents in syntactic representions and phrase-structure rules, and the resulting complications for a standard parser of phrase-structure grammar.It is argued, first, that discontinuous constituents seem inevitable in a phrase-structure grammar which is acceptable from a semantic point of view.
The notions .of linear precedence and adjacency are reexamined, and the concept of "n-place adjacency sequence" is introduced.Finally, the resulting form ofphrase-structure grammar, called "Discontinuous Phrase-Structure Grammar", is shown to be parsable by an algorithm for context-free parsing with relatively minor adaptations.
Natural language systems based on Categorial Unification Grammar (CUG) have mainly employed bottom- up parsing algorithms for processing.
We show how these summaries can be factored into a set of document features, and how an implemented content planner uses the topicality document feature to create indicative multidocument query-based summaries.
Task based evaluation shows a 26% F-measure improvement in named entity recognition when using truecasing.
We describe a method for annotating spoken dialog corpora using both automatic and manual annotation.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
It is based on word-document co- occurrence statistics in the training corpus and a dimensionality reduction technique.
We present here an approach called Syntactically Enhanced LSA (SELSA) which generalizes LSA by considering a word along with its syntactic neighborhood given by the part-of-speech tag of its preceding word, as a unit of knowledge representation.
This paper discusses the application of Unification Categorial Grammar (UCG) to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen.
In experiments on a natural language information retrieval system that retrieves images based on textual captions, we show that syntactic complexity actually aids retrieval.
Like previous work in this area, our approach exploits orthographic regularities in a search for possible morphological segmentation points.
The semantic web (SW) vision is one in which rich, ontology-based semantic markup will become widely available.
AquaLog novel ontology-based relation similarity service makes sense of user queries.
We present a computational model of contextual facilitation based on word co-occurrence vectors, and empirically validate the model through simulation of three representative types of context manipulation: single word priming, multiple-priming and contextual constraint.
These features were extracted from a 23M word WSJ corpus based on part-of-speech tags and phrasal chunks alone.
In this paper, we discuss the phenomenon of logical polysemy in natural language as addressed by Generative Lexicon Theory.
We discuss generally the role of type and sortal coercion operations in the semantics, and specifically the conditions on the application of coercion in aspectual predicates and other contexts.
The paper presents a morpho-lexical environment, designed for the management of root- oriented natural language dictionaries.
We present a small set of attachment heuristics for postnominal PPs occurring in full-text articles related to enzymes.
We look at self-triggerability across hyperlinks on the Web.
We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.
This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component.
We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.
This paper describes a semi-automatic method for associating a Japanese lexicon with a semantic concept taxonomy called an ontology, using a Japanese-English bilingual dictionary as a "bridge".
The ontology supports semantic processing in a knowledge-based machine translation system by providing a set of language-neutral symbols and semantic information.
Evaluations using a large-scale test collection on Japanese- English and different weighting schemes of SMART retrieval system confirmed the effectiveness of the proposed combination of two-stages comparable corpora and linguistics-based pruning on Cross- Language Information Retrieval.Keywords: Cross-Language Information Retrieval, Comparable corpora, Translation, Disambiguation, Part-of-Speech.
In order to examine the domain dependence of parsing, in this paper, we report 1) Comparison of struc-ture distributions across domains; 2) Ex-amples of domain specific structures; and 3) Parsing experiment using some domain dependent grammars.
In this paper, we describe a reversible letter-to-sound/soundto-letter generation system based on an approach which combines a rule-based formalism with data-driven techniques.
We adopt a probabilistic parsing strategy to provide a hierarchical lexical analysis of a word, including information such as morphology, stress, syllabification, phonemics and graphemics.
This paper proposes a structurally based method for computing the relative scope of such modifiers, based on their order, type, and syntactic complexity.
This project measures and classifies language variation.
We measure phonetic (un)relatedness between dialects using Levenshtein distance, and classify by clustering distances but also by analysis through multidimensional scaling.
We present results of our work in relevancy visualization, news visualization, world events visualization and sensor/battlefield visualization to enhance user interaction in information access and exploitation tasks.
The Situated Constructional Interpretation Model is one of these attempts.
We present an analysis of Dutch cross serial dependencies in Head-driven Phrase Structure Grammar UP&S(1994)]).
The speech synthesis group at the Computer- Based Education Research Laboratory (CERL) of the University of Illinois at Urbana-Champaign is developing a diphone speech synthesis system based on pitch-adaptive short-time Fourier transforms.
The UTTER (for "Unmarked Text Transcription by Expert Rule") system maps English text onto a phoneme string, which is then used as an input to the diphone speech synthesis system.
This paper analyzes the functionality of different distance metrics that can be used in a bottom-up unsupervised algorithm for automatic word categorization.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
We discuss shift- reduce parsing of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses.
This paper focuses on anaphora interpreted as referring to entities of type event and action.
Finite-state transducers give efficient representations of many Natural Language phenomena.
A relatively self-contained subtask of natural language generation is sentence realization: the process of generating a grammatically correct sentence from an abstract semantic / logical representation.
Using basic machine learning algorithms as Naive Bayes and K-Nearest Neighbors, we have developed a real-time system which generates questions on English grammar and vocabulary from on-line news articles.
This paper describes ongoing research on the lexicalisation problem in a multilingual generation framework.
keywords: Multilingual generation, lexical choice, controlled languages.
The model is then used to classify the unknown Named Entities in the test set.
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.
The parser uses bit-vector operations to parallelise the basic parsing operations.
We outline a general architecture for word learning, in which structural alignment coordinates this contextual information in order to restrict the possible interpretations of unknown words.
The idea behind our method is to utilize certain layout structures and linguistic pattern.
In our splitting method, we generate candidates for sentence splitting based on N-grams, and select the best one by measuring sentence similarity.
We show how categorial deduction can be implemented in higher-order (linear) logic programming, thereby realising parsing as deduction for the associative and non-associative Lambek calculi.
We aim to show here how such unfolding allows compilation into programs executable by a version of SLD resolution, implementing categorial deduction in dynamic linear clauses.
We aim to indicate here how higher-order logic programming can provide for such a need.After reviewing the "standard" approach, via sequent proof normalisation, we outline the relevant features of (linear) logic programming and explain compilation and execution for associative and non-associative calculi in terms of groupoid and binary relational interpretations of categorial connectives.
We go on to briefly mention multi- modal calculi for the binary connectives.The parsing problem is usually construed as the recovery of structural descriptions assigned to strings by a grammar.
This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.
We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.
This paper describes the current state of work on unification-based semantic interpretation in HARC (for Hear and Recognize Confinous speech) the BBN Spoken Language System.
It presents the implementation of an integrated syntax/semantics grammar written in a unification formalism similar to Definite Clause Grammar.
This paper discusses an approach to modeling monolingual and bilingual dictionaries in the description logic species of the OWL Web Ontology Language (OWL DL).
In this paper we provide a quantitative evaluation of information automatically extracted from machine readable dictionaries.
In this paper, we construct a biomedical semantic role labeling (SRL) system that can be used to facilitate relation extraction.
We only annotate the predicate-argument structures (PAS抯) of thirty frequently used biomedical predicates and their corresponding arguments.
Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) model.
Thirdly, we automatically generate argument-type templates which can be used to improve classification of biomedical argument types.
The discussion focusses on the generation of complex Boolean descriptions and sentence aggregation.
This article outlines a quantitative method for segmenting texts into thematically coherent units.
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.
This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification.
The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules.
In the new Head-driven Phrase Structure Grammar (HPSG) language processing system that is currently under development at Hewlett-Packard Laboratories, the Montagovian semantics of the earlier GPSG system (see IGawron et al.
The paper describes a numerical scoring system used in Slot Grammar for ambiguity resolution, which not only ranks parses but also contributes to parsing efficiency through a parse space pruning algorithm.
We propose the use of multilingual corpora in the automatic classification of verbs.
We extend the work of (Merlo and Stevenson, 2001), in which statistics over simple syntactic features extracted from textual corpora were used to train an automatic classifier for three lexical semantic classes of English verbs.
The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus.
We report in this paper on an experiment on auto-matic extraction of a Tree Adjoining Grammar from the WSJ corpus of the Penn Treebank.
This paper will describe the phrasal knowledge base of FLUSH and its use in TRUMP.II.
First, intentional context is represented and inferred from user actions using probabilistic context free grammars.
This paper presents a corpus study of evaluative and speculative language.
This study yields knowledge needed to design effective machine learning systems for identifying subjective language.
The present paper reports the ongoing research on corpus representativeness.
We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
We describe a biographical multi-document summarizer that summarizes information about people described in the news.
We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.
And	the application of thecharacteristics to automatic content analysis is dicsussed.
This paper presents a language model and its application to sentence structure manipulations for various natural language applications including human-computer communications.
We present a derivation of the alignment template model for statistical machine translation and an implementation of the model using weighted finite state transducers.
We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers.
The approach is based on the statistical machine transliteration model to exploit the phonetic similarities between English words and corresponding Chinese transliterations.
For a given proper noun in English, the proposed method extracts the corresponding transliterated word from the aligned text in Chinese.
This paper' describes a method for chart parsing Lambek grammars.
The proposed method divides the documents into sentences, and categorizes each sentence using keyword lists of each category and sentence similarity measure.
Anaphora in multi-modal dialogues have different aspects compared to the anaphora in language-only dialogues.
In this paper, we define two kinds of anaphora: screen anaphora and referring anaphora, and propose two general methods to resolve these anaphora.
This paper investigates how to extend coverage of a domain independent lexicon tailored for natural language understanding.
We introduce two algorithms for adding lexical entries from VERBNET to the lexicon of the TRIPS spoken dialogue system.
This paper explores the use of a character segment based character correction model, language modeling, and shallow morphology for Arabic OCR error correction.
We introduce a stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation.
The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model.
Here, we view word alignment as matrix factorisation.
We then propose an algorithm for orthogonal non-negative matrix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment.
In this paper we analyze the types of errors produced by a tagger, distinguishing name classification and various types of name identification errors.
We present a joint inference model to improve Chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline: name structure parsing, cross-document coreference, semantic relation extraction and event extraction.
This paper describes a dialog based QA system, Dialog Navigator, which can answer questions based on large text knowledge base.
Another feature of the system is that it retrieves relevant texts precisely, using question types, synonymous expression dictionary, and modifier-head relations in Japanese sentences.
In the paper we focus on the notion of importance from a computational standpoint, and we propose a procedural, rule-based approach toimportance evaluation.
This paper proposes an automatic in-terpretation system that integrates free-style sentence translation and parallel text based translation.
Free-style sentence translation accepts natural language sen-tences and translates them by machine translation.
We developed a prototype of an au-tomatic interpretation system for Japanese overseas travelers with parallel text based translation using 9206 parallel bilingual sentences prepared in task-oriented man-ner.
This paper introduces an open computational framework for visual perception and grounded language acquisition called Experience-Based Language Acquisition (EBLA).
We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.
This paper' describes a flaw ral language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best," parse of a sentence according to a given grammar.
We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics.
It is connected to a POS tagging and word segmentation tool.
The algorithms included in this study are Hidden Markov Model, Maximum Entropy, Memory-Based Learning, and Transformation-Based Learning.
This paper investigates the potential for projectinglinguistic annotations including part-of-speech tagsand base noun phrase bracketings from one languageto another via automatically word-aligned parallelcorpora.
This paper focuses on subject shift and presents a method for extracting key paragraphs from documents that discuss the same event.
Our aim in this paper is to identify genre-independent factors that influence the decision to pronominalize.
One isa Probabilistic Context-Free Grammar (PCFG)approach, the other is a classification-basedMemory-Based Learning (MBL) approach.
This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news, teleconferences, and meetings.
These features are derived from algorithms including automatic speech recognition, automatic speech indexing, speaker identification, prosodic and audio feature extraction.
Finally, we discuss our approach to semantic, morphological, phonetic query expansion to improve audio retrieval performance and to access cross-lingual data.
This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline.
Dictionaries and word translation models are used by a variety of systems, especially in machine translation.
This paper presents a new approach to bitext correspondence problem (BCP) of noisy bilingual corpora based on image processing (IP) techniques.
Therefore, BCPs, including sentence and word alignment, can benefit from a wealth of effective, well established EP techniques, including convolution-based filters, texture analysis and Hough transform.
This paper describes a new program, PlotAlign that produces a word-level bitext map for noisy or non-literal bitext, based on these techniques.Keywords: alignment, bilingual corpus,image processing
Sentence understanding builds a model of the state of the world described, through the application of several knowledge modules: (i) LFG parsing, (ii) syntactic disambiguation based on lexical entry semantic components, (iii) assembly of semantic components and instantiation of domain entities, and (iv) construction of a world model through activation of common sense and domain knowledge.
Chinese word segmentation and POS tagging are two key techniques in many applications in Chinese information processing.
CSeg&Tag1.0, a Chinese word segmenter and POS tagger which unifies these two procedures into one model, is introduced in this paper.
In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus.
We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers.
Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text.
During decoding, we use a block unigram model and a word-based trigram language model.
The blocks are further filtered using unigram-count selection criteria.
We study a method for incorporating noun-class information, in the context of learning to resolve Prepositional Phrase Attachment (PPA) disambiguation.
A method for automatic plot analysis of narrative texts that uses components of both traditional symbolic analysis of natural language and statistical machine-learning is presented for the story rewriting task.
Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel.
We present a new model of the translation process: quasi-synchronous grammar (QG).
Given a source-language parse tree T1, a QG defines a monolingual grammar that generates translations of T1.
We describe experiments learning quasi-synchronous context-free grammars from bitext.
We place synchronous tree-adjoining grammars and tree transducers in the single overarching framework of bimorphisms, continuing the unification of synchronous grammars and tree transducers initiated by Shieber (2004).
In particular, the tree relations definable by synchronous tree-substitution grammars (STSG) were shown to be just those definable by linear complete bimorphisms, thereby providing for the first time a clear relationship between synchronous grammars and tree transducers.In this work, we show how the bimorphism framework can be used to capture a more powerful formalism, synchronous tree-adjoining grammars, providing a further uniting of the various and disparate formalisms.After some preliminaries (Section 1), we begin by recalling the definition of tree-adjoining grammars and synchronous tree-adjoining grammars (Section 2).
We turn then to a set of known results relating context-free languages, tree homomorphisms, tree automata, and tree transducers to extend them for the tree-adjoining languages (Section 3), presenting these in terms of restricted kinds of functional programs over trees, using a simple grammatical notation for describing the programs.
This paper reports on an investigation into representing tone unit boundaries (pauses) as well as words in a corpus of spoken English.
This paper describes PaTrans - a fully automatic production MT system de-signed for producing raw translations of patent texts from English into Dan-ish.
In this paper, we describe improved alignment models for statistical machine translation.
The statistical translation approach uses two types of information: a translation model and a language model.
The language model used is a bigram or general m-gram model.
The translation model is decomposed into a lexical and an alignment model.
The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.
We discuss a tagging schema and a tagging tool for labeling the rhetorical structure of texts.
We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses.
We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms: SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system.
The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model.
Phonologists employ a variety of contextual descriptors, based on factors such as stress and syllable boundaries, to explain phonological variation.
In order to incorporate a wide variety of factors in the creation of pronunciation networks, we used data-derived context trees, which possess properties useful for pronunciation network creation.
5000 parallel sentences of Tamil and English data.
It uses dynamic program-ming to efficiently compare weighted aver-ages of sets of adjacent scored component translations.
Our method combines shallow linguistic processing with machine learning to extract phrasal units that are representative of email content.
This paper presents a semantic model for Chinese garden-path sentences.
The second is a Markov model of syntax and is based on syntactic categories (tags) associated with words.
We present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers.
Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers.
We investigate why weights from generative models underperform heuristic estimates in phrase- based machine translation.
This paper describes a Verb Phrase Ellipsis (VPE) detection system, built for robustness, accuracy and domain independence.
We investigate the use of machine learning in combination with feature engineering techniques to explore human multi- modal clarification strategies and the use of those strategies for dialogue systems.
move+ed).
Unknown word recognition is an important problem in Chinese word segmentation systems.
This paper presents and analyzes an incremental	algorithm	for	theconstruction of Acyclic Non-deterministic Finite-state Automata (NFA).
Focusing on bottom-up chart generation, we describe how the notions of chart algorithms relate to the knowledge base and Rete network of production systems.
We draw on experience gained in two research projects on natural language generation (NLG), one involving surface realization, the other involving both a content determination task (referring expression generation) and surface realization.
This paper proposes the application of finite-state approximation techniques on a unification-based grammar of word formation for a language like German.
This paper describes an implemented mechanism for handling bound anaphora, disjoint reference, and pronominal reference.
In this paper, we adapt the new approach of contrast classifiers for semi-supervised learning.
This paper shows that a class of Combinatory Categorial Grammars (CCGs) augmented with a linguistically-motivated form of type raising involving variables is weakly equivalent to the standard CCGs not involving variables.
The German joint research project Verb-mobil (VM) aims at the development of a speech to speech translation sys-tem.
In this paper we discuss the use of cascaded finite state transducers for machine translation.
The system uses uses syntactic information from Penn Treebank parse trees.
We have evaluated the appropriateness of sentence concatenations in summaries by using evaluation measures used for evaluating word concatenations in summaries through word extraction.
We investigate that claim by adopting a simple MT- based paraphrasing technique and evaluating QA system performance on paraphrased questions.
In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm.
This paper presents AsdeCopas, a module designed to interface syntax and semantics.
We present a MT system that applies graph unification in transfer from English to Finnish.
The transfer system is responsible for generating target language phrase structure.
The lexicon system consists of separate transfer and monolingual lexicons and a common lexicon of language independent definitions.Keywords: unification, machine translation, transfer, bilingual lexicon
This paper describes text meaning represen-tation for Chinese.
It integrates lexical, textual and world knowledge into a single hierarchical frame-work.
Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.
The IBM flight information system is a speech based conversational system which allows users to create multi-leg airline travel itineraries based on live flight availability information.
This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best" parse of a sentence.
We present a domain-independent topic segmentation algorithm for multi-party speech.
Our feature-based algorithm combines knowledge about content using a text-based algorithm as a feature and about form using linguistic and acoustic cues about topic shifts extracted from speech.
The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information.
This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases.
In this paper, we propose a topic detection method using a dialogue history for a speech translator.
The method uses a k-nearest neighbor method for the algorithm, automatically clusters target topics into smaller topics grouped by similarity, and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases.
We describe a language learner that extracts distributional information from a corpus annotated with parts of speech and is able to use this extracted information to accurately parse short sentences.
We define a more general class of unification grammars, which admits x-bar grammars while preserving the desirable properties of offline parsable grammars.Consider a unification grammar based on term unification.
The parsing problem for offline parsable grammars is solvable.
This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains.
We propose a formal model that estimates the lexical similarity between sources and potential destinations of hyperlinks.
In this paper, we present a new phrase break prediction architecture that; integrates proba-bilistic approach with decision-tree based error correction.
This paper proposes an automatic method of reading proper names with multiple pronunciations.
We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model.
In this paper, we focus on two distinctive features of the K3 system: plan-based anaphora resolution and handling vagueness in spatial ex-pressions.
In this paper, we first describe how we have customized our data-driven multilingual discourse module within our text understanding system for different languages and for a particular NLP application by utilizing hierarchically organized discourse KB's.
One of the most active and promising areas of statistical machine translation (SMT) research are tree-based SMT approaches.
Tree-based SMT has the potential to overcome the weaknesses of early SMT architectures which (a) do not handle long-distance dependencies well, and (b) are underconstrained in that they allow too much flexibility in word reordering.In this tutorial, we will review the various possible approaches to tree-based SMT, ranging from the original Inversion Transduction Grammar (ITG) models to later models such as alignment templates, dependency models, tree-to-string models, tree-to-tree models, and also probabilistic EBMT models.
This paper describes a method of pro-cessing unknown words in a HPSG-based dialogue system, with acquisi-tion of lexical semantics via clarifica-tion questions answered by the user.
Successful examples include subword models with many smoothing techniques.
We propose to model subphonetic events with Markov states and treat the state in phonetic hidden Markov models as our basic sub- phonetic unit ?senone.
In this approach we integrate a symbolic se-mantic segmentation parser with a learn-ing dialog act network.
The paper describes a particular approach to multi- engine machine translation (MEMT), where we make use of voted language models to selectively combine translation outputs from multiple off-theshelf MT systems.
We describe two choices for the representation of lexical items and two choices for the representation lexical relations.
We describe how these comparison strategies are used within the PEBA-II hypertext generation system to generate descriptions of animals.
We describe a computational system which parses discourses consisting of sequences of simple sentences.
These contain a range of temporal constructions, including time adverbials, progressive aspect and various aspectual classes.
We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization, etc., which change verb valency.
We are aiming to acquire named entity (NE) translation knowledge from nonparallel, content-aligned corpora, by utilizing NE extraction techniques.
Structural (attachment.)
We have proposed a method of word segmen-tation for non-segmented language using Induc-tive Learning.
The method predicts unknown words by recursively extracting common character strings.
This paper reports the present results of a research on unsupervised Persian morpheme discovery.
We utilized a Minimum Description Length (MDL) based algorithm with some improvements and applied it to Persian corpus.
^ The method uses a k-nearest neighbor method for the algorithm, automatically clusters target topics into smaller topics grouped by similarity, and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases.
CRFs are log-linear, allowing the incorporation of arbitrary features into the model.
To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist.
In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue.
We first apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries.
Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries, the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries, the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues, such as cue phrases and overlapping speech, are better indicators for the top- level prediction task.
This paper examines the role that summaries can play in document retrieval.
We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts.
We explored a simple, fast and effective learning algorithm, the uneven margins Perceptron, for Chinese word segmen-tation.
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
We investigate generalizations of the allsubtrees "DOP" approach to unsupervised parsing.
Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
This paper describes a semi-automatic method of inducing underspecified semantic classes from WordNet verbs and nouns.
An underspecified semantic class is an abstract semantic class which encodes systematic polysemy: a set of word senses that are related in systematic and predictable ways.
We show the usefulness of the induced classes in the semantic interpretations and contextual inferences of real-word texts by applying them to the predicate-argument structures in Brown corpus.
We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction.
Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words.
We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features.
We present a framework for the integrated analysis of the textual and prosodic characteristics of information structure in the Switchboard corpus of conversational English.
theme/rheme and background/kontrast.
This paper presents a character-based model of automatic sense determination for Chinese compounds.
The model adopts a sense approximation approach using synonymous compounds retrieved by measuring similarity of semantic template in compounding.
In this position paper we describe the scopes of two schools in lexical semantics, which we call syntax-driven lexical semantics and ontology-driven lexical semantics, respectively.
In this paper we propose an integrated knowledge management system in which terminology-based knowledge acquisition, knowledge integration, and XML-based knowledge retrieval are combined using tag information and ontology management tools.
Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval.
We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains.
In this paper a method to compile unification grammars into speech recognition packages is presented, and in particular, rules are specified to transfer the compositional semantics stated in unification grammars into speech recognition grammars.
The method was tested on a medium-sized unification grammar for English using Nuance speech recognition software on a corpus of 131 utterances of 12 different speakers.
Introductionin the field of natural language analysis, Unification Grammars are a main research topic.
Presently, Unification is defined as extension of context-free grammars.Knowing the formalism of Tree Adjoining Grammars (in the following called TA Gs in short), which is closely related to context-free grammars (in the following abbreviated CFG), the idea arises to replace the context-free grammar in a Unification Grammar by a TAG.
A TAG is a tree generation system.
Then we analyse the results of this data collection and estimate the efficiency of the collected list for the resolution of morphosyntactic and semantic ambiguity in a corpus.
This paper describes a model for a lexical knowledge base (LKB).
Thus, we propose a model for an LKB focusing on dictionary knowledge such as that obtained from machine-readable dictionaries.When an LKB is given a key from a user, it accesses the stored knowledge associated with that key.
They include machine translation systems, style analyzers, personal databases, an electronic book [Weyer82], and an electronic encyclopedia [VVeyer85].
We call an access unit of stored dictionary knowledge a lexical knowledge unit (LKU).
Headwords in machine-readable dictionaries are usually standardized; i.e.
without inflections or conjugations.
This paper presents a hybrid rule-based and statistical model for morpho-syntactic annotation of German, a highly ambiguous inflectional language.
Its dependency-based shallow parsing approach provides significant robustness in the face of language learners?ungrammatical compositions.
The lexicon now plays a central role in our implementation of a Head-driven Phrase Structure Grammar (HPSG), given the massive relocation into the lexicon of linguistic information that was carried by the phrase structure rules in the old GPSG system.
SemFrame generates FrameNet-like frames, complete with semantic roles and evoking lexical units.
The semantic relations are detected by checking selectional constraints.
The ability to represent cross-serial dependencies is one of the central features of Tree Adjoining Grammar (TAG).
This paper describes a prototype mul-timodal spoken natural language dia-log system for capturing a comman-der's expectations about planned mil-itary actions.
To provide practical spelling checkers on micro-computers, good compression algorithms are essential.
Experiments with multilingual dictionaries show a significant reduction rate attributable to our logic compression alone and even better results when using our method in conjunction with existing methods.KEY WORDS: Spelling checkers, Multilinguism, Compression, Dictionary, Finite-state machines.
In this paper, we look into extending standard dialogue move taxonomies for the genre of tutorial dialogues.
We suggest a way of investigating tutorial dialogue phenomena robustly.
Each TCT describes a translation example (a pair of bilingual sentences).
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects.
To overcome this limitation, this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them.
We present a dialogue manager for 揅all for Fire?training dialogues.
In this paper, using a similar syntactic analysis for wh pied-piping as in Han (2002) and further developed in Kallmeyer and Scheffler (2004), I propose a compositional semantics for relative clauses based on Synchronous Tree Adjoining Grammar.
In this paper	unification and	transductionmechanisms are applied in a new approach to phonological parsing.
Then a linear- unification parser for English syllables is introduced.
This parser takes phonetic input in Lie form of feature bundles and uses phonological rules represented by networks of transduction relations together with unification, and an iterative finite-state process to produce phonemic output with marled syllable boundaries.
parsers and transducers) for the interpretation of such networks.
A new language model incorporating both N-gram and context-free ideas is proposed.
This constrained context-free model is specified by a stochastic context-free prior distribution with N-gram frequency constraints.
The resulting distribution is a Markov random field.
This paper discusses automatic text summarization based on GDA.
In order to calculate the importance score of a text element, the algorithm uses spreading activation on an intradocument network which connects text elements via thematic, rhetorical, and coreferential relations.
We show how idioms can be parsed in lexicalized TAGs.
We thus consider idioms of different syntactic categories : NP, S, adverbials, compound prepositions... in both English and French.In lexicalized TAGs, the same grammar is used for idioms as for 'free' sentences.
We also show how regular 'transformations' are taken into account by the parser.Topics: Parsing, Idioms.
This paper presents the motivating goals, acoustic data design, text processing steps, lexicons, and testing paradigms incorporated into the multi-faceted WSJ CSR Corpus.
This paper describes an accurate and efficient algorithm for very-large-vocabulary continuous speech recognition based on an HMM-LR algorithm.
The HMM-LR algorithm uses a generalized LR parser as a language model and hidden Markov models (HMMs) as phoneme models.
To cope with the problem of background noise, an HMM composition technique which combines a noise- source HMM and a clean phoneme HMM into a noise-added phoneme HMM was investigated and incorporated into the system.
A brief description of the bilingual dictionary is given, followed by descriptions of grammar rules representation and the main processes involved in translation.SYSTEM CONFIGURATIONECTST consists of a translation program, a bilingual dictionary and a rule-data base.
The analysis phase involves syntactic and semantic parsing, which are accomplished through linguistic models and case frame.
In this paper, we present a named entity recognition system in the biomedical domain, called PowerBioNE.
We demonstrate an implemented disambiguator for a certain class of three-clause sentences based on our theory.
This paper describes the scalability and portability of a Belief Network (BN)-based mixed initiative dialog model across application domains.
We have also enhanced our dialog model with the capability of discourse context inheritance.
In this paper, we describe a system that applies maximum entropy (ME) models to the task of named entity recognition (NER).
We propose a method for semi-automatic classi-fication of verbs to Levin classes via the seman-tic network of WordNet.
We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.
This paper describes our actual and ongoing work in supporting semi-automatic ontology ac-quisition from a corporate intranet of an in-surance company.
GermaNet, WordNet).
the Italian wordnet in EuroWordNet (ItalWordNet).
This paper presents a system for unsupervised verb sense disambiguation using small corpus and a machine-readable dictionary (MRD) in Korean.
First, extending word similarity measures from direct co-occurrences to co- occurrences of co-occurred words, we compute the word similarities using not co-occurred words but co- occurred clusters.
We describe novel aspects of a new natural language generator called Nitrogen.
In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding- window method and maximum entropy classifiers for phrase recognition in each level of chunking.
In this paper we develop an automatic classifier for a very large set of labels, the WordNet synsets.
An input string is parsed by combining subtrees from the corpus.
shotat (hl, 13) .
We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial and final contributions.
Further analysis reveals that using dialogue exchanges versus dialogue contributions improves topic segmentation quality.
In this paper we investigate an application of feature clustering for word sense disambiguation, and propose a semi- supervised feature clustering algorithm.
This paper addresses the problems of movement transformation in Prolog-based bottom-up parsing system.
Three principles of Government-Binding theory are employed to deal with these problems.
For the treatment of this phenomenon, we cannot just write down the rules:sentence --> noun-phrase,verb-phrase.verb-phrase --> transitive-verb,noun-phrase.verb-phrase --> transitive-verb.verb-phrase --> intransitive-verb.This is because many ungrammatical sentences will be accepted.
A Government-Binding based Logic GrammarFormalism2.1 The specifications of grammar formalismThe Government-Binding based Logic Grammar (GBLG)formalism is specified informally as follows:(1) the general grammar rules -(a) c (Arg) --> ci(Argi ),c2(Arg2),...,cn(Argn).
We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.
This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be effectively carried out.
We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags.
We then applied an automatic algorithm to detect errors in the dialogues.
Our algorithm extends the information-bottleneck soft clustering method for a suitable setting consisting of several datasets.
We term this task cross-dataset (CD) clustering.In this article we demonstrate CD clustering through detecting corresponding themes across three different religions.
In this paper we present a brief look at some of the knowledge-based processes used in generating referring expressions in the natural language advisory system WISBER.
We apply a gene and protein name tagger trained on Medline abstracts (ABGene) to a randomly selected set of full text journal articles in the biomedical domain.
This paper presents a corpus-based method to assign grammatical subject/object relations to ambiguous German constructs.
We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes.
This paper will introduce the tasks of tokenization, normalization before introducing BAccHANT, a system built for bioscience text normalization.
Casting tokenization / normalization as a problem of punctuation classification motivates using machine learning methods in the implementation of this system.
Word alignment plays a crucial role in statistical machine translation.
We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support user- directed multidocument summarization.
We present a statistical shift-reduce parser that bridges the gap between deterministic and probabilistic parsers.
The method is based on explanation-based learning EBL.
This paper proposes a method for extracting key paragraph for multi-document summarization based on distinction between a topic and an event.
A topic and an event are identified using a simple criterion called domain dependency of words.
bus information system, an experimental system designed to study the use of spoken dialogue interfaces by non-native speakers.
This paper presents a disambiguation approach for translating non-segmented-Kana into Kanji.
The focus of this article is the integra-tion of two different perspectives on lexi-cal semantics: Discourse Representation Theory's (DRT) inferentially motivated approach and Semantic Emphasis The-ory's (SET) lexical field based view.
We present a new statistical language model based on a combination of individual word language models.
In particular, we describe the integration of a high-level HPSG parsing system with different high-performance shallow components, ranging from named entity recognition to chunk parsing and shallow clause recognition.
We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars.
Leximancer is a software system for performing conceptual analysis of text data in a largely language independent manner.
The system is modelled on Content Analysis and provides unsupervised and supervised analysis using seeded concept classifiers.
Unsupervised ontology discovery is a key component.
This paper applies machine learning techniques to acquiring aspects of the meaning of discourse markers.
causal, temporal or additive).
We investigate methods that add syntactically motivated features to a statistical machine translation system in a reranking framework.
The goal is to analyze whether shallow parsing techniques help in identifying ungrammatical hypotheses.
We show that improvements are possible by utilizing supertagging, lightweight dependency analysis, a link grammar parser and a maximum-entropy based chunk parser.
We create two bilingual pronunciation dictionaries for the language pairs German-Dutch and German- English.
The data is used for automatically learning phonological similarities between the two language pairs via EM- based clustering.
We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments.
While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns.
In this paper, a novel kernel-based method is presented for the problem of relation extraction between named entities from Chinese texts.
By em-ploying the Voted Perceptron and Sup-port Vector Machine (SVM) kernel ma-chines with the IED kernel as the clas-sifiers, we tested the method by extract-ing person-affiliation relation from Chi-nese texts.
We present a treebank conversion method by which we construct an RMRS bank for HPSG parser evalu-ation from the TIGER Dependency Bank.
We present the semantics construction mechanism, and focus on some spe-cial phenomena.
The model can be classified as a Maximum Entropy model and simultaneously uses many contextual "features" to predict the PUS tag.
These rules are noun sequences with part-of-speech tags.
In this paper we investigate the adaptations of users when they encounter recognition errors in interactions with a voice-in/voice-out spoken language system.
Extracting sentences that contain important information from a document is a form of text summarization.
This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs).
This paper studies a strategy for identifying and using multi-word expressions in Statistical Machine Translation.
We have developed a model based on a hierarchy of plans and metaplans that accounts for the clarification subdialogues while maintaining the advantages of the plan-based approach.I.
Quantified mass noun phrases is one such part.
The decision procedure is based on a tableau calculus.
We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization.
Semantic similarity measures have focused on individual word senses.
We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text.
We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG).
We present this architecture and the synchro-nization issues we encountered in building a truly distributed, agent-based dialogue architecture.
The HKUST word sense disambiguation systems benefit from a new nonlinear Kernel Principal Component Analysis (KPCA) based disambiguation technique.
The present article demostrates the implementation of a psycholinguistic model of second language learners' interlanguage in an Intelligent Computer Assisted Language Learning (ICALL) system for studying second language acquisition.
The algorithm uses a trellis-based structure as opposed to the binary branching tree structure used by the I/O algorithm.
1973] for the procedural embedding of knowledge.
We introduce stereotypes as an actor version of a frame theory.
We also present an inductive learning approach to the automatic discovery of lexical and semantic constraints necessary in the disambiguation of causal relations that are then used in question answering.
This work presents the data model we adopted for annotating coreference.
Our data model includes different levels of annotation, such as part-of-speech, syntax and discourse.
113y checking lexical cohesion between the current word and lexical chains in the order of the salience, in tandem with generation of lexical chains, we realize incremental word sense disattr, biguation based on contextual information that lexical chains,reveal.
A novel technique for automatic thesaurus construction is proposed.
It is based on the complementary use of two tools: (1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions).
Natural language processing systems need large lexicons containing explicit information about lexical-semantic relationships, selection restrictions, and verb categories.
This paper describes methods for finding taxonomy and set-membership relationships, recognizing nouns that ordinarily represent human beings, and identifying active and stative verbs and adjectives.
We present an implemented concept-to-speech (CTS) system that offers original proposals for certain couplings of dialogue computation with prosodic computation.
We introduce a MetaGrammar, which allows us to automatically generate, from a single and compact MetaGrammar hierarchy, parallel Lexical Functional Grammars (LFG) and Tree-Adjoining Grammars (TAG) for French and for English: the grammar writer specifies in compact manner syntactic properties that are potentially framework-, and to some extent language-independent (such as subcategorization, valency alternations and realization of syntactic functions), from which grammars for several frameworks and languages are automatically generated offline.1
Automatic Multi-Document summarization is still hard to realize.
Based on the observation that recognition errors may result in ungrammatical sentences, especially in dictation application where an acceptable level of accuracy of generated documents is indispensable, we propose to incorporate two kinds of linguistic features into error detection: lexical features of words, and syntactic features from a robust lexicalized parser.
Transformation-based learning is chosen to predict recognition errors by integrating word confidence scores with linguistic features.
It proposes that two widely used kinds of relations, lexical dependen-cies and structural relations, have complementary disambiguation ca-pabilities.
Linearization-based HPSG theories are widely used for analyzing languages with relatively free constituent order.
This paper introduces the Generalized ID/LP (GIDLP) grammar format, which supports a direct encoding of such theories, and discusses key aspects of a parser that makes use of the dominance, precedence, and linearization domain information explicitly encoded in this grammar format.
This paper describes Acorn, a sentence planner and surface realizer for dialogue systems.
We are using a computational learning sys-tem that is composed of a Universal Grammar with associated parameters, and a learning al-gorithm, following the Principles and Parame-ters Theory.
The Universal Grammar is imple-mented as a Unification-Based Generalised Cat-egorial Grammar, embedded in a default inher-itance network of lexical types.
We present a simple, but surprisingly effective, method of self-training a two- phase parser-reranker system using readily available unlabeled data.
We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker.
We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs.
We quantify the degree to which gazetteers, web resources, encyclopedia, web documents and web-based query expansion can help Question Answering in general and specific question types in particular.
This paper describes a system which uses a decision tree to find and classify names in Japanese texts.
The decision tree uses part-of-speech, character type, and special dictionary information to determine the probability that a particular type of name opens or closes at a given position in the text.
One is a phonemic based transcription of sounds for acoustic modelling in Automatic Speech Recognizers and for Text to Speech synthesizer, using ASCII based symbols, rather than International Phonetic Alphabet symbols.
This paper proposes a new term weighting method for summarizing documents retrieved by IR system.
This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recall- based evaluation measures.
This paper presents in implemented theory for quantifying noun phrases in clauses containing copular verbs (e.g.. 'be' and 'become.).
We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language.
A monotone phrasal decoder generates contextual replacements.
This paper describes a dependency based tagging scheme for creating tree banks for Indian languages.
It is based on Paninian grammatical model.
Frequently, such methods are based on Centering Theory, which deals with the resolution of anaphoric pronouns.
This paper presents a maximum entropy-based named entity recognizer (NER).
I'This work was supported by CEC Telernatics Applications Programme project LEI4111 "SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering".
The parser uses information about co-occurence and domination of linguistic elements as well as concept hierarchies in the domain to construct a parse tree and, subsequently, a derivation in quasi- logical form.
This paper evaluates the effect of using different lexical and syntactic features both individually and in combination.
This paper introduces a supersonic Korean morphological analyzer named MACH.
The connectivity between two sentences is measured based on correference between a pronoun and a preceding (pro)noun, and on lexical cohesion of lexical items.
This paper explores the usefulness of a technique from software engineering, code instrumentation, for the development of large-scale natural language grammars.
The first is a baseline condition using only training data available from NIST on CD-ROM and a word-based statistical bi-gram grammar developed at MIT/Lincoln.
In the second condition, we added training data from speakers collected at BBN and used a 4-gram class grammar.
The Microsoft Research translation system is a syntactically informed phrasal SMT system that uses a phrase translation model based on dependency treelets and a global reordering model based on the source dependency tree.
In this paper a new, tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition is proposed.
The Multi-Class Composite N-gram main-tains an accurate word prediction ca-pability and reliability for sparse data with a compact model size based on multiple word clusters, called Multi-Classes.
Fur-thermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams.
POS-tagging, lemmatisation and a thesaurus-like thematic dictionary.
(MILE).
We present a Chinese word seg-mentation system submitted to the closed track of Sighan bakeoff 2005.
In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on "most similar" words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model.
In this paper we present the results for building a grapheme-based speech recognition system for Thai.
In this paper, a treatment of Czech phonological rules in two-level morphology approach is described.
In this work, we apply a clustering technique to integrate the contents of items into the item-based collaborative filtering framework.
A hierarchical hidden Markov model (HHMM) based approach of product named entity recognition (NER) from Chinese free text is presented in this pa-per.
This paper presents a method for word• sense disambiguation and coherence understanding of prepositional relations.
We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures.
This paper proposes a machine learning based question classification method using a kernel function, Hierarchical Directed Acyclic Graph (HDAG) Kernel.
This paper describes a method for retrieving patterns of words and expressions frequently used in a specific domain and building a dictio-nary for machine translation(MT).
The method uses an untagged text corpus in retrieving word.
sequences and simplified part-of-speech tem-plates in identifying their syntactic categories.
The paper presents experimental results for ap-plying the words and expressions to a pattern-based machine translation system.
Multimodal Functional Unification Grammar (MUG) is a unification-based formalism that uses rules to generate content that is coordinated across several communication modes.
This paper describes a prototype of a multi- modal railway information system that was built by extending an existing speech-only system.
The language analysis strategy used by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing.
We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words.
We present results of two methods for assessing the event profile of news articles as a function of verb type.
the event profile.
Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents.
The proposed method makes use of two kinds of word correspondences in aligning bilingual texts.
One is a bilingual dictionary of general use.
Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness.
This paper proposes a method for retrieving 搈eaning-equivalent sentences?to overcome these two problems.
The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation.
The correct attachment of prepositional phrases (PPs) is a central disambiguation problem in parsing natural languages.
This paper describes the usage of XML for representing cross-language phrase alignments in parallel treebanks.
We describe a method for obtaining subject-dependent word sets relative to some (subject) domain.
This paper presents a new web mining scheme for parallel data acquisition.
Based on the Document Object Model (DOM), a web page is represented as a DOM tree.
N e implemented this idea as an unsupervised tokenization of Japanese with extended Hidden-Markov-Models (HNIMs), where hidden n-gram probabilities (i.e., state transition probabilities) are affected by co-occurring words in the English part.
The translation part of PIVOT is the rule-based system and adopts the interlingua method.
OF COLING-92, NANTES, Auo.
In this paper, we explore facets of instructional texts: general prototypical structures, rhetorical structure and natural argumentation.
We study the problem of learning to recognise objects in the context of autonomous agents.
First, the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure.
Here we address the problem of mapping phrase meanings into their conceptual representations.
They are formed based on finite state machine specifications thus resulting in a fast grouper.
We describe a corpus-based induction algorithm for probabilistic context-free grammars.
The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside- Outside algorithm.
This paper presents a new bootstrapping approach to named entity (NE) classification.
We utilize meta-patterns of high- frequency words and content words in order to discover pattern candidates.
Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets.
This paper presents a hybrid model for restoring an elided entry word for en-cyclopedia QA system.
A rule-based approach uses caseframes and sense classes.
In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French.
We describe a new approach to syntactic generation with Head-Driven Phrase Structure Grammars (HPSG) that uses an extensive off-line preprocessing step.
Direct generation algorithms apply the phrase-structure rules (schemata) of the grammar on-line which is an computationally expensive step.
This process is known as 'compiling HPSG to TAG' and derives a Lexicalized Tree-Adjoining Grammar (LTAG).
In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.
This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.
Constraint Grammar framework divides parsing into two different modules: morphological disambiguation and determination of syntactic functions.
Document indexing and representation of term-document relations are very important for document clustering and retrieval.
In this paper, we combine a graph-based dimensionality reduction method with a corpus-based association measure within the Generalized Latent Semantic Analysis framework.
We evaluate the graph-based GLSA on the document clustering task.
In this paper two different approaches to the resolution of lexical ambiguities are applied to a multi-domain corpus of speech recognition output produced from spontaneous utterances in a spoken dialogue system.
We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.
Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation.
This paper presents a network formalism for representing the meaning of noun phrases occurring in the context of intensional verbs such as seek and want.
This paper focuses on the automated processing of temporal information in written texts, more specifically on relations between events introduced by verbs in finite clauses.
This paper focuses on the transformation of grammar checking technology into a learning environment for second language writing.
Our starting point is a grammar checker for Swedish, called Granska.
Then we use lin-guistic patterns and HTML structures to ex-tract text fragments describing the term.
We present the results of experiments aimed at assigning domains to speech recognition hypotheses (SRH).
The methods rely on high-level linguistic representations of SRHs as sets of ontological concepts.
In this paper we examine the issues that arise from the annotation of the discourse connectives for the Chinese Discourse Treebank Project.
We then examine the types of syntactic units that can be arguments to the discourse connectives.
This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences.
Graph unification is the most expensive part of unification-based grammar parsing.
This research is aimed at the problem of disambiguating toponyms (place names) in terms of a classification derived by merging information from two publicly available gazetteers.
We present a usage consultation tool based on Internet searching.
GRADE allows a grammar writer to control the process of a machine translation.
This paper presents a novel approach to extracting phrase-level answers in a question answering system.
This approach uses structural support provided by an integrated Natural Language Processing (NLP) and Information Extraction (IE) system.
Phrase-level answer extraction is modelled by comparing the structural similarity involving the question-phrase and the candidate answer-phrase.There are two types of structural support.
The second type involves generic grammatical relationships such as V-S (verb-subject), V-O (verb-object).Preliminary experimental results show an improvement in both precision and recall in extracting phrase-level answers, compared with a baseline system which only uses Named Entity constraints.
Throughout, we use feature cooccurence restrictions as illustration and linguistic motivation.
Word- Net) is here proposed.
A methodology aiming to support semantic bootstrapping in a NLP application is defined.
We describe a method for augmenting unification-based deep parsing with statistical methods.
We extend and adapt the Bikel parser, which uses head-driven lexical statistics, to dialogue.
Methodological problems	in Montague Grammar arediscussed.
sentencedenotations.Turner[161 extended intensional logic in the sense88
Attentional focus is simulated in both models to select relevant subnetworks for Bayesian propagation.
This paper presents an enhanced model of plan-based dialogue understanding.
We call these features shared domain plan constraints.
We exploit and extend the Generative Lexicon Theory to develop a formal description of adnominal constituents in a lexicon which can deal with linguistic phenomena found in Japanese adnominal constituents.
We classify the problematic behavior into "static disambiguation" and "dynamic disambiguation" tasks.
Using the similarities, classes are built using hierarchical agglomerative clustering.
We compare two feature sets: one with lexical features only, and another with a mixture of lexical and semantic features.
This paper describes an attribute grammar specification of the Government-binding theory.
The paper focuses on the description of the attribution rules responsible for determining antecedent-trace relations in phrase-structure trees, and on some theoretical implications of those rules for the GB model.
We select word format as target linguistic feature and propose an HMMbased approach to this issue.
This paper concerns the design of cooperative response generation (CRG) systems, NLQA systems that are able to produce integrated cooperative responses.
This paper presents a unification-based approach to Japanese honorifics based on a version of HPSG (Head-driven Phrase Structure Grammar)l11121.
Utterance parsing is based on lexical specifications of each lexical item, including honorifics, and a few general PSG rules using a parser capable of unifying cyclic feature structures.
The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser.
The task of template filling is cast as constrained parsing using the SLM.
We propose a generic paraphrase-based approach for Relation Extraction (RE), aiming at a dual goal: obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervised configuration for RE.
Our findings reveal a high potential for unsupervised paraphrase acquisition.
In recent years there is much interest in word cooccurrence relations, such as n-grams, verb- object combinations, or cooccurrence within a limited context.
This paper presents a deterministic parsing algorithm for projective dependency grammar.
This position paper sketches the author's research in six areas related to speech translation: interactive disambiguation; system architecture; the interface between speech recognition and analysis; the use of natural pauses for segmenting utterances; dialogue acts; and the tracking of lexical co-occurrences.
A coin puter-based course addressing the topic of applying Natural Language resources and techniques to Information Retrieval is presented.
This paper talks about the deciding practical sense boundary of homonymous words.
We are interested in the problem of modeling and evaluating spoken language systems in the context of human-machine dialogs.
Spoken dialog corpora allow for a multidimensional analysis of speech recognition and language understanding models of dialog systems.
This paper investigates different ways for encoding dialogues into multidimensional structures and different clustering methods.
This paper outlines a high-level language FUNDPL for expressing functional structures for parsing dependency constraints.
Grammars for parsing have predominantly used generative rewrite rules.
This paper describes a high-level language FUNDPL (FUNctional DPL) we have designed on top of DPL.
CATEGORY assigns names in hierachies.
This paper presents a knowledge-based method for measuring the semantic- similarity of texts.
In this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.
We also report on an experimental result of case structure analysis using the constructed case frame dictionary.
This paper discusses three different but related large-scale computational methods for the transformation of machine readable dictionaries (MRDs) into machine tractable dictionaries, i.e., MRDs converted into a format usable for natural language processing tasks.
The MRD used is The Longman Dictionary of Contemporary English.
We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area.
The solution lies in a grammar design in which lexicalized grammar rules defined in terms of semantic categories and syntactic rules defined in terms of part-of-speech are utilized together.
We present an integrated architecture for word-level and sentence-level processing in a unification-based paradigm.
The core of the system is a CLP implementation of a unification engine for feature structures supporting relational values.
In this framework an IIPSG-style grammar is implemented.
Word-level processing uses X2MoRF, a morphological component based on an extended version of two-level morphology.
In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented.
We propose a detection method for orthographic variants caused by transliteration in a large corpus.
One is string similarity based on edit distance.
The other is contextual similarity by a vector space model.
This paper proposes a method of automatic back transliteration of proper nouns, in which a Japanese transliterated-word is restored to the original English word.
We confirmed the effectiveness of using the target English context by an experiment of personal-name back transliteration.
Chinese NE (Named Entity) recognition is a difficult problem because of the uncertainty in word segmentation and flexibility in language structure.
We then treat the process of selecting the best possible NEs as a multi-agent negotiation problem.
In tins paper, temporal expressions and the context for disambiguation which is called local context are represented using lexical data extracted from corpus and the finite state transducer.
We train a decision tree inducer (CART) and a memory-based classifier (MBL) on predicting prosodic pitch accents and breaks in Dutch text, on the basis of shallow, easy-to-compute features.
We design and test a sentence comparison method using the framework of Robust Minimal Recursion Semantics which allows us to utilise the deep parse information produced by Jacy, a Japanese HPSG based parser and the lexical information available in our ontology.
Our method was used for both paraphrase detection and also for answer sentence selection for question answering.
This paper describes a domain-independent, machine-learning based approach to temporally anchoring and ordering events in news.
We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences.
We have developed an effective probabilistic classifier for document classification by introducing the concept of the differential document vectors and DLSI (differential latent semantics index) spaces.
A simple posteriori calculation using the intra- and extra-document statistics demonstrates the advantage of the DLSI space-based probabilistic classifier over the popularly used LSI space-based classifier in classification performance.
This paper introduces a new type of grammar learning algorithm, inspired by string edit dis-tance (Wagner and Fischer, 1974).
This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology.
In its simplest version, the network consists of separate simple recurrent subnetworks for root and inflection identification; both networks take the phone sequence as inputs.
In a more elaborate version of the model, the network learns to use separate hidden-layer modules to solve the separate tasks of root and inflection identification.
We describe the CoNLL-2003 shared task: language-independent named entity recognition.
This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
We argue that probabilistic generative grammars are demonstrably a more psychologically realistic model of phonological competence than standard generative phonology or Optimality Theory.
in a prepositional phrase.
This paper represents initial work on corpus methods for acquiring lexical/semantic pattern lexicons for text understanding.
In this paper, we propose an optimized strategy, called Bottom-Up Filtering, for parsing GPSGs.
This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, part-of-speech and a small-vocabularycharacter-lists feature for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model.
We look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic relatedness plays a role in discourse structure.
After description of the tag system used, we show the results of four experi-ments using a simple probabilistic model to tag Czech texts (unigram, two bigram experiments, and a trigram one).
The experiments use the source channel model and maxi-mum likelihood training on a Czech hand-tagged corpus and on tagged Wall Street Journal (WSJ) from the LDC collection.
In order to compare two different approaches to text tagging ?statistical and rule-based ?we modified Eric Brill's rule-based part of speech tag-ger and carried out two more experiments on the Czech data, obtaining similar results in terms of the error rate.
Our system performs two procedures: Out-of-vocabulary extraction and word segmenta-tion.
We compose three out-of-vocabulary extraction modules: Character-based tag-ging with different classifiers ?maximum entropy, support vector machines, and con-ditional random fields.
We also com-pose three word segmentation modules ?character-based tagging by maximum en-tropy classifier, maximum entropy markov model, and conditional random fields.
We describe the development of a gen-erator for German built by reusing and adapting existing linguistic data and software.
In the context of lexicalized grammars, we propose general methods for lexical disambiguation based on polarization and abstraction of grammatical formalisms.
We describe the architecture of the ILEX system, which supports opportunistic text generation.
This paper presents a glue language account of how negative polarity items (e.g.
Spoken language understanding is a critical component of automated customer service applications.
It adopts dependency decision making and example-based approaches.
憁anner?and roles for nominalmodifiers.
The design of role assignment algorithm is based on the different decision features, such as head-argument/modifier, case makers, sentence structures etc.
It labels semantic roles of parsed sentences.
We describe a resource-based method of morphological annotation of written Korean text.
We show that morphological annotation of Korean text can be performed directly with a lexicon of words and without morpho-logical rules.
We are concerned with dependency- oriented morphosyntactic parsing of running text.
Using SHARDS ~?a semantically-based HPSG approach to the res-olution of dialogue fragments ?~ wewill show how to generate full paraphrases for fragments in dialogue.
This paper shows how higher levels of generalization can be introduced into unification grammars by exploiting methods for typing grammatical objects.
We describe here an algorithm for detecting subject boundaries within text based on a statistical lexical similarity measure.
We also sketch a theoretical foundation for unification- based semantic interpretation, and compare the unification-based approach with more conventional techniques based on the lambda calculus.
Named entity recognition is a fundamental task in biological relationship mining.
This paperemploys protein collocates extracted from a biological corpus to enhance the performance of protein name recognizers.
This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval.
We then present machine learn-ing results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone.
The syntactic processing model posits four modules, recovering phrase structure, long-distance dependencies, coreference, and thematic structure.
In this paper we discuss cascaded Memory- Based grammatical relations assignment.
In this paper we view Lexicalized Tree Adjoining Grammars as the compilation of a more abstract and modular layer of linguistic description : the metagrammar (MG).
MG provides a hierarchical representation of lexicosyntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions.
This paper describes an automatic, context-sensitive, word-error correction system based on statistical language modeling (SLM) as applied to optical character recognition (OCR) post- processing.
The system exploits information from multiple sources, including letter n-grams, character confusion probabilities, and word-bigram probabilities.
Letter n-grams are used to index the words in the lexicon.
Given a sentence to be corrected, the system decomposes each string in the sentence into letter n-grams and retrieves word candidates from the lexicon by comparing string n-grams with lexicon-entry n-grams.
Finally, the word-bigram model and Viterbi algorithm are used to determine the best scoring word sequence for the sentence.
In terms of both speed and mem-ory consumption, graph unification remains the most expensive com-ponent of unification-based gram-mar parsing.
This paper describes automatic tech-niques for mapping 9611 entries in a database of English verbs to Word-Net senses.
Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes;(2) word sense probabilities, from frequency counts in a tagged corpus;(3) semantic similarity of WordNet senses for verbs within the same class;(4) probabilistic correlations between WordNet data and attributes of the verb classes.
In this paper we propose a unified framework for automatic evaluation of NLP applications using N-gram co-occurrence statistics.
The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system.
A computational approach to metonymy and metaphor is proposed that distinguishes between them, literalness, and anomaly.
This paper presents example-based machine translation (MT) based on syntactic transfer, which selects the best translation by using models of statistical machine translation.
In order to incorporate this kind of long distance context dependency, this paper proposes a new MI-Ngram modeling approach.
The MI-Ngram model consists of two components: an ngram model and an MI model.
It is found that MI-Ngram modeling has much better performance than ngram modeling.
In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model.
Pragmatic.
This paper describes a hybrid Chinese word segmenter that is being developed as part of a larger Chinese unknown word resolution system.
The segmenter consists of two components: a tagging component that uses the transforma-tion-based learning algorithm to tag each character with its position in a word, and a merging component that transforms a tagged character sequence into a word-segmented sentence.
This paper outlines the linguistic semantic cornmitments underlying an application which automatically constructs depictions of verbal spatial descriptions.
The paper proposes an information-theory-based method for feature types analysis in probabilistic evaluation modelling for statistical parsing.
We demonstrate that exploiting the contextual information in the messages can noticeably improve email-act classification.
of deep cases relations (or thematic relations).
The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.
The original Rosetta grammar formalism, called M-gramrnars, was a computational variant of Montague grammar.
The result of the M-Parser algorithm is a syntactic derivation tree which reflects the history of the analysis process.
The M-Generator algorithm generates a set of S-trees by bottom-up application of M-rules, the names of which are mentioned in a syntactic derivation tree.
Next, each semantic derivation tree is mapped onto a set of S-trees of the target language.
The new formalism was called controlled M-grammars.
For the analysis part we use an ITPSG-based (Pollard and Sag 87) syntax and semantics that is further developed for German.
85.
We introduce a new approach to create concept-based text representations, and apply it to a standard text categorization collection.
This paper describes a statistical model for extraction of events at the sentence level, or "semantic tagging", typically the first level of processing in Information Extraction systems.
solving a task requiring data retrieval.
Our approach is the reverse- engineer text categorization to supply mappings from ordinary language vocabulary to specialist vocabulary by constructing maximum likelihood mappings between words and phrases and classification schemes.
We apply BioAR to the protein names in the biological interactions as extracted by our biomedical information extraction system, or BioIE, in order to construct protein pathways automatically.
This paper describes efforts to improve an automatic tagging system which identifies and classifies discourse markers in Chinese texts by applying machine learning (ML) to the disambiguation of discourse markers, as an integral part of automatic text summarization via rhetorical structure.
Encouraging results are reported.Keywords: discourse marker, Chinese corpus, rhetorical relation, automatic tagging, machine learning
We consider a logicist approach to natural language understanding based on the translation of a quasi-logical form into a temporal logic, explicitly constructed for the representation of action and change, and the subsequent reasoning about this semantic structure in the context of a background knowledge theory using automated theorem proving techniques.
The approach is substantiated through a proof-ofconcept question answering system implementation that uses a head-driven phrase structure grammar developed in the Linguistic Knowledge Builder to construct minimal recursion semantics structures which are translated into a Temporal Action Logic where both the SNARK automated theorem prover and the Allegro Prolog logic programming environment can be used for reasoning through an interchangeable compilation into first-order logic or logic programs respectively.
This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser.
This paper explores the automatic construction of a multilingual Lexical Knowledge Base from pre-existing lexical resources.
We present a new and robust approach for linking already existing lexical/semantic hierarchies.
The Smooth Injective Map Recognizer (SIMR) algorithm presented here is a generic pattern recognition algorithm that is particularly well-suited to mapping bitext correspondence.
This paper introduces a new approach to morpho-syntactic analysis through Humor 99 (High-speed Unification Morphology), a reversible and unification-based morphological analyzer which has already been integrated with a variety of industrial applications.
Occurence patterns of words in documents can be expressed as binary vectors.
We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.
In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci.
For example, we use the form "X of Y" for estimating referents of demonstrative adjectives.
We describe a re- estimation process which uses the accumulated counts of hypernyms of the alternative senses in order to redistribute the count.
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank.
This paper introduces a new paradigmatic similarity measure and presents a minimally supervised learning approach combining effective se-lection and weighting methods based on paradigmatic and contextual similarity measures populated from large quanti-ties of inexpensive raw text data.
Language processing of the corpus texts so far included morpho-syntactic analysis, POS tagging and shallow syntactic parsing.
The main application of name searching has been name matching in a database of names.
This paper investigates semantic and pragmatic presupposition in Discourse Representation Theory (DRT) and enhances the pragmatic perspective of presupposition in DRT.
We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text.
One method is adapted from POS tagging, the other is based on finite state transducers.We show experimental results for letter e on the French version of the Medical Subject Headings thesaurus.
Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling.
We describe a program for assigning correct stress contours to nominals in English.
We have also investigated the related issue of parsing complex nominals in English.
This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information sources.
Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events, transformation- based learning is used to detect edit disfluencies and conversational fillers.
We evaluate the inequality ME model using text categorization datasets.
We address the text-to-text generation problem of sentence-level paraphrasing ?a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing.
Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences.
This paper describes a prototype for automatically scoring College Board Advanced Placement (AP) Biology essays.'.
One hundred training essays were used to build an example-based lexicon and concept grammars.
Thus the proper mode of representation for discourse particles in a system coincides with the framework of cooperative question-answering.
?Edison Invented the telegraph.
discourse particles, onto subsets of DR.
We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose.
This paper proposes a method for generating a logicalconstraint-based internal representation from a unification grammar formalism with disjunctive information.
We present our work on open-domain multi-document summarization in the framework of Web search.
We present a task-based extrinsic evaluation of the quality of the produced multi-document summaries.
We describe our initial investigations into generating textual summaries of spatiotemporal data with the help of a prototype Natural Language Generation (NLG) system that produces pollen forecasts for Scotland.
We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT shared- task.
This paper describes a new templaterepresentation and generalization method.
Combing a semantic diction-ary, it uses multiple semantic codes to represent a paraphrase template.
This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs).
We compare the performance of the learned PCFG grammars and log linear models over the same features.
We propose a lexical organisation for multilingual lexical databases (MLDB).
We also present our current work in defining and prototyping a specialised system for the management of acception-based MUM.Keywords: multilingual lexical database, acception, linguistic structure.
This paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation.
The translation model suggested here first performs chunking.
This paper deals with the discovery, representation, and use of lexical rules (LRs) during large-scale semi-automatic computational lexicon acquisition.
One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.
In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.
We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
This paper then applies the graph-structured stack to various natural language parsing methods, including ATN, LA parsing, categorial grammar and principle- based parsing.
The paper gives an overview of repair sequences used in Estonian spoken information dialogues.
The lexicon is defined in terms of a lexicalized Tree Adjoining Grammar, which is subsequently mapped to a FS representation.
We present results using Parsli on an application that creates 3D-images from typed input.
-- the problem of diacritics and digraphs.
We present a scheme for efficiently representing a lexicalized tree-adjoining grammar (LTAG).
Furthermore, the lexical and syntactic rules can be used to derive new elementary trees from the default structures specified in the hierarchical lexicon.In the envisaged scheme, the use of a hierarchical lexicon and of lexical and syntactic rules for lexicalized tree-adjoining grammars will capture important linguistic generalizations and also allows for a space efficient representation of the grammar.
We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus.
We propose a solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar (CCG).
The parsing model uses predicate-argument dependencies for training, which are derived from sequences of CCG lexical categories rather than full derivations.
This paper presents an approach to identifying conjuncts of coordinate conjunctions appearing in text which has been labelled with syntactic and semantic tags.
We describe the CoNLL-2002 shared task: language-independent named entity recogni-tion.
This paper presents an LTAG account for binding of reflexives and reciprocals in English.
These include a QA Typology with answer patterns, WordNet, information about typical numerical answer ranges, and semantic relations identified by a robust parser, to filter out likely-looking but wrong candidate answers.
We present a new grammatical formalism called Constraint Dependency Grammar (CDG) in which every grammatical rule is given as a constraint on wordto-word modifications.
CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees.
In this paper, we propose ellipsis handling method for follow-up questions in Information Access Dialogue (IAD) task of NTCIR QAC3.
We report here the contribution of default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, selectional preferences, and topics or subject fields.
This paper presents baseline unsupervised techniques for performing word alignment based on geometric and word edit distances as well as supervised fusion of the results of these techniques using the nearest neighbor rule.
This paper describes a word and phrase alignment approach based on a dependency analysis of French/English parallel corpora, referred to as alignment by 搒yntax-based propagation.
This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations.
We show how adjunction allows us to clexicalize' a CFG freely.We then show how a `lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for `lexicalized' grammars is discussed.
Zellidja grant.
lexical rules in LFG, used also by HPSG, or Gross 1984's lexicon- grammar).
We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases.
FERGUS uses twodistinct stochastic models, a tree model which refersto a grammar, and a linear language model.
We useautomatic grammar extraction techniques to extractgrammars from different-sized tree banks, and thenuse these extracted grammars to train the tree mod-els.
Major components of the structure are discourse relations and "rhetorical types" (pragmatic objects similar to speech acts) to be discovered via paraphrase relations.
As illustration, we list the discourse relations and rhetorical types needed to generate a paragraph-length discourse studied by Mann and Thompson.
The SENSEVAL-3 task to perform word-sense disambiguation of WordNet glosses was designed to encourage development of technology to make use of standard lexical resources.
This paper describes a new method for aligning real bilingual texts using sentence pair location information.
In the GPSG framework.
In word sense disambiguation, a system attempts to determine the sense of a word from contextual features.
We call our system SNOOD (Hop-kins APL Inductive Retargetable Named Entity Tagger) .
This paper	presents	a	declarative,	dependencyconstraint model for parsing an inflectional free word order language, like Finnish.
In this paper we demonstrate that speech recognition can be effectively applied to information retrieval (IR) applications.
Termed "Semantic Co-occurrence Filtering" this enables the system to simultaneously disambiguate word hypotheses and find relevant text for retrieval.
The system is built by integrating standard IR and speech recognition techniques.
The paper is concerned with automatic classification of new lexical items into synonymic sets on the basis of their co- occurrence data obtained from a corpus.
Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary.
We offer a semantics and pragmatics of the pluperfect in narrative discourse.
In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result.
These probabilities are estimated using statistical decision tree models.
This paper reports on two contributions to large vocabulary continuous speech recognition.
In this paper we present word sense disambiguation (WSD) experiments on ten highly polysemous verbs in Chinese, where significant performance improvements are achieved using rich linguistic features.
For a few verbs, semantic role information actually hurt WSD performance.
Our character-based best match retrieval method can retrieve translation examples similar to the given input.
We show the retrieval examples with the following characteristic features: phrasal expression, long-distance dependency, idiom, synonym, and semantic ambiguity.
This paper presents Archivus, a multi- modal language-enabled meeting browsing and retrieval system.
We present two semi-supervised learning techniques to improve a state-of-the-art multi-lingual name tagger.
We describe effective measures to automatically select documents and sentences.
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.
We present a novel approach to parsing phrase grammars based on Eric Brill's notion of rule sequences.
Our information extraction prototype, Alembic, is in fact based on a pipeline of rule sequence processors thatrun the gamut from part-of-speech tagging, to phrase identification, to sentence parsing, to inference (Aberdeeen et al.
The morphological rules arc followed by contextual transformations: these rules inspect lexical context to relabel lexemes that are ambiguous with respect to part-of-speech.
This paper describes a prototype news analysis system which classifies and indexes news stories in real time.
Approaches are wide and include key wording, statistical analysis, pattern matching, and a method using lexical, syntactic, and semantic filters.
1988/ does describe a strictly pattern matching approach to news categorization.)
Several example stories and their indexes are also provided.3 0 The Architecture of NASNAS consists of four major subsystems, viz., a stream filter, a lexical scanner, a parser, and a semantic processor or filter working sequentially as listed.
This paper presents a statistical method for finger-printing text.
Our method exploits the characteristic distribution of word trigrams, and measures to determine similarity are based on set theoretic principles.
Compound noun phrases provide typical examples.
Traditionally, supervised machine learning approaches adopt the single- candidate model.
By contrast, our approach adopts a twin-candidate learning model.
The impact of word alignment on MT quality is investigated, using a phrase-based MT system.
This paper discusses a system for grammat-ically describing and parsing entries from machine-readable dictionary tapes and a lexical data base representation for storing the dictionary information.
We describe the design and implementation of the dialogue management module in a voice operated car-driver information system.
This paper reports on ongoing worka CALL system to facilitate foreign lan-guage learning: GLOSSER-RuG.
Follow-ing a brief introduction to the project, the paper describes the architecture of GLOSSER-RuG.
This paper introduces to the finite-state calculus a family of directed replace operators.
Other useful applications of directed replacement include tokenization and filtering of text streams.
This paper proposes a statistical, treeto-tree model for producing translations.
Two main contributions are as follows: (1) a method for the extraction of syntactic structures with alignment information from a parallel corpus of translations, and (2) use of a discriminative, feature- based model for prediction of these target- language syntactic structures梬hich we call aligned extended projections, or AEPs.
This paper presents an approach for detecting semantic relations in noun phrases.
A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation.
In this paper, the search engine Intuition is described.
In order to realize multi-document summarization focused by multiple questions, we propose a method to calculate sentence importance using scores produced by a Question-Answering engine in response to multiple questions.
We also describe an integration of it into a generic multi-document summarization system.
This paper presents our work on accumulation of lexical sets which includes acquisition of dictionary resources and production of new lexical sets from this.
The paper also proposes a combinatorial and dynamic organization for lexical systems, which is based on the notion of virtual accumulation and the abstraction levels of lexical sets.Keywords: dictionary resources, lexicalacquisition,	lexical	production,	lexicalaccumulation, computational lexicography.
Unlike conventional methods that use optical character recognition (OCR), we con-vert document images into word shape tokens, a shape-based representation of words.
We report the impact of speech recognition errors on speech act identification and discuss how standard control mechanisms can participate to robustness by assisting the user in repairing the consequences of speech recognition errors.
This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique.
Second, we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which finds non- local dependencies while parsing.
The first method involves generating a set of (unimodal) PELs for a given speaker by clustering the hypothetical frames found in the spectral models for that speaker, and then constructing speaker-dependent PEL sequences to represent each PIC.
We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic.
Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues.
Our experiments show that punctuation is of little help in parsing spoken language and extracting sub- categorization cues from spoken language.
This paper describes the English朒indi Multilingual lexical sample task in SENSEVAL?.
A method for generating a machine translation (MT) dictionary from parallel texts is described.
We report on a head-driven way to generate a language- specific representation for a language-independent conceptual structure.
This paper presents LexPhon, a computational framework for modelling segmental aspects of Lexical Phonology (LP).
This paper describes a text mining technique to aid an Ontology Engineer to identify the important concepts in a Domain Ontology.
We present a novel approach to the problem of overfitting in the training of stochastic mod-els for selecting parses generated by attribute-valued grammars.
We also developed an alignment algorithm of graphemes and phonemes for both ordinary text and OCR out-put.
We show, by experiment, that the combination of the grapheme-phoneme tuple ngram model and the grapheme-phoneme alignment algorithm significantly improve character recognition accuracy if both grapheme and phoneme representations are given.
It is embedded to the C-value approach for automatic term recognition (ATR), in the form of weights constructed from statistical characteristics of the context words of the candidate string.
The usefulness of a statistical approach suggested by Church and Hanks (1989) is evaluated for the extraction of verb-noun ( V-N) collocations from German text corpora.
We present precision and recall results for V-N collocations with support verbs and discuss the consequences for further work on the extraction of collocations from German corpora.
This paper describes an n-gram based reinforcement approach to the closed track of word segmentation in the third Chinese word segmentation bakeoff.
Character n-gram features of unigram, bigram, and trigram are extracted from the training corpus and its frequencies are counted.
Definite and loose segmentation are performed simply based on the bigram and trigram statistics.
We describe a distributed, modular architecture for platform independent natural language systems.
It features automatic interface generation and self-organization.
We present a constancy rate principle governing language generation.
This paper discusses interactions between negative concord and restructuring/clause union in Palestinian Arabic.
Analysis of a subset of tickets, guided by sublanguage theory, identified linguistic patterns, which were translated into rule-based algorithms for automatic identification of tickets?discourse structure.
Anaphora resolution for dialogues is a difficult problem because of the several kinds of complex anaphoric references generally present in dialogic discourses.
In this paper, we describe a system for anaphora resolution in multi-person dialogues.
In our system, we propose a new technique based on the use of anaphora chains to enable resolution of a large variety of anaphors, including plural anaphora and cataphora.
Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based methods to the output of an Automatic Speech Recognition (ASR) system.
In this paper, we propose a hybrid approach to chunking Chinese base noun phrases (base NPs), which combines SVM (Support Vector Machine) model and CRF (Conditional Random Field) model.
This paper presents the integration of a large-scale, reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer.
Summarization and Question Answering need precise linguistic information with a much higher coverage than what is being offered by currently available statistically based systems.
The heart of the system is a rule-based top-down DCG-style parser, which uses an LFG oriented grammar organization.
Most natural language processing tasks require lexical semantic information.
This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information.
We begin by proposing a typology of paraphrases.
This paper discusses an approach to planning the content of instructional texts.
In this paper we describe a way to discover Named Entities by using the distribution of words in news articles.
This paper describes an hierarchical approach to WordNet sense distinctions that provides different types of automatic Word Sense Disambiguation (WSD) systems, which perform at varying levels of accuracy.
Finally, we discuss how lexical selection is influenced by thematic (focus) information in the input.
The objective of this paper is to present a new dimension of Game Theoretic Semantics (GTS) using the idea of the coordination problem game to explain the semantics of metaphor.
Research in Named Entity extraction is no exception.
We report results for training and test-ing an automatic classifier to label the in-formation provider抯 utterances in spoken human-computer and human-human dia-logues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.
In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue.
This paper focuses on the use of Finite State morphological analysis (FST) resources for Irish and Part of Speech (POS) taggers for German.
Latent Semantic Analysis (LSA) is a statistical Natural Language Processing (NLP) technique for inferring meaning from a text.
We introduce a first-order language for semantic underspecification that we call Constraint Language for Lambda-Structures (CLLS).
CLLS features constraints for dominance, lambda binding, parallelism, and anaphoric links.
In this paper, we apply this technique to build micro- planning rules for preventative expressions in instructional text.
We call sentences like (2) semantically disambiguatable garden path sentences (SDGPs).
We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation.
We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs.
We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution.
This paper proposes a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse.
We investigate the extension of basic feature logic with subsumption (or matching) constraints, based on a weak notion of subsumption.
This paper presents a way to compute rela-tive social status of the individuals involved in Korean dialogue.
Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results.
This paper explores the role of information retrieval in answering 搑elationship?questions, a new class complex information needs formally introduced in TREC 2005.
The paper describes GEMS, a system for Generating and Expressing the Meaning of Sentences, focussing on the generation task, i.e.
GEMS is lexically distributed.
In this paper we consider the problem of analysing sentence-level discourse structure.
We introduce discourse chunking (i.e., the identification of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing.
We also demonstrate how discourse chunking can be successfully applied to a sentence compression task.
We present a new compositional tense-aspect deindexing mechanism that makes use of tense trees as components of discourse contexts.
In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).
We present an extension of the classic A* search procedure to tabular PCFG parsing.
The strategic lazy incremental copy graph unification method is a combination of two methods for unifying feature structures.
This paper describes a method to automatically create and maintain gazetteers for Named Entity Recognition (NER).
Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).
This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).
This paper studies and evaluates disambiguation strategies for the translation of tense between German and English, using a bilingual corpus of appointment scheduling dialogues.
It describes a scheme to detect complex verb predicates based on verb form subcategorization and grammatical knowledge.
We introduce BLANC, a family of dynamic, trainable evaluation metrics for machine translation.
Towards this end, we discuss ACS (all common skipngrams), a practical algorithm with trainable parameters that estimates reference- candidate translation overlap by computing a weighted sum of all common skipngrams in polynomial time.
We have found that it is possible to identify many of these interesting co-occurrence relations by computing simple summary statistics over millions of words of text.
This paper describes the roles of disjunctions and inheritance in the use of feature structures and their formal semantics.
We illustrate the approach for the acquisition of lexical information for several classes of nomi n als.Keywords: Knowledge Acquisition, Information Retrieval, Lexical Semantics.
This paper investigates stacking and voting methods for combining strong classifiers like boosting, SVM, and TBL, on the named-entity recognition task.
Finally, we combine the sense proximity estimation with a classification of semantic relations between senses.
Most recent research in trainable part of speech taggers has explored stochastic tagging.
In this paper, we describe a number of extensions to this rule-based tagger.
First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express.
We present a procedure which, given a cfg and phrase-structure semantics for a source language and a cfg and phrase-structure semantics for a target language, will (usually) produce the finite set of ttee-replacement rules for tne translation, if the translation exists.
We present an LFG-DOP parser which uses fragments from LFG-annotated sentences to parse new sentences.
We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes.
We have implemented a system for generating and applying corpus-based patterns.
The method is based on a structured probabilistic model of the domain, and unsupervised learning is performed with the EM algorithm.
This paper proposes an input-splitting method for translating spoken-language which includes many long or ill-formed expressions.
In this study we propose a method of segment-ing a sentence.
This paper describes a system for un-supervised learning of morphological af-fixes from texts or word lists.
The system is composed of a generative probability model and a search algorithm.
This paper describes NJFun, a real-time spoken dia-logue system -Out-provides users with information about things to do in New Jersey.
We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.
This paper describes a statistics-based Chinese parser, which parses the Chinese sentences with correct segmentation and POS tagging information through the following processing stages: 1) to predict constituent boundaries, 2) to match open and close brackets and produce syntactic trees, 3) to disambiguate and choose the best parse tree.
of Computing Tech., CAS in the ACL- SIGHAN-sponsored First International Chinese Word Segmentation Bake- off.
The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks.
To perform this task, we build decision tree classifiers that use a combination of simple speech features (speech lengths and spoken keywords) extracted from the participants?speech in meetings.
We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel).
We propose a plan-based approach for responding to user queries in a collaborative environment.
We present a novel application of NLP and text mining to the analysis of financial documents.
In this paper, we introduce a new approach to lexical or-ganization that leads to more compact and flexible lexicons.
We show how a data- driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.
Our dictation system uses character-trigram probabilities as a source model obtained from a text database consisting of both ICanji and Kana, and generates Kanji-and-Kana sequences directly from input speech.
An anaphor resolution algorithm is pre-sented which relies on a combination of strategies for narrowing down and select-ing from antecedent sets for reflexive pro-nouns, nonreflexive pronouns, and com-mon T101MS.
Applications to speech-to-document alignment and more generally to meeting processing and retrieval are finally discussed.
This paper describes the open-source Phrase-Based Statistical Machine Translation Decoder - Phramer.
This paper describes a heuristic approach to automatically identifying which senses of a machine- readable dictionary (MRD) headword are semantically related versus those which correspond to fundamentally different senses of the word.
It provides a large amount of semantic information such as semantic hierarchy and collocation features for 66,539 Chinese words and their English counterparts.
Its POS and semantic classification represent the latest progress in Chinese linguistics and language engineering.
The paper introduces the outline of SKCC, and indicates that it is effective for word sense disambiguation in MT applications and is likely to be important for general Chinese language processing.Key words: Semantic knowledge-base, lexical semantic, computational lexicography, word sense disambiguation ^WSD^ , Chinese language processing
We describe the IG (In-teraction Grammar) formalism, an extension of DT-D's which permits powerful linguistic manipulations, and show its application to the production of multi-lingual versions of a certain class of pharmaceutical documents.
This paper introduces SENSELEARNER ?a minimally supervised sense tagger that attempts to disambiguate all content words in a text using the senses from WordNet.
We propose a question answering system which uses an encyclopedia as a knowledge base.
Then linguistic patterns and HTML structures are used to extract text fragments describing the term.
Covering ambiguity is one of the two basic types of ambiguities in Chinese word segmentation.
We select 90 frequent cases of covering ambiguities as the target.
State of the art in statistical machine translation is currently represented by phrase- based models, which typically incorporate a large number of probabilities of phrase-pairs and word n-grams.
In this work, we investigate data compression methods for efficiently encoding n-gram and phrase-pair probabilities, that are usually encoded in 32-bit floating point numbers.
This paper also proposes an objective method of classifying these con-structs using a large amount of linguistic data.
The feasibility of this was verified with a self-organizing semantic map based on a neural net-work model.
We have participated in three open tracks of Chinese word segmentation and named entity recognition tasks of SIGHAN Bakeoff3.
Our named entity recognizer achieved the highest F measure for MSRA, and word segmenter achieved the medium F measure for MSRA.
We find effective combining of the external multi-knowledge is crucial to improve performance of word segmentation and named entity recognition.
Finally, I outline a theory of lexical semantics embodying a richer notion of compositionality, termed cocomposition, which aims to spread the semantic load more evenly throughout the lexicon.
We investigate the lexical and syntactic flexibility of a class of idiomatic expressions.
We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones.
This paper introduces the new grammar formalism of Extensible Dependency Grammar (XDG), and emphasizes the benefits of its methodology of explaining complex phenomena by interaction of simple principles on multiple dimensions of linguistic description.
problem oriented systems.
The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SMARTKOM - is presented.
Prepositions and verbs are defined as semantic functions that explicate spatial relations among noun images.
domain.
system to analyze Hungarian texts using a morphological.
MORPHOLOGICAL ANALYSISThe first phase is the morphological analysis of word forms.
SIOMS from the archiphonemes of the lexicon.
the grammatical morphemes  of the language.
RULES  normalized sequence of categoriesPARSERinput sentence [MORPHOLOGICAL ANALYZER.
Third, we propose two models (BOTW and BOF) which use domain knowl-edge as textual features for text catego-rization.
We describe the generation of communicative ac-tions in an implemented embodied conversational agent.
User modeling is an important components of dialog systems.
Most previous approaches are rule-based methods.
This paper analyses the intonation of polar questions extracted from a corpus of task- oriented dialogues in the Bari variety of Italian.
This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs).
This paper extends the base noun phrase(BNP) identification into a research on Chinese base phrase identification.
This paper discusses word choice for natural language generation.
The paper describes a new unification based grammar formalism called Regular Unification Grammar (RUG).
The RUG formalismRUG constitutes a unification based grammar formalism /Shieber86/.
This paper describes the formalism for incorporating emerging linguistic theory in a joint model of the acoustic/prosody/concept relationships.
We present a novel approach to the unsupervised detection of affixes, that is, to extract a set of salient prefixes and suffixes from an unlabeled corpus of a language.
In this paper we compare the use of machine translation (MT) to the commonly used dictionary term lookup (DTL) method for Reuter news article alignment in English and Japanese.
This paper presents Japanese morphological analysis based on conditional random fields (CRFs).
First, flexible feature designs for hierarchical tagsets become possible.
We have developed a Text Structurer module which recognizes text-level structure for use within a larger information retrieval system to delineate the discourse-level organization of each document's contents.
An algorithm for the morphological decomposition of words into morphemes is presented.
The application area is information retrieval, and the purpose is to find morphologically related terms to a given search term.
First, the parsing framework is presented, then several linguistic decisions are discussed: morpheme selection and segmentation, morpheme classes, morpheme grammar, allomorph handling, etc.
A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates.
Bagging and boosting, two effective machine learn-ing techniques, are applied to natural language pars-ing.
An automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank.
It con-sists of an approximate word match-ing method and an N-best word seg-mentation algorithm using a statistical language model.
We present an implemented compilation algorithm that translates HPSG into lexicalized feature-based TAG, relating concepts of the two theories.
We propose to handle this by breaking up the morhosyntactic tags into inflectional groups, each of which contains the inflectional features for each (interme(hiate) derived form.
Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflection groups in a trigram model.
The basic mechanism of CLaRK for linguistic pro-cessing of text corpora is the cascade regular gram-mar processor.
In this paper, we present the design of a lexical resource focusing on German verb phrase idioms.
Dictionary-based searching is useful for retrieving biological information in gene units.
In our laboratory, we have developed a gene name dictionary:GENA and a family name dictionary.
The effect of our gene/protein/family recognition method on protein-interaction and protein-function ex-
We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG.
This paper presents a technique for sentence genera-tion.
In this paper we present an approach to structure learning in the area of web documents.
This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics.
Much previous research on topic tracking use machine learning techniques.
This paper presents an empirical analysis of prosodic phenomena (intonation and timing) in 'common ground units' (Nakatani & Traum 1999).
ASL natural language generation (NLG) is a special form of multimodal NLG that uses multiple linguistic output channels.
In this paper, we identify semantic, pragmatic and syntactic features that are required to support a motivated choice of German temporal subordinating conjunctions and prepositions during text production.
A Chinese word segmentation algorithm based on forward maximum matching and word binding force is proposed in this paper.
This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction.
The proposed method builds an explicit error model for word pronunciations.
By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction.
The system makes use of partial and full syntactic information and converts the task into a sequential BIO-tagging.
Building on a state-of-the-art set of features, a binary classifier for each label is trained using AdaBoost with fixed depth decision trees.
This paper presents an HMM-based chunk tagger for Hindi.
Contextual information is incorporated into the chunk tags in the form of part-of-speech (POS) information.
We use logical inference techniques for recognising textual entailment.
This paper discusses one of the problems of machine translation, namely the translation of idioms.
In this paper, we discuss how a paraphrase maw be used as a heuristic device, viz.
We describe an experimental instruction swaths in mathematics incorporating this, feature.
We present the new multilingual version of the Columbia Newsblaster news summarization system.
In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.
In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web.
We start with an unlexicalized PCFG as a baseline model, which is enriched to the level of Collins?Model 2 by adding lexicalization and subcategorization.
Using data from WordNet, we generate 6 types of vocabulary questions.
This paper describes an approach to extract the aspectual information of Japanese verb phrases from a monolingual corpus.
We introduce an 搊racle?score, based on the probability distribution of unigrams in human summaries.
The paper presents a concise description of the LFG-ParserGenerator developed at the EWH in Koblenz.
We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy.
We describe and present evaluation results for Talk抧扵ravel, a spoken dialogue language system for making air travel plans over the telephone.
We present an unsupervised approach to recognizing discourse relations of CON-TRAST, EXPLANATION-EVIDENCE, CONDITION and ELABORATION that hold between arbitrary spans of texts.
A new approach to bottom-up parsing that extends Augmented Context-Free Grammar to a Process Grammar is formally presented.
A Process Grammar (PG) defines a set of rules suited for bottom-up parsing and conceived as processes that are applied by a PG Processor.
The aim of this paper is to introduce a new approach to bottom-up parsing starting from a well known and based framework - parallel bottom-up parsing in immediate constituent analysis, where all possible parses are considered - making use of an Augmented Phrase-Structure Grammar (APSG).
PARSE GRAPH STRUCTUREThe Parse Graph Structure (PGS) is built by the parser while applying grammar rules.
are described.Keywords: Computational Lexicography, Lexical Knowledge Base, Lexical Semantics.
This paper presents a method of automatically constructing information extraction patterns on predicate-argument structures (PASs) obtained by full parsing from a smaller training corpus.
The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns.
Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality.
In particular, we describe an algorithm to obtain the semantic dependencies on a TAG parse forest and construct a target derivation forest with isomorphic or locally non-isomorphic dependencies in 0(n7) time.
Complexity theoretic notion of feasible learnability called "polynomial learnability" to the evaluation of grammatical formalisms for linguistic description.
Specifically, we present a novel, nontrivial constraint on grammacs called "k-locality", which enables a rich class of mildly context sensitive grammars called Ranked Node Rewriting Grammars (RNR( ) to be feasibly learnable.
We describe a parallel implementation of a chart parser for a shared-memory multipro-cessor.
We extend the existing corpus-based measures for identifying LVCs between verb-object pairs in English, by proposing using new features that use mutual information and assess other syntactic properties.
This paper demonstrates two methods to improve the performance of instance- based learning (IBL) algorithms for the problem of Semantic Role Labeling (SRL).
Two IBL algorithms are utilized: k-Nearest Neighbor (kNN), and Priority Maximum Likelihood (PML) with a modified back-off combination method.
In this paper we exhibit a novel approach to the problems of topic and speaker identification that makes use of a large vocabulary continuous speech recognizer.
Our method is based on stochastic incremental parsing, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis.
In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold.
Then we will show how syntactic decisions interact with an intonation grammar.
We shall also introduce two functional notions: STRUCTURE REVERSIBILITY vs. FUNCTIONAL REVERSIBILITY in Italian.
In this paper, we will present the referent identification component of XTRA, a system for a natural-language access to expert systems.
Lexical rules are used in constraint-based grammar formalisms such as Head-Driven Phrase Structure Grammar (IIPSG) (Pollard and Sag 1994) to ex-press generalizations among lexical en-tries.
In this paper, we present a parser based on a stochastic structured language model (SLM) with aflexible history reference mechanism.
An SLM is an alternative to an n-gram model as a language model for a speech recognizer.
Thus SLMs are expected to play an important part in spoken language understanding systems.
We introduce a flexible history reference mechanism called an ACT (arboreal context tree; an extension of the context tree to tree-shaped histories) and describe a parser based on anSLM with ACTs.
This paper describes how a machine- learning named entity recognizer (NER) on upper case text can be improved by using a mixed case NER and some unlabeled text.
We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar.
?fhis paper proposes a new method for Chinese language corpus processing.
Unlike the past researches, our approach has following charactericsties : it blends segmentation with tagging and integrates rule-based approach with statistics-based one in grammatical disambiguation.
In this article we describe research on the development of large dictionaries for natural language processing.
We describe the process of restructuring the information in the dictionary and our use of the Longman grammar code system to construct dictionary entries for the PATR-H parsing system and our use of the Longman word definitions for automated word sense classification.
in this paper, we present a summarization system for spontaneous dialogues which consists of a. novel multi-stage architecture.
In this paper we present an overview of recent developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic theory of discourse structure.
We give a novel approach to the problem of discourse segmentation based on discourse semantics and sketch a limited but robust approach to symbolic discourse parsing based on syntactic, semantic and lexical rules.
Pronunciation-by-analogy (PbA) is an emer-ging technique for text-phoneme conversion based on a psychological model of read-ing aloud.
This paper explores the impact of certain basic implementational choices on the performance of various PbA mod-els.
We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words.
A method is presented for segmenting text into subtopic areas.
The lexical cohesion relations of reiteration and collocation are used to identify related words.
These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.
Prepositional Phrase-attachment is a common source of ambiguity in natural language.
We propose to solve the PP-attachment ambiguity with a Support Vector Machines learning model that uses complex syntactic and semantic features as well as unsupervised information obtained from the World Wide Web.
This paper presents an algorithm for extracting invertible probabilistic translation grammars from bilingual aligned and linguistically bracketed text.
This paper proposes a Japanese/English cross- language information retrieval (CUR) system targeting technical documents.
To counter the first problem, we use a compound word translation method, which uses a bilingual dictionary for base words and collocational statistics to resolve translation ambiguity.
For the second problem, we propose a transliteration method, which identifies phonetic equivalents in the target language.
One model with some experimental support uses distributional statistics of sound sequence predictability (Saffran et al.
We briefly describe a two-way speech-tospeech English-Farsi translation system prototype developed for use in doctor- patient interactions.
This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al.
The Chinese character-based maximum entropy model, which switches the word segmentation task to a classification task, is adopted in system developing.
This paper describes the technology and an experiment of subcategorization acquisition for Chinese verbs.
A crucial problem in topic recognition is how to identify topic continuation.
We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation.
Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora.
We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.
This paper describes an innovative evaluation regimen developed for the text meaning representations (TMRs) produced by the Ontological Semantic (OntoSem) general	purpose	syntactic-semanticanalyzer.
The overall architecture of the OntoSem semantic analyzer.
We address the problem of automati-cally acquiring case frame patterns (se-lectional patterns) from large corpus data..
We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution, where random vari-ables represent case slots.
This	paper	describes	a	syllable-basedcomputational model for the Korean morphology.
They are morphological transformation and morpheme Identification.
Two-level model and syllable--based formalism focussed on the problem of morphological transformation(13ear88, Cahi90, Kosk83i.
Analysis candidates are generated as a reverse process of word formation rules: morpheme isolation and morphological transformation.
In this paper, I describe SAGE and its components in the context of event descriptions.
This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features.
Second, a statistical grammar is used for determining the conceptual roles of the noun responses.
We present prominent syntax- semantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions.
The SAMMIE' system is an in-car multi-modal dialogue system for an MP3 application.
A new type of stochastic grammars is introduced for investigation: weakly restricted stochastic grammars.
In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure.
We develop a framework for formaliz-ing semantic construction within gram-mars expressed in typed feature struc-ture logics, including HPSG.
A model of Japanese honorific expressions in situation semantics is proposed.
This paper describes informally an algorithm for the generation from under- and overspecified feature structures.
A new model for statistical translation is presented.
We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.
First, we investigate how well the addressee of a dialogue act can be predicted based on gaze, utterance and conversational context features.
We present a study of the mappings from semantic content to syntactic ex-pression with the aim of isolating the precise locus and role of pragmatic infor-mation in the generation process.
The study reveals how multilin-gual NLG can be informed by language-specific principles for syntactic choice.
We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.
Our formulation uses a factorization analogous to the standard dynamic programs for parsing.
In this paper we present results on developing robust natural language interfaces by combining shallow and partial interpretation with dialogue management.
The system uses a structural trans-fer approach in translating the domain of IBM computer manuals.
We present a machine learning approach to the problem of extracting roots of Hebrew words.
This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling.
We report two applications of this approach: PP-attachment and POStagging.
This paper argues for the development of parallel treebanks.
This paper describes the symbolic and statistical hybrid approaches to solutions of problems of the previous English-to-Korean machine translation system in terms of the improvement of translation quality.
In this paper, a Generalized Probabilistic Semantic Model (GPSM) is proposed for preference computation.
An effective semantic tagging procedure is proposed for tagging semantic features.
A semantic score function is derived based on a score function, which integrates lexical, syntactic and semantic preference under a uniform formulation.
bilingual dictionaries).
We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system.
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.
Recent results have established that there is a family of languages that is exactly the class of languages generated by three independently developed grammar formalisms: Tree Adjoining Grammars, Head Grammars, and Linear Indexed Grammars.
We discuss the structural descriptions produced by Combinatory Categorial Grammars and compare them to those of grammar formalisms in the class of Linear Context-Free Rewriting Systems.
In this paper we propose a small set of lexical conceptual relations which allow to encode adjectives in computational relational lexica in a principled and integrated way.
Language understanding work at Paramax focuses on applying general-purpose language understanding technology to spoken language understanding, text understanding, and document processing, integrating language understanding with speech recognition, knowledge-based information retrieval and image understanding.
SPLAT (Sentence Plan Language Authoring Tool) is an authoring tool intended to facilitate the creation of sentence-plan specifications for the Penman natural language generation system.
SPLAT uses an example- based approach in the form of sentence-plan templates to aid the user in creating and maintaining sentence plans.
In this paper we apply conditional random fields (CRFs) to the semantic role labelling task.
We define a random field over the structure of each sentence抯 syntactic parse tree.
They used root-based clusters to substitute for dictionaries in indexing for information retrieval.
This paper presents a new approach to partial parsing of context-free structures.
This mechanism is implemented in an interactive argumentation sys-tem.
We show on the task of non-anaphoric it identification how to overcome these handicaps with the Bayesian Network (BN) formalism.
We view the QA problem as a classification problem and as a re- ranking problem.
Our method integrates two probability parameters to perform zero pronoun detection and resolution in a single framework.
This paper presents the LiLFeS system, an efficient feature-structure description language for HPSG.
The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics, proposed by Carpenter and Qu.
The paper discusses two implementations of the LiLFeS.
This paper describes an operational se-mantics for DATR theories.
The seman-tics is presented as a set of inference rules that axiomatises the evaluation relation-ship for DATR expressions.
In this paper we present a polynomial time parsing algorithm for Combinatory Categorial Grammar.
The recognition phase extends the CKY algorithm for CFG.
This paper presents a detailed study of the integration of knowledge from both dependency parses and hierarchical word ontologies into a maximum-entropy-based tagging model that simultaneously labels words with both syntax and semantics.
We propose a novel method for inducing monolingual semantic hierarchies and sense clusters from numerous foreign-language-to-English bilingual dictionaries.
The method exploits patterns of non-transitivity in translations across multiple languages.
We then propose a monolingual synonymy measure derived from this aggregate resource, which is used to derive multilinguallymotivated sense hierarchies for monolingual English words, with potential applications in word sense classification, lexicography and statistical machine translation.
In this paper we investigate the incorporation of Tree Adjoining Grammars (TAG) into the systemic framework.
A formalism is presented for lexical specification in unification-based grammars which exploits defeasible multiple inheritance to express regularity, sub- regularity, and exceptions in classifying the properties of words.
Variants of these using feature weighting by entropy reduction were systematically compared, as was the representation of diphthongs (as one symbol or two).
We describe a new algorithm for compiling rewrite rules into FSTs.
We present a phonological probabilistic context-free grammar, which describes the word and syl-lable structure of German words.
Both grammars are evaluated qualitatively showing that probabilistic context-free grammars can contribute linguistic knowledge to phonology.
Generalizing from efforts parsing natural language sentences using the grammar formalism, Dependency Grammar (DG) has been emulated by a context-free grammar (CFG) constrained by grammatical function annotation.
This paper describes an experimental implementation of this approach using unification to realize grammatical function constraints imposed on a dependency structure backbone emulated by a context-free grammar.
is expanding the repertoire of commercial user interfaces by incorporating multimodal techniques combining traditional point and click interfaces with speech recognition, speech synthesis, and gesture recognition.
This article is devoted to the problem of quantifying noun groups in German.
In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.
In this paper, we present a method for stochastic finite-state ma-chine translation that is trained automatically from pairs of source and target utterances.
We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances.
A word-based morphological analyzer and a dictionary for recognizing inflected forms of French words have been built by adapting the UDICT system.
This work lays the groundwork for doing French derivational morphology and morphology for other languages.
We applied an automatic lexical acquisition technique over parsed texts to identify semantically similar words.
After that, we made use of this lexical knowledge to resolve coreferent definite descriptions where the head-noun of the anaphor is different from the head-noun of its antecedent, which we call indirect anaphora.
Several parsers have been proposed that utilize the all-subtrees representation (e.g., tree kernel and data oriented parsing).
A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector.
We start by defining a classification schema for adjectives based on their syntactic and semantic properties.
Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.
This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT).
Coverage: The project adopted a corpus-based approach.
The goal of the on-going project described in this paper is evaluation of the utility of Latent Semantic Analysis (LSA) for unsupervised word sense discrimination.
Our results indicate that we can generate 抔ood?summaries even when using only acoustic/prosodic information, which points toward the possibility of text-independent summarization for spoken documents.
It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model.
3.3 GB ytes of text.
We present TISC, a multilingual, language- independent and context-sensitive spelling checking and correction system designed to facilitate the automatic removal of non- word spelling errors in large corpora.
Its lexicon is derived from raw text corpora, without supervision, and contains word unigrams and word bigrams.
A second, related theme is the use of speech and text-image recognition to retrieve arbitrary, user-specified information from documents with signal content This paper discusses three research initiatives at PARC that exemplify these themes: a text-image editor[1], a wordspotter for voice editing and indexing [12], and a decoding framework for scanned-document content retrieval[4].
1 The discussion focuses on key concepts embodied in the research that enable novel signal-based document processing functionality.
We present a systematic comparison of memory-based learning (MBL) and support vector machines (SVM) for inducing classifiers for deterministic dependency parsing, using data from Chinese, English and Swedish, together with a variety of different feature models.
The results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models.
In this paper, we propose a tree annotation tool using a parser in order to build a treebank.
We present an unsupervised learning strategy for word sense disambiguation (WSD) that exploits multiple linguistic resources including a parallel corpus, a bilingual machine readable dictionary, and a thesaurus.
The approach is based on Class Based Sense Definition Model (CBSDM) that generates the glosses and translations for a class of word senses.
We consider here the problem of Chinese named entity (NE) identification using statistical language model(LM).
In this research, word segmentation and NE identification have been integrated into a unified framework that consists of several class-based language models.
Our experiments further demonstrate the improvement after seamlessly integrating with linguistic heuristic information, cache-based model and NE abbreviation identification.
This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.
Fine- grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.
<li> tag for HTML.
This paper describes experiments in Machine Learning for text classification using a new representation of text based on WordNet hypernyms.
Six binary classification tasks of varying difficulty are defined, and the Ripper system is used to produce discrimination rules for each task using the new hypernym density representation.
Rules are also produced with thecommonly used bag-of-words representation,incorporating no knowledge from WordNet.
This paper describes the use of statistical analyses of untagged corpora to detect similarities and differences in the meaning of words in text.
The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling.
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
Most of errors in Korean morphological analysis and POS (Part-of-Speech) tagging are caused by unknown morphemes.
This paper presents a generalized unknown morpheme handling method with POSTAG(POStech TAGger) which is a statistical/rule based hybrid POS tagging system.
The generalized unknown morpheme guessing is based on a combination of a morpheme pattern dictionary which encodes general lexical patterns of Korean morphemes with a posteriori syllable tri-gram estimation.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined.
The characteristics of SLTAG are unique and novel since it is lexically sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars).Then, two basic algorithms for SLTAG are introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outsidelike iterative algorithm for estimating the parameters of a SLTAG given a training corpus.Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars.
We present a memory-based learning (MBL) approach to shallow parsing in which POS tagging, chunking, and identification of syntactic relations are formulated as memory-based modules.
This paper investigates the syntax of extraposition in the HPSG framework.
We present a system for computing similarity between pairs of words.
Our tests focus on the identification of cognates ?words of common origin in related languages.
The methodology is centered on the use of a dependency grammar based parser.
We present a syntax-based statistical translation model.
We describe a case study in the ap-plication of symbolic machine learning techniques for the discovery of linguis-tic rules and categories.
A supervised rule induction algorithm is used to learn to predict the correct diminutive suffix given the phonological representation of Dutch nouns.
We discuss the relevance of our method for linguis-tics and language technology.
A new algorithm is presented for estimating the parameters of a stochastic context-free grammar (SCFC) from ordinary unparsed text.
The trellis is a generalization of that used by the Baum-Welch algorithm which is used for estimating hidden stochastic regular grammars.
This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input.
We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented.
We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks.
In this paper, we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus.
The DLSI method first narrows down the search space of a sought-after patent document by content search and the template matching technique then pins down the documents by exploiting the words-based template matching scheme by syntactic search.
The former feature provides a good basis for testing techniques of collocation detection.
We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling.
In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and language- independent heuristics to find links between single words and multiword units in sentence-aligned parallel texts.
So we developed a system, SEGAPSITH, that acquires it automatically from text segments by using an unsupervised and incremental clustering method.
We assume that the first step towards content driven synthetic prosody generation (Concept-to-speech) is invariably to determine the perceptually relevant prosodic features.
In previous work, we have developed hidden Markov model (HMM) and maximum entropy (Maxent) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries.
We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups.
Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures.
Word Relation Matrices are then mapped across the corpora to find translation pairs.
Event detection and recognition is a complex task consisting of multiple sub-tasks of varying difficulty.
We present a prototype natural-language problem-solving application for a financial services call center, developed as part of the Amiti閟 multilingual human-computer dialogue project.
Discourse structure here refers to informational relations that hold between sentences in a discourse (cf.
This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.
In this paper, we describe clarification dialogues as one method to deal with incomplete or inconsistent information.
We present a task-level evaluation of the French to English version of MedSLT, a medium-vocabulary unidirectional controlled language medical speech translation system designed for doctor-patient diagnosis interviews.
In this paper, we report results from using machine learn-ing to train and test a nominal-expression generator on a set of 393 nominal descriptions from the CO-CONUT corpus of task-oriented de-sign dialogues.
This paper proposes an unsupervised training approach to resolving overlapping ambiguities in Chinese word segmentation.
We present an ensemble of adapted Na飗e Bayesian classifiers that can be trained using an unlabelled Chinese text corpus.
In this work, we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation (HMM-LDA) to obtain syntactic state and semantic topic assignments to word instances in the training corpus.
From these context-dependent labels, we construct style and topic models that better model the target document, and extend the traditional bag-of-words topic models to n-grams.
phonological rules or metrical systems).
In this paper, we describe temporal information retrieval in distributed search engines.
In our system, Trigram model with smoothing algorithm is the core module in word segmentation, and Maximum Entropy model is the basic model in Named Entity Recognition task.
Our environment accepts grammars consisting of binary dependency relations andgrammatical functions.
We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1).
Traditional vector-based models use wordco-occurrence counts from large corporato represent lexical meaning.
In this pa-per we present a novel approach for con-structing semantic spaces that takes syn-tactic relations into account.
We introducea formalisation for this class of modelsand evaluate their adequacy on two mod-elling tasks: semantic priming and auto-matic discrimination of lexical relations.
This paper proposes a method for incrementally translating English spoken language into Japanese.
To resolve the problem of generating a grammatically incorrect sentence, our method uses dependency structures and Japanese dependency constraints to determine the word order of a translation.
This paper describes some preliminary results about Word Domain Disambigua-tion, a variant of Word Sense Disam-biguation where words in a text are tagged with a domain label in place of a sense label.
The paper presents a constraint based semantic formalism for HPSG.
It clusters sequences of tags together based on local distributional information, and selects clusters that satisfy a novel mutual information criterion.
This paper compares different bilexical tree-based models for bilingual alignment.
We present a novel algorithm for Japanese dependency analysis.
In this paper we propose a methodology for investigating the relationship between architectures of natural language generation (NLG) systems and stylistic properties of texts.
The system that we have been developing is a hybrid system with rule-based, example-based, and statistical components.
Resolving anaphora is an important step in the identification of named entities such as genes and proteins in biomedical scientific articles.
This paper addresses the problem of iden-tifying likely topics of texts by their posi-tion in the text.
It describes the automated training and evaluation of an Optimal Posi-tion Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure.
This article approaches syntactical analysis of Portuguese language based upon a lbrinalism called Tree Adjoining Giammars ('/A Gs) 1.10S111 851.
Speech research focuses on narrowband algorithm development and uses human based intelligibility to evaluate success.
In this paper, we present a clustering experiment directed at the acquisition of semantic classes for adjectives in Catalan, using only shallow distributional features.We define a broad-coverage classification for adjectives based on Ontological Semantics.
We describe an approach to text planning that uses the XSLT template-processing engine to create logical forms for an external surface realizer.
This paper addresses the problem of automatically selecting the best among outputs from multiple machine translation (MT) systems.
This paper presents work on using Bayesian networks for the dialogue act recognition module of a dialogue sys-tem for Dutch dialogues.
Random Indexing is a vector space technique that provides an efficient and scalable approximation to distributional similarity problems.
This paper presents a method for auto-matically recognizing local cohesion be-tween utterances, which is one of the dis-course structures in task-oriented spoken dialogues.
In this paper we focus on speech act type-based local cohesion.
We present two methods of interpo-lating the plausibility of local cohesion based on surface information on utter-ances.
Based on our survey, we then used the rhythm feature in a practical shallow parsing task by using rhythm as a statistical feature to augment a PCFG model.
This paper presents the adaptation and customization of two lexical resources: Brill tagger, Brill (1992), and EuroWordNet, Vossen et al.
A new statistical method called 揵ilingual chunking?for structure alignment is proposed.
The alignment is finished through a simultaneous bilingual chunking algorithm.
We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks.
The platform incorporates a lexicon, a morphological analyzer/generator, and a DCG parser/generator that translates Turkish sentences to predicate logic formulas, and a knowledge base framework.
We describe and evaluate a method for performing backwards transliterations by machine.
The paper presents an NLU system developed in the context of the Geometry Explanation Tutor.
The system combines unification-based syntactic processing with description logics based semantics to achieve the necessary accuracy level.
This paper presents a Chinese word segmentation system that uses improved source- channel models of Chinese sentence generation.
Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities.
Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition.
In general, the recognition problem is undecidable for unification grammars.
We first show that non-reentrant unification grammars generate exactly the class of context- free languages.
Sentences are translated into production rules using the methodology of lexical-functional grammar.
text comprehension, this paper presents a recent advance in multilingual knowledge- based machine translation (KBMT).
A pilot implementation of our KBMT architecture using functional grammars and entity-oriented semantics demonstrates the feasibility of the new approach.1
This article examines this task within the context of a sub-sentential translation-memory system, i.e.
This paper describes various types of semantic ellipsis and underspecification in natural language, and the ways in which the meaning of semantically elided elements is reconstructed in the Ontological Semantics (OntoSem) text processing environment.
We present a novel representation of parse trees as lists of paths (leafprojection paths) from leaves to the top level of the tree.
We define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation.
We define a new feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes.
In this paper, we investigate the practical applicability of Co-Training for the task of building a classifier for reference resolution.
Both rhetorical structure and punctuation have been helpful in discourse processing.
Based on a corpus annotation project, this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon.
In this ppr, w dsrib a first prototyp of a pttrn-bsd nlyzr dvlopd in th ontxt of sph-to-sph trnsltion projt using pivot-bsd pproh (th pivot is lld IF).
Th nlyzr pplis "phrs spotting" mhnism on th output of th sph rognition modul.
We present an algorithm that automatically learns context constraints using statistical decision trees.
We then use the acquired constraints in a flexible PUS tagger.
Objectives: To explore the phenomenon of adjectival modification in biomedical discourse across two genres: the biomedical literature and patient records.
Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment.
In this paper we show that an account for coordination can be constructed us-ing the derivation structures in a lexical-ized Tree Adjoining Grammar (LTAG).
This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data.
We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations.
The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs.
From the literature on probabilistic context-free grammars (CFGs), we know precisely the conditions which ensure that consistency is true for a given CFG.
This paper looks at treebank-training of generators, an alternative method for building statistical models for NLG from raw corpora, and two different ways of using treebank-trained models during generation.
distinctive features.
This paper describes how we use the arrows properties from the 5P Paradigm to generate a dependency structure from a surface analysis.
This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and T╱Ba-D/Z tree- banks.
Experiments with the Stanford parser, which uses a factored PCFG and dependency model, show that, contrary to previous claims for other parsers, lexicalization of PCFG models boosts parsing performance for both treebanks.
This paper describes the treatment of nominal compounds in a transfer based machine translation system; it presents a new approach for resolving ambiguities in compound segmentation and constituent structure selection using a combination of linguistic rules and statistical data.
OF COL1NG-92, NANTEs, AUG. 23-28, 1992
This paper describes a learning method that exploits unlabeled data to tackle data sparseness problem.
Synchronous Tree Adjoining Grammars can be used for Machine Translation.
I present a mechanism to translate scrambled Korean sentences into English by combining the concepts of Multi-Component TAGs (MC-TAGs) and Synchronous TAGs (STAG s) .by looking in the transfer lexicon.
formalism such as Tree Adjoining Grammar.
This paper describes the syntactic rules which are applied in the Japanese speech recognition module of a speech-to-speech translation system.
The grammar-formalism generating the new class - the DI-grammars - cover unbound dependencies in a rather natural way.
It will be shown that, apart from DI-grammars, DI-languages can equivalently be characterized by a special type of automata - DI-automata.
For biomedical information extraction, most systems use syntactic patterns on verbs (anchor verbs) and their arguments.
We propose to use predicate-argument structures (PASs), which are outputs of a full parser, to obtain verbs and their arguments.
In this paper, we evaluated PAS method by comparing it to a method using part of speech (POSs) pattern matching.
In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications.
The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ("uniform lexico-structuralprocessing").
When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task.
This paper investigates the usefulness of a large medical database (the Unified Medical Language System) for the translation of dialogues between doctors and patients using a statistical machine translation system.
An algorithm is proposed to determine antecedents for VP ellipsis.
We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-the- art constituency-based parsers.
This paper examines the generation problem for a certain linguistically relevant subclass of LFG gram-mars.
We use the co杘ccurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in Word- Net.
The method we have used in this paper for language and encoding identification uses pruned character n-grams, alone as well augmented with word n-grams.
We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords.
Unification of disjunctive feature descriptions is important for efficient unification-based parsing.
This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints.
We evaluated the topic signatures on a WSD task, where we trained a second-order vector co- occurrence algorithm on standard WSD datasets, with promising results.
This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.
Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme.
Semi-supervised learning addresses the problem of utilizing unlabeled data along with supervised labeled data, to build better classifiers.
The approach has been used to supplement a maximum entropy model for semi-supervised training of the ACE Relation Detection and Characterization (RDC) task.
We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora.
This paper outlines research on processing strategies being developed for a language understanding system, designed to interpret the structure of arguments.
In this paper, we describe how to use such constraints for parsing ID/LP grammars and propose an implementation in Prolog III.Keywords : constraints, syntax, ID/LP formalism, bottom-up altering, Prolog III
Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation.
Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter.
Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries.
employs a unified statistical process to map from words to semantic structures.
The architecture uses a natural language processing module to extract entities, dependencies and simple semantic relationships from texts, and then feeds these features into a probabilistic reasoning module which combines the semantic relationships extracted by the NLP module to form new semantic relationships.
Every phrase structure language has a description using a regular grammar and a context free semantics.
For every description D with an gnrestricted grammar and context sensitive semantics there is a description D' using a context free grammar and context free semantics such that L(D) = L(D').
We prove the following results: Eveiy computable translation is definable as a syntax-controlled translation.
For a syntax-controlled translation which produces a
In this paper, we present a method for the semantic tagging of word chunks extracted from a written transcription of conversations.
Our purpose is to automatically annotate parts of texts with concepts from a SAR ontology.
Our approach combines two knowledge sources a SAR ontology and the Wordsmyth dictionary- thesaurus, and it uses a similarity measure for the classification.
This paper describes a principled approach for analyzing relations between constituent words of compound nouns whose heads are deverbal nouns.
Our approach is based on the classification of deverbal nouns by their lexical conceptual structure (LCS) and the classifica-tion of nouns in general (to appear in the mod-ifier position) vis-a-vis a few core LCS types (of head deverbal nouns).
A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
This purely syntax-based similarity measure shows remarkably plausible semanticrelations.2.
This paper presents a Constraint Grammar- inspired machine learner and parser, Ling?Pars, that assigns dependencies to morphologically annotated treebanks in a function- centred way.
The system not only bases attachment probabilities for PoS, case, mood, lemma on those features' function probabilities, but also uses topological features like function/PoS n-grams, barrier tags and daughter-sequences.
This paper proposes that sentence analysis should be treated as defeasible reasoning, and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning, that includes arguments and defeat rules that capture defensibility.
This model is a classification and coding system of medical procedures.
We derive a measure based on an extension of multinomial na飗e Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.
In this paper, we present asimilarity-based framework to model the taskof backward transliteration, and provide alearning algorithm to automatically acquirephonetic similarities from a corpus.
The learning algorithm is based on Widdrooww-Hoffrule with some	modifications.
Backwardtransliteration is more challenging than forwardtransliteration.
We mainly focus onbackward transliteration here.In this paper, we propose a similarity-basedframework to model the task of backwardHuo4-ge2-hua2-zi 1(CChhinnese)Hoguwaatsu(Jappaneese))Harry Potter(EEnngglishh)Haa1-li4-bboo1-te4(CChhinnese)Hard Pottaa(J^appannese)Hogwarts(EEnngglishh)
To avoid grammar coverage problems we use a fully-connected first-order statistical class grammar.
We report in this paper the observation of one tokenization per source.
He thus formulated aPreferred Argument Structure (PAS) for the preferential structural configurations of arguments.
In this paper, we provide an account of how to generate sentences with coordination constructions from clause-sized semantic representations.
These include the use of partof-speech tags to guide the generalization, Named Entity categories inside the patterns, an edit-distance-based pattern generalization algorithm, and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora.
In this paper we investigate the impact of morphological features on the task of au-tomatically extending a dictionary.
We used a boosting clas-sifier to compare the performance of mod-els that use different sets of features.
We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word.
Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model.
The annotation in-formation consists of speech, tran-scription delimited by slash units, prosodic, part of speech, dialogue acts and dialogue segmentation.
This paper presents F-PATR, a generalization of the PATR-II unification-based formalism, which incorporates relational constraints expressed as user-defined functions.
It is designed particularly for unification- based formalisms implemented in functional programming environments such as Lisp.
Tomita's	parsing	algorithm[Tomita 86], which adapted the LR parsing algorithm to context free grammars, makes use of a breadth-first strategy to handle LH table conflicts.
Thus we obtain a parallel generalized LR parser implemented in GHC.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic para-phrases.
We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
The model combines full and partial parsing techniques to reach full grammar coverage on unseen data.
sys-tem.
Many linguistse.
Candide uses methods of information theory and statistics to develop a probability model of the translation process.
We report on our work to build a discourse parser (SemDP) that uses semantic features of sentences.
We use an Inductive Logic Programming (ILP) System to exploit rich verb semantics of clauses to induce rules for discourse parsing.
In this work, we present three information extraction methods, one based on hand-crafted rules, one based on maximum entropy tagging, and one based on probabilistic trans-ducer induction.
A broad-coverage dependency parser is first used to extract the lexical relations from the definitions.
We propose a method 揑nteractive Paraphrasing?which enables users to interactively paraphrase words in a document by their definitions, making use of syntactic annotation and word sense annotation.
Our strategy is to apply grammatical constraints at the phrase level and to use semantic rather than lexical grammars.
We associate phrases by frame-based semantics.
This paper proposes a method for iden-tifying probable real words among out-of-vocabulary (OOV) words in text.
The identification of real words is done based on entropy of probability of char-acter trigrams as well as the morpho-logical rules of English.
It also gener-ates possible parts-of-speech (POS) of the identified real words on the basis of lexical formation rules and word end-ings.
Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate.
As a result, SNAP employs an extended marker- passing model and a dynamically modifiable network model.
We have discovered that the SNAP provides milliseconds or microseconds performance on several important applications such as the memory-based parsing and translation, classification-based parsing, and VLKB search.
We first identify semantic arguments, and then assign semantic roles to the identified semantic arguments.
We present CARPANTA, an e-mail summarization system that applies a knowledge intensive approach to obtain highly coherent summaries.
This paper describes a series of cepstral-based compensation procedures that render the SPHINX-II system more robust with respect to acoustical environment.
Back-off N-gram language models[11] are an effective class of word based stochastic language model.
The first part of this paper describes our experiences using the back-off language models in our time-synchronous decoder CSR.
The second part of this paper describes our experiences with our prototype stack decoder CSR using no grammar, the word-pair grammar, and N-gram back-off language models.N-GRAM BACK-OFF LANGUAGEMODELSN-gram language models[2, 101 are an attractive method for estimating the probability of the sentence W by successively estimating the probability of the next word in the sentence:P(W) wt_i)where N is the order of the model.
We present work on the automatic generation of short indicative-informative abstracts of scientific and technical articles.
Our work combines intentional and social accounts of discourse, unifying theories of speech act production, interpretation, and the repair of misunderstandings.
In this paper we present a means of defining morphonological phenomena in an inheritance based lexicon.
SPUD simultaneously constructs the semantics and syntax of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG).
We represent questions as frequency weighted vectors of salient terms.
We compare our approcah to related work that uses relatively complex syntactic/semantic processing to create features and a sparse network of linear units to classify questions.
Recent studies in word sense induction are based on clustering global co-occurrence vectors, i.e.
We describe and evaluate hidden understanding models, a statistical learning approach to natural language understanding.
We discuss the properties of the system components and report results on the translation of spoken dialogues in the VERBMOBIL project.
We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm.
We describe a proposal for an extensible, component- based software architecture for natural language engineering applications.
This paper presents a new view of Explanation-Based Learning (EBL) of natural language parsing.
(Bod, 1995, Charniak, 1996, Sekine and Grishman, 1995).The present method consists of an EBL algorithm for learning partial-parsers, and a parsing algorithm which combines partial- parsers with existing "full-parsers".
The learned partial-parsers, implementable as Cascades of Finite State Transducers (CFSTs), recognize and combine constituents efficiently, prohibiting spurious overgeneration.
The technique is demonstrated on the task of learning word and letter bigram pairs from text.
The BYBLOS continuous speech recognition system is applied to on-line cursive handwriting recognition.
The text mining system first identifies protein names in the text using a trained Conditional Random Field (CRF) and then identifies interactions through a filtered co-citation analysis.
We also report two new strategies for mining interactions, either by finding explicit statements of interactions in the text using learned pattern-based rules or a Support-Vector Machine using a string kernel.
Statistical machine translation (SMT) is currently one of the hot spots in natural language processing.
Word clustering is important for automatic thesaurus construction, text classification, and word sense disambiguation.
This paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web counts.
By calculating the similarity of words, a word co- occurrence graph is obtained.
We demonstrate COSMA, a fully im-plemented German language server for ex-isting appointment scheduling agent sys-tems.
There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences.We have built a Japanese corpus of over three million words with both lexical and structural semantic information.
Transfer-Driven Machine Translation (TDMT) is presented as a method which drives the translation processes according to the nature of the input.
TDMT effectively utilizes an example-based framework for transfer and analysis knowledge.
In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model.
A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM).
In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions.
In order to extract the best- matched documents from several parallel corpora, we propose ranking individual documents by using a length-normalized Okapi-based similarity score between them and the target corpus.
Several non-speech-recognition based techniques were employed.
We address these requirements by a hybrid word/phoneme search in lattices, and a supporting indexing scheme.
We will introduce the ranking criterion, a unified hybrid posterior-lattice representation, and the indexing algorithm for hybrid lattices.
We describe finite-state constraint relaxation, a method for applying global constraints, expressed as automata, to sequence model decoding.
We present algorithms for both hard constraints and binary soft constraints.
In this paper we introduce two methods for deriving the intentional structure of complex questions.
This paper describes the structure and evaluation of the syntactico-semantic lexicon (SSL) of the German NaturalLanguage Understanding System VIE-LANG[3].
The SSL contains the rules according to which the mapping between net-structures and surface structures of a sentence is carried out.
The paper develops a constraint-based the-ory of prosodic phrasing and prominence, based on an HPSG framework, with an implementation in ALE.
The general aim is to define prosodic structures recursively, in parallel with the definition of syntactic structures.
We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system.
We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model.
The paper describes an analogy-based measure of word-sense proximity grounded on distributional evidence in typical contexts, and illustrates a computational system which makes use of this measure for purposes of lexical disambiguation.
This work describes the implementation of an email summarisation system for use in a voice-based Virtual Personal Assistant developed for the EU FASiL Project.
We discuss existing approaches to train LR parsers, which have been used for statistical resolution of structural ambiguity.
This demo abstract describes the SmartWeb Ontology-based Annotation system (SOBA).
Question Classification is an important task in Question Answering Systems.
We have made experiments using lexical, syntactic and semantic features to test which ones made a better performance.
Our conclusion about the features is that a lexical, syntactic and semantic features combination obtains the best result.
This paper describes how NOMLEX, a dictionary of nominalizations, can be used in Information Extraction (IE).
The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill抯 rule-based part-of-speech tagger.
The main novelty in the system is the method for generating the semantic features, based on word co-occurrence probabilities.
This paper presents Trace & Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (uG) and ideas of Government & Binding Theory (GB) in an undogmatic way.
We find that tree-entropy can significantly reduce the amount of training annotation for both a history-based parser and an EM-based parser.
We explore the major functionalities andarchitectural implications of natural language generation for three key classes of interactive 3D worlds: self- explaining 3D environments, habitable 3D /earning environments, and interactive 3D narrative worlds.
This paper proposes a corpus-based language model for topic identification.
We analyze the association of noun-noun and noun-verb pairs in LOB Corpus.
The word association norms are based on three factors: 1) word importance, 2) pair co-occurrence, and 3) distance.
This paper reports our application of three naive models of centering theory for dialog.
One corpus is now available, and current plans call for corpora of increasing size and complexity over the next few years.Large vocabulary speech recognition requires transcribed speech, pronouncing dictionaries, and language models.
This paper presents a method for inducing transla-tion lexicons based on transduction models of cog-nate pairs via bridge languages.
Bilingual lexicons within languages families are induced using proba-bilistic string edit distance models.
The first method is used to detect any type of word segments.
superordinate substitution, and definite noun phrase reiteration.
The paper is about the issue of addressing in multi-party dialogues.
A method for the automatic prediction of the addressee of speech acts is discussed.
We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.
We identify two types of opinion-related entities ?expressions of opinions and sources of opinions ?along with the linking relation that exists between them.
Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities.
In this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks.
This paper presents a corpus-based approach to word sense disambiguation where a decision tree as-signs a sense to an ambiguous word based on the bigrams that occur nearby.
This approach is evalu-ated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.
In this paper we present a supervised Word Sense Disambiguation methodology, that exploits kernel methods to model sense distinctions.
A good decoding algorithm is critical to the success of any statistical machine translation system.
This paper presents a method of Japanese dependency structure analysis based on Sup-port Vector Machines (SVMs).
We apply SVMs to Japanese dependency structure iden-tification problem.
This paper presents a probabilistic framework, QARLA, for the evaluation of text summarisation systems.
This paper describes a standardised XML tagset (DTD) for annotated biomedical corpora and other resources, which is based on the Text Encoding Initiative Guidelines P4, a general and parameterisable standard for encoding language resources.
It is common practice to use tables of probabilities of single words, pairs of words, and triples of words (n-grams) as a prior model.
Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items.
This paper presents a method for parsing associative Lambek grammars based on graph- theoretic properties.
The method amounts to find alternating spanning trees in graphs.
This paper addressees the problem of eliminating unsatisfactory outputs from machine translation (MT) systems.
Confidence measures for MT outputs include the rank-sum-based confidence measure (RSCM) for statistical machine translation (SMT) systems.
Most of the previous Korean noun extraction systems use a morphological analyzer or a Partof-Speech (POS) tagger.
morphosyntactic rules and morphological rules).This paper proposes a new noun extraction method that uses the syllable based word recognition model.
It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries.
(TSR trees).
In this paper, we present ParaEval, an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations.
Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching.
ParaEval addresses three important issues: support for paraphrase/synonym matching, recall measurement, and correlation with human judgments.
The work includes (1) the development of an integer "break index" representation of prosodic phrase boundary information, (2) the automatic detection of prosodic phrase breaks using a hidden Markov model on relative duration of phonetic segments, and (3) the integration of the prosodic phrase break information in SRI's Spoken Language System to rule out alternative parses in otherwise syntactically ambiguous sentences.
In this paper, we propose a new probabilistic model for text catego-rization, that is based on a Single random Variable with Multiple Values (SVMV).
Here, we propose a scoring algoritlun to rank candidate parses based on an analysis-by-synthesis method which compares the observed prosodic phrase structure with the predicted structure for each candidate parse.
This paper describes a set of computer programs for Chinese corpus analysis.
This paper describes SemNet the in-ternal Knowledge Representation for LOIATAl.
We describe and evaluate the application of a spectral clustering technique (Ng et al., 2002) to the unsupervised clustering of German verbs.
Our previous work has shown that standard clustering techniques succeed in inducing Levin- style semantic classes from verb subcategorisation information.
We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.
In contrast to previous work, we particularly focus on clustering polysemic verbs.
It addresses the problem of learning from examples rules for word-forms analysis and synthesis.
This paper proposes two new meth-ods to identify the correct meaning of Japanese homonyms in text based on the noun-verb co-occurrence in a sentence which can be obtained easily from cor-pora.
The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system.
We deal with not only conjunctive noun phrases, but also conjunctive predicative clauses created by "Renyoh chuuslii-ho".
We present a progress report on our research on nominal compounds (NC's).
We examine a number of constraints on the semantic interpretation rules for NC's.
Results on this data strongly suggest that images can help with word sense disambiguation.
Automatic word segmentation is a basic requirement for unsupervised learning in morphological analysis.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is defined and basic algorithms for SLTAG are designed.
The basic idea of the method is to ap-ply bootstrapping to an existing corpus-based cross-language information retrieval (CLIR) approach.
This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other抯 output.
Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets.
We demonstrate an original and successful approach for both resolving and generating definite anaphora.
We propose and evaluate unsupervised models for extracting hypernym relations by mining co- occurrence data of definite NPs and potential antecedents in an unlabeled corpus.
It also substantially outperforms recent related work using pattern-based extraction of such hypernym relations for coreference resolution.
This paper presents an analysis of temporal anaphora in sentences which contain quantification over events, within the framework of Discourse Representation Theory.
We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences.
A method is presented here for calculating such finite-state approximations from context-free grammars.
This paper describes corpus analysis and a psycholinguis-tic experiment regarding the acceptability of using non-restrictive NP modifiers to express semantic re-lations that might normally be signalled by `because' and 'then'.
This paper describes a new approach to the generation of referring expressions.
We propose to formalize a scene as a labeled directed graph and describe content selection as a subgraph construction problem.
Three features are unusual in PLUM's architecture: a domain- independent deterministic parser, processing of (the resulting) fragments at the semantic and discourse level, and probabilistic models.
unscripted) speech.
The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and so on.
In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT).
We use a maximum likelihood criterion to train a log-linear block bigram model which uses real- valued features (e.g.
block bigram features.
This mechanism encompasses both accommodation by discourse attachment and accommodation by temporal addition.
We present a hybrid text understanding methodology for the resolution of textual ellipsis.
It integrates language-independent conceptual criteria and language-dependent functional constraints.
SEM encodes logical semantics.
[PUS951 posits the lexical entry to be constituted by four structures: Event, Argument, Lexical-Inheritance and QUALIA.
Here we present work on using spatial knowledge in conjunction with information extraction (IE).
In this paper, we exploit these automata- theoretic results to obtain a characterization of the tree-adjoining languages by definability in the monadic second-order theory of these three-dimensional tree manifolds.
In this paper we describe a prosody- dependent duration model as a first step toward incorporating a prosodic consistency constraint into a speech recognizer.
As part of this model, we describe a text- based prosody prediction scheme, novel in its use of a preliminary integrated commaprediction/POS tagging step.
This final result suggests a benefit to integrating a prosodic consistency constraint into a speech recognition system.
We present a model and an experimental platform of a bootstrapping approach to statistical induction of natural language properties that is constraint based with voting components.
The procedure has been implemented for large-scale systemic grammars and builds on the formal equivalence between systemic grammars and typed unification based grammars.
The system consists of a new words recognizer, a base segmentation algorithm, and procedures for combining single characters, suffixes, and checking segmentation consistencies.
There is a real annotated corpus of Czech ?Prague Dependency Treebank (PDT).
We have adapted the letter-to-phoneme component of a text-to-speech synthesizer to a web-based software system that can teach word decoding to non-native speakers of English, English-speaking children, and adult learners.
We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.
This paper describes an algorithm for unifying disjunctive feature structures.
We present various linguistic applications of default unification.
The PANGLOSS system addresses this dilemma by integrating MT with machine-aided translation (MAT).
This article describes the implementation of I2R word sense disambiguation system (I2R ^ W5D) that participated in one senseval3 task: Chinese lexical sample task.
Our core algorithm is a supervised Naive Bayes classifier.
The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context.
The senses are ranked using two sources of information: (1) the Internet for gathering statistics for word-word co- occurrences and (2) WordNet for measuring the semantic density for a pair of words.
This paper describes the NECA MNLG; a fully implemented Multimodal Natural Language Generation module.
This paper presents a semantically oriented, rule based method for single sentence text generation and discusses its implementation in the Kafka generator.
This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.
We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH-questions.
The framework uses customized morpho-syntactic and syntactic analysis where the lexicons and their alphabets correspond to the symbol set acquired from the recognition process.
The paper outlines the methods of morpho-syntactic and syntactic post-processing currently in use.
This paper analyzes principles of human conversation based on the conversational goals of the participants.
We propose a distribution-based pruning of n-gram backoff language models.
Our method is based on then -gram distribution i.e.
In this paper, we look at comparing high- accuracy context-free parsers with high- accuracy finite-state (shallow) parsers on several shallow parsing tasks.
We also demonstrate that context- free parsers can train effectively on relatively little training data, and are more robust to domain shift for shallow parsing tasks than has been previously reported.
We introduce a generalization of categorial grammar extending its descriptive power, and a simple model of categorial grammar parser.
We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules.
The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries.
This paper addresses recent results on Mandarin spoken dialogues and introduces the collection of a large Mandarin conversational dialogue corpus.
We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora.
In this paper, we mainly present building a semantically tagged bilingual corpus to word sense disambiguation (WSD) in English texts.
To assign semantic tags, we have taken advantage of bilingual texts via word alignments with semantic class names of LLOCE (Longman Lexicon of Contemporary English).
This semantically annotated corpus will be used to extract disambiguation rules automatically by TBL (Transformation-based Learning) method.
This paper describes a method for learning the countability preferences of English nouns from raw text corpora.
The method maps the corpus-attested lexico-syntactic properties of each noun onto a feature vector, and uses a suite of memory-based classifiers to predict membership in 4 countability classes.
The model is based on a EM clustering model with word classes and selectional restrictions as hidden features.
In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense disambiguation.
Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models.
We compute frequencies of unigrams, bigrams, and trigrams of word classes in order to further refine the disambiguation.
This new approach gives a more efficient representation of the data in order to disambiguate word part-of-speech.
The MPM part of PolyphraZ has 3 main web interfaces.
This paper presents a reestimation algorithm and a best-first parsing (BFP) algorithm for probabilistic dependency grammars (PDC).
The inside-outside algorithm is a probabilistic parameter reestimation algorithm for phrase structure grammars in Chomslcy Normal Form (CNF).
The reestiotation and BFP algorithms utilize CYK-style chart and the non- constituent objects as chart entries.
Its basic data structures are typed feature structures.
We describe a method and its implementation for self-monitoring during natural language generation.
This paper describes the structural an-notation of a spoken dialogue corpus.
In this paper, we present a solution to the prob-lem of generating Japanese numeral classifiers us-ing semantic classes from an ontology.
This paper shows how these problems may be circumvented using a rule-based, wait-and-see parsing strategy.
In this paper we present URES (Unsupervised Relation Extraction System), which extracts relations from the Web in a totally unsupervised way.
This paper presents a word-pair (WP) identifier that can be used to resolve homonym/segmentation ambiguities and perform syllable-to-word (STW) conversion effectively for improving Chinese input systems.
This paper presents an approach to automatically build a semantic perceptron net (SPN) for topic spotting.
It uses context at the lower layer to select the exact meaning of key words, and employs a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words within a topic to form basic semantic nodes.
It detects mistypes, Kana-to-Kanji misconversions, and stylistic errors.
This paper presents an empirically motivated theory of the discourse focusing nature of accent in spontaneous speech.
This paper addresses two issues concerning lexical access in connected speech recognition: 1) the nature of the Fe-lexical representation used to initiate lexical lookup 2) the points at which lexical look-up is triggered off this representation.
The few explicit and well- developed models of lexical access and word recognition in continuous speech (e.g.
Degraded text recognition is a difficult task.
In this paper, we propose that visual inter-word constraints can be used to facilitate candidate selection.
Our system improves other proposals presented so far due to the fact that we are able to solve and generate intersentential anaphora, to detect coreference chains and to generate Spanish zero-pronouns into English, issues that are hardly considered by other systems.
Based on the integration of the domain-specific named entity knowledge and syntactic as well as statistical information, this work mainly focuses on how to evaluate a proper multiword verb candidate.
This paper presents a unified approach to parsing, in which top-down, bottom- up and left-corner parsers are related to preorder, postorder and inorder tree traversals.
In the nominal compound analysis area, some corpus-based approaches have reported successful results by using statistal co- occurrences of nouns.
This paper presents a new model for Korean nominal compound analysis on the basis of linguistic and statistical knowledge.
The structure of a nominal compound is analyzed based on the linguistic lexical information extracted.
We describe a framework for the evaluation of summaries in English and Chinese using similarity measures.
This paper describes the representation of Basque Multiword Lexical Units and the automatic processing of Multiword Expressions.
The schema method is a framework for correcting grammatically ill-formed input.
We investigate the logical structure of concepts generated by conjunction and disjunction over a monotonic multiple inheritance network where concept nodes represent linguistic categories and links indicate basic inclusion (isA) and disjointness (IsNoTA) relations.
We apply our logical analysis to the sort inheritance and unification system of HPSG and also to classification in systemic choice systems.
This paper present a method based on the behavior of nonnative speaker for reduction sentence in foreign language.
This paper presents BIAS (Bahasa Indonesia Analyzer System), an analysis systemfor Indonesian language suitablefor multilingual machine translation system.
In this paper we present a logical treatment of semi- free word order and bounded discontinuous constituency.
We extend standard feature value logics to treat word order in a single formalism with a rigorous semantics without phrase structure rules.
This permits a natural interpretation of implicational universals in terms of theories, subtheories and implicational axioms.
In this paper we describe the extraction of thesaurus information from parsed dictionary definition sentences.
The dictionary is parsed using a head-driven phrase structure grammar of Japanese.
Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG).
In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text.
We also show how the parser can be used to parse questions for Question Answering.
In this paper we present a grammar formalism that combines the insights from Combinatory Categorial Grammar with feature structure unification.
We focus on the way theme, rheme, and focus are integrated in the compositional semantics, using Discourse Representation Theory as first-order semantic theory.
NLP systems for tasks such as question answering and information extraction typically rely on statistical parsers.
We present several statistical models of syntactic constituent order for sentence realization.
We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models.
This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a natural language interface to database query ap-plications.
Multimedia answers include videodisc im-ages and heuristically-produced complete sentences in text or text-to-speech form.
We present an input understanding method for a tutoring system teaching mathematical theorem proving.
In interpreting multilingual queries to databases whose domain information is described in a particular language, we must address the problem of word sense disambiguation.
Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.
In this paper, we analyze two particular multi- action constructions, utterances with means clauses and utterances with rationale clauses.
In this paper, we report on our experiment to extract Chinese multiword expressions from corpus resources as part of a larger research effort to improve a machine translation (MT) system.
We define noun phrase translation as a subtask of machine translation.
This enables us to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation methods by incorporating special modeling and special features.
Hebrew includes a very productive noun-compounding construction called smixut.
Because smixut is marked morphologically and is restricted by many syntactic constraints, it has been the focus of many descriptive studies in Hebrew grammar.We present the treatment of smixut in HUGG, a FUF-based syntactic realization system capable of producing complex noun phrases in Hebrew.
We contrast the treatment of smixut with noun-compounding in English and illustrate the potential for paraphrasing it introduces.We specifically address the issue of determining when a smixut construction can be generated as opposed to other semantically equivalent constructs.
We describe an in-depth study of using a dictionary (WordNet) and web search engines (Altavista, MSN, and Google) to boost the performance of an automated question answering system, Webclopedia, in answering definition questions.
It consists of a word-based statistical language model, an initial estimation procedure, and a re-estimation procedure.
This paper is a first step towards a computational account of Binding Theory (BT).
Listen-Communicate-Show (LCS) is a new paradigm for human interaction with data sources.
We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources.
The	notion	of	a	commonsensealgorithm is presented as a basic ' data structure for modeling humancognition.
The natural language database query system incorporated in the KNOBS interactive planning system comprises a dictionary driven parser, APE-II, and script interpreter which yield a conceptual dependency conceptualization as a representation of the meaning of user input.
Conditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks.
We present the results of an analysis of a corpus of 331 TFRs, with particular attention to discourse segmentation and focusing.
In this paper we demonstrate that it is possible to parse dependency structures deterministically in linear time using syntactic heuristic choices.
We first prove theoretically that deterministic, linear parsing of dependency structures is possible under certain conditions.
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.
Graph unification remains the most expensive part of unification-based grammar parsing.
This paper proposes a method to resolve the reference of deictic Japanese zero pronouns which can be implemented in a practical machine translation system.
This method focuses on semantic and pragmatic constraints such as semantic constraints on cases, modal expressions, verbal semantic attributes and conjunc-tions to determine the deictic reference of Japanese zero pronouns.
In this paper, we describe the rationale and operation of the Computerized Oral Proficiency Instrument (COPI), a multimedia, computer- administered oral proficiency test.
?acctqacy of :text.
-a .framework fdr fr?
each sentence in the text astext information.
Thus, our context Model consists of parsed trees that are obtained by using an exist-ug syntactic parser.
the processing objectYenee: to	sentences .
:reSOlving-pronoun refrence-oii ie'fOe*?.
This paper presents the results of converting a standard Graharn/Hanison/Ruzzo (GHR) parser for a unification grammar into an agenda-driven parsing system.
This paper presents an algorithm for text summarization using the the-matic hierarchy of a text.
The algorithm first detects the thematic hierarchy of a source text with lexical cohe-sion measured by term repetitions.
This paper discusses an approach to incremental learning in natural language processing.
We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.
We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.
In this paper, we apply its idea to the sense classifica-tion of Japanese verbal polysemy in case frame acquisition from Japanese-English parallel corpora.
Measures of bilin-gual class/class association and bilingual class/frame association are introduced and used for discovering sense clusters in the sense distribution of English pred-icates and Japanese case element nouns.
In this paper we introduce a computer- assisted writing tool for deaf users of American Sign Language (ASL).
Our approach is based on the novel application of the longest common substring and string edit distance metrics.
In this paper a method to incorporate linguistic information regarding single-word and compound verbs is proposed, as a first step towards an SMT model based on linguistically-classified phrases.
We present the implementation of a system which extracts not only lexicalized grammars but also feature-based lexicalized grammars from Korean Sejong Treebank.
We report on some practical experiments where we extract TAG grammars and tree schemata.
In addition, we modify Treebank for extracting lexicalized grammars and convert lexicalized grammars into tree schemata to resolve limited lexical coverage problem of extracted lexicalized grammars.
OLACMS (stands for Open Language Archives Community Metadata Set) is a standard for describe language resources.
This pa-per addresses the reduction of parsing com-plexity by intra-sentence segmentation, and presents maximum entropy model for deter-mining segmentation positions.
This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem.
The grammar checking component, which is the focus of this paper, uses island processing (or chunking) rather than a full parse.
In this work we study the influence of corpus homogeneity on corpus-based NLP system performance.
We de-scribe a method to represent corpus homogeneity as a distribution of sim-ilarity coefficients based on a cross-entropic measure investigated in previ-ous works.
This paper presents a novel nonlocal language model which utilizes contextual information.
A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors.
The sum of word co-occurrence vectors represents the context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the long-distance lexical de-pendencies.
We describe a demonstration system built upon Topic Detection and Tracking (TDT) technology.
In this paper, we exploit non-local features as an estimate of long-distance dependencies to improve performance on the statistical spoken language understanding (SLU) problem.
An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented.
In order to make use of lemmatisation and morphological and syntactic tagging of Czech texts, author proposes a method for construction of dependency word microcontexts fully automatically extracted from texts, and several ways how to exploit the microcontexts for the sake of increasing retrieval performance.
This paper describes the key characteristics of the parser and the distributed grammar.
In this paper, we present a general purpose model for both Chinese word segmentation and named entity recognition.
This model was built on the word sequence classification with probability model, i.e., conditional random fields (CRF).
The method of organization of word meanings is a crucial issue with lexical databases.
In order to find adjective hyperonyms, we utilize abstract nouns.
We constructed linguistic data by extracting semantic relations between abstract nouns and adjectives from corpus data and classifying abstract nouns based on adjective similarity using a self-organizing semantic map, which is a neural network model (Kohonen 1995).
In this paper we describe how to hierarchically organize abstract nouns (adjective hyperonyms) in a semantic map mainly using CSM.
We compare three hierarchical organizations of abstract nouns, according to CSM, frequency (Tf.CSM) and an alternative similarity measure based on coefficient overlap, to estimate hyperonym relations between words.
This paper deals with search algorithms for real-time speech recognition.
Word graphs have various applications in the field of machine translation.
We will describe the generation of word graphs for state of the art phrase-based statistical machine translation.
We will evaluate the quality of the word graphs using the well-known graph word error rate.
Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score.
First, we introduce the 揹ependency tree path?(DTP).
Next, we calculate the similarity between DTPs based on the ESK (Extended String Subsequence Kernel), which considers sequential patterns.
To resolve the problem, we propose "term distillation", a framework for query term selection in cross-database retrieval.
The experiments using the NTCIR-3 patent retrieval test collection demonstrate that term distillation is effective for cross-database retrieval.
We propose a lexicon smoothing method that takes the word base forms explicitly into account.
This study addresses pronominal anaphora resolution, including zero pronouns, in Chinese.
A syntactic, rule-based pronoun resolution algo-rithm, the 揌obbs algorithm?was run on 揼old standard?hand parses from the Penn Chinese Treebank.
We develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering.
This design is used in the TRANSLATOR machine translation project.
The interlingua.
Any approach that attempts to relate directly various syntactic structure trees between SI.
We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems.
We describe an approach to tagging a monolingual dictionary with linguistic features.
In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus.
We present an algorithm for automatically disambiguating noun-noun compounds by deducing the correct semantic relation between their constituent words.
Keywords: question answering, information retrieval, user modelling, readability.
One method is based on majority vote, while the other is a memory-based approach that integrates maximum entropy and conditional random field classifiers.
A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.
The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log- linear disambiguation component and provides much richer representations theory.
We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings.
statistical lexical head- outward transducers.
The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer.
We present a corpus-based study of the sequential ordering among premodifiers in noun phrases.
We propose and evaluate three approaches to identify sequential order among pre- modifiers: direct evidence, transitive closure, and clustering.
In this paper, I discuss issues pertinent to the design of a task-based evaluation methodology for a spoken machine translation (MT) system processing human to human communication rather than human to machine communication.
In this paper we present a Two-Phase LMR-RC Tagging scheme to perform Chinese word segmentation.
The purpose of this paper is to identify effective factors for selecting discourse organization cue phrases in instruction dialogue that signal changes in discourse structure such as topic shifts and attentional state changes.
We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
This paper describes the experiment of the English generation from interlingua by the example-based method.
The generator is implemented by using English Word Dictionary and Concept Dictionary developed in EDR.
Sumo is a formalism for universal segmentation of text.
This framework relies on a layered struc-ture representing the possible segmentations of the document.
JANUS is a multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous con-versation in a limited domain.
This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan.
LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification).
We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment.
This paper presents an overview of the TIPSTER/SHOGUN project, the major results, and the SHOGUN data extraction system.
The original SHOGUN design integrated several different approaches by combining different knowledge sources, such as syntax, se-mantics, phrasal rules, and domain knowledge, at run-time.
In this paper seven algorithms are compared using a word alignment approach based on association clues and an English-Swedish bitext together with a handcrafted reference alignment used for evaluation.
We describe in this paper the MU- MIS Project (Multimedia Indexing and Searching Environment)1, which is concerned with the development and integration of base technologies, demonstrated within a laboratory prototype, to support automated multimedia indexing and to facilitate search and retrieval from multimedia databases.
This paper describes a project to construct a terminological knowledge base, called COGNITERM.
In particular we will introduce the Extensible MultiModal Annotation (EMMA) language specification.
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications.
The task is the on-demand generation of dialogue scripts and result summaries of dialogues.
The COR.UDIS system (COreference R.Ules with DIsambiguation Statistics) combines syntactico-semantic rules with statistics derived from an annotated corpus.
Then, the coreference resolution algorithm and the involved statistics are explained.
The schema argues for linearizing nonlinear representations before applying phonological and morphological rules.
This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.
The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes.
This paper presents a hybrid approach for location normalization which combines (i) lexical grammar driven by local context constraints, (ii) graph search for maximum spanning tree and (iii) integration of semi-automatically derived default senses.
The Korean Combinatory Categorial Grammar (KCCG) formalism can uniformly handle word order variation among arguments and adjuncts within.
In this paper, incremental pars-ing technique of a morpheme graph is devel-oped using the KCCG.
We present techniques for choosing the most plausible parse tree us-ing lexical information such as category merge probability, head-head co-occurrence heuristic, and the heuristic based on the coverage of sub-trees.
Probabilistic Recursive Transition Network(PRTN) is an elevated version of RTN to model and process languages in stochastic parameters.
We describe an extension of the WYSiwYm technology for knowledge editing through natural language feedback.
The extension will be included in a Java-based library package for producing WYSiwYm applications.
The method we propose then uses these trained parameters to define a kernel for reranking parse trees.
In experiments, we use a neural network based statistical parser as the probabilistic model, and use the resulting kernel with the Voted Perceptron algorithm to rerank the top 20 parses from the probabilistic model.
This resource can be used in machine translation and cross-lingual IR systems.
We coerce the English FrameNet into Chinese using a bilingual lexicon, frame context in FrameNet and taxonomy structure in HowNet.
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.
104-106) described a new methodology for testing lexical transfer in machine translation.
We describe a method for con-verting a document image into character shape codes and word shape tokens.
We present a new word-alignment approach that learns errors made by existing word alignment systems and corrects them.
By adapting transformation- based learning to the problem of word alignment, we project new alignment links from already existing links, using features such as POS tags.
This paper proposes a new type of transfer system, called a Similarity-driven 7iyinsfee System (SiniTran), for use in such case-based MT (CBMT).
This paper introduces new specificity determining methods for terms using compositional and contextual information.
We present an HMM-based approach and a maximum entropy model for speaker role labeling using Mandarin broadcast news speech.
During decoding, we use a block unigram model and a word-based trigram language model.
We show experimental results on block selection criteria based on unigram counts and phrase length.
We present a novel approach for generating the ideographic representations of a CJK name written in a Latin script.
We illustrate the approach with English-to-Japanese back-transliteration.
We extended this algorithm to recover extragrammatical sentence into grammatical one in running text.
This paper describes a hybrid model and the corresponding algorithm combining support vector machines (SVMs) with statistical methods to improve the performance of SVMs for the task of Chinese Named Entity Recognition (NER).
This paper explores the extent to which phoneme sequence constraintz can be used to identify word boundaries in continuous speech recognition.
One of these criteria is the anaphoric conditions of verbs described as a lexicon-grammar of anaphoric verbs.The present paper investigates atransformational analysis of verbs related to their anaphoric behaviour, and the adequacy of extension of the lexicon-grammar of M.GROSS to anaphoric conditions on verbs.
We propose a parser for constraint- logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top- down control.
Recognizing idioms in a sentence is important to sentence understanding.
This paper discusses the lexical knowledge of idioms for idiom recognition.
We propose a set of lexical knowledge for idiom recognition.
In this paper, we represent singular definite noun phrases as functions in logical form.
This paper presents a first experience with evaluating sytems that address the issue of Logic Form Identification (LFi).
We have developed a formal description of Arabic syntax in Definite Clause Grammar.
This paper addresses issues in automated treebank construction.
We show how such a domain model can be used for topic identification of unseen calls.
Some Terminology In the discussion below, we shall use the term grammar to mean a system consisting of a lexicon, a syntax, a meaning representation lan-guage, and a semantic mapping.
We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data.
Through this reformulation we gain full reversibility for the SFG description and access for unification-based grammar descriptions to the rich semantic levels of description that SFG supports.
This paper reports the development of log- linear models for the disambiguation in wide-coverage HPSG parsing.
Hence we investigate the automatic categorization of text segments of scientific articles with XML markup into 16 topic types from a text type structure schema.
In the framework of statistical machine transla-tion (SMT), correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so-called alignment models.
We found the argumentative interpretation of utterances on a semantics defined at the linguistic level.
We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation.
We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation.
There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach.
We describe our analysis and modeling of the summarization process of Japa-nese broadcast news.
We then developed a summarization model on which we intend to build a summa-rization system.
The paper presents an unlexicalized probabilistic parsing model for German trained on the Negra treebank.
I present a modified version of Centering, Dynamic Centering, in which this assumption is removed.
This paper describes the tree-structured maximum mutual information (MMI) encoders used in SSI's Phonetic Engine?to perform large-vocabulary, continuous speech recognition.
We evaluated these MMI encoders by comparing them against a standard minimum distortion (MD) vector quantizer (encoder).
Both encoders produced code streams, which were used to train speaker-independent discrete hidden Markov models in a simplified version of the Sphinx system [3].
A small application called Whole Word Morpholo-gizer which does just this is outlined and discussed.
This paper describes the initial development of a natural language text processor, as the first step in an INRS dialogue-by-voice system.
This paper reports results in processing the textual version of ATIS (Air Travel Information System) queries.
We investigate the relative impact of pre- and post- translation document expansion for cross-language spoken document retrieval in Mandarin Chinese.
A method for automatic lexical acquisition is outlined.
These templates represent grammatically correct sentence patterns.
We propose a generalization of the supervised DOP model to unsupervised learning.
This new model, which we call U-DOP, initially assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from (a large subset of) these binary trees to compute the most probable parse trees.
We consider the problem of training logistic regression models for binary classification in information extraction and information retrieval tasks.
We describe thus a microsemantic parser which is uniquely based on an associative network of semantic priming.
This paper describes a new, large scale discourse-level annotation project ?the Penn Discourse TreeBank (PDTB).
This paper explores the usefulness of a technique from software engineering, namely code instrumen-tation, for the development of large-scale natural language grammars.
In 1989, our group first reported on the development of SUMMIT, a segment-based speaker-independent continuous-speech recognition system [13] .
In this paper we compare two grammar-based generation algorithms: the Semantic-Head-Driven Generation Algorithm (SHDGA), and the Essential Arguments Algorithm (EAA).
We propose a treatment of coordination based on the concepts of functor, argument and sub categorization.
This paper presents a proposal for iCLEF 2006, the interactive track of the CLEF cross-language evaluation campaign.
We are trying to find paraphrases from Japanese news articles which can be used for Information Extraction.
In this paper, we propose adding long-term grammatical information in a Whole Sentence Maximun Entropy Language Model (WSME) in order to improve the performance of the model.
One is the Viterbi algorithm for linear-chain models, such as HMMs or CRFs.
The other is the CKY algorithm for probabilistic context free grammars.
We conducted experiments with an EBMT system.
We introduce a new method of feature selection for text categorization.
This paper studies issues related to the compilation of a bilingual lexicon for technical terms.
As a method of translation estimation for technical terms, we employ a compositional translation estimation technique.
We present the concepts of weighted language, transduction and automaton from algebraic automata theory as a general framework for describing and implementing decoding cascades in speech and language processing.
Applications to speech recognition and to Chinese text segmentation will be discussed.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
The major features of MM-DCG include capability to handle an arbitrary number of !nodes and temporal information in grammar rules.
Further, we have developed MM-DCG translator to transfer rules in MM-DCG into Prolog predicates.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.
We describe a similarity calculation model called IFSM (Inherited Feature Similarity Measure) between objects (words/concepts) based on their com-mon and distinctive features.
We pro-pose an implementation method for ob-taining features based on abstracted triples extracted from a large text corpus utilizing taxonomical knowledge.
relation based sim-ilarity measure and distribution based similarity measure.
We show that the concatenation cost can be accurately estimated from this corpus using a statistical n-gram language model over units.
A LOGICAL SEMANTICSFOR NONMONOTONIC SORTSMark A.
In this paper, we present a logical basis for NSs and non- monotonically sorted feature structures (NSFSs).
Then we could use default theories to describe feature structures.
We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic finite automaton.
the cross-entropy.
We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata.
The paper describes several possibilities of using finite-state automata as means for speeding up the performance of a grammar-and-parsing-based (as opposed to pattern-matching-based) grammar-checker able to detect errors from a predefined set.
We demonstrate the feasibility of using unary primes in speech-driven language processing.
Translation memories are promising devices for automatic translation.
In this paper, the use of a hierarchical transla-tion memory, consisting of a cascade of finite state transducers, is proposed.
Natural language parsing requires ex-tensive lexicons containing subcategori-sation information for specific sublan-guages.
This paper describes an unsuper-vised method for acquiring both syntac-tic and semantic subcategorisation restric-tions from corpora.
For more generalized applications, in this paper we extend our previous approach by adding a phase of transitive (indirect) translation via an intermediate (third) language, and propose a transitive model to further exploit anchor-text mining in term translation extraction applications.
We present a system for identifying and tracking named, nominal, and pronominal mentions of entities within a text document.
Our maximum entropy model for mention detection combines two pre-existing named entity taggers (built to extract different entity categories) and other syntactic and morphological feature streams to achieve competitive performance.
We developed a novel maximum entropy model for tracking all mentions of an entity within a document.
We participated in the Automatic Content Extraction (ACE) evaluation and performed well.
A method is described by which a rhetorical-structure tree can be realized by a text structure made up of sections, paragraphs, sentences, verti-cal lists, and other textual patterns, with discourse connectives added (in the correct positions) to mark rhetorical relations.
We explore the use of Support Vector Ma-chines (SVMs) for biomedical named en-tity recognition.
In ad-dition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning.
We compare our SVM-based recognition system with a system using Maximum Entropy tagging method.
We provide D-ITG grammars to search these spaces completely and without redundancy.
We describe results based on the stan-dard logfile metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme.
This paper describes a natural language query engine that enables users to search for entities, relationships, and events that are extracted from biological literature.
We focus on the usability of the natural language interface to users who are used to keyword- based information retrieval.
This paper describes an indexing substrate for typed feature structures (ISTFS), which is an efficient retrieval engine for typed feature structures.
This paper extends earlier work on the relation between syntax and intonation in language understanding in Combinatory Categorial Grammar (CCG).
Hdrug is an environment to develop grammars, parsers and generators for natural languages.
The system provides a graphical user interface with a command interpreter, and a number of visualisation tools, including visualisation of feature structures, syntax trees, type hierarchies, lexical hierarchies, feature structure trees, definite clause definitions, grammar rules, lexical entries, and graphs of statistical information of various kinds.Hdrug is designed to be as flexible and extendible as possible.
Grammatical formalisms that have been used range from context-free grammars to concatenative feature-based grammars (such as the grammars written for ALE) and nonconcatenative grammars such as Tree Adjoining Grammars.
The most prominent of these include decomposition trees, linear representations such as the Predicate Calculus, and semantic networks.
This paper presents a study on optimizing sentence pair alignment scores of a bilingual sentence alignment module.
This paper concentrates on the class of action verbs of movement, and builds on earlier work on lexical correspondences between languages and specific to this verb class.
We first examine the way prototypical verbs of movement are translated in the Collins- Robert (Collins 1978, henceforth CR) bilingual dictionary.
We take advantage of the results of linguistic research on verb types (e.g.
Motion Verbs.
In this paper, we report on data for movement verbs (or motion verbs).
Bilingual Corpus-based Analysis.
We propose a bottom-up variant of Earley deduction.
In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text.
This paper discusses the partitive-genitive case alternation of Finnish adpositions.
Interactive spoken dialog provides many new challenges for spoken language systems.
The repair pattern is built by finding word matches and word replacements, and identifying fragments and editing terms.
Natural language image sense annotation involves increased semantic complexities compared to disambiguating word senses when annotating text.
This paper identifies the types of sentence fragments found In the text of two domains: medical records and Navy equipment status messages.
We present a new recall-based evaluation methodology for determining the effectiveness of different similarity measures on drug names.
In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summarization system.
This paper proposes an efficient example selection method for example-based word sense disambiguation systems.
We present a simple approach for Asian language text classification without word segmentation, based on statistical -gram language modeling.
This paper describes a prototype system to visualize and animate 3D scenes from car accident reports, written in French.
This paper presents a representation language in the notation of FOPC whose form facilitates the design of a semantic-network-like retriever.
sidesg.
oppositesg.
The paper presents a tabular interpretation for a kind of 2-Stack Automata.
This machine can extract morphemes from 10,000 character Japanese text by searching an 80,000 morpheme dictionary in 1 second.
Kenji characters are Chinese ideographs.
This morpheme extraction machine is designed as the first step toward achieving the natural language parsing accelerators.2 MACHINE DESIGNSTRATEGY2.1 MORPHEME EXTRACTIONMorphological analysis methods are generally composed of two processes: (1) a morpheme extraction process and (2) a morpheme determination process.
This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems.
The algorithm is compared with three other pronoun resolution methods: Hobbs' syntax-based algorithm, Strube's S-list approach, and the BFP Centering algorithm.
The first method uses a standard HMM part-of-speech tagger with variable context length.
In this paper we show that an unsupervised method for ranking word senses automatically can be used to identify infrequently occurring senses.
We demonstrate this using a ranking of noun senses derived from the BNC and evaluating on the sense-tagged text available in both SemCor and the SENSEVAL-2 English all-words task.
We address the representation of nouns having complex argument structures like deverbal nominalisations.
The first recog- nition pass extracts letter hypotheses from the spelled part of the waveform and maps them to phonemic hypotheses via a hierarchical sub- lexical model capable of generating grapheme- phoneme mappings.
In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classifiers.
We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar.
The model parameters were learned from unlabelled training data by a probabilistic context-free parser.
A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called 揷lassifier predicates.
As the first step in an automated text summarization algorithm, this work presents a new method for automatically identifying the central ideas in a text based on a knowledge-based concept counting paradigm.
This paper describes a system that provides customer service by allowing users to retrieve identification numbers of parts for medical systems using spoken natural language dialogue.
This result is achieved by combining a modified ca,seframe approach to linguistic knowledge representation with a parsing strategy able to integrate expectations from the language model and predictions from words.
We present a set of algorithms that en-able us to translate natural language sentences by exploiting both a trans-lation memory and a statistical-based translation model.
We describe a simple approach for integrating shallow and deep parsing.
We use phrase structure bracketing obtained from the Collins parser as filters to guide deep parsing.
In this paper, the usage and function of Chinese punctuations are studied in syntactic parsing and a new hierarchical approach is proposed for parsing long Chinese sentences.
In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction — selecting the part of speech of a word.
This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon.
We show how elements of the Qualia structure can be incorporated into semantic composition rules to make explicit the semantics of the combination adjective + noun.
We then discuss our method, which applies SVM induction over lexical and morphological features.
We present a FrameNet-based semantic role labeling system for Swedish text.
Values of lexical correlates lead to other word senses.
Thematic knowledge is a basis of semantic interpretation.
Using this way, a syntactic processor may become a thematic recognizer by simply deriving its thematic knowledge from its own syntactic knowledge.Keywords: Thematic Knowledge Acquisition, Syntactic Clues, Heuristics-guided Ambiguity Resolution, Corpus-based Acquisition, Interactive AcquisitionI.
syntactic processing systems and syntactically processed corpora).
machine translation), thematic role recognition is a major step.
This paper presents a system which automatically generates shallow semantic frame structures for conversational speech in unrestricted domains.We argue that such shallow semantic representations can indeed be generated with a minimum amount of linguistic knowledge engineering and without having to explicitly construct a semantic knowledge base.
To this end, we propose an aesthetically 揷lean?Bayesian inference scheme for exploiting lexical relations for passage-scoring for QA .
Current NER approaches include: dictionary-based, rule-based, or machine learning.
We represent shallow linguistic information as linguistic features in our ME model.
In this paper, we study aparsing technique whose purpose is to improve the practical efficiency of RCL parsers.
In this paper we propose a novel approach for ontology alignment and domain ontology extraction from the existing knowledge bases, WordNet and HowNet.
This paper evaluates a direct speech trans-lation Method with waveforms using the Inductive Learning method for short con-versation.
We define the notion of relative singularity of world objects as an abstraction class of the layer- membership relation
In this paper, I will discuss the effect of using variables in lexical category assignments in CCGs.
It will be shown that using variables in lexical categories can increase the weak generative capacity of CCGs beyond the class of grammars listed above.A Formal Definition for CCGsIn categorial grammars, grammatical entities are of two types: basic categories and functions.
This paper describes a system for analysing	naturallanguage based on the concept of case.
This paper presents a query tool for syntacti-cally annotated corpora.
The tool uses a query language that allows to search for tokens, syntactic cat-egories, grammatical functions and binary re-lations of (immediate) dominance and linear precedence between nodes.
Then we propose the rules to extract subjects and predicates from those sentences.
This paper introduces GLARF, a framework for predicate argument structure.
We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities.
as a hybrid system.
The standard design for a computer-assisted translation system consists of data entry of source text, machine translation,and revision of raw machine translation.
Knowledge-Based Report Generation is a technique for automatically generating natural language reports from computer databases.
The first application of the technique, a system for generating natural language stock reports from a daily stock quotes database, is partially implemented.
Three fundamental principles of the technique are its use of domain-specific semantic and linguistic knowledge, its use of macro-level semantic and linguistic constructs (such as whole messages, a phrasal lexicon, and a sentence-combining grammar), and its production system approach to knowledge representation.I.
WHAT IS KNOWLEDGE-BASEDREPORT GENERATIONA knowledge-based report generator is a computer program whose function • is to generate natural language summaries from computer databases.
Finally, it holds that macro-level knowledge units, such as whole seman tic messages, a phrasal lexicon, clausal grammatical categories, and a clause-combining grammar, provide an appropriate level of knowledge representation for generating that type of text which may be categorized as periodic summary reports.
These three tenets guide the design and implementation of a knowledge-based report generation system.II.
SAMPLE OUTPUT FROM AKNOWLEDGE-BASED REPORT GENERATORThe first application of the technique ofknowledge-based report generation is a partially implemented stock report generator called Ana.
We review studies of reference resolution, word recognition, and pragmatic effects on syntactic ambiguity resolution.
We present discriminative reordering models for phrase-based statistical machine translation.
The models are trained using the maximum entropy principle.
This paper presents the implementation of the Vietnamese generation module in ITS3, a multilingual machine translation (MT) system based on the Government & Binding (GB) theory.
Self-Organizing Maps (SOMs) are a good method to cluster and visualize large collections of documents, but they are computationally expensive.
In this paper, we investigate linguistically motivated reductions on the usual bagof-words representation, to improve efficiency.
Disambiguation is carried out with the latest version (April 1996) of the Constraint Grammar Parser (CGP).
The method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces.
SCNE is very common in written Chinese text.
This paper formulates the SCNE recognition within the source- channel model framework.
We construct two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively.
We used two conditional probabilistic models for this task, including conditional random fields (CRFs) and maximum entropy models.
In particular, we trained two conditional random field recognizers and one maximum entropy recognizer for identifying names of people, places, and organizations in unsegmented Chinese texts.
This paper presents a method for inducing the parts of speech of a language and partof-speech labels for individual words from a large text corpus.
Vector representations for the part-of-speech of a word are formed from entries of its near lexical neighbors.
A neural net trained on these spatial representations classifies individual contexts of occurrence of ambiguous words.
In this paper we describe EFLUF - an implementation of FLUF.
In this paper, we propose a method for ex-tracting key paragraphs in articles based on the degree of context dependency.
We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts.
We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model.
We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses.
We solve the problem of hierarchical structure in our tagging model by modeling underspecified tags, which are fully determined only at decoding time.
In creating an English grammar checking software product, we implemented a large-coverage grammar based on the dependency grammar formalism.
In this paper we sketch a decidable inference-based procedure for lexical dis-ambiguation which operates on semantic representations of discourse and concep-tual knowledge.
Content selection is a key factor of any successful document generation system.
This paper shows how a content selection algorithm has been implemented using an efficient combination of XML/XSL technology and the framework of RST for discourse modeling.
In this paper we present an approach to dialogue management that supports the generation of multifunctional utterances.
It is based on the multidimensional dialogue act taxonomy and associated context model as developed in Dynamic Interpretation Theory (DIT).
This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction.
In this paper we describe our experience of using post-edit data to evaluate SUMTIME-MOUSAM, an NLG system that produces marine weather forecasts.
This paper describes using RDF/RDFS/XML to create and navigate a metadata model of relationships among entities in text.
In this paper we consider extended similarity metrics for documents and other objects embedded in graphs, facilitated via a lazy graph walk.
We provide a detailed instantiation of this framework for email data, where content, social networks and a timeline are integrated in a structural graph.
The suggested framework is evaluated for the task of disambiguating names in email documents.
This paper presents a data-driven language independent word segmentation system that has been trained for Chinese corpus at the second Chinese word segmentation bakeoff.
At discourse level, anaphoric and coreference expressions are annotated.
ConTroll is a grammar development system which supports the implementation of current constraint-based theories.
It uses strongly typed feature structures as its principal data structure and offers definite relations, universal constraints, and lexical rules to express grammar constraints.
This paper proposes a principled approach for analysis of semantic relations between constituents in compound nouns based on lexical semantic structure.
All results presented were generated by using the Ngram-based statistical machine translation system which has been enhanced from the last year抯 evaluation with a tagged target language model (using Part-Of-Speech tags).
This paper proposes a two-phase shift-reduce dependency parser based on SVM learning.
We demonstrate a proof-of-concept system that uses a shallow chunking-based technique for knowledge extraction from natural language text, in particular looking at the task of story understanding.
This technique is extended with a reasoning engine that borrows techniques from dynamic ontology refinement to discover the semantic similarity of stories and to merge them together.
To support context-based multimodal interpretation in conversational systems, we have developed a semantics-based representation to capture salient information from user inputs and the overall conversation.
We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.
The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.
This is a proposal for a an XML mark-up of argumentation.
The knowledge extraction process utilizes the structure property, statistical property as well as partial linguistic knowledge of the organization names to extract new organizations from domain texts.
We discuss impli-cations of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems.
Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.
Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.
We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.
The paper presents a measure, based on the x2 statistic, for measuring both corpus similarity and corpus homogeneity.
We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora.
This paper presents the story understanding mechanism for creating computer animation scenarios.
Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance.
A generative HMM/CFG composite model, which integrates easy-toobtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement.
The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework.
We also study and compare conditional random fields (CRFs) with perceptron learning for SLU.
The first is the grammar of stress, or metrical phonology, which has received much attention in the learning model literature.
In this paper an architecture and an implementation for a linguistically based prosodic analyser is presented.
We report on the role of the Urdu grammar in the Parallel Grammar (ParGram) project (Butt et al., 1999; Butt et al., 2002).1 The ParGram project was designed to use a single grammar development plat-form and a unified methodology of grammar writ-ing to develop large-scale grammars for typologi-cally different languages.
domain-independent, semantic information for question interpretation.
We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text.
The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model.
In this paper I outline Type-inheritance Combinatory Categorial Grammar (TCCG), an implemented feature structure based CCG fragment of English.
TCCG combines the fully lexical nature of CCG with the type-inheritance hierarchies and complex feature structures of Head- driven Phrase Structure Grammars (HPSG).
Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts.
This framework was applied to a text retrieval system for MEDLINE.
We present a method for compiling grammars into efficient code for head-driven generation in ALE.
Memory-based learning is a form of supervised learning based on similarity-based reasoning.
Memory-based tagging shares this advantage with other statistical or machine learning approaches.
Given a spoken query, we generate a transcription and detect OOV words through speech recognition.
In this paper, we explore how the addition of information to the text, in particular part of speech and dysfluency annotations, can be used to .
build more complex language models.
To answer these questions, we present a variety of kinds of analysis, from vocabulary distributions to perplexities on language models.
This additional assumption allows us to identify sentiment-bearing terms very reliably.
We then use these newly identified terms in various scenarios for the sentiment classification of sentences.
In contrast, this paper describes a new approach toward using contextual, dialog-based knowledge for speech recognition.
The grammars used for speech recognition dictate legal word sequences.
The mapping between syntactic structure and prosodic structure is a widely discussed topic in linguistics.
We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text.
This paper aims to analyze word dependency structure in compound nouns appearing in Japanese newspaper articles.
This paper presents a corpus-based approach which scans a corpus with a set of pattern matchers and gathers co-occurrence examples to analyze compound nouns.
Can we do text analysis without phrase structure?
In this paper, we propose a supervised learning-based approach which does coreference resolution by exploring the relationships between NPs and coreferential clusters.
Personal MT (PMT) is a new concept in dialogue- based MT (DBMT) , which we are currently studying and prototyping in the LIDIA project Ideally, a PMT system should run on PCs and be usable by everybody.
The paper briefly presents some of them (HyperText, distributed architecture, guided language, hybrid transfer/interlingua, the goes on to study in more detail the structure of the dialogue with the writer and the place of speech synthesis [1].KeywordsPersonal Machine Translation, dialogue-based Machine Translation, Man-Machine Dialogue, Ambiguity Resolution, Speech Synthesis.
In this paper we address the question of assigning semantic roles to sentences in Chinese.
Finally, we compare English and Chinese semantic-parsing performance.
We describe and evaluate an implemented system for general-knowledge question answering.
We present an automatic approach to learning criteria for classifying the parts-of-speech used in lexical mappings.
Associations among these and the parts-of-speech are learned using the lexical mappings contained in the Cyc knowledge base as training data.
The paper describes work on applying a general purpose natural language processing system to transfer-based interactive translation.
This paper proposes a method of finding correspondences of arbitrary length word sequences in aligned parallel corpora of Japanese and English.
Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations.
This paper deals with the use of computational linguistic analysis techniques for information access and ontology learning within the legal domain.
This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.
We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.
The topic of the paper is the introduction of a formalism that permits a homogeneous representation of definite temporal adverbials, temporal quantifications (as frequency and duration), temporal conjunctions and tenses, andof their combinations with propositions.
The formal representation is based on the notions "phase-set" and "phase-operator", and it involves an interval logic.
The parser of QPATR uses a left-corner algorithm for context-free grammars and includes a facility for identifying new lexical items in input on the basis of contextual information.
In statistical machine translation, the generation of a translation hypothesis is computationally expensive.
Therefore, we present an extension to the ITG constraints.
We introduced a new linguistic representation, the Dynamic Hierarchical Phrasal Lexicon (DHPL) [Zemik88], to facilitate language acquisition.
We present a transliteration algorithm based on sound and spelling mappings us-ing finite state machines.
We apply our translit-eration algorithm to the transliteration of names from Arabic into English.
We trained two parsers with the training corpus in which the semantic argument information is attached to the constituent labels, we then used the resulting parse trees as the input of the pipelined SRL system.
We present our results of combining the output of various SRL systems using different parsers.
This paper will address the emerging standards for evaluation of spoken language systems.
By representing discourse graphically, we also show that interruptions are part of the local and global coherence that is brought about through the systematic phrase-to-phrase prosodic patterns of discourse.
This paper introduces a standard setting of binary features, inspired by the literature on named-entity recognition and text chunking, and derives corresponding real- valued features based on smoothed log- probabilities.
This paper proposes a discriminative HMM (DHMM) with long state dependence (LSDDHMM) to segment and label sequential data.
The LSD-DHMM overcomes the strong context independent assumption in traditional generative HMMs (GHMMs) and models the sequential data in a discriminative way, by assuming a novel mutual information independence.
In this paper, a variable-length mutual information- based modeling approach and an ensemble of kNN probability estimators are proposed to capture the long state dependence and the observation dependence respectively.
This suggests that the LSD-DHMM can effectively capture the long context dependence to segment and label sequential data.
We have been developing our own modified TBL learner initially to tackle the Part-of-Speech tagging problem, for integration in a hybrid NLL and rule- based system for information extraction (Ciravegna et al., 1999).
A variety of statistical methods for noun compound analysis are implemented and compared.
We illustrate tins point with the presentation of ALLiS, a learning system winch generates a regular expression grammar of non-recursive phrases from bracketed corpora.
The parser uses application-restricted grammars and lexicons obtained from ontologies representing the application specific knowledge.
This paper describes how recent linguistic results in explaining Japanese short and long distance scrarnbling can be directly incorporated into an existing principles-and-parameters-based parser with only trivial modifications.
Pronouns are the most common NPs in the speech that children hear.
Taken together, these results suggest that children might use regularities in pronoun/verb co-occurrences to help learn verbs, though whether this is actually so remains a topic for further research.
In this paper, we explore a number of alternatives for how these parse tree paths are encoded, focusing on the difference between automatically generated constituency parses and dependency parses.
This paper presents a genetic algorithm based approach to the automatic discovery of finite- state automata (FSAs) from positive data.
We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging.
Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.
We deal with the question as to whether there exists a polynomial time algorithm for computing the most probable parse tree of a sentence generated by a data-oriented parsing (DOP) model.
Therefore we describe DOP as a stochastic tree-substitution grammar (STSG).
This paper presents a grammar formalism designed for use in data-oriented approaches to language processing.
Both full-text information retrieval and large scale parsing require text preprocessing to identify strong lexical associations in textual databases.
In order to associate linguistic felicity with computational efficiency, we have conceived FASTR a unification-based parser supporting large textual and grammatical databases.
The grammar is composed of term rules obtained by tagging and lemmatizing term lists with an on-line dictionary.
This paper describes a new approach for estimating term weights in a text classification task.
The approach uses term co- occurrence as a measure of dependency between word features.
We present a uniform computational architecture for developing reversible grammars for parsing and generation, and for bidirectional transfer in MT.
The system chosen was the Caption Generation System.
Categorial Dependency Grammars (CDG) introduced in this paper make clear-cut distinction between local and distant word driven dependencies.
Besides this, CDGs represent a convenient frame for relating dependency grammar with linguistic semantics.
An outline of our approach to storing situation-to-language associations will then be provided.
In this paper, we presented a new technique for the semi梐utomatic tagging of Chinese text.
We use dependency grammar and employ a stack based shift/ reduce context梔ependent parser as the tagging mechanism.
Further more, if you want to obtain the co梠ccurrence frequency of each two adjacent part of speeches, which is helpful to the study of part of speech (POS) tagging, you must annotate the corpora with POS information.
This paper presents a maximum entropy method for the disambiguation of word senses as defined in HowNet.
The maximum entropy model treats semantic tags like parts-of-speech tags and achieves an overall accuracy of 89.39%, outperforming a baseline system, which picks the most frequent sense.
The system employs an interval-based temporal network for storing histor-ical information.
Lexical chains capture the semantic relations between words that occur throughout a text.
We focus on Example based machine translation and the automatization of the translation examples extraction by means of RDFrepositories.
In this paper, we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information.
We introduce a new method for disambiguating word senses that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique to achieve accuracy superior to the best published individual models.
We also contrast against another type of kernel method, the support vector machine (SVM) model, and show that our KPCA-based model outperforms the SVM-based model.
One of the currently most used statistics is the mutual information ratio.
This paper compares the mutual information ratio and a measure that takes temporal ordering into account.
Non-compositional expressions present a special challenge to NLP applications.
In this paper we describe a novel approach to lexical chain based segmentation of broadcast news stories.
The key proposal is to incorporate into lexical heads the WOC (Word Order Constraints) feature, which is used to constrain the word order of its projection.
In order to deal with ambiguity, the MORphological PArser MORPA is provided with a probabilistic context-free grammar (PCFG), i.e.
it combines a "conventional" context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse.
MORPA is a fully implemented parser developed for use in a text-to-speech conversion system.
Information Extraction (1E) systems are com-monly based on pattern matching.
The "Noisy Channel" 's are the promoters of statistically based approaches to language learning.
This paper describes a machine learning approach to classifying n-best speech recognition hypotheses as either correctly or incorrectly recognised.
Decanter illustrates a heuristic approach to extraction for information retrieval and question answering.
A parse of discourse is defined as a set of semantic dependencies among sentences that make up the discourse.
We also study effects of features such as clue words, distance and similarity on the performance of the discourse parser.
Identifying sentiments (the affective parts of opinions) is a challenging problem.
The system contains a module for determining word sentiment and another for combining sentiments within a sentence.
Currently, our goal is to provide a language model using transition statistics to disambiguate alternative parses for a speech recognition device.
The first description is treated by extending type symbol lattices to include complement type symbols.
Algorithms for augmented-WS unification have been developed using graph unification, and programs using these algorithms have been written in Common Lisp.
We have studied three aspects of robustness in such a system: accent differences, mixed language input, and the use of common feature sets for HMM-based speech recognizers for English and Cantonese.
In this paper, we explore how the taxo-nomic inheritance hierarchy in a seman-tic net can contribute to the resolution of associative anaphoric expressions.
In this paper we analyze story link detection and new event detection in a retrieval framework and examine the effect of a number of techniques, including part of speech tagging, new similarity measures, and an expanded stop list, on the performance of the two detection tasks.
We present an approach using syntactosemantic rules for the extraction of relational information from biomedical abstracts.
This list of names was included in the lexicon of our retrained part-of-speech tagger for use on molecular biology abstracts.
Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
We employ both unsupervised clustering and semi-supervised learning to recognize pitch accent in English and tones in Mandarin Chinese.
We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments.
We have developed a summarization method that creates a summary suitable for the process of sifting information retrieval results.
The model is a mixture of the regeneration-based model and the rewriting-based model.
This paper constitutes an investigation into the generative capabilities of two-level phonology with respect to unilevel generative phonological rules.
We explore features of hand gesture that are correlated with coreference.
In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text.
The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation.
We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.
Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.
The logic behind parsers for categorial grammars can be formalized in several different ways.
We present an empirical corpus study of the meaning and usage of time phrases in weather forecasts; this is based on a novel corpus analysis technique where we align phrases from the forecast text with data extracted from a numerical weather simulation.
In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed.
This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora resolution.
Finally, an experimental evaluation of these variations is reported.Subject Areas: corpus-based language modeling, computational lexicography
This paper describes our word segmentation system and named entity recognition (NER) system for participating in the third SIGHAN Bakeoff.
This paper describes our effort on the task of edited region identification for parsing disfluent sentences in the Switchboard corpus.
We explore new feature spaces of a partof-speech (POS) hierarchy and relaxed for rough copy in the experiments.
To overcome the problem of not having enough manually labeled relation instances for supervised relation extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data.
That system employs constraint satisfaction, branch-and-bound and solution synthesis techniques to produce near linear-time processing for knowledge-based semantic analysis.
This is accomplished using a computational tool known as solution synthesis.
Mechanisms of compressidn in medical records for a collaborative study of breast cancer are described.
This paper describes an empirical study on the optimal granularity of the phrase structure rules and the optimal strategy for interleaving CFG parsing with unification in order to Unplement an efficient unification-based parsing system.
We claim that using "medium-grained" CEG phrase structure rules, which balance the computational cost of CPC; parsing and unification, are a cost-effective solution for making unification-based grammar both efficient and easy to maintain.
The grammar and the parser described in this paper are fully implemented and used as the Japanese analysis module in SL-TRANS, the speech-to-speech translation system of ATR.
A hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structures for semantic role labeling (SRL).
The hybrid kernel consists of two individual convolution kernels: a Path kernel, which captures predicate- argument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments.
We also combine our new hybrid tree kernel based method with the standard rich flat feature based method.
In this paper we deal with learning and forgetting of speech commands in speech dialogue systems.
We discuss two mathematical models for learning and four models for forgetting.
This paper introduces a new method for identifying candidate phrasal terms (also known as multiword units) which applies a nonparametric, rank-based heuristic measure.
We propose an approximation algorithm, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and ex-ecution time of thesaurus extraction with only a marginal performance penalty.
This part-ofspeech predictor will be used in a part-of-speech tagger to handle out-of-lexicon words.
We have developed an automated Japanese essay scoring system called Jess.
This paper presents SemFrame, a system that automatically induces the names and internal structures of semantic frames.
After SemFrame identifies sets of frame- evoking verb synsets, the conceptual density of nodes in the WordNet network for corresponding nouns and noun synsets is computed and analyzed.
We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages.
In this paper, we discuss the generation of paraphrases from predicate/argument structure using a simple, uniform generation methodology.
Central to our approach are lexico-grammatical resources which pair elementary semantic structures with their syntactic realization and a simple but powerful mechanism for combining resources.
We can also tokenize, lemmatise and part-of-speech tag the corpus, and load the data into a corpus query tool which supports sophisticated linguistic queries.
We have been developing ro-bust natural language tools for the au-tomated extraction of structured infor-mation from biomedical texts as part of a project we call MEDSTRACT.1 Here we will describe an architecture for developing databases for domain spe-cific information servers for research and support in the biomedical commu-nity.
In this paper, we want to describe a tag-ger/lemmatiser for Dutch medical voca-bulary, which consists of a full-form dic-tionary and a morphological recogniser for unknown vocabulary coupled to an expert system-like disambiguation mo-dule.
The tag-ger/lemmatiser currently functions as a lexical front-end for a syntactic parser.
This paper offers a provisional mathematical typology of metrical representations.
We present a cost-based (or energy-based) model of disambiguation.
This method of ambiguity resolution is implemented in DMTRANS PLUS, which is a second generation bi-directional English/Japanese machine translation system based on a massively parallel spreading activation paradigm developed at the Center for Machine Translation at Carnegie Mellon University.
This paper describes a system for the un-supervised learning of morphological suf-fixes and stems from word lists.
The sys-tem is composed of a generative probabil-ity model and a novel search algorithm.
This paper describes discriminative language modeling for a large vocabulary speech recognition task.
The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer.
This paper proposes a new discriminative training method, called minimum sample risk (MSR), of estimating parameters of language models for text input.
We have developed a parser generator for natural language processing.
This paper describes a speech interface to the Google search engine.
Rohrer.
One of the necessary tasks of a machine translation system is lexical transfer.
This paper contributes to the theory of substructural logics that are of interest to categorial grammarians.
In this paper, we describe a method of extracting information from an on-line resource for the construction of lexical entries for a multi-lingual, interlingual MT system (ULTRA).
We have been able to automatically generate lexical entries for interlingual concepts corresponding to nouns, verbs, adjectives and adverbs.
This paper proposes a description of German word order including phe-nomena considered as complex, such as scrambling, (partial) VP fronting and verbal pied piping.
This paper describes our work-in-progress in au-tomatic English-to-Korean text translation.
We tackle the ambiguity problem by incorporating syntactic and semantic categories in the anal-ysis grammar.
For system robustness, integration of two subsystems is under way: (i) a rule-based part-of-speech tagger to handle un-known words/constructions, and (ii) a word-for-word translator to handle other system failures.
plex constraints, that is, constraint disjunction (Crowhurst and Hewitt, 1995) and local constraint conjunction (Smolensky, 1995).
This paper presents a complete integrated NLG system which uses a Description logic for the content determination module, Segmented Discourse Representation Theory for the document structuring module and a lexicalized formalism for the tactical component.
We describe a bidirectional framework for natural language parsing and genera-tion, using a typed feature formalism arid an HPSG-based grammar with a parser arid generator derived from parallel pro-cessing algorithms.
We present an ap-proach to delayed lexical choice in gener-ation, based on subsumption within the sort hierarchy, using a lexicon of under-instantiated signs which are derived from the normal lexicon by lexical rules.
Schwa deletion is an important issue in grapheme-to-phoneme conversion for Indo- Aryan languages (IAL).
This paper describes a novel instance- based sentence boundary determination method for natural language generation that optimizes a set of criteria based on examples in a corpus.
its aggregation and referring expression generation capability).
This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes.
We here explore a "fully" lexicalized Tree-Adjoining Grammar for discourse that takes the basic elements of a (monologic) discourse to be not simply clauses, but larger structures that are anchored on variously realized discourse cues.
1.n experiments comparing the four previously available machine-learning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best.
In this paper we discuss the notions of hypertext, blog, wiki and cognitive mapping in order to find a solution to the main problems of processing text data stored in these forms.
The paper also discusses problems of representing the valency information in case-frames arising in a spoken language environment.0.
This paper presents a transfer framework called LFT (Lexical-functional Transfer) for a machine translation system based on LFG (Lexical-functional Grammar).
Since LFG is a grammatical framework for sentence structure analysis of one language, for the purpose, we propose a new framework for specifying transfer rules with LFG schemata, which incorporates corresponding lexical functions of two different languages into an equational representation.
Multiple-class annotation is more challenging than single- class annotation.
In this paper, we took a single word classification approach to dealing with the multiple-class annotation problem using Support Vector Machines (SVMs).
This paper presents a multidimensional Dependency Grammar (DG), which decouples the dependency tree from word order, such that surface ordering is not determined by traversing the dependency tree.
We develop the notion of a word order domain structure, which is linked but structurally dissimilar to the syntactic dependency tree.
We then discuss the implementation of such a DG using constructs from a unification-based phrase-structure approach, namely Lexical-Functional Grammar (LFG).
We briefly present the Multi- Engine Machine Translation (MEMT) architecture, describing how it is well- suited for such an application.
We then describe our incorporation of interactive error correction throughout the system design.
A corpus-based analysis shows the existence of surface regularities related to metaphors.
These clues can be characterized by syntactic structures and lexical markers.
We present an object oriented model for representing the textual clues that were found.
We present a Korean question answering framework for restricted domains, called K-QARD.
This papers extends previous statistical models to class-to-class preferences, and presents a model that learns selectional preferences for classes of verbs.
The model is tested on a word sense disambiguation task which uses subject-verb and object-verb relationships extracted from a small sense-disambiguated corpus.
We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity.
Based on this formulation, different models for case identification and word-sense disambiguation are derived.
These results clearly demonstrate the superiority of the proposed models for deep-structure disambiguation.
This paper describes the use of a system of semantic rules to generate noun compounds, vague or polysemous words, and cases of metonymy.
This paper describes a hybrid approach to spontaneous speech parsing.
We have implemented an interactive, Web-based, chat-style machine translation system, supporting speech recognition and synthesis, local- or third-party correction of speech recognition and machine translation output, and online learning.
In this paper we describe the Senseval 3 Basque lexical sample task.
In this paper we describe two independent methods of improving speech recognizers: a machine translation (MT) method and a topic-based one.
morphological derivation and synonymy expansion) in web search strategies.
We first model standard language text using standard Chinese corpora and apply these models to detect anomalous chat text.
We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.
The REQUEST System is an experimental natural language query system based on large transformational grammar of English.
This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG).
In this paper we outline a lexical organization for Turkish that makes use of lexical rules for inflections, derivations, and lexical category changes to control the proliferation of lexical entries.
A lexical inheritance hierarchy facilitates the enforcement of type constraints.
A class of constraint-based categorial grammars is proposed in which the construction of both logical forms and strings is specified completely lexically.
In particular, we outline the structure of the geometrical scene description, the representation of events in a logic-oriented semantic representation language, the case-frame lexicon and the representation of the referential semantics based on the Flavor system.
We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs.
We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs.
We present a system for unsupervised tagging of words into classes produced by a distributional clustering technique called co-clustering.
We also show how state-level term emission models canbe augmented to account for morphological patterns using features automatically derived from the output of co-clustering.
Two classifiers -- Support Vector Machine (SVM) and Conditional Random Fields (CRFs) are applied here for the recognition of biomedical named entities.
This paper presents an original method and its implementation to extract terminology from corpora by combining linguistic filters and statistical methods.
MedSLT is a unidirectional medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different language pairs and subdomains.
This paper describes an all level approach on statistical natural language translation (SNLT).
Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a generalized form of a word alignment (WA).
The implementation of all the approaches combines generation algorithms in Prolog and HPSG grammars in ProFIT.
It is natural to combine a head-driven HPSG grammar with a head- driven generation algorithm.
We then switch to non-head-driven approaches.
for use with categorial grammar and indexed QLF, can be used with HPSG and Minimal Recursion Semantics.
The informing principles are those of modern 'lexicalist' unification-based linguistic theories: the English analysis grammar is based on Lexical-Functional Grammar (Bresnan, ed.
1982) and Generalized Phrase Structure Grammar (Gazdar et al 1985), the Japanese generation grammar on Categorial Grammar (Ades & Steedman 1982, Steedman 1985, Whitelock 1986).
We	propose	abootstrapping framework in which soft and hard matching pattern rules are combined in a cascading manner to realize a weakly supervised rule induction scheme.
Applying the noisy channel model to search query spelling correction requires an error model and a language model.
Typically, the error model relies on a weighted string edit distance measure.
Furthermore, we outline an exemplar-based approach in which se se-views are developed gradually and incrementally.
This paper argues for a novel data structure for the representation of discourse referents.
A so-called hashing list is employed to store discourse referents according to their grammatical features.
We introduce two general techniques to address the search problem, expectation-driven search and dy-namic grammar rule selection, and present the archi-tecture of an implemented generation system called IGEN.
Our approach uses a domain-specific genera-tion grammar that is automatically derived from a semantically tagged treebank.
In this paper we introduce an integrated approach for named entity translation deploying phrase-based translation, word-based translation, and transliteration modules into a single framework.
OF COLING-92, NANTES, Atm.
information technology test reports and medical finding reports.
Besides centering-based discourse analysis mechanisms for pronominal, nominal and bridging anaphora, SYNDIKATE is supplied with a learning module for automatically bootstrapping its domain knowledge as text analysis proceeds.
Semantic entropy is a measure of semantic ambiguity and uninformativeness.
This paper presents a method for measuring semantic entropy using translational distributions of words in parallel text corpora.
We identify problems with the Penn Tree- bank that render it imperfect for syntax- based machine translation and propose methods of relabeling the syntax trees to improve translation quality.
Prosodic patterns of discourse markers occurring in the recorded corpus have been analyzed.
This paper suggests a method to align Korean-English parallel corpus.
The proposed alignment al-gorithm is based on dynamic program-ming.
In this paper we present a model for statistical English-to-Korean transliteration that generates transliteration candidates with probability.
We investigate independent and relevant event-based extractive mutli-document summarization approaches.
replaced by case relations, using a a catalogue of patterns/interpretation pairs, a concept type hierarchy, and a set of selectional restriction rules on semantic interpretation types.
Two main problems in natural language generation are lexical selection and syntactic structure determination.
In this paper, a knowledge-based computational model which handles these two problems in interlingua approach is presented.
The developed system takes interlingua representations of individual sentences, performs lexical selection, and produces frame-based syntactic structures.
In this paper we show how two standard outputs from information extraction (IE) systems ?named entity annotations and scenario templates ?can be used to enhance access to text collections via a standard text browser.
We propose a method of organizing reading materials for vocabulary learning.
In this paper I will show how to use bidirectional unification grammars to define rever.sible relations between language dependent meaning representations.
As an alternative, we have developed an ef-ficient, trainable algorithm that uses a lex-icon with part-of-speech probabilities and a feed-forward neural network.
This work demonstrates the feasibility of using prior probabilities of part-of-speech assignments, as opposed to words or definite part-of-speech assignments, as contextual infor-mation.
This paper presents a trainable sentence planner for the MATCH dialog system.
A new type of thesaurus for word process-ing is proposed.
This paper focuses on IE tasks designed to support information discovery applications.
This paper describes the construction of language choice models for the microplanning of discourse relations in a Natural Language Generation system that attempts to generate appropriate texts for users with varying levels of literacy.
We describe how the design of microplanner is evolving.
This paper presents an implemented multi-tape two- level model capable of describing Semitic non-linear morphology.
This paper describes recent improvements in the weight estimation technique for sentence hypothesis rescoring using the N-Best formalism.
Automatically acquiring synonymous words (synonyms) from corpora is a challenging task.
Contextual information and the mapping from WordNet synsets to Cilin sense tags deal with word sense disambiguation.
Each generalized metaphor contains a recognition network, a basic mapping, additional transfer mappings, and an implicit intention component.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
Many probabilistic models for natural language are now written in terms of hierarchical tree structure.
We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
Relaxation labelling is an optimization technique used in many fields to solve constraint satisfaction problems.
In this paper we present our recent work on harvesting English-Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subpara-graph level via utilizing the number-ing system in the legal text hierarchy.
It is designed to provide telephone speech suitable for the development of automatic voice-interactive telephone services.
We describe a method for acquiring semantic cooccurrence restrictions for tuples of syntactically related words (e.g.
verb-object pairs) from text corpora automatically.
WordNet) to ambiguous words occurring in a syntactic dependency.
This paper proposes a generic mathematical formalism for the combination of various structures: strings, trees, dags, graphs and products of them.
After a short recall of our view of dependency grammars, we present two dependency parsers.
The second uses typed feature structures to add some semantic knowledge on dependency trees and parses in a more robust left to right manner.
A novel formalism is presented for Earley-like parsers.
It accommodates the simulation of non-deterministic pushdown automata.
In particular, the theory is applied to non-deterministic LR-parsers for RTN grammars.
A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English, French, Spanish and Italian corpora.
We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors' of noun phrases.
By using these referential properties, our system determined the referents of noun phrases in Japanese sentences.
Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases.
In this paper, a novel machine learning approach for the identification of named entity relations (NERs) called positive and negative case-based learning (PNCBL) is proposed.
This approach has been applied to the identification of domain-specific and cross-sentence NERs for Chinese texts.
In this paper, we propose and implement a model for bootstrapping parallel wordnets based on one monolingual wordnet and a set of cross-lingual lexical semantic relations.
In particular, we propose a set of inference rules to predict Chinese wordnet structure based on English wordnet and English-Chinese translation relations.
We automatically annotated documents with document structure and semantic tags by using taggers, and retrieve information by specifying structure represented by tags and words using ranked region algebra.
In this paper we discuss recent results from our efforts to make SPHINX, the CMU continuous-speech speaker- independent recognition system, robust to changes in the environment.
(Way and Gough, 2005) provide an in- depth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools.
This paper deals with the reference choices involved in the generation of argumentative text.
JANUS is a multi-lingual speech-to-speech translation system, which has been designed to translate spontaneous spoken language in a limited domain.
We end the paperby mentioning some projects for long-term research.
The second step interprets logical expression to generate network structure.
We have implemented set of programs which performs the stepwise translation.Experiments are in progress	for machinetranslation and question answering.
We argue that flexibility is an important property for unification-based formalisms.
Collocation translation is important for machine translation and many other NLP tasks.
First, dependency triples are extracted from Chinese and English corpora with dependency parsers.
Then, a dependency triple translation model is estimated using the EM algorithm based on a dependency correspondence assumption.
The generated triple translation model is used to extract collocation translations from two monolingual corpora.
We distinguish between context-free repre-sentability and context, free processing.
We examine the benefits of using multi-ple agents to produce explanations.
In this paper, we describe a corpus study of CRs in task-oriented dialogue and compare our findings to those reported in two prior studies.
Finally we identify form- function correlations which can inform the generation of CRs.
An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.
We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies.
Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.
The method has been applied in a speech translation project with large HPSG grammars.
For each language, the system detects the major news stories of the day using a group-average unsupervised agglomerative clustering process.
This paper describes named entity (NE) extraction based on a max-imum entropy (M.E.)
model and transformation rules.
This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature.
This paper introduces PhraseNet, a context- sensitive lexical semantic knowledge base system.
PhraseNet makes use of WordNet as an important knowledge source.
Immediate Dominance rules, metarules, and lexical rules are clearly distinguished in their form but all serve to capture the phenomenon of subcategorization.This paper proposes the extension of cooccurrence restrictions in GPSG to express constraints on the cooccurrence of categories within local trees.
We describe a simple sentence length approach to sentence alignment and a hybrid, multi-feature approach to perform word alignment.
We use a multi-feature approach with dictionary lookup as a primary technique and other methods such as local word grouping, transliteration similarity (edit-distance) and a nearest aligned neighbours approach to deal with many-to-many word alignment.
In order to incorporate long-distance information into the ME frame-work in a language model, a Whole Sentence Maximum Entropy Language Model (WSME) could be used.
In this paper, we propose the applica-tion of another sampling technique: the Perfect Sampling (PS).
Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
All the current rule-based and machine learning- based approaches for this task operate at the document level.
Polysemy is one of the major causes of difficulties in semantic clustering of words in a corpus.
The Hidden Markov Model (HMM) for part-of-speech (POS) tagging is typically based on tag trigrams.
This paper describes an ongoing effort to parse the Hebrew Bible.
We first constructed a cantillation treebank which encodes the prosodic structures of the text.
First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of pos-sible sentence plans for a given text-plan input.
Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan.
The SPR uses ranking rules automatically learned from train-ing data.
In this paper, we investigate the use of multivariate Poisson model and feature weighting to learn naive Bayes text classifier.
This paper describes several key experiments in large vocabulary speech recognition.
We compare the performance of two feature preprocessing algorithms for microphone independence and we describe a new microphone adaptation algorithm based on selection among several codebook transformations.
Fixed weight smoothing of the mixture weights allowed the use of word-boundary-context-dependent triphone models for both speaker-dependent (SD) and speaker- independent (SI) recognition.
A new form of phonetic context model, the semiphone, is also introduced.
Word processors or computers used in Japan employ Japanese input method through keyboard stroke combined with Kana (phonetic) character to Kanji (ideographic, Chinese) character conversion technology.
The key factor of Kana-to-Kanji conversion technology is how to raise the accuracy of the conversion through the homophone processing, since we have so many homophonic Kanjis.
In this paper, we report the results of our Kana-to-Kanji conversion experiments which embody the homophone processing based on large scale collocation data.
We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.
The system divides query processing into the following phases:?Lexical lookup?Syntactic parsing?Semantic interpretation and selectional filtering?Quantifier scoping?Database query generation?Query optimization?Database retrievalThe syntactic and semantic rules used in the parsing and interpretation phases are expressed in a unification-based formalism.
Grammatical unification is then implemented simply as term unification in Prolog, which is the implementation language used in the system.In the semantic interpretation phase, logical form expressions are computed bottom-up by applying semantic interpretation rules keyed to the syntax rules.
We also demonstrate that an implementation of this algorithm is capable of learning auxiliary fronting in polar interrogatives (AFIPI) in English.
This document proposes a new taxon-omy for describing the quality of services which are based on spoken dialogue sys-tems (SDSs), and operated via a telephone interface.
We investigate the performance of the Structured Language Model (SLM) in terms of perplexity (PPL) when its components are modeled by connectionist models.
The EM training procedure can improve the connectionist models further, by using hidden events obtained by the SLM parser.
We developed a prototype information retrieval system which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval.
It utilizes the domain independent semantic form of The Rochester Interactive Planning System (TRIPS) with a domain independent stochastic surface generation module.
We show that a written text language model can be used to predict dialogue utterances from an over- generated word forest.
We present the concept of a "Segmental Neural Net" (SNN) for phonetic modeling in continuous speech recognition.
We segment Chinese text into words based on a word-based Chinese language model.
The Grapheme-to-Phoneme (G2P) conversion model achieves 98 % accuracy.
We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser.
We also further reduce the derivation space using constraints on category combination.
This paper proposes a probabilistic chunker to help the development of a partially bracketed corpus.
The chunker partitions the part-of-speech sequence into segments called chunks.
This paper focuses on mining tables from large-scale HTML texts.
Heuristic rules and cell similarities are employed to identify tables.
We also propose an algorithm to capture attribute-value relationships among table cells.
An analysis of English tense and aspect is presented that specifies temporal precedence relations within a sentence.
In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque.
This method learns empirical associations between free-form texts and canonical terms from human-assigned matches and determines a Linear Least Squares Fit (LLSF) mapping function which represents weighted connections between words in the texts and the canonical terms.
We introduce a learner capable of automatically extending large, manually written natural language Definite Clause Grammars with missing syntactic rules.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system.
We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the base-line.
This paper introduces a class of statistical mechanisms, called hidden understanding models, for natural language processing.
We address this problem by introducing hypergraphs for speech processing.
By converting ordinary word graphs to hyper- graphs one can reduce the number of edges considerably.
In this paper, we present a chunk based partial parsing system for spontaneous, conversational speech in unrestricted domains.
We show that the chunk parses produced by this parsing system can be usefully applied to the task of reranking Nbest lists from a speech recognizer, using a combination of chunk-based n-gram model scores and chunk coverage scores.The input for the system is Nbest lists generated from speech recognizer lattices.
The hypotheses from the Nbest lists are tagged for part of speech, "cleaned up" by a preprocessing pipe, parsed by a part of speech based chunk parser, and rescored using a backpropagation neural net trained on the chunk based scores.
We present a new framework for classifying common nouns that extends named- entity classification.
We propose a supervised, two-phase framework to address the problem of paraphrase recognition (PR).
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.
BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.
BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.
We present a novel method to identify effective surface text patterns using an inter- net search engine.
The paper deals with a new approach to importance evaluation of descriptive texts developed in the framework of SUSY, an experimental system in the domain of text summarization.
We present a comparative evaluation of two data-driven models used in translation selection of English-Korean machine translation.
Latent semantic analysis(LSA) and probabilistic latent semantic analysis (PLSA) are applied for the purpose of implementation of data-driven models in particular.
We have used k- nearest neighbor (k-NN) learning to select an appropriate translation of the unseen instances in the dictionary.
Thus, the indexation of such data requires indexing weighted automata.We present a general algorithm for the indexation of weighted automata.
We have used a recursively structured finite state machine for handling the changes in constituent order, implemented as a right-linear grammar backbone.
This paper presents an effective method of Korean compound noun segmen-tation based on lexical data extracted from corpus.
We perform a linguistic analysis of documents during indexing for information retrieval.
We describe the design and implementation of a question answering (QA) system called QARAB.
This paper describes an algorithm for open text shallow semantic parsing.
The algorithm relies on a frame dataset (FrameNet) and a semantic network (WordNet), to identify semantic relations between words in open text, as well as shallow semantic features associated with concepts in the text.
In this study wesuggest a practical dynamic semantic network available for NLP, which has the structure from associative concept dictionaries and the dynamics from a pulsed neural network.
We built the semantic network by means of constructing the platform called 'Brain Memory Model" based on a pulsed neural network first, then encoding data of associative concept dictionaries into it.
This paper describes a rule-based machine learning approach to morphological processing in the system called XMAS.
A Korean version of XMAS is effectively working in the English-Korean machine translation system KSHALT.
Parallel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages.
It promises to bridge the gap be-tween practical dialogue management and (pattern-based) dialogue model through integrating interaction patterns with the underling tasks and modeling interaction patterns via utterance groups using a high level construct different from dialogue act.
We present TISC, a language-independent and context-sensitive spelling checking and correction system designed to facilitate the automatic removal of non-word spelling errors in large corpora.
Its lexicon is derived from a very large corpus of raw text, without supervision, and contains word unigrams and word bigrams.
We introduce our system, FADA, which relies on question parsing, web page classification/clustering, and content extraction to find reliable distinct answers with high recall.
This paper explores the possibilities and limits of a discourse grammar applied to spontaneous speech.
Most discourse grammars (e.g.
Subjects describe the pixel-per-pixel development of sketch- maps on a computer screen.
Recently, we initiated a project to develop a phonetically-based spoken language understanding system called SUMMIT.
We describe a purely confidence-based geographic term disambiguation system that crucially relies on the notion of 損ositive?and 搉egative?context and methods for combining confidence-based disambiguation with measures of relevance to a user抯 query.
This paper presents a simple, general method for using the Mutual Information (MI) statistic trained on unannotated trivia questions to estimate question class/semantic tag correlation.
This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger.
This paper describes machine learning based parsing and question classification for question answering.
This paper describes and compares a number of statistical and machine learning techniques for ordering se-quences of adjectives in the context of a natural language generation system.
The rule-based transfer and generation module takes the parsing tree as the input and operates on the information of POS tag, semantics or even the lexicon.
A named entity tagger and a dependency parser are used to analyze the question accurately.
This paper establishes a framework un-der which various aspects of prosodic morphology, such as templatic morphol-ogy and infixation, can be handled under two-level theory using an implemented multi-tape two-level model.
The paper provides a new computational analysis of root-and-pattern morphology based on prosody.
In the paper we describe a project to develop a system which has word-sense acquisition from information contained in computerized dictionaries and knowledge organization as its main objectives.
The first is support vectors which are extracted from the training samples by a machine learning technique, Support Vector Machines(SVM).
Graph grammars are a multidimensional generalization of linear string grammars.
In graph grammars string rewrite rules are generalized into graph rewrite rules.This paper presents a graph grammar formalism and parsing scheme for parsing languages with inherent configurationalflavor.
In this chapter a graph grammar formalism based on the notions of relational graph grammars (Rajlich 1975) and attributed programmed graph grammars (Bunke 1982) is developed for parsing languages with configurational structure.Definition 1.1 (relational graph, r-graph)Let ARCS, NODES, and PROPS be finite setsof symbols.
Morphological	r-graphrepresentation of word "ihmisten"	(thehumans).Definition 1.2 (r-production) An r-production RP is a pair: RP = (LS, RS)LS (left side) and RS	(right	side)	arer-graphs.
ethnologue.com).
Some familiarity with Montague's PTO and the basic DCG mechanism is assumed.Ke words Compositional Semantics, Definite Clause Grammar, Friedman Warren Algorithm, Intensional Logic, Montague Grammmar, Natural Language Processing.
PROLOG.
which are obtained mainly from word alignment.
We use blocks to infer better word alignment and improved word alignment which, in turn, leads to better inference of blocks.
We propose two new probabilistic models based on the inner- outer segmentations and use EM algorithms for estimating the models?parameters.
Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees.
Figure 1 shows five examples of implicit parse trees.
One is a transfer model with monolingual head automata for analysis and generation; the ocher is a direct transduction model based on bilingual head transducers.
This paper presents a project report on NP, a working Natural language Plan inference system that uses feature structures and is based on assumptions.
The plan inference component is implemented using a feature-structure-based inference engine and models of plan recognition, prediction, and inference.
The inference engine is implemented using a rewriting system for pattern-matching, and an Assumption-based Truth Maintenance System (ATMS) for conjunctions.
The NP system is used to infer dialog- and domain-level plans, among other types.Original contributions include: a plan inference system that works directly from feature structures; a plan inference System that uses an ATMS and plan schema actions with preconditions and effects to infer hierarchical and chained plans; and, an inference engine that works with multiple feature- structure assertions and rules.Project Goal.
We present a method that integrates term variation in a hybrid ATR approach, in which term candidates are recognised by a set of linguistic filters and termhood assignment is based on joint frequency of occurrence of all term variants.
The quantitative language and translation models are based on relations between lexical heads of phrases.
This paper introduces a semi-supervised learning framework for creating training material, namely active annotation.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
We address the role of two phenomena with respect to the recovery of these relations: (1) the referential properties of tense, and (2) the role of temporal constraints imposed by coherence relations.
We propose a fast and reliable Question-answering (QA) system in Korean, which uses a predictive answer indexer based on 2-pass scoring method.
JAVOX provides a mechanism for the development of spoken-language systems from existing desktop applications.
In Combinatory Categorial Granunar (CCG) [Ste90, Ste911, semantic function-argument structures are corn- positionally produced through the course of a derivation.
In this paper we focus on how to improve pronoun resolution using the statistics- based semantic compatibility information.
In this paper, we present a system which can extract syntactic feature structures from a Korean Treebank (Sejong Tree- bank) to develop a Feature-based Lexicalized Tree Adjoining Grammars.
III we present results of our studies on voice modifications and transformations using the basic system.
IV results from our studies to determine the factors responsible for unnatural quality of synthetic speech from our system.
We propose a signal dependent analysis-synthesis scheme in Sec.
The lexical acquisition system presented in this paper incrementally updates linguistic properties of unknown words inferred from their surrounding context by parsing sentences with an HPSG grammar for German.
This paper presents on-going research on the building of an electronic dictionary of frozen sentences of European Portuguese.
We incorporate multiple anaphora resolution factors into a statistical framework — specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.
The second experiment investigates a method for unsupervised learning of gender/number/animaticity information.
Next, zero pronouns within a Japanese sentence are identified by using the syntactic and semantic stricture of the Japanese sentence and their antecedents within the English sentence are identified by using the characteristics of anaphoric and deictic expressions in English.
In this paper, I revise generalized phrase structure grammar (GPSG) linguistic theory so that it is more tractable and linguistically constrained.
Conventional algorithms are based on word-by-word models which require bilingual data with hundreds of thousand sentences for training.
The algorithm also poses the advantage of producing a tagged corpus for word sense disambiguation.
The model takes into account the effects of linguistic features, such as tense/aspect, temporal connectives, and discourse structures, and makes use of the fact that events are represented in different temporal structures.
A machine learning approach, Weighted Bayesian Classifier, is developed to map their combined effects to the corresponding relations.
An empirical study is conducted to investigate different combination methods, including lexical- based, grammatical-based, and role-based methods.
We investigate a series of graph-theoretic constraints on non-projective dependency parsing and their effect on expressivity, i.e.
In particular, we define a new measure for the degree of non-projectivity in an acyclic dependency graph obeying the single-head constraint.
Dialect groupings can be discovered objectively and automatically by cluster analysis of phonetic transcriptions such as those found in a. linguistic atlas.
This article presents a compression-based adaptive algorithm for Chinese Pinyin input.
Compression by Partial Match (PPM) is an adaptive statistical modelling technique that is widely used in the field of text compression.
In this paper, we describe a novel approach to spoken language analysis for translation, which uses a combination of grammar-based phrase-level parsing and automatic classification.
In this paper, we present a hybrid Chi-nese language model based on a com-bination of ontology with statistical method.
To evaluate the performance of this language model, we completed two groups of experiments on texts re-ordering for Chinese information re-trieval and texts similarity computing.
In this paper, we propose a method to extract descriptions of techni-cal terms from Web pages in order to utilize the World Wide Web as an encyclopedia.
We use linguistic patterns and HTML text structures to extract text fragments contain-ing term descriptions.
This paper presents our implemented computational model for interpreting and generating indirect answers to Yes-No questions.
This paper describes a method for detecting the boundaries of quotations and inserted clauses and that for improving the dependency accuracy by applying the detected boundaries to dependency structure analysis.
The quotations and inserted clauses are determined by using an SVM-based text chunking method that considers information on morphemes, pauses, fillers, etc.
We present translation results on the shared task 擡xploiting Parallel Texts for Statistical Machine Translation?generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories.
We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence.
We present BAYESUM (for 揃ayesian summarization?, a model for sentence extraction in query-focused summarization.
In this paper, we evaluate a two-pass parsing strategy proposed for the so-called `lexicalized' grammar.
Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization.
A number of machine translation systems based on the learning algorithms are presented.
To overcome this problem, we propose a method of machine translation using a Recursive Chain-linktype Learning.
From the results of evaluation experiments, we confirmed the effectiveness of Recursive Chain-link-type Learning.
This paper presents a computational model of incremental utterance produc-tion in task-oriented dialogues.
We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors.
Word co-occurrences form a graph, regarding words as nodes and co-occurrence relations as branches.
Thus, a co-occurrence graph can be constructed by co-occurrence relations in a corpus.
This paper discusses a clustering method of the co-occurrence graph, the decomposition of the graph, from a graph-theoretical viewpoint.
This paper describes our method based on Support Vector Machines for automatically assigning semantic roles to constituents of English sentences.
Automatic thesaurus construction is developed to solve the problem.
Conventional way to automatically construct thesaurus is by finding similar words based on context vector models and then organizing similar words into thesaurus structure.
Latent Semantic Index (LSI) was commonly used to overcome the problems.
We introduce an algorithm for scope resolution in underspecified semantic representations.
Scope preferences are suggested on the basis of semantic argument structure.
We develop a factored-model statistical parser for the Penn Chinese M-eebank, showing the implications of gross statistical differences between WSJ and Chinese M-eebanks for the most general methods of parser adaptation.
We combined a Japanese-Japanese dictionary, an English-Japanese dictionary, an acronym dictionary, an information science dictionary, and our office telephone directory.This paper is constructed as follows.
From an annotated corpus 126 def-inite clause grammar rules were constructed.
Tagging compound verb groups in an annotated corpus exploiting the verb rules is described.Keywords: compound verb groups, chunking, morphosyntactic tagging, inductive logic pro-gramming
The DCG (Definite Clause Grammar) formalism (Pereira & Warren 80) is adopted.
Section 2 gives a grouping of sentences containing coordinate conjunctions.
We present a prototype system called PROFILE which uses a client-server architecture to ex-tract noun-phrase descriptions of enti-ties such as people, places, and organiza-tions.
We present an evaluation of the approach and its applications to natural language generation and summarization.
This work investigates the use of text analysis in predicting the location of intonational phrase boundaries in natural speech, through analyzing 298 utterances from the DARPA Air Travel Information Service database.
For statistical modeling, we employ Classification and Regression Tree (CART) techniques.
to integrate media indexing with computer visualization to achieve effective content-based access to video information.
Text metadata.
This approach shows how to make multimedia information accessible to a text-based visualization system.
The IDAS natural-language generation system uses a KL-ONE type classifier to perform content determination, surface realisation, and part of text planning.
The first is a sentence extraction- based approach while the second is a language generation-based approach.
We present a new search algorithm for very large vocabulary continuous speech recognition.
TBLbased error correction is used to further improve chunking performance.
We also find that the hardest point of Chinese chunking is to identify the chunking boundary inside noun-noun sequences1.
In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence.
We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text.
We explore the descriptive power, in terms of syntactic phenomena, of a formalism that extends Tree- Adjoining Grammar (TAG) by adding a fourth level of hierarchical decomposition to the three levels TAG already employs.
Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation.
We extend Combinatory Categorial Grammar (CCG) with a generalized notion of multidimensional sign, inspired by the types of representations found in constraint-based frameworks like HPSG or LFG.
Specifically, the score is the probability of acoustic features of a hypothesized word sequence given an associated syntactic parse, based on acoustic and "language" (prosody/syntax) models that represent probabilities in terms of abstract prosodic labels.
This allows the extension of Translation Memory systems towards Example-based Machine Translation.
We describe the use of XML tokenisa-tion, tagging and mark-up tools to pre-pare a corpus for parsing.
This paper reports progress in development of evaluation methodologies for natural language systems.
This paper presents an algorithm for the compilation of regular formalisms with rule features into finite-state automata.
Tree Unification Grammar is a declarative unification-basid linguistic framework.
This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL).
This paper is concerned with learning categorial grammars in Gold抯 model.
In tins paper, we describe a search procedure for sta-tistical machine translation (MT) based on dynamic programming (DP).
We present CarmelTC, a novel hybrid text classification approach for automatic essay grading.
We explore a novel computational approach to identifying 揷onstructions?or 搈ulti-word expressions?
(MWEs) in an annotated corpus.
The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing.
We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank.
Our system disambiguates senses of a target word in a context by selecting a substituent among WordNet relatives of the target word, such as synonyms, hypernyms, meronyms and so on.
This proposal is considered with respect to two types' of discourse phenomena; anaphoric reference to event entities, and temporal binding.
Our method is based on a theoretically clear statistical model that integrates linguistic, acoustic and situational information.
We report tagging experiments on Japanese and English dialogue corpora manually labeled with SA tags.
We also report on some translation experiments on positive response expressions using SA tags.
This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguat-ing dependencies between subordinate clauses.
Word forming units are thus relevant cues for the identification of terms in domain- specific texts.
This article describes a method for the automatic extraction of terms relying on the detection of classical prefixes and word-initial combining forms.
Word-forming units are identified using a regular expression.
This paper will present an enhanced probabilistic model for Chinese word segmentation and part-of-speech (POS) tagging.
We analyze humorous spoken conversations from a classic comedy television show, FRIENDS, by examining acoustic- prosodic and linguistic features and their utility in automatic humor recognition.
We propose a kana-kanji conversion system with input support based on prediction.
Experiments are presented which measure the perplexity reduction derived from incorporating into the predictive model utilised in a standard tag-n-gram part-of-speech tagger, contextual information from previous sentences of a document.
The baseline model utilized is a maximum entropy-based tag-ngram tagging model, embodying a standard tag-n-gram approach to tagging: i.e.
constraints for tag trigrams, bigrams, and and the word-tag occurrence frequency of the specific word being tagged, form the basis of prediction.
But it is another thing to demonstrate that extrasentential context supports an improvement in perplexity vis-a-vis a part-of-speech tagging model which employs state-of-the-art techniques: such as, for instance, the tagging model of a maximum entropy tag-n-grambased tagger.The present paper undertakes just such a demonstration.
In this paper, two approaches are presented which generalize the verification of coindexing constraints to deficient descriptions.
Variants of tree-distance are considered, including whole-vs-sub tree, node weighting, wild cards and lexical emphasis.
We derive string-distance as a special case of tree-distance and show that a particular parameterisation of tree-distance outperforms the string-distance measure.
We propose a new lexicalized grammar formalism called Lexicalized Tree Automata-based Grammar, which lexicalizes tree acceptors instead of trees themselves.
We discuss the properties of the grammar and present a chart parsing algorithm.
We have implemented a translation module for conversational texts using this formalism, and applied it to an experimental automaticinterpretation system (speech	translationsystem).
This paper presents a method to resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks.
Most previous word sense disambiguation approaches based on neural networks have limitations due to their huge feature set size.
In addition, a pattern construction method is described through which paraphrasing patterns can be efficiently learned from a paraphrase corpus and human experience.
Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated.
Three types of syntactic variations were studied : permutation, expansion and substitution.
The graph obtained reveals the global organisation of research topics in the corpus.
The syntactic preferences such coded can be easily used to compute with semantic preferences.
Assuming that the goal of a person name query is to find references to a particular person, we argue that one can derive better relevance scores using probabilities derived from a language model of personal names than one can using corpus based occurrence frequencies such as inverse document frequency (idf).
We describe our experience with automatic alignment of sentences in parallel English-Chinese texts.
This paper discusses an information extraction (IE) system, Textract, in natural language (NL) question answering (QA) and examines the role of IE in QA application.
We have explored statistical models that use different representational units for parsing.
The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem.
We describe a method of text summarization that produces indicative-informative abstracts for technical papers.
We have carried out an evaluation to assess indicative-ness and text acceptability relying on human judgment.
Sentence ranking is a crucial part of generating text summaries.
We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches.
Our results suggest poor performance for the simple paragraph-based approach, whereas word- based approaches perform remarkably well.
We present a novel automatic approach to constructing a bilingual semantic network梩he BiFrameNet, to enhance statistical and transfer-based machine translation systems.
We automatically induce Chinese example sentences and their semantic roles, based on semantic structure alignment from the first stage of our work, as well as shallow syntactic structure.
We describe a method for retrieving counts for adjective-noun, noun-noun, and verb- object bigrams from the web by querying a search engine.
We deal with the automated acquisition of a Spanish medical subword lexicon from an already existing Portuguese seed lexicon.
Using two non-parallel monolingual corpora we determined Spanish lexeme candidates from Portuguese seed lexicon entries by heuristic cognate mapping.
We validated the emergent lexical translation hypotheses by determining the similarity of fixed-window context vectors on the basis of Portuguese and Spanish text corpora.
We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution).
In information extraction systems, pattern matchers are widely used to identify information of interest in a sentence.
In this paper, pattern matching in the TEXTRA CT information extraction system is described.
It comprises a concept search which identifies key words representing a concept, and a template pattern search which identifies patterns of words and phrases.
This paper proposes a new unsupervised learning method for obtaining English part-ofspeech(POS) disambiguation rules which would improve the accuracy of a PUS tagger.
This method has been implemented in the experimental system APRAS (Automatic POS Rule Acquisition System), which extracts PUS disambiguation rules from plain text corpora by utilizing different types of coded linguistic knowledge, i.e., PUS tagging rules and syntactic parsing rules, which are already stored in a fully implemented MT system.In our experiment, the obtained rules were applied to 1.7% of the sentences in a non-training corpus.
This paper presents a report processing method with object-centered semantics.
Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering.
A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking.
To cope with the unit problem, we propose a character-based chunking method.
Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs.
This paper presents a methodology for discovering domain-specific concepts and relationships in an at-tempt to extend WordNet.
This paper describes a system (RAREAS) which synthesizes marine weather forecasts directly from formatted weather data.
Synthesis of Arctic Marine Weather ForecastsThe RAREAS system was developed during a five-month effort to explore the feasibility of synthesizing marine weather bulletins from formatted weather forecast data.
We describe Talk'n'Travel, a spoken dialogue language system for making air travel plans over the telephone.
Literal movement grammars (LMGs) provide a general account of extraposition phenomena through an attribute mechanism allowing top-down displacement of syntactical information.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry.
One improvement incorporates syntactic knowledge.
We predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data.
We present two methods for developing segmentation algorithms from training data: hand tuning and machine learning.
We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics.
In our interface architecture the machine learning module replaces an elaborate semantic analysis component.
We use an existing interface to a production planning and control system as evaluation and compare the results achieved by different instance-based and model-based learning algorithms.
This paper describes DialogueView, a tool for annotating dialogues with utter-ance boundaries, speech repairs, speech act tags, and discourse segments.
The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
The main purpose of this paper is the exploitation and application of an audio and video bimodal corpus of the Chinese language in broadcasting.
This paper presents a method to construct Japanese KATAKANA variant list from large corpus.
At step 2, our system collects candidate pairs of KATAKANA variants from the collected KATAKANA words using a spelling similarity which is based on the edit distance.
This technique adopts an efficient LR parsing method and uses a reverse LR table constructed besides a standard LR table.
Triphone analysis is a new correction strategy which combines phonemic transcription with trigram analysis.
In this paper a new version of Montague Grammar (MG) is developed, which is suitable for application in question-answering systems.
It generates NL sentences and their logicalforms 'in parallel'.
This paper describes our attempt at NomBank-based automatic Semantic Role Labeling (SRL).
The framework uses a linear, pipeline based, bottom-up parsing algorithm, with a look ahead local search that serves to make the local predictions more robust.
In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results.
The statistical translation uses two sources of information: a translation model and a language model.
All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.
We present an upcoming web-based centre for information and documentation on language technology (LT) in Sweden and/or in Swedish.
Some ways to collect and represent data using data harvesting and XML are suggested.
In this work we present a method for Named Entity Recognition (NER).
One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities梞entions?and labeling them with three types of information: entity type, entity subtype and mention type.
In this paper, a generic system that generates parsers from parsing schemata is applied to the particular case of the XTAG English grammar.
In order to be able to generate XTAG parsers, some transformations are made to the grammar, and TAG parsing schemata are extended with feature structure unification support and a simple tree filtering mechanism.
We propose a new method to improve the accuracy of Text Categorization using two- dimensional clustering.
Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing.
We report on an investigation of the pragmatic category of topic in Danish dialog and its correlation to surface features of NPs.
We discovered that NPs in epistemic matrix clauses (e.g.
We apply machine learning techniques to classify automatically a set of verbs into lexical semantic classes, based on distributional approximations of diatheses, extracted from a very large annotated corpus.
IntroductionOne useful way to look at computational morphology and phonology is in terms of transductions, that is, n-ary word relations definable by the element-wise concatenation of n-tuple labels along paths in a finite directed labeled graph.
coreferential) relations.
We present the design and development of a Hidden Markov Model for the division of news broadcasts into story segments.
In this paper, we propose keyword extraction method for dictation of radio news which consists of several domains.
This paper presents the use of probabilistic class-based lexica for disambiguation in target-word selection.
That is, only information provided by the target-verb, enriched by the condensed information of a probabilistic class-based lexi-con, is used.
'This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted gram-mar and a statistical technique.
This utilization of alter-native trees enables us to construct a new statis-tical model called nipleilQuadruplet Model.
In this work, we present a new semantic language modeling approach to model news stories in the Topic Detection and Tracking (TDT) task.
We also cast the link detection sub- task of TDT as a two-class classification problem in which the features of each sample consist of the generative log-likelihood ratios from each semantic class.
This paper reports on the development of two spoken language collaborative interface agents built with the Collagen system.
This paper describes an exploration of the implicit synonymy relationship expressed by synonym lists in an on-line thesaurus.
This paper reports on the development of a head-lexicalized PCFG for the disambiguation of German morphological analyses.
The grammar is trained on unlabeled data using the Inside-Outside algorithm.
In this paper we investigate the use of linguistic knowledge in passage retrieval as part of an open-domain question answering system.
We use annotation produced by a deep syntactic dependency parser for Dutch, Alpino, to extract various kinds of linguistic features and syntactic units to be included in a multi-layer index.
We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers.
A simple method is described for the detection of semantically self-contained word phrase segments in title-like texts.
The method is based on a predetermined list of acceptable types of nominative syntactic patterns which can be recognized using a small domain-indepen-dent dictionary.
The records are used for the compilation of Key Word Phrase subject indexes (KWPSI).
We proposed a novel summarization system architecture which employs Gene Expression Programming technique as its learning mechanism.
A major architectural decision in designing a disambiguation model for segmentation and Part-of-Speech (POS) tagging in Semitic languages concerns the choice of the input-output terminal symbols over which the probability distributions are defined.
In this paper we develop a segmenter and a tagger for Hebrew based on Hidden Markov Models (HMMs).
We describe a probabilistic approach to content selection for meeting summarization.
We use skip- chain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as QUESTION-ANSWER that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task.
We also discuss different approaches for ranking all utterances in a sequence using CRFs.
verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction.
We have also devised an automatic acquisition process for Web-derived answer patterns (AP) which utilizes question-answer pairs from TREC QA, the Google search engine and the Web.
This paper describes recent work on the Unisys ATIS Spoken Language System, and reports benchmark results on natural language, spoken language, and speech recognition.
After discussing various approaches to pho-netic alignment, I present a new algorithm that com-bines a number of techniques developed for se-quence comparison with a scoring scheme for com-puting phonetic similarity on the basis of multival-ued features.
In particular, we motivate a semantic-rich discourse representation and discuss the impact of refined discourse structure on question answering.
In this paper we explore the idea of using natural language generation systems for corpus annotation.
We focus here on exploring the use of the KPML (Komet-Penman MultiLingual) generation system for corpus annotation.
We describe the kinds of linguistic information covered in KPML and show the steps involved in creating a standard XML corpus representation from KPML抯 generation output.
Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity.
We present results on the relation discovery task, which addresses some of the shortcomings of supervised relation extraction by applying minimally supervised methods.
Previous work on relation discovery used a semantic space based on a term-bydocument matrix.
We find that representations based on term co-occurrence perform significantly better.
This paper describes a set of comparative exper-iments, including cross朿orpus evaluation, be-tween five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW, Decision Lists, and Boosting.
This paper proposes a two-phase example-based machine translation methodology which develops translation templates from examples and then translates using template matching.
This method improves translation quality and facilitates customization of machine translation systems.
This paper focuses on the automatic learning of translation templates.
Two-Phase Example-based Machine TranslationFigure 1 outlines our two-phase example-based machine
A system under construction is outlined which incorporates morphological checks (using new two-level error rules) over a directed letter graph, tag positional trigrams and partial parking.
Hence 'co-operative' error processing.
This paper presents methods for a qual-itative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples ex-tracted from German corpora.
In this document, we present a language which associates type construction principles to constraint logic programming.
Finally, we give the procedural semantics of our language, combining type construction with SLDresolution.
In the context of the Papillon project, which aims at creating a multilingual lexical database (MLDB), we have developed Jeminie, an adaptable system that helps automatically building interlingual lexical databases from existing lexical resources.
In this paper, we will present a rather ,simplified description of an algorithm for transformational analysis (decomposition) of English sentences.
This paper describes GENIE, an object-oriented architecture that generates text with the intent of extending user expertise in interactive environments.
This paper presents a method for extracting subcorpora documenting different subcategorization frames for verbs, nouns, and adjectives in the 100 mio.
word British National Corpus.
Usual plan-based approaches to speech act interpretation require that performing a speech act implies its success.
For the task of recognizing dialogue acts, we are applying the Transformation-Based Learning (TBL) machine learning algorithm.
We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense, aspect, temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure.
The method combines a constraint-based approach with an approach based on preferences: we exploit the HPSG type hierarchy and unification to arrive at a temporal structure using constraints placed on that structure by tense, aspect, rhetorical structure and temporal expressions, and we use the temporal centering preferences described by (Kameyama et al., 1993; Poesio, 1994) to rate the possibilities for temporal structure and choose the best among them.The starting point for this work was Scha and Polanyi's discourse grammar (Scha & Polanyi 1988; Priist et a/ 1994).
This paper will focus on our temporal processing algorithm, and in particular on our analysis of narrative progression, rhetorical structure, perfects and temporal expressions.
There has been much interest in using phrasal movement to improve statistical machine translation.
Next, we consider the problem of detecting miscues during oral reading.
It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination.
This paper introduces new specificity determining methods for terms based on information theoretic measures.
We present our work on combining large- scale statistical approaches with local linguistic analysis and graph-based machine learning techniques to compute a combined measure of semantic similarity between terms and documents for application in information extraction, question answering, and summarisation.
We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors.
We study unsupervised methods for learning refinements of the nonterminals in a treebank.
The e-rater system Tm I is an operational automated essay scoring system, developed at Educational Testing Service (ETS).
We fo-cus in this paper on the part of the se-mantic analyser which deals with seman-tic composition.
We explain how we use the domain model to handle metonymy dynamically, and more generally, to un-derlie semantic composition, using the knowledge descriptions attached to each concept of our ontology as a kind of concept-level, multiple-role qualia struc-ture.
We rely for this on a heuristic path search algorithm that exploits the graphic aspects of the conceptual graphs formalism.
This suggests a potentially broader role for disfluency modeling in adapting text-based tools for processing conversational speech.
Coupled- Context--Grammars are a generalization of context-free grammars obtained by combining limiter- Initials to parentheses which can only be an simultaneously.
Pos811D.
Second, the Coupled-Context-Free Grammars consider elements rewritten simultaneously as components of a parenthesis.
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.
Dependency StructureThe structuring principle of constituency trees is concatenation and the part-whole .relationship.
by a x-bar notation.
We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
This paper describes a prototype English-Japanese machine translation (MT) system developed at the Science Institute of IBM Japan, Ltd.
We present a model for sentence compression that uses a discriminative large- margin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.
To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames.
In this paper we present an empirical study of the potential and limitation of sentence extraction in text summarization.
Term recognition and clustering are key topics in automatic knowledge acquisition and text mining.
Beside features that represent contextual patterns, we use lexical and functional similarities between terms to define a combined similarity measure.
We introduce a multi-language named-entity recognition system based on HMM.
In this paper, we describe the architecture and accuracy of the named-entity system, and report preliminary experiments on automatic bilingual named-entity dictionary construction using the Japanese and English named-entity recognizer.
In this paper we present ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology.
We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence.
We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system.
We present a machine-learning approach to modeling the distribution of noun phrases (NPs) within clauses with respect to a fine-grained taxonomy of grammatical relations.
Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy.
representation methods, hypermedia maps.
hypermedia writing.
?a									Address of author									Adjective 										Agent	.
1			Back梤eferencing in text		?.
11			Belief structure	.
.
46 Chaining, inference method .
In the paper the use of the notion "obligatory complement" in syntactic analysis is discussed.
We present a system which retrieves answers to queries based on coreference relationships between entities and events in the query and documents.
Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task.
We argue that the detection of entailment and contradiction relations between texts is a minimal metric for the evaluation of text understanding systems.
This paper describes a system to create animated 3D scenes of car accidents from reports written in Swedish.
The text-to-scene conversion process consists of two stages.
handwriting).
We present the final MIAMM system, a multimodal dialogue system that employs speech, haptic interaction and novel techniques of information visualization to allow a natural and fast access to large multimedia databases on small handheld devices.
This paper reports preliminary experiments on part-whole extraction from a corpus of anatomy definitions, using a fully automatic iterative algorithm to learn simple lexico-syntactic patterns from multiword terms.
The experiments show that meronyms can be extracted using these patterns.
In this paper we describe a method of classifying Japanese text documents using domain specific kanji characters.
We extracted these domain specific kanji characters by x2 method.
This paper proposes an LR parsing algorithm modified for grammars with feature-based categories.
The description of lexical predicates within the framework of frame semantics provides a natural method for selecting and structuring appropriate tagsets.
It represents two classes of concepts: objects of discourse and action schemata, the former resulting from nominal syntagms and the latter from the 'processes'.
The current paper introduces an intonational analysis of mono- and di-syllabic words based upon such a framework and compares results in progress with previous work on intonation.
The paper describes the construction of a lexical transducer for Korean that can be used for stemming and generation.
We will show by detailed examples the working of synchronous TAGs and some of its applications, for example in generation and in machine translation.The second development is the design of LR-style parsers for TAGs.
In this paper a method for controlling the dialog in a natural language (NL) system is presented.
It provides a deep modeling of information processing based on time dependent propositional attitudes of the interacting agents.
First, we describe the CU Communicator system that integrates speech recognition, synthesis and natural language understanding technologies using the DARPA Hub Architecture.
Next, we describe our more recent work on the CU Move dialog system for in-vehicle route planning and guidance.
We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics.
We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities.
In this paper we propose the trigger language model based IR system to resolve the problem.
We introduce the relative parameters into the document language model to form the trigger language model based IR system.
This paper presents a pipelined ar-chitecture for the generation of German monologues with contextually appropriate word order and accent placements for the realization of focus/background structures.
Word order is realized by grammatical competition based on linear prece-dence (LP) rules which are based on the discourse-relational features.
This paper presents a formal account of the temporal interpretation of text.
The distinct natural interpretations of texts with similar syntax are explained in terms of defeasible rules characterising causal laws and Gricean-style pragmatic maxims.
In syntax, the trend nowadays is towards lexicalized grammar formalisms.
This paper will present an algorithm for learning a link granunar of German.
a link grammar parse): Figure 1 shows a linkage for an English sentence.
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 11-18.
The inadequacy of some common strategies to answer this question in Machine Translation (MT) systems is shown.
In this paper, a flexible annotation schema called (SSTC) is introduced.
In order to describe the correspondence between different languages, we propose a variant of SSTC called synchronous SSTC (S-SSTC).
in Machine Translation).
The S-SSTC is very well suited for the construction of a Bilingual Knowledge Bank (BKB), where the examples are kept in form of S-SSTCs.KEYWORDS: parallel text, Structured String-Tree Correspondence (SSTC), Synchronous SSTC, Bilingual Knowledge Bank (BKB), Tree Bank Annotation Schema.
We explain dialogue management tech-niques for collaborative activities with hu-mans, involving multiple concurrent tasks.
Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document.
In this model, a sentence connectivity matrix is constructed based on cosine similarity.
Existing speech recognizers work best for "high-quality close-talking speech."
We consider in depth the semantic analysis in learning systems as well as some information retrieval techniques applied for measuring the document similarity in eLearning.
This paper describes and evaluates a detector of presuppositions (DP) for survey questions.
DP performs well using local characteristics of presuppositions.
We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system.
These include context-dependent phonetic modelling, the use of a bigram language model in conjunction with a probabilistic LR parser, and refinements made to the lexicon.
This paper provides a model theoretic semantics to feature terms augmented with set descriptions.
We provide constraints to specify HPSG style set descriptions, fixed cardinality set descriptions, set-membership constraints, restricted universal role quantifications, set union, intersection, subset and disjointness.
It is shown that determining consistency of terms is a NP-complete problem.Subject Areas: feature logic, constraint-based grammars, HPSG
This paper presents some techniques that provide a standard parsing system for the analysis of ill-formed utterances.
The proposed framework involves two processes, partial grammar acquisition and grammar refinement.
We present the results of an experiment on extending the automatic method of Machine Translation evaluation BLUE with statistical weights for lexical items, such as tf.idf scores.
Island parsing is a powerful technique for parsing with Augmented Transition Networks (ATNs) which was developed and successfully applied in the HWIM speech understanding project.
Tins paper explores two directions for the next step beyond the state of the art of statistical parsing: probabilistic partial parsing and committee-based decision making.
We propose a new method for reformatting web documents by extracting semantic structures from web pages.
Our approach is to extract trees that describe hierarchical relations in documents.
We present a novel system for automatically marking up text documents into XML and discuss the benefits of XML markup for intelligent information retrieval.
Statistical part-of-speech(POS) taggers achieve high accuracy and robustness when based on large scale manually tagged corpora.
Currently we use a fine-grained POS tag set with about 500 tags.
Moreover, to cope with the datasparseness problem caused by exceptional phenom-ena, we introduce several other techniques such as word-level statistics, smoothing of word-level and POS-level statistics and a selective tri-grain model.
This paper presents a pilot study on how Population Test Method (PTM) may be used as an effective, empirical tool to define near-synonyms in a quantifiable manner.
This paper presents the results of an empirical investigation of temporal reference resolution in scheduling dialogs.
A pattern in the translation of locative prepositional phrases between English and Spanish is presented.
We briefly describe our cross document co-reference resolution algorithm and discuss applications these resolved references enable.
We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al., 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure.
We describe work in progress on using quantitative methods to classify writing systems according to Sproat抯 (2000) classification grid using unannotated data.
An overview of the DLT (Distributed Language Translation) project is given.
The sys-tem we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences.
Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity.
We propose an approach that uses semantically motivated preposition selection and frequency information to determine if a locative PP is an argument or an adjunct.
This paper proposes an algorithm for causality inference based on a set of lexical knowledge bases that contain information about such items as event role, is-a hierarchy, relevant relation, antonymy, and other features.
These lexical knowledge bases have mainly made use of lexical features and symbols in HowNet.
Our partial parser for Chinese uses a learned classifier to guide a bottom-up parsing process.
We describe improvements in performance obtained by expanding the information available to the classifier, from POS sequences only, to include measures of word	association	derived	fromco-occurrence statistics.
We present here a new monolingual sentence alignment algorithm, combining a sentence-based TF*IDF score, turned into a probability distribution using logistic regression, with a global alignment dynamic programming algorithm.
We show how to do this using , -reduction constraints in the constraint language for A-structures (CLLS).
we diseuss a method for using automated corpus analysis to acquire word sense information l'or' text interpretatiou.
Our approach focuses on tying together word senses, us- lag a combination of world knowledge (ontology) with word knowledge (corpus data).
We will present results in the case of a transducer D representing a dictionary and R representing phonological rules.Keywords: ambiguity, deterministic, dictionary, transducer.
As mentioned in Mitkov (1996), solving the anaphora and extracting the antecedent are key issues in a correct translation.In this paper, we propose an Interlingual mechanism that we have called Interlingual Slot Structure (ISS) based on Slot Structure (SS) presented in FerrAndez et al.
The tool makes use of part-of-speech tagging and word-alignment programs to extract candidate terms and their translations.
V磂ronis (2004) has recently proposed an innovative unsupervised algorithm for word sense disambiguation based on small-world graphs called HyperLex.
Our method uses features for the translation model which use the translation dictionary, the number of words, part-of-speech, constituent words and neighbor words.
A semi-continuous hidden Markov model based on the multiple vector quantization codebooks is used here for large-vocabulary speaker-independent continuous speech recognition In the techniques employed here, the semi-continuous output probability density function for each codebook is represented by a comhinotion of the corresponding discrete output probabilities of the hidden Markov model and the continuous Gaussian density functions of each individual codebook.
continuous output probability by the combination of multiple codewords and multiple codebooks For a 1000-word speaker- Independent continuous speech recognition using a word-pair grammar.
The combination of a rule-based system and a statistical one is not parallel but serial: the rule-based system performing partial disambigua-tion with recall close to 100% is applied first, and a trigram HMM tagger runs on its results.
This paper describes adaptations of unsupervised word sense discrimination techniques to the problem of name discrimination.
We evaluated the proposed method with a task of recognizing named entities (genes) in biology text involving three species.
The structures covered include lists, narratives, subordinating and coordinating rhetorical relations, topic chains and interruptions.
The paper discusses the problem of parsing discourse, and compares different grammatical formalisms which could be used for describing discourse structure.
We demonstrate that in a realistically difficult authorship attribution scenario, deep linguistic analysis features such as context free production frequencies and semantic relationship frequencies achieve significant error reduction over more commonly used 搒hallow?features such as function word frequencies and part of speech trigrams.
We present an efficient, broad-coverage, principle-based parser for English.
Principles are constraints over X-bar structures.
Sentence analysis is divided into three steps.
The lexical analyser first converts the input sentence into a set of lexical items.
Then, a message passing algorithm for GB-parsing is used to construct a shared parse forest.
The revised grammar of this model combines the phrase structure and transformational rules of the underlying grammar into a single efficient component.
The underlying grammar in this system is a recognition grammar based on tne generative approach in linguistic theory.
Among the transformational grammarsi.
We present a constraint-based syntax-semantics interface for the construction of RMRS (Robust Minimal Recursion Semantics) representations from shallow grammars.
incomplete syntactic analysis.
We present RMRS semantics construction principles that can be applied to flat syntactic structures with various degrees of partiality.2 RMRS ?For Partial Semantic RepresentationCopestake (2003) presents a formalism for partial semantic representation that is derived from MRS semantics (Copestake et al., 2003).
(1990) Modelling the perception of concurrent vowels: vowels with different fundamental frequencies.
We present three systems for surface natural lan-guage generation that are trainable from annotated corpora.
We present experiments in which we generate phrases to describe flights in the air travel domain.
Multiset-CCG is a combinatory categorial formalism that can capture the syntax and interpretation of "free" word order in languages such as Turkish.
obtained from supervised training) by iterative ascent.
This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations.
We construct an objective function based on Minimum Description Length.
We present a comparative study of corpus- based methods for the automatic synthesis of email responses to help-desk requests.
We also describe the multiplicative combination of this dependency model with a model of linear constituency.
The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing.
This paper describes the basic philosophy and implementation of MPLUS (M+), a robust medical text analysis tool that uses a semantic model based on Bayesian Networks (BNs).
In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet.
In this work, we present a learning approach that correctly determines the hierarchical structure of information filtering expressions 78.30% of the time.
In addition, we investigate the use of monolingual morpho-syntactic knowledge i.e.
base forms and POS tags.
The important part of semantics of complex.
(Lytinen 86) distinguishes two approaches to NI, processing.
This paper proposes machine learning techniques, which help disambiguate word meaning.
Context information is produced from rule-based translation such as part-of-speech tags, semantic concept, case relations and so on.
In this paper, we test on ParSit, which is an interlingual-based machine translation for English to Thai.
This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination.
The model utilized rich linguistic features that capture predicate- argument structure information of the target verbs.
We further enhanced the model with certain fine-grained semantic categories called lexical sets.
We describe a method for discriminative training of a language model that makes use of syntactic features.
The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.
We propose a new inference system which oper-ates on underspecified semantic representations of scope and anaphora.
This system exploits anaphoric accessibility conditions from dynamic semantics to disambiguate scope ambiguities if possible.
without enumerating readings.
Most information extraction (IE) systems treat separate potential extractions as independent.
This allows for "collective information extraction" that exploits the mutual influence between possible extractions.
In this paper, we propose a method of dynamic programming matching for information retrieval.
It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging.
We will demonstrate the GIST system, which generates social security forms in English, Italian and German.
Named entities form the major components in a document.
This paper employs different types of information from different levels of text to extract named entities, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache and n-gram model.
In this paper, we show how to induce head-driven probabilistic parsers with latent heads from a tree-bank.
This paper describes a novel statistical named- entity (i.e.
"proper name") recognition system built around a maximum entity framework.
These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e.
A key task in an extraction system for query-oriented multi-document summarisation, necessary for computing relevance and redundancy, is modelling text semantics.
In the Embra system, we use a representation derived from the singular value decomposition of a term co-occurrence matrix.
We present evidence that head-driven parsing strategies lead to efficiency gains over standard parsing strategies, for lexicalist, concatenative and unification-based grammars.
A head-driven parser applies a rule only after a phrase matching the head has been derived.
We have used two different head-driven parsers and a number of standard parsers to parse with lexicalist grammars for English and for Dutch.
This paper describes a machine translation architecture that integrates the use of examples for flexible, idiomatic translations with the use of linguistic rules for broad coverage and grammatical accuracy.
Collocational word similarity is considered a source of text cohesion that is hard to measure and quantify.
An implementation, the VecTile system, produces similarity curves over texts using pre-compiled vector representations of the contextual behavior of words.
We propose a two-layered model for computing semantic and conceptual interpretations from dependency structures.
We model the distortion in the spelling of a name due to the speech recognizer as the effect of a noisy channel.
We also test a string edit distance based method.
The effectiveness of these models is evaluated on a name query retrieval task.
We are presenting a new, hybrid alignment architecture for aligning bilingual, linguistically annotated parallel corpora.
It is able to align simultaneously at paragraph, sentence, phrase and word level, using statistical and heuristic cues, along with linguistics-based rules.
We describe a system which automatically generates multimedia briefings from high-level outlines.
Example-based machine translation (EBMT) is based on a bilingual corpus.
In this paper, we de-scribe a method to acquire synonymous expres-sions from a bilingual corpus and utilize them to expand retrieval of similar sentences.
We offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification.
We offer an analy-sis couched in a version of Head-Driven Phrase Structure Grammar combined with a theory of information states (IS) in dialogue.
We discuss which recent word sense disambiguation algorithms are appropriate for sense tagging.
We describe an algorithm for recovering non-local dependencies in syntactic dependency structures.
The pattern- matching approach proposed by Johnson (2002) for a similar task for phrase structure trees is extended with machine learning techniques.
For English- Japanese machine translation, the syntax directed approach is effective where the Heuristic Parsing Model (HPM) and the Syntactic Role System play important roles.
In this paper we show how interaction of lexical and derivational semantics at the lexico-syntactic interface can be precomputed as a process of off-line lexical compilation comprising Cut elimination in partial proof-nets.
We propose more efficient algorithms approximating Tree Kernel: Tree Overlapping and Subpath Set.
In this paper, we discuss a method to screen inconsistencies in ontologies by applying a nat-ural language processing (NLP) technique, es-pecially, those used for word sense disambigua-tion (WSD).
The phrasal approach to language processing emphasizes the role of the lexicon as a knowledge source.
In this paper, we propose a novel paradigm for the Chinese-to-English speech-to-speech (S2S) translation, which is interactive under the guidance of dialogue management.
In this paper1 we introduce eXtensible MetaGrammar, a system that facilitates the development of tree based grammars.
To segment Japanese text, systems typically use knowledge-based methods and large lexicons.
The algorithm utilizes a hidden Markov model, a stochastic process, to determine word boundaries.
The extraction is performed in two major steps: incremental finite-state parsing and extraction of subject/verb and object/verb relations.
We also provide some error analysis; in particular, we evaluate the impact of POS tagging errors on subject/object dependency extraction.
Previous research on temporal anchoring and ordering has focused on the annotation and learning of temporal relations between events.
This paper describes the first steps in acquiring metric temporal constraints for events.
This paper describes techniques for unsupervised word sense disambiguation of English and German medical documents using UMLS.
In RST, the linguistic discourse structure is modeled recursively as a tree of related seg-ments.
In this paper we propose a cluster-specific NE transliteration framework.
We group name origins into a smaller number of clusters, then train transliteration and language models for each cluster under a statistical machine translation framework.
We also propose a phrase- based name transliteration model, which effectively combines context information for transliteration.
This paper briefly describes our rule-based heuristic analyzer for Finnish nominal and verb forms.
This paper describes an extension of the DAR-algorithm (Navarretta, 2004) for resolving intersentential pronominal anaphors referring to individual and abstract entities in texts and dialogues.
It concerns more particularly the lexical item representation using phonograins (i.e.
In this paper, a Markov chain Monte-Carlo method is proposed for learning Stochastic OT Grammars.
In this paper, we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus.
This paper presents a Named Entities (NE) recognition system for the English written lan-guage, which combines the wealth of the WORD-NET taxonomy and the effectiveness of tradi-tional rule-based approaches.
The paper describes two parsing schemes: a shallow approach based on machine learning and a cascaded finite-state parser with a hand-crafted grammar.
What is the relationship between syntax, prosody and phonetics?
In this paper, we address the issue of combining data-driven and grammar-based models for rapid prototyping of robust speech recognition and understanding models for a multimodal conversational system.
This paper addresses two major problems in closed task of Chinese word segmentation (CWS): tagging sentences interspersed with non-Chinese words, and long named entity (NE) identification.
For the latter problem, we apply postprocessing to our CWS output using automatically generated templates.
Three approaches to measuring text similarity are considered: n- gram overlap, Greedy String Tiling, and sentence alignment.
This paper presents a speech understanding component for enabling robust situated human-robot communication.
We discuss how deep interpretation and generation can be integrated with a knowledge representation designed for question answering to build a tutorial dialogue system.
We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging.
The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods.
We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments.
In addition, we proposed a confidence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmentation.
We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
We show that a baseline statistical machine translation system is significantly improved using this approach.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
We evaluate the model on entity extraction and noun-phrase chunking and show that it is more accurate for overlapping and non-contiguous segments, but it still performs well on simpler data sets for which sequential tagging has been the best method.
We introduce here a new technique which employs M-NLG during the phase of knowledge editing.
It is based on a general- purpose ngram model for word segmentation and a case-based learning approach to disambiguation.
We present the architecture and data model for TEXTRACT, a document analysis framework for text analysis components.
This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism.In parsers for unification-based grammar formalisms, complex phrase types are derived by incremental refinement of the phrase types defined in grammar rules and lexical entries.
This paper explores the effect of improved morphological analysis, particularly context sensitive morphology, on monolingual Arabic Information Retrieval (IR).
It also compares the effect of context sensitive morphology to non- context sensitive morphology.
We present a new appr.oach, illustrated by two algorithms, for parsing not only Finite State Grammars but also Context Free Grammars and their extension, by means of finite state mach'ines.
a finite-state transducer.
There has recently been considerable interest in the use of lexically-based statistical techniques to resolve prepositional phrase attachments.
In this paper we describe a method for performing word sense disambiguation (WSD).
The method relies on unsupervised learning and exploits functional relations among words as produced by a shallow parser.
We automatically classify verbs into lexical se-mantic classes, based on distributions of indica-tors of verb alternations, extracted from a very large annotated corpus.
We conclude that corpus-driven extraction of grammatical features is a promising methodology for fine-grained verb classification.
This paper describes the development of a rule-based computational model that describes how a feature-based representation of shared visual information combines with linguistic cues to enable effective reference resolution.
This work explores a language-only model, a visual- only model, and an integrated model of reference resolution and applies them to a corpus of transcribed task-oriented spoken dialogues.
This demo presents LeXFlow, a workflow management system for cross- fertilization of computational lexicons.
LeXFlow is a web-based application that enables the cooperative and distributed management of computational lexicons.
This paper proposes a method for detecting and correcting Japanese homophone errors in compound nouns.
A left-associative operation is used to build a lexicon of extended elementary trees.
In this paper, we define an event as one or more event terms along with the named entities associated, and present a novel approach to derive intra- and inter- event relevance using the information of internal association, semantic relatedness, distributional similarity and named entity clustering.
This paper describes a method for obtaining the semantic representation for a syntax tree in Systemic Grammar (SG).
We define a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights.
The quantity is computed in a parse for-est representation of the set of tree anal-yses for a given sentence, using vector sums and scaling by inside probability and flow.
In this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) tagging framework.
We also describe an exten-sion of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling.
Due to the phenomenon of zero anaphora occurring in Chinese texts frequently, in addition to the centering model, we further employ the constraint rules to identify the antecedents of zero anaphors.
Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge, we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics from sentences.
We describe a number of experiments that demonstrate the usefulness of prosodic information for a processing module which parses spoken utterances with a feature-based grammar employing empty categories.
We introduce adaptivity in QA and IR by creating a hybrid system based on a dialogue interface and a user model.
Keywords: question answering, information retrieval, user modelling, dialogue interfaces.
We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine- produced translation and human-produced reference translations.
A maximum entropy classifier can be used to extract sentences from documents.
This paper discusses general heuristics to control computation on symbolic constraints represented in terms of first-order logic programs.
Efficient computation for sentence parsing and generation naturally emerge from these heuristics, capturing the essence of standard.
parsing procedures and semantic head-driven generation.
This paper describes and evaluates the Metalinguistic Operation Processor (MOP) system for automatic compilation of metalinguistic information from technical and scientific documents.
Dialogue system for 3D virtual environ-ments).
2 Types of AlignmentWe evaluate the English桭rench word alignment data of the shared tasks from a phrase alignment perspective.
We show that phrase-based evaluation is closely related to word-based evaluation.
Example 1 shows the word-to-word alignment data of sample 91 for submission 12 and a plot of the data.
The analysis is based on a statisti-cal method and employs a beam search strategy.
This paper describes the application of discriminative reranking techniques to the problem of machine translation.
We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.
This paper presents a fully automated linguistic approach to measuring distance between phonemes across languages.
Two phonological distances are statistically derived from lexical frequency measurements.
The phonetic distance is combined with the phonological distances to produce a single metric that quantifies cross-language phoneme distance.The performances of target-language phoneme HMMs constructed solely with source language HMMs, first selected by the combined phonetic and phonological metric and then by a data-driven, acoustics distance-based method, are compared in context-independent automatic speech recognition (ASR) experiments.
This paper presents an evaluation of an ensemble朾ased system that participated in the English and Spanish lexical sample tasks of SENSEVAL-2.
The system com-bines decision trees of unigrams, bigrams, and co杘ccurrences into a single classi-fier.
This paper presents a specific part of HUGG, a generation grammar for Hebrew.
This part deals with determiners and quantifiers.
This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.
This paper discusses the possibility of building an ontology-based question answering system in the context of the Semantic Web presenting a proof-of-concept system.
This is done by using k-means with EM.
For meaning representations in NLP, we focus our attention on thematic aspects and conceptual vectors.
Then, we define antonymy functions which allows the construction of an antonymous vector and the enumeration of its potentially antinomic lexical items.
This paper presents a new application of the recently proposed machine learning method Alternating Structure Optimization (ASO), to word sense disambiguation (WSD).
Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems.We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models.
Here, a two-step talker-location algorithm is introduced.
This paper develops a method for recognizing relations and entities in sentences, while taking mutual dependencies among them into account.
Using probability distributions over verb subcategorisation frames, we obtained an intuitively plausible clustering of 57 verbs into 14 classes.
The automatic clustering was evaluated against independently motivated, hand- constructed semantic verb classes.
General structure of PORTUGA: A - analysis, G - generation.
Transfer: L - lexical, S - structural, T - tense, Sty - style.
This work investigates techniques for predicting the location of intonational phrase boundaries in natural speech, through analyzing a utterances from the DARPA Air Travel Information Service database.
For statistical modeling, we employ Classification and Regression Tree (CART) techniques.
We have been investigating an interactiveapproach for Open-domain QA (ODQA)and have constructed a spoken interactiveODQA system, SPIQA.
The combination is then used for an-swer extraction.
This paper describes a pilot version of a commercial application of natural language processing techniques to the problem of categorizing news stories into broad topic categories.
Its categorizations are dependent on fragmentary recognition using pattern-matching techniques.
This paper describes a unified framework for bilingual text matching by combining existing hand-written bilingual dictionaries and statistical techniques.
The process of bilingual text matching consists of two major steps: sentence alignment and structural matching of bilingual sentences.
Statistical techniques are applied to estimate word correspondences not included in bilingual dictionaries.
An ATN-Parser is presented with emphasis on the treatment of those phenomena which in the framework of transformational grammar are subsumed under the concept of WH-movement.
These gaps are modeled using a mixture of exponential distributions.
In this paper we present and evaluate a novel unsupervised approach, SALAAM, which exploits translational correspondences between words in a parallel Arabic English corpus to annotate Arabic text using an English WordNet taxonomy.
Using XML as a reference format, we approach the schema generation problem by application of inductive inference theory.
We evaluate the result of an inference process using the concept of Minimum Message Length.
In this paper, we used syntactic sub- trees that span potential argument structures of the target predicate in tree kernel functions.
We describe a branch of dictionary science, and recommend the term lexicomptry for it, that deals with the mathematical and statistical aspects of dictionaries.
In this paper, we compare two approaches to modeling subtask structure in dialog: a chunk-based model of subdialog sequences, and a parse-based, or hierarchical, model.
We evaluate these models using customer agent dialogs from a catalog service domain.
We present preliminary results of experiments with two types of recurrent neural networks for a natural language learning task.
The neural networks, Elman networks and Recurrent Cascade Correlation (RCC), were trained on the text of a first-year primary school reader.
In this paper, we present preliminary results of experiments with recurrent neural networks for a natural language learning task.
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 3-10.
The three-tiered discourse representation defined in (Luperfoy, 1991) is applied to multimodal human- computer interface (HCI) dialogues.
The first tier holds a linguistic analysis of surface forms.
We call this the G-view of constraints.
This paper describes a prototype disambiguation module, KANKEI, which was tested on two corpora of the TRAINS project.
Unlike previous statistical disambiguation systems, this technique thus combines evidence from bigrams, trigrams, and the 4-gram around an ambiguous attachment.
We apply the C4.5 decision tree learner in interpreting Japanese relative clause constructions, based around shallow syntactic and semantic processing.
In this paper, we describe our approach to intermediate semantic representations in the interpretation of temporal expressions.
In this paper, we describe TANGO as a collocational concordancer for looking up collocations.
In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction.
We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines.
This paper explores the large-scale acquisition of sense-tagged examples for Word Sense Disambiguation (WSD).
This paper introduces an efficient analyser for the Chinese language, which efficiently and effectively integrates word segmentation, part-of-speech tagging, partial parsing and full parsing.
The Chinese efficient analyser is based on a Hidden Markov Model (HMM) and an HMM-based tagger.
In this paper, we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation.
The Head-driven Phrase Structure Grammar project (HPSG) is an English language database query system under development at Hewlett-Packard Laboratories.
1982I), in four significant respects: syntax, lexical representation, parsing, and semantics.
We propose to score phrase translation pairs for statistical machine translation using term weight based models.
These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs.
The translation probability is then modeled by similarity functions defined in a vector space.
Using these models in a statistical machine translation task shows significant improvements.
Experiments on movie review data from Epinions.com demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data.
We describe a method for identifying systematic patterns in translation data using part-ofspeech tag sequences.
This paper provides an evaluation of the OntoLearn ontology learning system.
We propose a draft scheme of the model formalizing the structure of communicative context in dialogue interaction.
To achieve a more pragmatic DRT model, this paper extends standard DRT framework to incorporate more pragmatic elements such as representing agents?cognitive states and the complex process through which agents recognize utterances employing the linguistic content in forming mental representations of other agent抯 cognitive states.
We describe how movement is handled in a class of computational devices called active production networks (APNs).
This paper introduces a Chinese word tokenization system through HMM-based chunking.
Word alignment using recency-vector based approach has recently become popular.
Through two experiments, three methods for constructing word vectors, i.e., LSA-based, cooccurrence-based and dictionary-based methods, were compared in terms of the ability to represent two kinds of similarity, i.e., taxonomic similarity and associative similarity.
The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity, while the LSAbased and the cooccurrence-based word vectors better reflect associative similarity.
The patterns we derived are rules which map surface syntactic structures to semantic case frames, which serve as the canonical representation of questions.
This paper presents a method, based on Turney (2003), for inferring the SO of a word from its statistical association with strongly-polarized words and morphemes in Chinese.
In this paper, a pragmatics-first approach to specifying the meaning of utterances in terms of plans is presented.
The Dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause.
Here, we investigate Chinese word segmentation for statistical machine translation.
In the paper, we extend CBS for decision tree classifiers.
This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system.
The word sense disambiguation is applied to verbs and nouns.
We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis.
We integrated this global word sense disambiguation into our zero pronoun resolution system, and conducted experiments of zero pronoun resolution on two different domain corpora.
In this paper we present a method for transforming the WordNet glosses into logic forms and further into axioms.
The paper demonstrates the utility of the WordNet axioms in a question answering system to rank and extract answers.
This paper reports the first part of a project that aims to develop a knowledge extrac-tion and knowledge discovery system that extracts causal knowledge from textual da-tabases.
We classified compound nouns into three classes: countable, uncountable, plural only.
We present a semi-automatic method for extracting fine-grained semantic relations between verbs.
We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.
Verbal idioms can be divided into two main groups: non-compositional id-ioms as kick the bucket and composi-tional/decomposable idioms as spill the beans.
Taking these facts into account we propose an adequate way to represent the idiomatic meaning by Kamp's Discourse Represen-tation Theory (DRT).
Furthermore, we show how to parse idiomatic sentences and how to process the proposed seman-tic representation.
Lexicalized Tree Adjoining Grammars have proved useful for NLP.
We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms.
This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.
We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.
The (ii-system is a tree-to-tree transducer developed for teaching purposes in madhine.translation.
We refer to intentions derived from structural differences as objective intentions and intentions derived from contents differences as subjective intentions.
In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora.
The applied techniques include building deep semantic representations, application of categories of patterns underlying a formal reconstruction, and using pragmatically- motivated and empirically justified preferences.
This paper describes new default unification, lenient default unification.
We extract grammar rules from the results of robust parsing using lenient default unification.
The nearest related technologies are information retrieval (search engines), document categorization, information extraction and named entity detection.
This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG)project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use.
We focused on syntax, esp.
noun phrase (NP) syntax.
Based on the concepts of bidirectional conversion and automatic evaluation, we propose two user- adaptation mechanisms, character-preference learning and pseudo-word /earning, for resolving Chinese homophone ambiguities in syllable-to-character conversion.
This paper presents a trainable approach to making these attachments through transformation sequences and error-driven learning.
Our goal is to develop fast speech recognition algorithms, and supporting hardware capable of recognizing continuous speech from a bigram- or trigram-based 20,000-word vocabulary or a 1,000- to 5,000- word SLS.
We describe an MDL based grammar of a language that contains morphology and lexical categories.
We present an algorithm for collapsing morphological classes (signatures) by using syntactic context.
This paper presents an ontology-based semanticframework to question answering.
Answer retrieval is done using subsumption and unification, and queries are expanded incrementally using ontological rules.
This paperl decribes a computational treatment of the semantics of relational nouns.
Transfer-based Machine Translation systems re-quire a procedure for choosing the set of transfer rules for generating a target language transla-tion from a given source language sentence.
We propose a solution to this problem based on current best-first chart parsing algorithms.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
A system for the automatic production of controlled index terms is presented using linguistically-motivated techniques.
This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unification- based shallow-level parser using transformational rules over syntactic patterns.
This paper presents a method for searching the web for sentences expressing opinions.
We collected declaratively subjective clues in opinion- expressing sentences from Japanese web pages retrieved with opinion search queries.
The presentation focuses on the major methodological principles underlying the design of TOPIC: a frame representation model that incorporates various integrity constraints, text parsing with focus on text cohesion and text coherence properties of expository texts, a lexically distributed semantic text grammar in the format of word experts, a model of partial text parsing, and text graphs as appropriate representation structures for text condensates.
This paper presents a technique to deal with multiword nominal terminology in a computational Lexical Functional Grammar.
This method treats multiword terms as single tokens by modifying the preprocessing stage of the grammar (tokenization and morphological analysis), which consists of a cascade of two-level finite-state automata (transducers).
Explanation-based Learning (EBL) is a technique to speed-up parsing.
Abductive EBL allows extending the deductive closure of the parser.
We describe a statistical technique for assigning senses to words.
This paper compares two approaches to computational semantics, namely semantic unification in Lexicalized Tree Adjoining Grammars (LTAG) and Lexical Resource Semantics (LRS) in HPSG.
This paper describes the SyntaLex entries in the English Lexical Sample Task of SENSEVAL-3.
There are four entries in all, where each of the different entries corresponds to use of word bigrams or Part of Speech tags as features.
The systems rely on bagged decision trees, and focus on using pairs of lexical and syntactic features individually and in combination.
The first is for acquiring conjunctive relationships from corpora, as measures of word similarity that can be used in addition to thesauruses.
We describe a machine learning system for the recognition of names in biomedical texts.
In this paper we present some novel applications of Explanation-Based Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars.
The paper describes a localized connectionist model of language generation, focusing on the representation and use of sequencing information.
We assume a two-stage language analysis system.
This paper presents PROVERB a text planner for argumentative texts.
We describe a statistical technique for assigning senses to words.
We adapt van Noord's Prolog generator for use with an HPSG grammar in ProFIT.
We must adopt recent theoretical proposals for lexicalized scoping and context.
We briefly describe a word alignment system that combines two different methods in bitext correspondences identification.
This paper discusses the treatment of fixed word expressions developed for our ITS-2 French- English translation system.
Inter-sentence similarity is estimated by latent semantic analysis (LSA).
Boundary locations are discovered by divisive clustering.
We have implemented a restricted domain parser calledPlume.
These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects.
We describe the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking.
In order to incorporate this kind of long distance context dependency in the ngram model of our Mandarin speech recognition system, this paper proposes a novel MI-Ngram modeling approach.
This new MI-Ngram model consists of two components: a normal ngram model and a novel MI model.
That is, the MI-Ngram model incorporates the word occurrences beyond the scope of the normal ngram model.
We present an approach to bounded constraint- relaxation for entropy maximization that corresponds to using a double-exponential prior or E1 regularizer in likelihood maximization for log-linear models.
This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA).
Our approach thus represents a combination of grammar-based and empirical natural language processing.
This paper examines several key issues in computing and using anaphoricity information to improve learning-based coreference systems.
In particular, we present a new corpus-based approach to anaphoricity determination.
We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear models.
These results have implications for automating discourse annotation based on syntactic annotation.
We extend default unification to non-parallel structures, which is important for speech and multimodal dialog systems.
These features include lexical, lexico grammatical and semantic phenomena.
Perhaps the most popularly known are psychodynamic, Cognitive Behavioural Therapy(CBT), and interpersonal therapies such as family therapy, narrative therapy, emotion-focussed therapy, solution-focussed therapy.
This paper investigates the usefulness of sentence-internal prosodic cues in syntactic parsing of transcribed speech.
We compared the accuracy of a statistical parser on the LDC Switchboard treebank corpus of transcribed sentence-segmented speech using various combinations of punctuation and sentence-internal prosodic information (duration, pausing, and f0 cues).
This paper proposes an unsupervised learning algorithm for Optimality Theoretic grammars, which learns a complete constraint ranking and a lexicon given only unstructured surface forms and morphological relations.
The learning algorithm, which is based on the Expectation- Maximization algorithm, gradually maximizes the likelihood of the observed forms by adjusting the parameters of a probabilistic constraint grammar and a probabilistic lexicon.
The paper presents the algorithm抯 results on three constructed language systems with different types of hidden structure: voicing neutralization, stress, and abstract vowels.
A novel bootstrapping approach to Named Entity ^NE^ tagging using concept-based seeds and successive learners is presented.
First, decision list is used to learn the parsing-based NE rules.
The normal method for representing anaphoric dependencies in Unification Based gram-mar formalisms is that of re-entrance.
A new approach to structure-driven generation is presented that is based on a separate semantics as input structure.
These two steps are implemented as SVM classifiers using LIBSVM.
Features take into account the static context as well as relations dynamically built during parsing.We experimented two main additions to our implementation of Nivre抯 parser: N- best search and bidirectional parsing.
To construct a single-head, rooted, and cycle-free tree, we applied the ChuLiu/Edmonds optimization algorithm.
By default, these classes run a generalization of agenda- based parsing, prioritizing the partial parses by some figure of merit.
Intra-syllabic constraints are encoded as acyclic finite automata with input alphabets of phonemic symbols.
These automata in turn form the transitions in cyclic finite automata that encode the inter-syllabic constraints of word-level phonology.
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs).
Using these models in a phrase-based SMT system, we show significant improvements in translation quality for certain language pairs.
This paper proposes an effective parsing method for example-based machine translation.
Transfer-Driven Machine Translation (TDMT) achieves efficient and robust translation within the example-based framework by adopting this parsing method.
Thus, pattern-matching achieves efficient parsing.
It is also useful in treating spoken language, which sometimes deviates from conventional grammar, while grammar-based parsing has difficulty treating unrestricted spoken language.This paper proposes a constituent boundary parsing method based on pattern-matching, and shows its effectiveness for spoken language translation within the example-based framework.
constituent boundary description by a part-of-speech bigram, and classification of patterns according to linguistic levels such as simple sentence and notIll phrase,Transfer-Driven Machine Translation (TDMT) (ruruse, 1992, 1994) uses the constituent boundary parsing method presented in this paper, as an alternative to gram in analysis, and makes the best use of the example-based framework.
Section 4 explains structural disambiguation using distance calculations in the ex air p1 framework.
This paper presents evidence from several natural languages that unification—variable-matching combined with variable substitution—is the wrong mechanism for effecting agreement.
variable-matching without variable substitution.
We study the interplay of the discourse structure of a scientific argument with formal citations.
This paper presents an effective approach for resume information extraction to support automatic resume management and routing.
A cascaded information extraction (IE) framework is designed.
We propose an expressive pattern language with extended semantics of the sequence pattern, supporting negation, permutation and regular patterns that is especially appropriate for querying XML annotated documents with multi-dimensional markup.
We introduce an algorithm for designing a predictive left to right shift-reduce non-deterministic push-down machine corresponding to an arbitrary unrestricted context-free grammar and an algorithm for efficiently driving this machine in pseudo-parallel.
This paper presents a method that measures the similarity between compound nouns in different languages to locate translation equivalents from corpora.
We present in this paper the method of English-to-Korean(E-K) transliteration and back-transliteration.
The E-K transliteration method has three steps.
This paper summarizes the formalism of Category Cooccurrence Restrictions (CCRs) and describes two parsing algorithms that interpret it.
in morphological analysis).
between source program and generated lexica.
These ratios can be considered as one overall language-specific criterion for the analysis, evaluation and validation of lexical dB-s in Arabic.Keywords: Arabic lexical databases ?Arabic script ?word-formatives grammar ?lemma- entries ?morphosyntactic specifiers.
hi this paper we present an integrated system for tagging and chunking texts from a certain language.
Tins includes bigram models or finite-state automata learnt using grammatical inference techniques.
We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures.
The system was implemented in the framework of finite-state transducer technology, using linguistic criteria as well as frequency distributions derived from a database.
Further we examine the use of sentence level syntactic pattern features to increase performance.
Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.
Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based).
In this paper, we adapt the TF-IDF model to the Japanese grapheme-phoneme alignment task, by way of a simple statistical model and an incremental learning method.
In the incremental learning method, grapheme-phoneme alignment paradigms are disambiguated one at a time according to the relative plausibility of the highest scoring alignment schema, and the statistical model is re-trained accordingly.
Emdros is a text database engine for linguistic analysis or annotation of text.
Emdros implements the EMdF text database model and the MQL query language.
Junction Grammar, a model of language structure developed by Eldon Lytle, is being used to define the interlingua for a machine- assisted translation project.
Junction Grammar representations (called junction trees) consist of word sense information interrelated by junctions, which contribute syntactic and semantic information.
This paper presents a comparison of a rule- based and a statistical semantic information modeling technique.
For the rule?based method we employ Embedded Grammar (EG) tagging and for the statistical method we use a previously proposed Semantic Structured Language Modeling (SSLM) technique.
Combining EG and SSLM using linear interpolation results in further improvement.
We also use the features obtained from EG and SSLM for confidence measurement.
We define a single class hierarchy for a metagrammar which allows us to automatically generate grammars for different languages from a single compact metagrammar hierarchy.
We show how 'delayed evaluation' techniques from constraint-logic programming can be used to process such lexical rules.
In this paper, we review SPHINX-II and summarize our recent efforts on improved speech recognition.
This paper describes a Chinese word segmentor (CWS) for the third International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2006).
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita抯 GLR parsing algorithm and extends it further.
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.
Furthermore our experiments are executed in two fields: Chinese base noun phrase identification and full syntactic parsing.
The experiments prove that the extended GLR parsing algorithm with PCFG* is an efficient	parsing	method	and	astraightforward way to combine statistical property with rules.
This method is implemented in the Terminology Extraction Sotware LEXTER.
In this paper, we propose methods for linking relevant segments in hypertext authoring of a set of related manuals.
idf based method.
The paper reports on progress in building computational models of a constructivist approach to language development.
Augmented phrase structure	grammarsconsist of phrase structure rules with embedded conditions and structure-building actions written in a specially developedlanguage.
INTRODUCTIONAn augmented phrase structure grammar (APSG) consists of a collection of phrase structure, rules which are augmented by arbitrary conditions and structure building actions.
decoding and encoding.
In this paper we address the issue of useradaptivity for annotation guidelines.
We present results of machine learning experiments designed to identify user corrections of speech recog-nition errors in a corpus collected from a train in-formation spoken dialogue system.
Ensuring consistency of Part-of-Speech (POS) tagging plays an important role in constructing high-quality Chinese corpora.
Our method builds a vector model of the context of multi-category words, and uses the k-NN al-gorithm to classify context vectors con-structed from POS tagging sequences and judge their consistency.
We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms.
In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar.
By including a chunker, a supertagger, a PP attacher, and a fast probabilistic parser we were able to improve upon the baseline by 3.2%, bringing the overall labelled accuracy to 9 1.
In this paper, we describe the extension of an existing monolingual QA system for English-to-Chinese and English-toJapanese cross-lingual question answering (CLQA).
There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven.
Recent state-of- the-art part-of-speech taggers are based on the data-driven approach.
Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the data- driven approach in part-of-speech analysis may appear surprising.
A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated.
Compared to the 95-97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis.
In this paper we describe an approach to reducing the complexity of Arabic morphology generation using discrimination trees and transformational rules.
This paper proposes a framework of language inde-pendent morphological analysis and mainly concen-trate on tokenization, the first process of morpholog-ical analysis.
We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.
A conviction arose that lexicalized PCFGs (where head words annotate phrasal nodes) were the key tool for high performance PCFG parsing.
This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model.
Here we describe a method for measuring inter-annotator agreement for these event duration distributions.
In this paper, subclasses of monadic context- free tree grammars (CFTGs) are compared.
The InfoXtract location normalization module consists of local pattern matching and discourse co-occurrence analysis as well as default senses.
Multiple knowledge sources are used in a number of ways: (i) pattern matching driven by local context, (ii) maximum spanning tree search for discourse analysis, and (iii) applying default sense heuristics and extracting default senses from the web.
We investigate Global Index Grammars (GIGs), a grammar formalism that uses a stack of indices associated with productions and has restricted context-sensitive power.
We show also how GIGs can represent structural descriptions corresponding to HPSGs (Pollard and Sag, 1994) schemas.
In this paper we propose a machine- learning approach to paragraph boundary identification which utilizes linguistically motivated features.
We investigate the relation between paragraph boundaries and discourse cues, pronominalization and information structure.
In this paper we present a semantic study of motion complexes (ie.
of a motion verb followed by a spatial preposition).
We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs, on the one hand, and of the spatial prepositions, on the other hand.
We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text.
In this paper we present an evaluation of Carmel-Tools, a novel behavior oriented approach to authoring and maintaining domain specific knowledge sources for robust sentence-level language understanding.
The availability of robust and deep syntactic parsing can improve the performance of Question Answering systems.
We define a measure for the observable amount of paradigmatic modifiability of terms and, subsequently, test it on bigram, trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus.
Based upon a statistically trained speech translation system, in this study, we try to combine distinctive features derived from the two modules: speech recognition and statistical machine translation, in a log- linear model.
This paper deals with the way temporal connectives affect Temporal Structure as well as Discourse Structure in Narratives.
We focus on the sequence labelling problem, particularly POS tagging and NER tasks.
This paper proposes a parser based fully upon the connectionist model(called "CM parser" hereafter).
In order to realize the CM parser, we use Sigma-PiUnits to implement a constraint of grammatical category order or word order, and a copy mechanism of sub-parse trees.
We have explored the usefulness of incorporating speech and discourse features in an automatic speech summarization system applied to meeting recordings from the ICSI Meetings corpus.
The authors collect lexical data for a module of English syntactic analysis in the context of a bilingual research project.
This method is based on mutual information and context dependency.
We describe an algorithm that recursively applies Latent Dirichlet Allocation with an orthogonality constraint to discover morphological paradigms as the latent classes within a suffix-stem matrix.
This paper compares the consistency- based account of agreement phenomena in 'unification-based' grammars with an implication-based account based on a simple feature extension to Lambek Categorial Grammar (LCG).
We describe a linguistically expressive and easy to implement parallel semantics for quasi-deterministic finite state transducers (FSTS) used as acceptors.
This paper describes strategies for automatic recognition of unknown variants of known words in a natural language processing system.
The types of lexical variants which are detectable include in-flexional aberrations, ad hoc abbreviations and spelling/typographical errors.
My nearest neighbor approach to lexical acquisition computes the distance between an unknown word and examples from the CiLin thesaurus based upon its morphological structure.
We apply a decision tree based approach to pronoun resolution in spoken dialogue.
Our system deals with pronouns with NP- and non-NP-antecedents.
We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features.
We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.
The models use syntactic and lexical features.
We discuss some sets of grammars whose generative power lies between that of the set of context-free grammars and that of the set of context- sensitive grammars.
This paper describes a Verb Phrase Ellipsis (VPE) detection system, built for robustness, accuracy and domain independence.
This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations.
Prepositional phrase attachment is a ma-jor cause of structural ambiguity in nat-ural language.
To cope with this problem, we introduce a hybrid method of integrating corpus-based approach with knowledge-based techniques, using a wide-variety of information that comes from annotated corpora and a machine-readable dictionary.
The grammar constructs the structural tree capturing the dialogical functions of the discourse using functional subcategorization.
We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output.
We show conclusive results on joint learning and inference of syntactic and semantic representations.
We call this model a. cache trigram language model (CTLM) since we are caching the recent history of words.
Tense, temporal adverbs, and temporal connectives provide information about when events described in English sentences occur.
To extract this temporal information from a sentence, it must be parsed into a semantic representation which captures the meaning of tense, temporal adverbs, and temporal connectives.
(2001) using English combinations.
We have constructed the IPA Lexicon of Basic Japanese Nouns (IPAL-BN), which has a hierarchical structure based on the syntactic and semantic proper-ties of nouns.
Thus we have developed a method for specifying the kind of relationship between subentries, using special cognitive devices such as metaphor, metonymy, and synecdoche.
We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.
We show that we can automatically classify semantically related phrases into 10 classes.
This scheme improves the robustness for statistical machine translation models.
Two HMMbased translation models are tested to use these bilingual clusters.
This paper describes a media-independent knowledge representation scheme, or content language, for describing the content of communicative goals and actions.
In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction.
The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training.
In this paper, we will investigate a cross-linguistic phenomenon referred to as complex prepositions (CPs), which is a frequent type of multiword expressions (MWEs) in many languages.
using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective.
Such kernels use a term similarity measure based on the WordNet hierarchy.
We have developed a word sense disambiguation algorithm, following Cheng and Wilensky (1997), to disambiguate among WordNet synsets.
Our goal is to improve retrieval precision through word sense disambiguation.
We report grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars.
We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both systems.
This paper presents a framework for clustering in text-based information retrieval systems.
Co-occurrence relations can serve two main purposes in language processing.
This paper discusses a method for collecting co-occurrence data, acquiring lexical relations from the data, and applying these relations to semantic analysis.
These operators include a subset of traditional deductive rules of inference, argumentation theoretic rules of refutation, and inductive reasoning patterns.
We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that trans-forms speech signals through successive representa-tions of linguistic, dialogue, and domain knowledge.
This	paper	is	concerned with thespecifications and the implementation of a particular concept of word-based lexicon to be used for large natural language processing systems such as machine translation systems, and compares it with the morpheme-based conception of the lexicon traditionally assumed in computational linguistics.It will	be argued that, although lessconcise, a relational word-based	lexicon issuperior to-a morpheme-based	lexicon	from atheoretical, computational and also	practicalviewpoint.
Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data.
High-quality lexical resources are needed to both train and evaluate Word Sense Disambiguation (WSD) systems.
This paper presents techniques for multimedia annotation and their application to video summarization and translation.
This paper discusses the advantages for practical .hi- directional grammars Of combining a lexical focus with the GPSG-originated principle of immediate-?dominanceilinear-precedence (11)/I ,P) rule partitioning.
Finally the characteristics of the algorithm are analysed.Keywords: segmentation, connection, character-net, ambiguity, unknown words.
This paper introduces a method for computational analysis of move structures in abstracts of research articles.
We also present a prototype concordancer, CARE, which exploits the move-tagged abstracts for digital learning.
The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation.
The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus.
We discuss a Japanese text input method for mobile phones.
In this paper, we propose a method to group brackets in a bracketed corpus (with lexical tags), according to their local contextual information, as a first step towards the automatic acquisition of a context-free grammar.
Two techniques, distributional analysis and hierarchical Bayesian clustering, are applied to exploit local contextual information for computing similarity between two brackets.
This paper describes an approach to adapt an existing multilingual Open-Domain Question Answering (ODQA) system for factoid questions to a Restricted Domain, the Geographical Domain.
The new system uses external resources like GNS Gazetteer for Named Entity (NE) Classification and Wikipedia or Google in order to obtain relevant documents for this domain.
This paper describes a system that produces extractive summaries of short works of literary fiction.
This paper proposes a unified Transformation Based Learning (TBL, Brill, 1995) framework for Chinese Entity Detection and Tracking (EDT).
It consists of two sub models: a mention detection model and an entity tracking/coreference model.
This paper provides a theory of performatives as a test case for our rationally based theory of illocutionary acts.
The paper introduces a dependency-based grammar and the associated parser and focusses on the problem of determinism in parsing and recovery from errors.
Relatedness between word senses is measured using the WordNet:: Similarity Perl modules.
SCsyn is the unweighted syntax score.
SCsem is the semantic weighting.
Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data.
We describe three analyses on the effects of spontaneous speech on continuous speech recognition performance.
Johnston 1998a proposes a modular approach to multimodal language processing in which spoken language parsing is completed before multimodal parsing.
This architecture greatly simplifies the spoken language parsing grammar and enables predictive information from spoken language parsing to drive the application of multimodal parsing and gesture combination rules.
We present the design of a practical context-sensitive glosser, incorporating current techniques for lightweight linguistic analysis based on large-scale lexical resources.
We discuss extensions to our plan-based discourse processor in order to make this possi-ble.
We evaluate those extensions and demonstrate the advantage of exploiting context-based predictions over a purely non context-based approach.
Human evaluations of machine translation are extensive but expensive.
The QCS information retrieval (IR) system is presented as a tool for querying, clustering, and summarizing document sets.
We present the dialogue component of the speech-to-speech translation system VERBMOBIL.
A dialogue memory is constructed incrementally.
This paper describes the LT NSL sys-tem (McKelvie et al, 1996), an architec-ture for writing corpus processing tools.
This paper proposes a grammar-based approach to semantic annotation which combines the notions of robust parsing and fuzzy grammars.
This paper shows that two uncertainty- based active learning methods, combined with a maximum entropy model, work well on learning English verb senses.
The linguistic framework of Generalized Phrase Structure Grammar offers tools for dealing with -word order variation.
We propose a NLP methodology for analyzing patent claims that combines symbolic grammar formalisms with data- intensive methods while enhancing analysis robustness.
The classification task is an integral part of named entity extraction.
In this paper, a general method of maximizing top-down constraints is proposed.
To accomplish this, the model introduces Viterbi parsing under two-dimensional stochastic CFGs.
We propose a novel context heterogeneity similarity measure between words and their translations in helping to compile bilingual lexicon entries from a non-parallel English-Chinese corpus.
Current algorithms for bilingual lexicon compilation rely on occurrence frequencies, length or positional statistics derived from parallel texts.
Based on this information, we derive statistics of bilingual word pairs from a non-parallel corpus.
These statistics can be used to bootstrap a bilingual dictionary compilation algorithm.
In this paper, we describe how quantifiers can be generated in a text generation system.
The use of two-way finite automata for Arabic noun stem and verb root inflection leads to abstractions based on finite-state transition network topology as well as the form and content of network arcs.
We present a 1000-word continuous speech recognition (CSR) system that operates in real time on a personal computer (PC).
(Unification Categorial Grammar).
UCG is a feature grammar incorporating some basic insights from GPSG [GAZDAR 85] and HPSG [POLLARD 84].
signsemantics ?
indexsemantics ?
An important problem that is related to phrase-based statistical translation models is the obtaining of word phrases from an aligned bilingual training corpus.
In this work, we propose obtaining word phrases by means of a Stochastic Inversion Translation Grammar.
It is a key technology of Information Extraction and Open-Domain Question Answering.
First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems.
We also present an SVM-based feature selection method and an efficient training method.
Thus the basic framework of weighted constraint dependency parsing is extended by the notion of dynamic dependency parsing.
We present a robust approach for linking already existing lexi-cal/semantic hierarchies.
Previous work has argued that memory-based learning is better than abstraction-based learning for a set of language learning tasks.
In this paper, we present an E-K transliteration model using pronunciation and contextual rules.
A PSG can be expressed by a tree graph where every node has a correspondent label.
We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.
Here, we motivate and illustrate our approach using discourse-level annotations of text and speech data drawn from the CALLHOME, COCONUT, MUC-7, DAMSL and TRAINS annotation schemes.
We show how annotation graphs can represent hybrid multi-level structures which derive from a diverse set of file formats.
This paper presents a brief overview of the bidirectional (Japanese and English) Transfer- Driven Machine Translation system, currently being developed at ATR.
The aim of this development is to achieve bidirectional spoken dialogue translation using a new translation technique, TDMT, in which an example-based framework is fully utilized to translate the whole sentence.
INTRODUCTIONTransfer-Driven Machine Translation[D,121(91, (TDMT) is a translation technique which utilizes empirical transfer knowledge compiled from actual translation examples.
With this transfer-centered translation mechanism together with the example- based framework[3],[4].
TRANSFER-DRIVENARCHITECTUREThe bidirectional TDMT system, shown in Figure 1, translates English into Japanese and Japanese into English.
1 Configuration of Bidirectional TDMTSystem64
This paper presents a method for diacritics restora-tion based on learning mechanisms that act at let-ter level.
Industrial applications of a reversible, string-based, unification approach called Humor (High-speed Unification Morphol-ogy) is introduced in the paper.
The seven categories of the scheme are based on rhetorical moves of argumentation.
We describe a framework for large-scale distributed corpus annotation using peerto-peer (P2P) technology to meet this need.
Assisting in foreign language learning is one of the major areas in which natural language processing technology can contribute.
Based on this approach, we conducted experiments on estimating the Test of English for International Communication (TOEIC) score.
documents with a major geographic component.
expressions denoting geographical localisations).
We present a linguistic analyser which recognises them, performing a semantic analysis and computing symbolic representations of their "content".
This report summarizes the system and its performance on the MUG-3 task.
This paper presents a study of the effects of syntax and melodic configuration on turn- taking in Southern British English.
The results suggest that syntactic completion or non-completion is the main factor in predicting turn-taking behaviour.
?Speech preprocessors are important.
We give a qualitative analysis of the zone identification (ZI) process in biology articles.
This paper proposes the use of situation theory as a basic semantic formalism for defining general semantic theories.
After a general description of Discourse Representation Theory an encoding of DRT in ASTL is given.
Advantages and disadvantages of this method are then discussed.Topic: computational formalisms in semantics and discourse
This work provides the essential foundations for modular construction of (typed) unification grammars for natural languages.
We find they can capture effectively allophonic variation, alternative pronunciation, word co-articulation and segmental durations.
We present an algorithm for the generation of sentences from the semantic representations of Unification Categorial Grammar.
We discuss a variant of Shieber's semantic mono tonicity requirement and its utility in our algorithm.
In this paper we first briefly describe our representation language Objtalk, and then illustrate how it is used for building an understanding system for processing German newspaper texts about the jobmarket situation.keywords: newspaper processing, ATN, frames semantic grammar, object-oriented programming, Obj TalkConcepts as active schemataWe have developped an object-orientedrepresentation language - called Obj Talk - in which objects are frame-like data structures which have behavioral traits and communicate through message passing.
An aspect of developing language interaction models is an investigation of dialogue structure.
In the paper a notion of elementary communicative triad (SR-triad) is introduced to model the "stimulus-reaction" relation between utterances in the dialogue.
In this paper a system for categorisation and automatic authoring of news streams in different languages is presented.
Authoring across documents in different languages is triggered by Named Entities and event recognition.
This kind of multilingual analysis relies on a lexical knowledge base of nouns(i.e.
the EuroWordnet Base Concepts) shared among English, Spanish and Italian lexicons.
Both components are needed for word sense resolution.
A contextual representation of a word sense consists of topical context and local context.
Our goal is to construct contextual representations by automatically extracting topical and local information from textual corpora.
We review an experiment evaluating three statistical classifiers that automatically extract topical context.
In this paper, we introduce an information- theoretic approach to automatic evaluation of summaries based on the Jensen-Shannon divergence of distributions between an automatic summary and a set of reference summaries.
The project is devoted to the evaluation of parallel text alignment techniques.
In its first period ARCADE ran a competition between six systems on a sentence-to-sentence alignment task which yielded two main types of results.
Dutch subordinate clauses.
Dutch subordinate clauses for non-transformational theories of grammar.
GPSG also includes the idea of rule schemata - rules with variables over categories.
We then examine the effect of integrating pausal and spontaneous speech phe-nomena into syntactic rules for speech recognition, using118 sentences.
This paper presents a format for representing the linguistic form of utterances, called situation schemata, which is rooted in the situation semantics of Barwise and Perry.
I also show that WordNet categories improve a system that performs aspectual classification with linguistically-based numerical indicators.
We present the newest implementation of the LINGSTAT machine-aided translation system.
INTRODUCTIONLINGSTAT is an interactive machine-aided translation system designed to increase the productivity of a translator.
Similarity of a word to a context is estimated using a proximity measure in corpus- derived "semantic space".
Polarized dependency (PD-) grammars are proposed as a means of efficient treatment of discontinuous construc-tions.
The discussion is motivated with a case of study in multimodal reference resolution.
In Section 3, an incremental model for multimodal reference resolution is illustrated.
Finally, in the conclusion of the paper, a reflexion on the relation between spacial deixis and anaphora is advanced.
The previous probabilistic part-of-speech tagging models for agglutinative languages have considered only lexical forms of morphemes, not surface forms of words.
By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HMM)-based tagging model.based tagging model.2 Korean POS tagging modelIn this section, we first describe the standard morpheme-unit tagging model and point out a mistake of this model.
Then, we describe the proposed model.2.1 Standard morpheme-unit modelThis section describes the HMM-based morpheme- unit model.
The morpheme-unit POS tagging model is to find the most likely sequence of morphemes M and corresponding POS tags T for a given sentence W, as follows (Kim et al., 1998; Lee et al., 2000):
Within the machine translation system Verb-mobil, translation is performed simultaneously by four independent translation modules.
This paper describes a method for converting a task-dependent grammar into a word predictor of a speech understanding system.
We have solved this problem by applying an algorithm for bottom-up parsing.
We study how two graph algorithms apply to topic-driven summarization in the scope of Document Understanding Conferences.
Our algorithms select sentences for extraction.
A bottom-up generation algorithm for principle-based grammars is proposed.
We describe a new technique for con-structing finite-state transducers that in-volves reapplying the regular-expression compiler to its own output.
Speaker independent phonetic transcription of fluent speech is performed using an ergodic continuously variable duration hidden Markov model (CVDHMM) to represent the acoustic, phonetic and phonotac tic structure of speech.
The transcription algorithm was then combined with lexical access and parsing routines to form a complete recognition system.
We consider the problem of extracting specified types of information from natural language text.
We therefore use preference semantics: selecting the analysis which maximizes the number of semantic patterns matched.
This paper describes an extremely lexicalized probabilistic model for fast and accurate HPSG parsing.
In the spoken language machine trans-lation project Verbmobil, the seman-tic formalism Language for Underspec-ified Discourse representation structures (LUD) is used.
We present a comparison of two formalisms for representing natural language utterances, namely deep syntactical Tectogrammatical Layer of Functional Generative Description (FGD) and a semantic formalism, MultiNet.
We define a semantic chunk as the sequence of words that fills a semantic role defined in a semantic frame.
We explore two semantic chunking tasks.
In the first task we simultaneously detect the target word and segments of semantic roles.
We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural- language processing.
Comparing the performance of PLTIGs with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart.
Furthermore, training of PLTIGs displays faster convergence than PCFGs.
This paper' presents an example-basedrescoring method that validates SMT translation candidates and judges whether the selected decoder output is good or not.
This paper describes the user expertise model in AthosMail, a mobile, speech-based e-mail system.
This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text.
LCP records mutual similarity of words in a sequence of text.
LCP may provide valuable information for resolving anaphora and ellipsis.
This paper presents an approach which exploits general-purpose algorithms and resources for domain-specific semantic class disambiguation, thus facilitating the generalization of semantic patterns from word-based to class-based representations.
Through the mapping of the domain- specific semantic hierarchy onto Word- Net and the application of general-purpose word sense disambiguation and semantic distance metrics, the approach proposes a portable, wide-coverage method for disambiguating semantic classes.
For topic identification, we will outline techniques based on stereotypical text structure, cue words, high-frequency indicator phrases, intratext connectivity, and discourse structure centrality.
For topic fusion, we will outline some ideas that have been proposed, including concept generalization and semantic association.
Semantic language model is a technique that utilizes the semantic structure of an utterance to better rank the likelihood of words composing the sentence.
Natural-Language Generation from flat semantics is an NP-complete problem.
We show how to convert TAG generation problems into dependency parsing problems, which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard.
State of the art spelling correction systems, e.g.
Traditionally, word sense disambiguation (WSD) involves a different context classification model for each individual word.
In this research, maximum entropy modeling is used to train the word independent context pair classification model.
Then based on the context pair classification results, clustering is performed on word mentions extracted from a large raw corpus.
trigger words and parsing structures.
(HLT-NAACL Workshop on Parallel Texts 2006).
We have studied different techniques to improve the standard Phrase-Based translation system.
Probabilistic Latent Semantic Analysis (PLSA) is an information retrieval technique proposed to improve the problems found in Latent Semantic Analysis (LSA).
We have applied both LSA and PLSA in our system for grading essays written in Finnish, called Automatic Essay Assessor (AEA).
We report the results comparing PLSA and LSA with three essay sets from various subjects.
The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution.
We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm.
While most work on parsing with PCFGs has focused on local correlations between tree configurations, we attempt to model non-local correlations using a finite mixture of PCFGs.
This paper presents results on experiments using this approach, in which statisti-cal models of the term selection and term ordering are jointly applied to pro-duce summaries in a style learned from a training corpus.
We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks.
We present an operational framework allowing to express a large scale Tree Adjoining Grammar (TAG) by using higher level operational constraints on tree descriptions.
This paper describes the results of a preliminary study of a Knowledge Engineering approach to Natural Language Understanding.
Classifier combination is a promising way to improve performance of word sense disambiguation.
As an option to the parsing, this paper discusses a way to identify the subject and the predicate, known as the pola-grammar technique.
Parsing spoken input introduces serious problems not present in parsing typed natural language.
This paper describes an extension of semantic caseframe parsing to restricted-domain spoken input.
The semantic caseframe grammar representation is the same as that used for earlier work on robust parsing of typed input.
We ran both Brill抯 rule-based tagger and TNT, a statistical tagger, with a default German newspaper-language model on a medical text corpus.
Concept grammar rules are built by mapping concepts from the lexicon onto the concept-structure patterns present in a set of training responses.
This paper analyses the syntax and semantics of English comparatives, and some types of ellipsis.
a range of current grammatical theories.
We investigate linguistic-knowledge-based word similarity measures while other previous works heavily rely on statistical information, and their limits will be discussed.
The first enhancement considers topic representations in terms of relevant topic relations instead of relevant terms.
The second enhancement is based on ranking the topic themes.
Topic representations are integrated in two NLP applications: Information Extraction and Multi- Document Summarization.
The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items.
Integrating puns into normal text may involve complex search.
We present a language-independent and unsupervised algorithm for the segmentation of words into morphs.
Our bottom-up deterministic parser adopts Nivre抯 algorithm (Nivre, 2004) with a preprocessor.
Support Vector Machines (SVMs) are utilized to determine the word dependency attachments.
In this paper, we propose a framework in which information about the multi-word expressions can be used in the word-alignment task.
The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus.
In this paper, we present a formal definition of reversible NLG systems and develop a classification of existing natural language dialog systems in this framework.
The three sources of information are appositives, compound nouns, and ISA clauses.
We apply heuristics to these syntactic structures, embed them in a bootstrapping architecture, and combine them with co-training.
Previous works on question classification are based on complex natural language processing techniques: named entity extractors, parsers, chunkers, etc.
This paper reviews four approaches to the annotation of reference in corpora.
Motivated by a systematic analysis of Chinese	semantic	relationships,	weconstructed a Chinese semantic framework based on surface syntactic relationships, deep semantic relationships and feature structure to express dependencies between lexical meanings and conceptual structures, and relations that underlie those lexical meanings.
This paper investigates model merging, a technique for deriving Markov models from text or speech corpora.
This paper proposes an alignment adaptation	approach	to	improvedomain-specific (in-domain) word alignment.
We compare representatives of two principal approaches to computing phonetic similarity: manually-designed metrics, and learning algorithms.
We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
In this paper we present a preliminary study aimed at automatically identifying 搃rrelevance?in the domain of telephone conversations.
We first created an HPSG treebank from the EDR corpus by using heuristic conversion rules, and then extracted lexical entries from the tree- bank.
We also trained a statistical parser for the grammar on the tree- bank, and evaluated the parser in terms of the accuracy of semantic-role identification and dependency analysis.
One task is the extraction of cognates from bilingual text.
This paper presents an algorithm for the unsupervised learning of a simple morphology of a natural language from raw text.
A generative probabilistic model is applied to segment word forms into morphs.
We present a simple approximation method for turn-ing a Head-Driven Phrase Structure Grammar into a context-free grammar.
GAMBL is a word expert approach to WSD in which each word expert is trained using memory- based learning.
A further innovation on earlier versions of memory- based WSD is the use of grammatical relation and chunk features.
This paper describes a computational model of concept acquisition for natural language.
Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words.
The acoustic models make use of dictionary phonetic spellings together with models for phonemes in context.
We present a computational model for generation and recognition of Georgian verb conjugations, relying on the analysis of Georgian verb structure as a word-level template.
The model combines a set of finite-state transducers with a default inheritance mechanism.1
In a rule based machine translation system, the grammar consists of a lot of rewriting rules.
We propose new approach to the problem that relies on term specificity and similarity measures.
DAAL03-89-00031.
In this paper, we introduce a large-scale test collection for multiple document summarization, the Text Summarization Challenge 3 (TSC3) corpus.
This paper addresses the issue of designing embodied conversational agents that exhibit appropriate posture shifts during dialogues with human users.
We present an analysis of human monologues and dialogues that suggests that postural shifts can be predicted as a function of discourse state in monologues, and discourse and conversation state in dialogues.
On the basis of these findings, we have implemented an embodied conversational agent that uses Collagen in such a way as to generate postural shifts.
A grammar formalism, Field and Category Grammar (FCG), is described, which beside constituent structure and functional structure recognizes a level of field structure.
We present a lexicon-free post-processing method for optical character recognition (OCR), implemented using weighted finite state machines.
We present an LG-based approach to recognizing of Proper Names in Korean texts.
IntroductionIn this paper, we present a description of the typology of nominal phrases containing Proper Names (PN) and the local grammars [Gro87],[Moh94] constructed on the basis of this description.
Current lexical semantic representations for natural language applications view verbs as simple predicates over their arguments.
The purpose of this study is to construct a semantic analysis method for disambiguating Japanese compound verbs.
We construct a method employing 110 disambiguation rules based on the semantic features of the first verb of a compound and syntactic patterns consisting of co-occurrence between verbs and nouns.
We describe iNeATS ?an interactive multi-document summarization system that integrates a state-of-the-art summarization engine with an advanced user interface.
In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on different approaches: Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features.
In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner.
The hierarchical structure is discovered wing lexical cohesion methods combined with hierarchical agglomerative clustering.
The list of concepts are discovered by n-gram analysis filtered by part-of-speech patterns.
This work reports on three human tense annotation experiments for Chinese verbs in Chinese-to-English translation scenarios.
The analyses also find the verb telicity feature, aspect marker presence and syntactic embedding structure to be strongly associated with tense, suggesting their utility in the automatic tense classification task.
This approach is successful for entailments.
Hence, we view the study of these functional units as lexical pragmatics rather than lexical semantics.
Default Logic is one formal method for performing default reasoning in the area of Artificial Intelligence called Knowledge Representation.
We suggest that the use of non-classical inferencing techniques such as default reasoning will prove fruitful in the realm of lexical reasoning.
We present here a methodology for obtaining semantic information on verb aspect by parsing a corpus and automatically applying linguistic tests with a set of structural analysis tools.
We present a computationally tractable account of the interactions between sentence markers and focus marking in Somali.
We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse.
We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG.
We describe an implementation of data- driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.
Practically applied in the DIRECT-INFO EC R&D project we show how such linguistic annotation contributes to semantic annotation of multi- modal analysis systems, demonstrating also the use of the XML schema of MPEG-7 for supporting cross-media semantic content annotation.
In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system.
Finally, the recognizer has been modified to use bigram back-off language models.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
The former models certain uses of multisetvalued feature structures in unification-based formalisms, while the latter is motivated by word order variation and by "quasi-trees", a generalization of trees.
Speech Recognition and Machine Translation are also changing the world.
Managing Gigabytes: Compressing and Indexing Documents and Images, Academic Press/Morgan Kaufmann.
A CYK-table-driven interactive relaxation parsing method oi spoken Korean, integrated with the CYK-based morphological analysis is introduced.
We argue that grammatical processing is a viable alternative to concept spotting for processing spoken input in a practical dialogue system.
We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input.
WORD- NET DOMAINS).
Statistical machine translation systems are based on one or more translation models and a language model of the target language.
While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems.In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary.
We consider the translation of European Parliament Speeches.
This paper reports on a new statistical approach to machine aided translation of terminology bank.
We introduce a new categorial formal-ism based on intuitionistic linear logic.
Incorporating these restrictions into Head-Driven Phrase Structure Grammar (HPSG) has caused us to examine the treatment of nominal modification in HPSG.
We evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization.
We developed a novel classification of concept attributes and two supervised classifiers using this classification to identify concept attributes from candidate attributes extracted from the Web.
We demonstrate a novel application of EM-based clustering to multivariate data, exemplified by the induction of 3- and 5-dimensional probabilis-tic syllable classes.
We then propose a novel approach to grapheme-to-pho-neme conversion and show that syl-lable structure represents valuable information for pronunciation sys-tems.
In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) sub- compounds from complex noun phrases using both corpus statistics and linguistic heuristics.
Our system for semantic role labeling is multi-stage in nature, being based on tree pruning techniques, statistical methods for lexicalised feature encoding, and a C4.5 decision tree classifier.
We use both shallow and deep syntactic information from automatically generated chunks and parse trees, and develop a model for learning the semantic arguments of predicates as a multi-class decision problem.
To translate SMS texts, traditional approaches model such irregularities directly in Machine Translation (MT).
This paper describes eight telephone-speech corpora at various stages of development at the Center for Spoken Language Understanding.
It combines word-based frequency and position method to get categorization knowledge from the title field only.
The system contains a multivalued part-of-speech disambiguator and a novel parser employing bottom-up recognition to find the constituent phrases of larger structures that might be too difficult to analyze.
We investigate the problem of learning a part-of-speech (POS) lexicon for a resource-poor language, dialectal Arabic.
We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method.
In this paper, we will present first the syntactic analysis, based on a chart parser that uses a LFG grammar for French, and the semantic analysis, based on conceptual graphs.
Then we will show how these two analyses collaborate to produce semantic representations and sentences.
Before concluding, we will show how these modules are used through a distributed architecture based on CORBA (distributed Smalltalk) implementing the CARAMEL multi-agent architecture.
We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees.
We report here on our experiments with POST (Part of Speech Tagger) to address problems of ambiguity and of understanding unknown words.
This paper presents an adaptive learning framework for Phonetic Similarity Modeling (PSM) that supports the automatic construction of transliteration lexicons.
This paper presents experiments with the evaluation of automatically produced summaries of literary short stories.
Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption.
STS is a small experimental sentence translation system developed to demonstrate the efficiency of our lexicalist model of translation.
Based on a GB-inspired parser, lexical transfer and lexical projection, STS provides real-time accurate English.
This paper proposes a new approach to segmentation of utterances into sentences using a new linguistic model based upon Maximum-entropy-weighted Bidirectional N-grams.
Language Understanding (NLU) systems,particularly for data-driven ones.
knowledge and language-specific heuristics.
This paper examines the properties of feature- based partial descriptions built On top of Halliday's systemic networks.
This paper describes the general architecture of generation in the ACORD project.
SemEval is an attempt to define a task-independent technology-based evaluation for language- understanding systems consisting of three parts: word-sense identification, predicate-argument structure determination, and identification of coreference relations.
This paper describes a Chinese word segmentor (CWS) based on backward maximum matching (BMM) technique for the 2nd Chinese Word Segmenta-tion Bakeoff in the Microsoft Research (MSR) closed testing track.
Our CWS comprises of a context-based Chinese unknown word identifier (UWI).
In this paper we discuss an algorithm for the assignment of pitch accent positions in text-to-speech conversion.
When a base-line speech recognition system generates al-ternative hypotheses for a sentence, we will utilize the word preferences based on topic coherence to select the best hy-pothesis.
This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora.
This explains the low results when performing word sense disambiguation across corpora.
In this paper, we report the adaptation of a named entity recognition (NER) system to the biomedical domain in order to participate in the 擲hared Task Bio-Entity Recognition?
As a measure of similarity, the concept of longest collocation couple is introduced, which is the basis of clustering similar words.
We proposed a method of machine transla-tion using inductive learning with genetic algorithms, and confirmed the effectiveness of applying genetic algorithms.
This paper discusses research on distinguishing word meanings in the context of information retrieval systems.
We conducted experiments with three sources of evidence for making these distinctions: morphology, part-of-speech, and phrases.
We have focused on the distinction between homonymy and polysemy (unrelated vs. related meanings).
Our results support the need to distinguish homonymy and polysemy.
This paper presents an algorithm for tagging words whose part-of-speech properties are unknown.
The algorithm is based on predicates defined for WordNet verb classes.
Figure 1: Dimensions of Language Engineering Complexity.
These generic systems include: (1) integrated multi-rate voice/data communications terminal; (2) interactive speech enhancement system; (3) voice-controlled pilot's associate system; (4) advanced air traffic control training systems; (5) battle management command and control support system with spoken natural language interface; and (6) spoken language translation system.
We demonstrate TextRank ?a system for unsupervised extractive summarization that relies on the application of iterative graph- based ranking algorithms to graphs encoding the cohesive structure of a text.
This paper presents a novel method of generating and applying hierarchical, dynamic topic-based language models.
It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques.
These combined models help capture long-distance lexical dependencies.
We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.
This paper describes a method to find phrase-level translation patterns from parallel corpora by applying dependency structure analysis.
We use statistical dependency parsers to determine dependency relations between base phrases in a sentence.
Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts.
We describe I/J-terms and how they have been incorporated into the Logic Grammar formalism.
Its taxonomic reasoning facilitates semantic type-class reasoning during grammatical analysis.
A semantic-representation-to--speech system communicates orally the information given in a semantic representation.
Such a system must Integrate a text generation module, a phonetic conversion module, a prosodic module and a speech synthesizer.
It is loosely based on a new psycholinguistic theory of coordinative ellipsis proposed by Kempen.
To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries.
In this paper, I argue for the use of a probabilistic form of tree-adjoining grammar (TAG) in statistical natural language processing.
One approach for achieving this is to represent the classification model as a weighted finite-state transducer (WFST).
In this paper, we present a compilation procedure to convert the rules resulting from an AdaBoost classifier into an WFST.
This paper investigates the linguistic application of pragmatic-based constraints to the 憇emantic?notion of presupposition in DRT.
In this paper, we report on an effort to provide a general-purpose spoken language generation tool for Concept-to-Speech (CTS) applications by extending a widely used text generation package, FUF/SURGE, with an intonation generation component.
As a first step, we applied machine learning and statistical models to learn intonation rules based on the semantic and syntactic information typically represented in FUF/SURGE at the sentence level.
In this paper, we introduce a generic approach to elliptic coordination modeling through the parsing of Ltag grammars.
This paper briefly outlines the WYSIWYM (What You See is What You Meant) approach to knowledge editing and focuses on the role of coreferring Noun Phrases in the feedback texts that are generated by a WYSIWYM system and which play a key role in this approach.
A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories.
By combining finite-state and unification-based formalisms, the grammar formalism used in SProUT offers both processing efficiency and a high degree of decalrativeness.
We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs.
Next section describes the prototype which is based on a Transformational System as well as on a rewriting system of c-graphs which constitutes the nodes of the Transformational System.
In this paper, DARPA Resource Management task is used as the domain to investigate the performance of speaker-adaptive speech recognition.
We describe an algorithm for Word Sense Disambiguation (WSD) that relies on a lazy learner improved with automatic feature selection.
This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE.
We study the effect of the Cartesian product operator on memory-based language learning, and demonstrate its effect on generalization accuracy and data compression for a number of linguistic classification tasks, using k-nearest neighbor learning algorithms.
We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string.
In this paper, a representation for syntactic dependency trees (D-trees) is defined through a finite set of axioms.
Bracketed D-tree representations (cf.
We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications.
We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries.
In this paper, we propose a classification of gram-mar development strategies according to two crite-ria: hand-written versus automatically acquired grammars, and grammars based on a low versus high level of syntactic abstraction.
In this paper, we describe the acquisition and organization of knowledge sources for machine translation (MT) systems.
Automatic summarization and information extraction are two important Internet services.
This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1.
We present an algorithm for generating appropriate anaphoric expressions which takes the temporal structure of texts and knowledge about ambiguous contexts into account.
We selected three typical word alignment models ranging over statistical-based ones and heuristic-based ones, to test whether cross language similarities can improve the performance of word alignment models.
In this paper interpretation principles for simple and complex frame-adverbial expressions are presented.
We also explore a potential application in document clustering that is based upon different types of lexical changes.
Our method is a statistical approach based on what we call textual entailment patterns, prototypical sentences hiding entailment relations among two activities.
This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues.
A generative probability model for unification- based grammars is presented in which rule probabilities depend on the feature structure of the expanded constituent.
We describe in this paper a boolean Information Retrieval system that adds word semantics to the classic word based indexing.
We consider lexical operations and their representation in a unification based lexicon and the role of lexical semantic information.
We introduce a framework for semantic interpreta-tion in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata.
information technology test reports and medical finding reports.
This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account.
We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations.
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model.
In (Hindle and Rooth, 1993) hereafter H&R, lexicalized rules are derived according to the probability of noun-preposition or verb-preposition bigrams for am-biguous structures like verb-noun-preposition-noun se-quences.
In (Resnik and Hearst, 1993) class-based trigrams are obtained by generalizing the PP head, using WordNet synonymy sets.
(Franz, 1995) uses a loglinear model to estimate preferred attachments according to the linguistic features of co-occurring words (e.g.
bigrams, the accompanying noun determiner, etc.).
(Brill and Resnik, 1994) use transformation- based error-driven learning (Brill, 1992) to derive disambiguation rules based on simple context information (e.g.
In this paper we report on our natural language informa-tion retrieval (NLIR) project as related to the recently concluded 5th Text Retrieval Conference (TREC-5).
In this paper we discuss both manual and automatic procedures for query expansion within a new stream-based information retrieval model.
We present an adaptive technique that enables users to produce a high quality dictionary parsed into its lexicographic components (headwords, pronunciations, parts of speech, translations, etc.)
In this paper, baseline speech recognition performance is determined both for a single remote microphone and for a signal derived from a delay-and-sum beamformer using an eight-microphone linear array.
We describe a novel technique and imple-mented system for constructing a subcate-gorization dictionary from textual corpora.
We compare two stories by finding three cosine similarities based on names, topics and the full text.
This paper presents a semantic class prediction model of Chinese two-character compound words based on a character ontology, which is set to be a feasible conceptual knowledge re-source grounded in Chinese characters.
Linear precedence (LP) rules are widely used for stating word order principles.
We show a type-based encoding in an HPSG-style formalism that supports processing.
We propose four new methods to accurately determine word sense dominance using raw text and a published thesaurus.
In the process, we create a word朿ategory co- occurrence matrix, which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses, as well.
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks).
We apply weighted voting of 8 SVMs-based systems trained with distinct chunk repre-sentations.
This paper describes a method for analyzing Japanese double-subject construction having an adjective predicate based on the valency structure.
We describe a word alignment platform which ensures text pre-processing (tokenization, POS-tagging, lemmatization, chunking, sentence alignment) as required by an accurate word alignment.
This paper reports on efforts to automatically identify and classify discourse markers in Chinese texts using heuristic-based and corpus-based data-mining methods, as an integral part of automatic text summarization via rhetorical structure and Discourse Markers.
A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model.
In this paper, we propose a recursive graph based scheme for semantic annotation of Chinese phrases.
This paper presents a practical method for a global structure analyzing algorithm of Japanese long sen-tences with lexical information, a method which we call Lexical Discourse Grammar (LDG).
Finally, we evaluate our system by real corpus and present the experiment results.Key Words: Word Alignment, Chunk Alignment, Bilingual Corpus, Lexicon Extraction
This paper describes a data source and methodology for producing customized test suites for molecular biology entity identification systems.
We first present a version of definite clauses (positive Horn clauses) that is based on this logic.
We also describe a higher-order logic programming language, called AProlog, which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter.
This is to be contrasted with the more common practice of using two entirely different computation paradigms, such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing.
We describe a formal framework for interpretation of words and compounds in a discourse context which integrates a symbolic lexicon/grammar.
word-sense probabilities, and a pragmatic component.
We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues.
We then automatically extract features representing acoustic-prosodic and other linguistic information from the speech signal and associated transcriptions.
We compare the results of machine learning experiments using different feature sets to predict the annotated emotions.
The supervised learning system is based on lexical features and bagged decision trees.
They are:?Vector-quantized energy-normalized Mel-cepstra?Vector-quantized smoothed 40-ms晅ime derivatives of the Mel-cepstra?Energy?Smoothed 40-ms energy differencesWe use 256-word speaker-independent code- books to vector-quantize the Mel-cepstra and the Melcepstral differences.
The resulting four-feature-perframe vector is used as input to the DECIPHER HMMbased speech recognition system.Pronunciation ModelsDECIPHER uses pronunciation models generated by applying a phonological rule set to word base- forms.
Speaker- independent pronunciation probabilities are then estimated using these bushy word networks and the forward- backward algorithm in DECIPHER.
We have shown in Cohen90 that this modeling improves system performance.Acoustic ModelingDECIPHER builds and trains word models by using context-based phone models arranged according to the pronunciation networks for the word being modeled.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
The components are implemented as SVM classifiers using libSVM.
Experiments were conducted to find kernel parameters for the Radial Basis Function (RBF) kernel.
This paper proposes a generation method for feature-structure-based unification grammars.
The method enables feature structure retrieval via multiple indices.
This paper discusses Korean morpho-logical analysis and presents three probabilistic models for morphological analysis.
part-of-speech tagged corpora).
Default inheritance is a useful tool for encoding linguistic generalisations that have exceptions.
In this paper we show how the use of an order independent typed default unification operation can provide non-redundant highly structured and concise representation to specify a network of lexical types, that encodes linguistic information about verbal sub- categorisation.
The method involves using a bilingual term list to learn source- target surface patterns.
We present a prototype called TermMine that applies the method to translate terms.
The S-graph is an acyclic directed graph with exactly one start node and one end node.
It includesa) a morphosyntactic type (MS), i.e.
), the C/O-concept is based on context-free rules.
Where GPSG employs meta-rules, derived categories, and the ID/LP-formalism, LFG uses different structural concepts (C- and F-structures) and - above all - lexical knowledge.
LFG and GPSG are augmented PS- grammars.
The algorithm works by first using a large corpus to find semantic neighbors of the unknown word, which we accomplish by combining latent semantic analysis with part-of-speech information.
We evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy.
The method we are currently working on uses an expectation-maximization (EM) based word-clustering algorithm, and we have evaluated the effectiveness of this method using Japanese verb phrases.
This paper investigates the application of co- training and self-training to word sense disambiguation.
sociated with competing hypotheses.
This article introduces a bidirectionalgrammar generation system called feature structure-directed generation, developed for a dialogue translation system.
We have been developing a spoken language system to recognize and understand spontaneous speech.
2 Previous workThe MT system described in this paper combines hand-built analysis and generation components with automatically learned example-based transfer patterns.
finite^state and probabilistic methods).
The approach combines a statistical model of named entity states with a lattice representation of hypothesized words and errors annotated with recognition confidence scores.
This paper proposes methods for extractingloanwords from Cyrillic Mongolian corporaand producing a Japanese朚ongolianbilingual dictionary.
We propose astemming method for Mongolian to extractloanwords correctly.
This paper describes PEGASUS, a spoken language interface for on-line air travel planning that we have recently developed.
This paper describes our preliminary research on "attention-sharing" in infants' language acquisition.
This paper outlines a formal computational semantics and pragmatics of the major speech act types.
The purpose of this paper is to auto-matically generate Chinese chunk bracketing by a bottom-to-top map-ping (BTM) model with a BTM data-set.
The BTM model is designed as a supporting model with parsers.
We de-fine a word-layer matrix to generate the BTM dataset from Chinese Tree-bank.
Our model matches auto-learned patterns and templates against seg-mented and POS-tagged Chinese sen-tences.
This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame.
Unknown words are recognized with reliability in role-based HMM.
An HHMM-based system ICTCLAS was accomplished.
We present a tool developed for annota-ting corpora with argument structure re-presentations.
Among others, we show how the as-signment of grammatical functions can be automatised using standard part-of-speech tagging methods.
We report that a proper employment of MWEs concerned enables us to put forth a tractable framework, which is based on a multiple nesting of semantic operations, for the processing of non-inferential, Non- propositional Contents (NPCs) of natural Japanese sentences.
We present a global sequence-processing method that repairs inconsistent local decisions.
In this document, we analyze several as-pects related to the semantics of preposi-tions.
In this paper, we present a stochastic language modeling tool which aims at retrieving variable-length phrases (multigrams), assuming bigram dependencies between them.
We present the first algorithm that computes optimal orderings of sentences into a locally coherent discourse.
In a learning phase, linguistic knowledge such as conceptual co-occurrence patterns and syntactic role distribution of antecedents is extracted from a large-scale corpus.
Unlike previous research based on co-occurrence patterns at the lexical level, we represent co-occurrence patterns with concept types in a thesaurus.
We describe a method for incorporating syntactic information in statistical machine translation systems.
In this paper we present a multiclassifier approach for multilabel document classification problems, where a set of k-NN classifiers is used to predict the category of text documents based on different training subsampling databases.
We investigate the change in performance of automatic subcategorization acquisition when a word sense disambiguation (WSD) system is employed to guide the acquisition process.
This paper discusses the design of the EuroWordNet database, in which semantic databases like WordNetl .5 for several languages are combined via a so-called inter-lingual-index.
This involves deriving abstract relationships among conceptual units.
This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning.
In our method, collocations which characterise every sense are extracted using similarity-based estimation.
For the results, term weight learning is performed.
Processing discourse connectives is important for tasks such as discourse parsing and generation.
This paper presents experiments into modelling the substitutability of discourse connectives.
It shows that substitutability effects distributional similarity.
A novel variance- based function for comparing probability distributions is found to assist in predicting substitutability.
We introduce a new interactive corpus exploration tool called InfoMagnets.
In particular, this paper presents an in-depth investigation of the entity detection and recognition (EDR) task for Arabic.
We start by highlighting why segmentation is a necessary prerequisite for EDR, continue by presenting a finite-state statistical segmenter, and then examine how the resulting segments can be better included into a mention detection system and an entity recognition system; both systems are statistical, build around the maximum entropy principle.
We apply our method to two tasks ?semantic role labeling and recognizing textual entailment ?and achieve useful performance gains from the superior pipeline architecture.
Examples show how word dependency across phrases can be modeled.
The verbgraphs may be used to support NL update.
This paper builds on recent research investigating sentence ordering in text production by evaluating the Centering-based metrics of coherence employed by Karamanis et al.
The Prague Czech-English Dependency Tree- bank (PCEDT) is a new syntactically annotated Czech-English parallel resource.
Using the formalism of generalized phrase structure grammar (GPSG) in an NI, system (e.g.
In this paper we present the RWTH FSA toolkit ?an efficient implementation of algorithms for creating and manipulating weighted finite-state automata.
The claim is substantiated through experimental data and an illustration of a word sense disambiguation system (SENSE) capable of using contextually-relevant semantic similarity.
OF COLING-92, NAN-rEs, Auc.
We address Question Answering (QA) for biographical questions, i.e., questions asking for biographical facts about persons.
We present BioGrapher, a biographical QA system that addresses this problem by machine learning algorithms for biography classification.
We present two novel paraphrase tests for automatically predicting the inherent semantic relation of a given compound nominalisation as one of subject, direct object, or prepositional object.
We compare these to the usual verb朼rgument paraphrase test using corpus statistics, and frequencies obtained by scraping the Google search engine interface.
This paper describes a system that performs hierarchical error repair for ill- formed sentences, with heterarchical control of chart items produced at the lexical, syntactic, and semantic levels.
The system uses an augmented context-free grammar and employs a bidirectional chart parsing algorithm.
This paper focuses on the heterarchical processing of integrated- agenda items (i.e.
Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others.
We apply a complementary similarity measure to find a hierarchical word structure.
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language.
Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections.Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91 % F-measure.
The second method learns less restrictive patterns that include bags of words and relation-specific named entity tags.
In this paper, we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models.
We propose a method which explicitly takes into account such interdependencies during the EM training of the statistical alignment models.
We investigate the usefulness of evolutionary al-gorithms in three incarnations of the problem of feature relevance assignment in memory-based language processing (MBLP): feature weight-ing, feature ordering and feature selection.
Both types of models are trained by Expectation- Maximization (EM) algorithms for maximum likelihood estimation.
Case dependencies and noun class generalization are represented as features in the maximum entropy approach.
In this paper we discuss and evaluate CTXMATCH, an approach to interoperability that discovers mappings among CHs considering the semantic interpretation of their nodes.
CTXMATCH performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.
We describe the baseline phrase-based translation system and various refinements.
We present translation results for three tasks: Verb- mobil, Xerox and the Canadian Hansards.
In this work, we report on supervised learning experiments to learn this distinction for the difficult case of prepositional phrases attached to the verb.
We develop statistical indicators of linguistic diagnostics for argumenthood, and we approximate them with counts extracted from an annotated corpus.
The WordNet lexical ontology, which is primarily composed of common nouns, has been widely used in retrieval tasks.
To support this claim, we build a fine-grained proper noun ontol-ogy from unrestricted news text and use this ontology to improve performance on a question answering task.
This paper proposes an empirical approach to the development of a computational model for assessing texts according to cohesiveness.
In this paper, we discuss inter-dialect MT in general and Cantonese-Mandarin MT in particular.
Synchronous Context-Free Grammars (SCFGs) have been successfully exploited as translation models in machine translation applications.
The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS.
It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies (projection of information structure into syntax) and prosodic context (performance- related modifications to intonation patterns).Phonological processing in our system corn- prises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation.
We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function.
Semantic relatedness is a special form of linguistic distance between words.
As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts.
This paper presents Trace dr Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (UG) and ideas of Government dr Binding Theory (on).
in GPSO and HPSG.
This process is called transliteration.
Specifically focusing on sequential segmentation tasks, i.e.
text chunking and named entity recognition, we introduce a loss function that closely reflects the target evaluation measure for these tasks, namely, segmentation F-score.
The Valency Lexicon of Czech Verbs, Version 1.0 (VALLEX 1.0) is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs.
The query translation approach is employed using the LDC bilingual wordlist.
In this paper we present a two-stage statistical word segmentation system for Chinese based on word bigram and word- formation models.
We present a general architecture for incremental interaction between modules in a speech-tointention continuous understanding dialogue system.
This paper presents a novel language-independent question/answering (Q/A) system based on natural language processing techniques, shallow query understanding, dynamic sliding window techniques, and statistical proximity distribution matching techniques.
Supervised learning methods for WSD yield better performance than unsupervised methods.
We develop compositionality and acceptability measures that draw on linguistic properties specific to LVCs, and demonstrate that these statistical, corpus-based measures correlate well with human judgments of each property.
This document shows how the factorized syntactic descriptions provided by Meta- Grammars coupled with factorization operators may be used to derive compact large coverage tree adjoining grammars.
Word Manager supports the definition, access and maintenance of lexical databases.
In this paper, we present three semantic grouping methods: similarity- based, verb-based and category-based grouping, and their implementation in the SLUI toolkit.
Computer games is an interesting application for spoken and multimodal dialogue systems.
the four axioms of the Dependency Grammar (DG).
hi this paper we describe how in NMG (Non-Monotonic Grammar), by monitoring a logic parser, a truth maintenance system can significantly.
Bayesian, decision tree and neural network classifiers) to discover language model errors.
We have examined 2 general types of features: model-based and language-specific features.
very large corpora of strings.
In this paper, we use a machine learning framework for semantic argument parsing, and apply it to the task of parsing arguments of eventive nominalizations in the FrameNet database.
We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines.
Unlike usual automatic text categorization systems, which rely on data- intensive models induced from large training data, our automatic text categorization tool applies data-independent classifiers: a vector-space engine and a pattern matcher are combined to improve ranking of Medical Subject Headings (MeSH).
The first part of the study focuses on the text to MeSH categorization task.
We investigate the impact of parse quality on a syntactically-informed statistical machine translation system applied to technical text.
Treating shallow parsing as part-of-speech tag-ging yields results comparable with other, more elaborate approaches.
A learning model is introduced based on the statistical analysis of the distribution of genitives?semantic relations on a large corpus.
This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaus-tively parsing the Penn Treebank with the Treebank抯 own CFG grammar.
In this paper, we explore the effects of data fusion on First Story Detection [1] in a broadcast news domain.
Using the TDT1 evaluation methodology we evaluate a number of document representation strategies and propose reasons why our data fusion experiment shows performance improvements in the TDT domain.KeywordsLexical Chaining, Data Fusion, First Story Detection.
We examine three different types of sense clustering criteria with an Information Retrieval application in mind: methods based on the wordnet structure (such as generalization, cousins, sisters...); co-occurrence of senses obtained from Sem-cor; and equivalent translations of senses in other languages via the EuroWordNet InterLingual Index (ILI).
b) co-occurrence of senses in Semcor provide strong evidence for Information Retrieval clusters, un-like methods based on wordnet structure and systematic polysemy.
This pa-per describes our approach to detecting and re-cording adjectival meaning, compares it with the body of knowledge on adjectives in literature and presents a detailed, practically tested methodolo-gy for the acquisition of lexical entries for adjec-tives.
We describe our ongoing efforts at adaptive statistical language modeling.
We describe a succession of ME models, culminating in our current Maximum Likelihood / Maximum Entropy (ML/ME) model.
We report here empirical results of a series of studies aimed at automatically predicting information quality in news documents.
We present a cut and paste based text summa-rizer, which uses operations derived from an anal-ysis of human written abstracts.
'This paper deals with the training phase of a Markov-type linguistic model that is based on transition probabilities between pairs and triplets of syntactic categories.
Word category prediction is used to implement an accurate word recognition system.
To solve this problem, NETgram, which is the neural network for word category prediction, is proposed.
We investigate the problem of summarizing text documents that contain errors as a result of optical character recognition.
We conclude by proposing possible ways of improving the performance of noisy document summarization.
This paper deals with query translation issue in cross-language information retrieval, proper names in particular.
Models for name identification, name translation and name searching are presented.
IntroductionThis paper suggests a formulation of events and actions that seems powerful enough to define a wide range of event and action verbs in English.
He provides a classification of event verbs that includes verbs of change cgo verbs) and verbs that assert a state remaining constant over an interval of time (STAY verbs), and defines a representation of action verbs of both types•hy introducing the notion of agentive causality and permission.
We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host.
We present two techniques for combining evidence from dictionary-based and corpus-based translation lexicons, and show that backoff translation outperforms a technique based on merging lexicons.
We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words.
It consists of a statistical language model and an efficient two-pass N-best search algorithm.
'this paper describes techniques to corn-pile lexical entries in HPSG (Pollard and Sag, 1987; Pollard and Sag, 1993)-style grammar into a set of finite state au-tomata.
In this paper we present an ambiguity preserving translation approach which transfers ambiguous LFG f-structure representations.
They contain information about linguistic and lexicographic properties of words, and word combinations.
The paper studies question classification through machine learning approaches, namely, different classifiers and multiple classifier combination method.
By using compositive statistic and rule classifiers, and by introducing dependency structure from Minipar and linguistic knowledge from Wordnet into question representation, the research shows high accuracy in question classification.
Multi-word terms are traditionally identified using statistical techniques or, more recently, using hybrid techniques combining statistics with shallow linguis-tic information.
GM 1)/I PSI [hate.
In order to find a unified approach for Chinese word segmentation, the author develop a Chinese lexical analyzer PCWS using direct maximum entropy model.
We propose a unified solution to detect unknown words in Chinese texts.
We present a thorough evaluation comprising supervised and unsupervised modes, and both lexical-sample and all-words tasks.
Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation.
First, we apply self-organized document maps for mod-eling the broader subject of dis-course based on the occurrence of content words in the dialogue con-text.
The approach is based on the idea of matching certain lexicosyntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines.
Probabilistic Latent Semantic Analysis (PLSA) models have been shown to provide a better model for capturing polysemy and synonymy than Latent Semantic Analysis (LSA).
In this paper we present a method for using LSA analysis to initialize a PLSA model.
This paper describes the NLMenu System, a menu-based natural language understanding system.
In this paper a new language processor is proposed, in which unification grammar and Markov language model are integrated in a word lattice parsing algorithm based on an augmented chart, and the island-driven parsing concept is combined with various preference-first parsing strategies defined by different construction principles and decision rules.
In this paper, we study the impact of a group of features extracted automatically from machine-generated parse trees on coreference resolution.
We report initial results on the relatively novel task of automatic classification of author personality.
We explore both binary and multiple classification, using differing sets of n-gram features.
Spelling recognition is an approach to enhance a speech recognizer抯 ability to cope with incorrectly recognized words and out-of-vocabulary words.
In order to implement Thai spelling recognition, Thai alphabets and their spelling methods are analyzed.
We describe a technique for automatically constructing a taxonomy of word senses from a machine readable dictionary.
Previous taxonomies developed from dictionaries have two properties in common.
We show that hierarchies of this type can be automatically constructed, by using the semantic category codes and the subject codes of the Longman Dictionary of Contemporary English (LDOCE) to disambiguate the genus terms in noun definitions.
This paper proposes a unified evaluation method for multiple reading support systems such as a sentence translation system and a word translation system.
This paper describes our preliminary attempt to automatically recognize zero adnominals, a subgroup of zero pronouns, in Japanese discourse.
The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work.
This paper presents an unsupervised method for choosing the correct translation of a word in context.
We describe an advanced text processing system for information retrieval from natural language document collections.
Two statistical measures are computed: the measure of informational contribution of words in phrases, and the similarity measure between words.APPROXIMATE PARSING WITH TTPTTP (Tagged Text Parser) is a top down English parser specifically designed for fast, reliable processing of large amounts of text.
We examine the relationship between the two grammatical formalisms: Tree Adjoining Grammars and Head Grammars.
A speech recogniser, based on Hidden Markov Models in forced segmenta-tion mode is used to outline phone boundaries within spoken logatoms.
coreference in corpora for such systems.
We present Evita, an application for recognizing events in natural language texts.
憆ealization?
In this paper we present an efficient context-free (CF) bottom-up, non deterministic parser.
This work attempts to provide a robust Thai morphological analyzer which can automatically assign the correct part-of-speech tag to the correct word with time and space efficiency.
It consists of preference based pruning, syntactic based pruning and semantic based pruning.
We present two methods for unsupervised segmentation of words into morpheme-like units.
In the second method, Max-imum Likelihood (ML) optimization is used.
In this paper we explore the power of surface text patterns for open-domain question answering systems.
EVALING is an 'Item Banking'2 system: exercise database allowing dynamic design of questionnaires.
A modal temporal logic is developed for this purpose.
This paper presents a named entity classification system that utilises both orthographic and contextual information.
Supervised and unsupervised learning techniques used in the recombination of models to produce the final results.
A compositional account of the semantics of German prefix verbs in HPSG is outlined.
We consider only those verbs that are formed by productive synchronic rules.
We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser.
Statistical machine translation systems use a combination of one or more translation models and a language model.
Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system.
We present a word alignment procedure based on a syntactic dependency analysis of French/English parallel corpora called 揳lignment by syntactic propagation?
This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE).
We have discovered a simple and effective filter, the Relevancy Signatures Algorithm, and demonstrated its performance in the domain of terrorist event descriptions.
The Relevancy Signatures Algorithm is based on the natural language processing technique of selective concept extraction, and relies on text representations that reflect predictable patterns of linguistic context.This paper describes text classification experiments conducted in the domain of terrorism using the MUC-3 text corpus.
Our algorithm automatically derives relevancy signatures from a training corpus using selective concept extraction techniques.
In this paper, we propose a method of gen-erating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora.
The method combines morphological and lex-ical processing, bilingual word alignment, and graph-theoretic cluster generation.
We study a number of natural language decipherment problems using unsupervised learning.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
The first is the derivation of precise probability scores for partial hypotheses containing islands, in the context of a Stochastic-Context-Free-Grammar (SCFG) for Language Modeling (LM).
We are interested in the computation of Pr(th) when th is a partial interpretation of a spoken sentence generated by a Stochastic Context-Free Grammar (SCFG) G,.
Interesting island-driven parsers have been proposed by [13], [12], [6] who have also discussed the motivations for considering these parsers for ASU.
In this paper we address the issue of automatically assigning information status to discourse entities.
Using an annotated corpus of conversational English and exploiting morpho-syntactic and lexical features, we train a decision tree to classify entities introduced by noun phrases as old, mediated, or new.
The present selection primarily focuses on speech and natural language systems for speech recognition and synthesis.
We describe the head transducer model used in an experimental English-toMandarin speech translation system.
Head transduction is a translation method in which weighted finite state transducers are associated with source- target word pairs.
In clas-sification tasks, we contrast the use of text and prosodic features.
Word fragments pose serious problems for speech recognizers.
building classifiers using acoustic-prosodic features.
Chinese Pinyin in our case.
To perform the task of dynamic chat language term normalization, we extend the source channel model by incorporating the phonetic mapping models.
ProAlign combines several different approaches in order to produce high quality word word alignments.
Like EM-based methods, a probability model is used to rank possible alignments.
We describe HAMSAH (HAifa Morphological System for Analyzing Hebrew), a morphological processor for Modern Hebrew, based on finite-state linguistically motivated rules and a broad coverage lexicon.
This paper presents a corpus-based approach for deriving heuristics to locate the antecedents of relative pronouns.
This paper presents an analysis of semantic association norms for German nouns.
We subsequently used the clustering to predict noun ambiguity and to discriminate senses in our target nouns.
This article presents a new semantic-based transfer approach developed and applied within the Verbmobil Machine Translation project.
of natural language texts (Bod, 1993c).
This paper deals with automatic classification of Arabic web documents.
plus petits.
ex: eireuter.2- le verbe syntaxique entre dans tine construction syntaxique.
Section 1 describes the creation of the WFDH Web-based Frequency Dictionary of Hungarian from the raw corpus.
A set of statistical measures are used to identify significant word units in both samples.
Identification of single word terms is based on the notion of word intervals.
Two-word terms are identified through the computation of mutual information, and the extension of mutual information assists in capturing multi-word terms.
One way to address the problems of the grammar-based ap-proach is to compile recognition gram-mars from grammars written in a more expressive formalism.
We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system.
Associated or related terms.
and complex content identifiers derived from thesauruses and knowledge bases, or constructed by automatic word grouping techniques.
have therefore been proposed for text identification purposes.The area of associative content analysis and information retrieval is reviewed in this study.
MLR, an extended LA parser, is introduced, and its application to natural language parsing is discussed.
An LA parser is a shift-reduce parser which is deterministically guided by a parsing table.
A parsing table can be obtained automatically from a context- free phrase structure grammar.
Our method also provides an elegant solution to the problem of multi-part-of -speech words such as "that".
This position paper looks critically at a number of aspects of current research into spoken language translation (SLT) in the medical domain.
We focus briefly on the issue of feedback in SLT systems, pointing out the difficulties of relying on text-based paraphrases.
Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.
This paper presents an answer selection method based on Support Vector Machines (SVM) for Open-Domain Question Answering (QA).
We evaluate the performance measured by mean reciprocal rank (MRR) and the correct ratio of answer ranked first.
Using word shape tokens composed of these charactershape codes, a properly trained text tagger can extract part-of-speech information from scanned document images.
This paper introduces a bilingual MRD (English-Chinese LDOCE) to help with the construction of a Chinese WordNet.
The module for mutations is based on syntax patterns and the one for protein-protein interactions on NLP.
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar.
Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.
Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.
Chinese word segmentation and Part-ofSpeech (POS) tagging have been commonly considered as two separated tasks.
In this paper, we present a system that performs Chinese word segmentation and POS tagging simultaneously.
We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features.
We propose an approximated joint decoding method by reranking the N-best segmenter output, based POS tagging information.
The system is based on lexicons and rules from an earlier KIMMO-style two-level morphological system, reworked exten-sively using Xerox Finite-State Morphol-ogy tools.
We implement, a num-ber of both bag-of-words and word order-sensitive similarity metrics, and test each (wet' character-based and word-based indexing.
Our results indicate that char aster-based indexing is consistently.
A methodology is presented for component-based machine translation (MT) evaluation through causal error analysis to complement existing global evaluation methods.
We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners.
This paper proposes a statistical model for finding domain specific words (DSW秙) in particular domains, and thus building the association among them.
This paper proposes a mistake-driven mixture method for learning a tag model.
To well reflect the data distribution, we represent each tag model as a hierarchical tag (i.e.,NTT1 < proper noun < noun) context tree.
We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models.
We present a method for improving dependency structure analysis of Chi-nese.
Support Vector Ma-chines (SVMs) are utilized to deter-mine the word dependency relations.
We utilize the global features to solve this.
We supply the top-down information by constructing SVMs based root node finder to solve this problem.
This paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules.
A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures.
We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.
In this paper, we argue that TAG naturally supports the integration of three main ways of reducing complexity: polarity filtering, delayed adjunction and empty semantic items elimination.
We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that trans-forms speech signals through successive representa-tions of linguistic, dialogue, and domain knowledge.
This paper describes a chunk-based parser/semantic analyzer used by a language learning model.
We present the first "sizable grammar written for TAG.
This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input.
This paper describes a multilingual data extraction system under development for the Department of Defense (Dot)).
This paper presents a method for identifying token instances of verb particle constructions (VPCs) automatically, based on the output of the RASP parser.
The proposed method pools together instances of VPCs and verb-PPs from the parser output and uses the sentential context of each such instance to differentiate VPCs from verb-PPs.
We also discuss the expansion of group common nouns and group proper nouns to enhance retrieval recall.
Successful proper noun boundary identification within the part of speech tagger is essential for successful categorization.
The proper noun classification module is designed to assign a category code to each proper noun entity, using 30 categories generated from corpus analysis.
Standardization of variant proper nouns occurs at three levels of processing.
The motivation behind the approach is to expand existing methods for content based information retrieval.
in content based information retrieval.
In this paper we discuss the formal relationship between the classes of languages generated by Tree Adjoining Grammars and Head Grammars.
NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order.
We propose models for semantic orientations of phrases as well as classification methods based on the models.
This paper describes a first attempt at a sta-tistical model for simultaneous syntactic pars-ing and generalized word-sense disambigua-tion.
Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.
We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models.
To exploit this contextual feature, we propose the technique of temporalfeature modification, which takes various sources of lexical change into account, including changes in term frequency, associative strength between terms and categories, and dynamic categorization systems.
This paper presents an unsupervised approach to lexical acquisition with the goodness measure description length gain (DLG) formulated following classic information theory within the minimum description length (MDL) paradigm.
This paper describes an approach to automatic text processing which is entirely basedon syntactic form.
We present some lessons we have learned from using software infrastructure to support coursework in natural language dialogue and embodied conversational agents.
In the project "Procedural Dialogue Models" being carried on at the University of Bielefeld we have developed an incremental multilevel parsing formalism to reconstruct task-oriented dialogues.
A new technique to locate content-represent-ing words for a given document image using abstract representation of character shapes is described.
Since most previous works for 11MM-based tagging consider only part-of-speech infOrmation in contexts, their models cannot utilize lexical information which is crucial for resolving some morphological ambiguity.
The lexicalized models use a simplified back-off smoothing technique to overcome data sparseness.
We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels.
This paper shows that default-based phonologies have the potential to capture morphophonological generalisations which cannot be captured by non-default theories.
We define two main features — possessor and ref-set and discuss how the grammar handles complex syntactic co-occurrence phenomena based on this input.
-We will discuss the-semi-recursive algorithm for text generation, as defined for the GTAG formalism, and its implementation in the CLEF project.
We will show how to use lexical choice constraints and properties of the LTAG grammar to minimize the backtracking of the semi-recursive algorithm.
We describe a new sentence realization framework for text-to-text applications.
This framework uses IDL-expressions as a representation formalism, and a generation mechanism based on algorithms for intersecting IDL-expressions with probabilistic language models.
We describe a multimodal application architecture which combines finite-state multimodal language processing, a speech-act based multimodal dialogue manager, dynamic multimodal output generation, and user-tailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output.
This paper describes an approach to translate rarely occurring named entities (NE) by combining phonetic and semantic similarities.
The phonetic similarity is estimated from a surface string transliteration model, and the semantic similarity is calculated from a context vector semantic model.
This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT).
The DP-based decoding algorithms are analyzed in terms of shortest path-finding algorithms, where the similarity to DP-based decoding algorithms in speech recognition is demonstrated.
Prepositional Phrase is the key issue in structural ambiguity.
Four different attach-ments are told out according to their functionality: noun attachment, verb attachment, sentence-level attachment, and predicate-level attachment.
Although single-document summarization is a well-studied task, the nature of multi- document summarization is only beginning to be studied in detail.
Are multi-document summaries less extractive than single- document summaries?
Ad-ditional consistency knowledge is discovered by a meta-bootstrapping algorithm applied to unlabeled texts.
This paper presents a decision-tree approach to the problems of part-ofspeech disambiguation and unknown word guessing as they appear in Modem Greek, a highly inflectional language.
We incorpo-rated information on participants' social roles and genders into transfer rules and dictionary entries.
In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy.
This paper describes a method for com-piling a constraint-based grammar into a potentially more efficient form for pro-cessing.
Various informations can be used to align parallel texts at word level: co-occurrence frequencies, position difference, part-of-speech, graphic resemblance, etc.
This paper introduces a method that generates simulated multimodal input to be used in testing multimodal system implementations, as well as to build statistically motivated multi- modal integration modules.
We present a statistical phrase-based translation model that uses hierarchical phrases?phrases that contain subphrases.
In this paper I present ongoing work on the data-oriented parsing (DOP) model.
We give a novel method for parsing these words by estimating the probabilities of unknown subtrees.
This paper presents a Bayesian model for unsupervised lixtruhnz, of verb selectional preferences.
We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences.
The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality.
This paper discusses ensembles of simple but het-erogeneous classifiers for word-sense disambigua-tion, examining the Stanford-CS224N system en-tered in the SENSEVAL-2 English lexical sample task.
First-order classifiers are combined by a second-order classifier, which variously uses ma-jority voting, weighted voting, or a maximum en-tropy model.
This paper presents the construction of a manually annotated Chinese shallow Treebank, named PolyU Treebank.
Phrase-based Grammar, proposed by Peking University, is used to guide the design and implementation of the PolyU Treebank.
The design issues, including corpus material preparation, standard for word segmentation and POS tagging, and the guideline for phrase bracketing and annotation, are presented in this paper.
We present a novel classifier-based deterministic parser for Chinese constituency parsing.
Our parser computes parse trees from bottom up in one pass, and uses classifiers to make shift-reduce decisions.
Semantic relations between text concepts denote the core elements of lexical semantics.
Our approach first identifies the syntactic patterns that encode intentions, then we select syntactic and semantic features for a SVM learning classifier.
In conclusion, we discuss the application of INTENTION relations to Q&A.
This paper proposes a statistical approach to Pinyin-based Chinese input.
This approach uses a trigram-based language model and a statistically based segmentation.
Also, to deal with real input, it also includes a typing model which enables spelling correction in sentence-based Pinyin input, and a spelling model for English which enables modeless Pinyin input.
For example, topic-comment structures, the ba-constructions, the bei-constructions, relative clause constructions, appositive clause constructions, and serial verb constructions.
The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy.
This paper discusses a system of translation of locative prepositions between English and French.
This paper introduces knowledge representations of conceptualizations of objects, and a method for translating prepositions based on these conceptual representations.
In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels.
This paper presents a unified theory of verbal irony for developing a computa-tional model of irony.
We then describe and evaluate a combined alignment and classification algorithm, which achieves an F-score on alignment of .85 (using EuroWordNet) and an F-score of .80 on semantic relation classification.
database tuples).
Computational models in the form of finite state automata (FSA) also illustrate the describable regularity of German and Mandarin Chinese speech repairs in a formal way.
This paper proposes a word spacing model using a hidden Markov model (HMM) for refining Ko-rean raw text corpora.
We consider word spacing problem as a classification problem such as Part-of-Speech (POS) tagging and have experimented with var-ious models considering extended context.
An algorithm, the Forward-Backward Word-Life Algorithm, is described.
This paper presents Exills, a true e- learning solution which integrates natural language processing tools and virtual reality1.
The software module GRAPHON (GRAPHeme-PHONeme-conversion) has been developed to convert any given German text into it phonetic transcription (I.P.A.
CommandTalk combines a number of separate components integrated through the use of the Open Agent Ar-chitecture, including the Nuance speech recognition system, the Gemini natural-language parsing and interpretation sys-tem, a contextual-interpretation module, a "push-to-talk" agent, the ModSAF battle-field simulator, and "Start-It" (a graph-ical processing-spawning agent).
This paper presents a probabilistic formalization of analogical matching, and describes how this model is applied to speech translation in the framework of translation by analogy.Figure 1: Example-based Translation Architecture
In this study, the idea of ECOC is applied to memory-based language learning with local (k-nearest neighbor) classifiers.
This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations.
We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation.
We report on a series of experiments with probabilistic context-free grammars predicting English and German syllable structure.
The analysis of one of our phonotactic grammars shows that interesting phonotactic constraints are learned.
The primary technical challanges relating to spoken dialogue systems that arise in this project are speech recognition in noise, open-microphone, and recording voice annotations.
'Fins paper describes a data-driven method for hierarchical clustering of words in which a large vocabulary of Nii-glish words is clustered bottom--up, with respect to corpora ranging in size from 5 to 50 million words, using a greedy al-gorithm that tries to riiinimize average loss of mutual information of adjacent classes.
This paper compares a number of generative probability models for a wide- coverage Combinatory Categorial Grammar (CCG) parser.
They are probability, rank, and entropy.
This paper explores the problem of identifying sen-tence boundaries in the transcriptions produced by automatic speech recognition systems.
This paper describes two algorithms which construct two different types of generators for lexical functional grammars (LFGs).
The first type generates sentences from functional structures and the second from semantic structures.
The latter works on the basis of extended LFGs, which contain a mapping from f-structures into semantic structures.
In the paper we propose a black-box method for comparing the lexical coverage of MT systems.
We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.
We describe a quantitative analysis of the information gain and the information redundancy for various combinations of feature types inspired by both dependency structure and bigram structure, using a Chinese treebank and taking word prediction as the object.
From this representation, a new text is generated, using a text model and action rules.This process is done in six steps : word analysis, sentence analysis using a Functional Grammar, reference solving and inference, construction of the text pattern, sentence generation, and word generation.
It consists of lexical data, a Functional Grammar, a knowledge network, action rules for reference solving and sentence generation, models of text, rules of structuration, and sentence schema.Text representation, included in the semantic network, is composed of different kinds of objects (not necessarily distinct) : text organization, syntactical information, objects introduced by the discourse, affirmations on these objects, and links between these affirmations.
Human face-to-face conversation is an ideal model for human-computer dialogue.
We have developed an experimental system that integrates speech dialogue and facial animation, to investigate the effect of introducing communicative facial expressions as a new modality in human-computer conversation.
This paper deals with contextual aspects of locative preposition processing.
Semantic role labeling (SRL) methods typically use features from syntactic parse trees.
We propose a novel method that uses Lexicalized Tree-Adjoining Grammar (LTAG) based features for this task.
We convert parse trees into LTAG derivation trees where the semantic roles are treated as hidden information learned by supervised learning on annotated data derived from PropBank.
We extracted various features from the LTAG derivation trees and trained a discriminative decision list model to predict semantic roles.
In this paper we propose to define selectional preference and semantic similarity as information-theoretic relationships involving conceptual classes, and we demonstrate the applicability of these definitions to the resolution of syntactic ambiguity.
We model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees.
We present new results on the relation between context-free parsing strategies and their probabilistic counter-parts.
This paper presents a detailed account of prepositional mismatch between our handcrafted verb lexicon and a semantically annotated corpus.
In this paper we report about an implemented system for supporting authoring claims for patents describing apparatuses.
The basic conceptual units of our system are Phonemes-in-Context (PICs), which are represented as Hidden Matkov Models, each of which is expressed as a sequence of Phonetic Elements (PELs).
We propose a collaborative framework for collecting Thai unknown words found on Web pages over the Internet.
Our main goal is to design and construct a Web- based system which allows a group of interested users to participate in constructing a Thai unknown-word open dictionary.
Our framework includes word segmentation and morphological analysis modules for handling the non-segmenting characteristic of Thai written language.
To take advantage of large available text resource on the Web, our unknown-word boundary identification approach is based on the statistical string pattern-matching algorithm.Keywords: Unknown words, open dictionary, word segmentation, morphological analysis, word-boundary detection.
Bilingual word alignment forms the foundation of most approaches to statistical machine translation.
Current word alignment methods are predominantly based on generative models.
It then describes a hybrid extraction system based on a multilingual parser.
This paper investigates the usefulness of prosodic features in classifying rhetorical relations between utterances in meeting recordings.
We explore the use of speculative language in MEDLINE abstracts.
In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs).
We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements.
We also propose a refinement of an existing LCS dictionary.
Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases.
We are developing stylistic grammars to provide the basis for a French and English stylistic parser.
Our stylistic grammar is a branching stratificational model, built upon a foundation dealing with lexical, syntactic, and semantic stylistic realizations.
Overall, we are implementing a computational schema of stylistics in French-to-English translation.
Most statistical parsers have used the grammar induction approach, in which a stochastic grammar is induced from a treebank.
Such controllers may be stochastic; here, we focus on greedy controllers, which result in deterministic parsers.
We also describe Markov parsing models, a general framework for parser modeling and control, of which the parsers reported here are a special case.
In this pa-per we study the problem of term selection and the performance of various features for unsuper-vised text classification.
The features studied are: principal components, independent com-ponents, and non-negative components.
Lasso is a regularization method for parameter estimation in linear models.
This paper explores the use of lasso for statistical language modeling for text input.
Therefore, we investigate two approximation methods, the boosted lasso (BLasso) and the forward stagewise linear regression (FSLR).
It integrates syntactic, semantic and contextual processing serially.
The syntactic analyzer obtains rough syntactic structures from the text.
Then, the contextual analyzer obtains contextual information from the semantic structure extracted by the semantic analyzer.
Our system uses a context-free grammar parser named Extended-Lingol as a syntactic analyzer to analyze the Japanese sentences and produce parsing trees.
We propose a method for identifying diathesis alter-nations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms.
The method uses selectional pref-erences acquired as probability distributions over WordNet.
Repair processing plays an important role in spoken language processing systems.
This paper proposes a method for correcting Chinese repetition repairs and demonstrates the effects of repair processing in Chinese homophone disambiguation.
We induce categorial type assignments from a dependency treebank (Torino University treebank, TUT) and use the obtained categories with annotated dependency relations to study the distributional behavior of Italian words and reach an empirically founded part-of-speech classification.
We developed a client-server speech translation system with mobile wireless clients.
We define state transition grammars (STG) as an intermediate formalism between grammars and parsing algorithms which is intended to separate the description of a parsing strategy from the grammar formalism.
Various grammar formalisms are characterized in terms of properties of STG's.
We define an Earley parsing schema for STG's and characterize the valid parse items.
We also discuss the usabil-ity of STG's for head-corner parsing and direct parsing of sets of tree constraints.
We propose projection models that exploit lexical and syntactic information.
This is a paper that describes computational linguistic activities on Philippines languages.
VOYAGER is a speech understanding system currently under development at MIT.
This paper describes the preliminary evaluation of VOYAGER, using a spontaneous speech database that was also recently collected.
We present a new approach to extracting keyphrases based on statistical language models.
We use typed feature structures for encoding linguistic knowledge.
We show the application of this representational device for the architecture of linguistic knowledge sources for multilingual generation.
As an example, we describe the use of interacting collocational and syntactic constraints in the generation of French and German sentences.
This paper presents a framework for unsupervised natural language morphology induction wherein candidate suffixes are grouped into candidate inflection classes, which are then arranged in a lattice structure.
In this paper, we propose a statistical dialogue analysis model based on speech acts for Korean-English dialogue machine translation.
The model uses syntactic patterns and N-grams reflecting the hierarchical discourse structures of dialogues.
The syntactic pattern includes the syntactic features that are related with the language dependent expressions of speech acts.
The N-gram of speech acts based on hierarchical recency approximates the context.
We present a document compression system that uses a hierarchical noisy-channel model of text production.
Our results support the claim that discourse knowledge plays an important role in document summarization.
In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.
We then extend the model to include a probabilistic treatment of both sub- categorisation and wh-movement.
We discuss some consequence relations in DRT useful to discourse semantics.
We incorporate some consequence relations into DRT using se- writ calculi.
One is how to ensure the quality of word segmentation and Part-of-Speech (POS) tagging, because its consequence has an adverse impact on the performance of NE recognition.
We discuss the potential of KPCA for leveraging unannotated data for partially-unsupervised training to address these issues, leading to a composite model that combines both the supervised and semi-supervised models.
We present Minimum Bayes-Risk word alignment for machine translation.
This paper represents one of the first steps towards an XDG-based integrated generation architecture by tackling what is arguably the most basic among generation tasks: lexicalization.
Herein we present a constraint-based account of disjunction in lexicalization, i.e.
We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random fields (CRFs).
We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
MONA is an automata toolkit providing a compiler for compiling formulae of monadic second order logic on strings or trees into string automata or tree automata.
The paper' presents a lightweight knowledge-based reasoning framework for the JAVELIN open-domain Question Answering (QA) system.
We propose a constrained representation of text meaning, along with a flexible unification strategy that matches questions with retrieved passages based on semantic similarities and weighted relations between words.
The semantics of Montague's The proper treatment of quan-tification in ordinary English (PTO) uses an intensional modelto evaluate formulas.
Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers.
Distributional similarity requires large volumes of data to accurately represent infrequent words.
In this paper, we explore statistical language modelling for a speech-enabled MP3 player application by generating a corpus from the interpretation grammar written for the application with the Grammatical Framework (GF) (Ranta, 2004).
We present an effective training algorithm for linearly-scored dependency parsers that implements online large- margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).
Our work addresses the integration of speech recognition and language processing for whole spoken dialogue systems.To filter ill-recognized words, we design an on-line computing of word confidence scores based on the recognizer output hypothesis.
To infer as much information as possible from the retained sequence of words, we propose a bottom-up syntacticosemantic robust parsing relying on a lexicalized tree grammar and on integrated repairing strategies.
We propose a new method to resolve ambiguity in translation and meaning interpretation using linguistic statistics extracted from dual corpora of source and target languages in addition to the logical restrictions described on dictionary and grammar rules for ambiguity resolution.
This paper presents an implemented, psychologically plausible parsing model for Government Binding theory grammars.
We treat nouns that behave adjectively, which we call adjectival nouns, extracted from large corpora.
We investigate how adjectival nouns are similar to adjectives and different from non-adjectival nouns by using self-organizing semantic maps.
We create five kinds of semantic maps, i.e., semantic maps of abstract nouns organized via (1) adjectives, (2) adjectival nouns, (3) non-adjectival nouns and (4) adjectival and adjectival nouns and a semantic map of adjectives, adjectival nouns and non-adjectival nouns organized via collocated abstract nouns, and compare them with each other to find similarities and differences.
In this paper, we propose a new stochastic language model that integrates local and global constraints effectively and describe a speech recognition system based on it.
We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area.
Sentence simi-larity is determined using both surface-oriented measures and semantic groups obtained from merging the output templates of an information extraction subsystem.
We present the first stage of the discursive annotation of a corpus in Spanish.
In this paper, we describe a system to rank sus-pected answers to natural language questions.
This paper describes a media-independent, compositional, plan-based approach to representing attributive descriptions for use in integrated text and graphics generation.
This paper shows how to formally characterize language learning in a finite parameter space as a Markov structure.
We choose as a starting point the GW Triggering Learning Algorithm (TLA).
We investigate a statistical sentence generation method which recombines words to form new sentences.
The environment includes automatic generation of Grammatical Framework (GF) grammars for robust interpretation of spontaneous speech, and uses application databases to generate lexical entries and grammar rules.
The GF grammar is compiled to an ATK or Nuance language model for speech recognition.
Log-linear models provide a statistically sound framework for Stochastic "Unification-Based" Grammars (SUBGs) and stochastic versions of other kinds of grammars.
We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical- Functional Grammar.
We examine one coarse category of named entities, persons, and describe a method for automatically classifying person instances into eight finer- grained subcategories.
We present a supervised learning method that considers the local context surrounding the entity as well as more global semantic information derived from topic signatures and WordNet.
The system also provides a framework for integrating spoken language information, namely intonation and prosody, into the speech act interpretation process.
We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts.
The prevailing dual-route model of oral read-ing claims that a lexical route is used for the pronunciation of words and a non-lexical route processes nonwords.
A variation of Belief Net named as Collocation Map is used to compute the probabilities.
The Belief Net captures the conditional independences of words, which is obtained from the cooccurrence relations.
Our learning algorithms build a construction grammar language model, and generalize using form-based patterns and the learners conceptual ystem.
Model-theoretic semantics provides acomputationally attractive means of representing the semantics of natural language.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
This paper addresses the problem of large-scale acquisition of computational-semantic lexicons from machine-readable resources.
We report on our results of disambiguating the verbs in the semantic filters by adding WordNeti sense annotations.
We investigate the suitability of sub-categorization acquisition for evalu-ation of word sense disambiguation (WSD) systems.
We modify an exist-ing subcategorization acquisition sys-tem to enable it to benefit from WSD.
We present a small scale experiment with manually sense annotated data which shows that accurate WSD in-deed does improve the accuracy of the acquired subcategorization frames (SCFs).
We also applied the lexicon to the lexical choice and realization component of a practical generation application by using a multi-level feedback architecture.
The paper presents a new way of accounting for the meaning of verbs in natural languages, using a diagrammatic notation based on the Unified Modeling Language (UML).
We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.
We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering.
A new natural language system, TINA, has been developed for applications involving speech understanding tasks, which integrates key ideas from context free grammars, Augmented Transition Networks (ATN's) [1], and Lexical Functional Grammars (LFG's) [2].
This paper describes an automated system for assigning quality scores to recorded call center conversations.
The system combines speech recognition, pattern matching, and maximum entropy classification to rank calls according to their measured quality.
We also describe and compare the effectiveness of three complementary methods of signal processing for robust speech recognition: acoustical pre-processing, microphone array processing, and the use of physiologically- motivated models of peripheral signal processing.
We have built an MEMM-based tagger using this training data set and evaluated the performance of acronym generation.
9000 Japanese scientific papers.
We propose the use of Lexicalized Tree Adjoining Grammar (LTAG) as a source of features that are useful for reranking the output of a statistical parser.
This paper presents Delphi, the natural language component of the BBN Spoken Language System.
Analysis components include an agenda-based best-first parser and a fall-back component for partial understanding that works by fragment combination.
We present our approach to this back-transliteration problem based on processes such as bilingual geographic name lookup, name suggestion using place name character and pair frequencies, and confirmation via a collection of monolingual names or the WWW.
This paper describes the AT&T ATIS data collection system, with emphasis on the development of the speech-in, speech- out interaction paradigm.
This paper proposes a method for dealing with repairs in action control dialogue to resolve participants?misunderstanding.
We extend Traum抯 grounding act model by introducing degree of groundedness, and partial and mid-discourse unit grounding.
This paper has proposed a special method that produces text summary by detecting thematic areas in Chinese document.
In addition, a novel parameter, which is known as representation entropy, is used for summarization redundancy evaluation.
This paper provides a cognitive basis for anaphora resolution and focusing.
In this article we outline a basic approach to treating metonymy properly in a multilingual machine translation system.
(4) The analysis and generation components treat metonymy differently using the patterns.
Pattern-Based Machine Translation is one of the machine translation methods which performs syntactic analysis and structure transfer at the same time using bilingual pat-terns.
Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have non- compositional meanings.
In order to reduce the parameter space, we pre-processed the training corpus using a word lemmatizer and a bilingual term extraction algorithm.
This paper describes the Mandarin-English Information (MEI) project, where we investigated the problem of cross-language spoken document retrieval (CL-SDR), and developed one of the first English-Chinese CL-SDR systems.
The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase- based translation with word-by-word translation.
Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval.KeywordsCross-language, spoken document retrieval, English-Chinese
Combining a naive Bayes classifier with the EM algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction.
We present a new framework for rapid development of mixed-initiative dialog systems.
Esfinge is a general domain Portuguese question answering system.
Distinctions are established between grammatical aspect, aspectual class and the aspectual perspective of a sentence in discourse; it is shown that in English, grammatical aspect under-determines the aspectual perspective.NARRATIVESThis paper investigates the varieties of temporal knowledge and temporal reasoning that are at work in understanding extended narratives.
In this paper, we present a semantic role labeler (or chunker) that groups syntactic chunks (i.e.
This amounts to tagging syntactic chunks with semantic labels using the IOB representation.
The chunker is realized using support vector machines as oneversus-all classifiers.
In this paper, we describe MITRE抯 contribution to the logical form generation track of Senseval-3.
In this paper, we examine semantic differences in cases of paraphrase and subsumption, in an effort to understand what makes one sentence significantly more informative than another.
These properties of variant transduction arise from combining techniques for paraphrase generation, classification, and example- matching.
Another factor in selecting which approach to use in a particular situation is whether there is sufficient uncertainty to warrant the need to make educated guesses (statistical approach) rather than assertions (symbolic approach).In our work in gisting, word spotting, and topic classification, we have successfully integrated symbolic and statistical approaches in a range of tasks, including language modeling for speech recognition, information extraction from speech, and topic and event spotting.
In this paper we report on a set of compu-tational tools with (n)SGML pipeline data flow for uncovering internal structure in natural language texts.
We inves-tigate different tree-based stochas-tic models for lexical choice.
In light of these benchmarks, we then consider three possible transformations to dialogue protocols, formulated within an issue-based approach to dialogue management.
We propose a context-sensitive method to predict noun- phrases in the next utterance of a telephone inquiry dialogue.
This paper examines some of the processes creating sets of polite expressions, deictiet expressions, and compound ions phrases, which are common in telephone inquiry dialogue.
This paper presents a novel statistical model for automatic identification of English baseNP.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
Our model also integrates lexical information.
This paper discusses an approach to augmenting a lexicon for knowledge-based machine translation (KBMT) with information derived from WordNet.
Several heuristics are used to find the WordNet synonym sets corresponding to the concepts in the Mikrokosrubs language-independent ontology.
The result is a lexicon acquisition tool that produces plausible lexical mappings from English words into the Milcrokosmos ontology.
We are trying to extend the boundary of Information Extraction (IE) systems.
This paper describes efforts underway to construct a large- scale ontology to support semantic processing in the PAN- GLOSS knowledge-base machine translation system.
We therefore use phonetic recognition of utterances and search for salient phonetic sequences within the decodings.
This paper proposes a novel, corpus- based, method for producing mappings between lexical resources.
lir this paper, we propose an al-gorithm for a semi-automatic extraction of nested uninterrupted and interrupted collocations, paying particular attention to nested collocation.
Phonetic baseforms are the basic recognition units in most large vocabulary speech recognition systems.
This paper describes a series of experiments in which the phonetic baseform is deduced automatically for new words by utilizing actual utterances of the new word in conjunction with a set of automatically derived spelling-tosound rules.
We present experiments aiming at an automatic classification of Spanish verbs into lexical semantic classes.
We present a learning framework for structured support vector models in which boosting and bagging methods are used to construct ensemble models.
In this paper, we present empirical data from a corpus study on the linear order of subjects and objects in German main clauses.
Our findings suggest an influence of grammatical functions on the ordering of verb complements.
This paper describes a system in PROLOG for the automatic transformation of a grammar, written in LFG formalism, into a DCG-based parser.
This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
In this paper, we describe an efficient A* search algorithm for statistical machine translation.
This paper presents a comprehensive NLP sys-tem by Melingo that has been recently developed for Arabic, based on MorfixTm ?an operational formerly developed highly successful comprehen-sive Hebrew NLP system.The system discussed includes modules for morphological analysis, context sensitive lemmati-zation, vocalization, text-to-phoneme conversion, and syntactic-analysis-based prosody (intonation) model.
This paper shows that in the context of statistical weblog classification for splog filtering based on n-grams of tokens in the URL, further segmenting the URLs beyond the standard punctuation is helpful.
This paper examines the phenomenon of consonant spreading in Arabic sterns.
There is a mismatch between the distribution of information in text, and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees.
In this paper we introduce MULTIVOC, a real-world text-to-speech product geared to the French language.Starting from a ordinary French text, MUL-TIVOC generates in real-time a high quality speech using a synthesis-by-diphone method.
Gene and protein named-entity recognition (NER) and normalization is often treated as a two-step process.
We have built a dictionary based gene and protein NER and normalization system that requires no supervised training and no human intervention to build the dictionaries from online genomics resources.
The paper describes the results of a compari-son of two annotation systems for intonation, the tone-based ToBI approach and the tune-based approach proposed by Systemic Func-tional Grammar (sFG).
The goal of this compar-ison is to define a mapping between the two sys-tems for the purpose of concept-to-speech gen-eration of English.
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.
Automatic acquisition of paraphrase knowledge for content words is proposed.
This interface is stated in terms of Extensible Dependency Grammar (XDG), a grammar formalism we newly specify.
This generalises the concept of under- specification.
This paper details a software architecture for discourse processing in spoken dialogue systems, where the three component tasks of discourse processing are (1) Dialogue Management, (2) Context Tracking, and (3) Pragmatic Adaptation.
A motivation of this work is reusable discourse processing software for integration with non-discourse modules in spoken dialogue systems.
As there are few annotated resources that can be used to develop a good P-Name extraction system, this paper presents a bootstrapping algorithm, called PN-Finder, to tackle this problem.
The algorithm uses a combination of P-Name and context word probabilities to identify new P-Names.
The system described here is a large-vocabulary continuous-speech recognition (CSR) system with results obtained using the Wall Street Journal-based database [15].
The recognizer uses a stack decoder-based search strategy[1, 7, 14] with a left-to-right stochastic language model.
In this paper, we introduce SEMTAG, a toolbox for TAG-based parsing and generation.
In this paper, a memory-based pars-ing method is extended for han-dling compositional structures.
An approximate word-matching algorithm for Chinese is presented.
Comparing the original sentence with the optimal string, spelling error detection and correction is realized simultaneously.
This paper describes the development of QuestionBank, a corpus of 4000 parse- annotated questions for (i) use in training parsers employed in QA, and (ii) evaluation of question parsing.
We introduce a new method for recovering empty nodes and their antecedents (capturing long distance dependencies) from parser output in CFG trees using LFG f-structure reentrancies.
In summary, QuestionBank provides a useful new resource in parser-based QA research.
This paper presents a novel training algorithm for a linearly-scored block sequence translation model.
We here investigate questions with implicatures related to biography facts in a web-based QA system, PowerBio.
We compare the performances of Decision Tree, Na飗e Bayes, SVM (Support Vector Machine), and ME (Maximum Entropy) classification methods.
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
passivisation.
subordinators and verbs).
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
GermaNet and FrameNet.
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
We name the algorithm minimummean-log-spectral-distance (MMLSD).
EBMT (Example-Based Machine Translation) is proposed.
Features include: morphological variants; WordNet synonyms and hypernyms; log-likelihood-based word pairings dy-namically obtained from baseline sen-tence alignments; and formal string features such as word-based edit dis-tance.
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
LFG f-structures or HPSG feature structures, to dependency triples simple.
at Grenoble.
the construction of domain- independent lexica.
possible structural and lexical attributes.
Processing stages include lexical lookup, syntactic parsing, semantic analysis, and pragmatic analysis.
groups of words).
LFG and PATR-II.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.
We describe experiments learning quasi-synchronous context-free grammars from bitext.
Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval.
CATEGORY assigns names in hierachies.
in a prepositional phrase.
Pragmatic.
We propose a lexical organisation for multilingual lexical databases (MLDB).
-- the problem of diacritics and digraphs.
problem oriented systems.
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
phonological rules or metrical systems).
distinctive features.
unscripted) speech.
block bigram features.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
Bilingual Corpus-based Analysis.
domain-independent, semantic information for question interpretation.
morphological derivation and synonymy expansion) in web search strategies.
information technology test reports and medical finding reports.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
WordNet) to ambiguous words occurring in a syntactic dependency.
We distinguish between context-free repre-sentability and context, free processing.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
PROLOG.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
word British National Corpus.
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.
1			Back梤eferencing in text		?.
handwriting).
base forms and POS tags.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
noun phrase (NP) syntax.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects.
These features include lexical, lexico grammatical and semantic phenomena.
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
idf based method.
Dutch subordinate clauses.
For topic identification, we will outline techniques based on stereotypical text structure, cue words, high-frequency indicator phrases, intratext connectivity, and discourse structure centrality.
trigger words and parsing structures.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
WORD- NET DOMAINS).
information technology test reports and medical finding reports.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
in GPSO and HPSG.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
Associated or related terms.
We present Minimum Bayes-Risk word alignment for machine translation.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
The strategy includes parsing and phrase prediction algorithms.
The paper presents the Constructive Dialogue Model as a new approach to formulate system goals in intelligent di-alogue systems.
This paper describes a. method for positioning un-known words in an existing thesaurus by using word-to-word relationships with relation (case) markers extracted from a large corpus.
We investigate prototype-driven learning for primarily unsupervised grammar induction.
This paper investigates the automatic identification of aspects of Information Structure (IS) in texts.
We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines.
In this paper, we explain a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling.
Syllable-to-word (STW) conversion is important in Chinese phonetic input methods and speech recognition.
We define two concept-level CMs, which are on content-words and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars.
We present an approach to statistical machine translation that combines ideas from phrase-based SMT and traditional grammar-based MT.
This paper describes the Italian all-words sense disambiguation task for Senseval-3.
This paper describes a multi-lingual phrase-based Statistical Machine Translation system accessible by means of a Web page.
This paper describes SMES, an informa-tion extraction core system for real world German text processing.
This paper presents a method of resolv-ing ambiguity by using a variant of cir-cumscription, prioritized circumscrip-tion.
information retrieval, translation unit discovery, and corpus studies.
This paper describes how to construct a finite-state machine (FSM) approximating a 'unification-based' grammar using a left-corner grammar transform.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency.
This paper introduces the N-th order Ergodic Multigram TIMM for language modeling of such languages.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
We present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections.
Lastly, methods for bilingual terminology extraction and thesaurus enrichment are discussed.
We present a novel, data-driven method for integrated shallow and deep parsing.
This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.
This paper proposes an English adverb ordering method based on adverb gram-matical functions (subjuncts, adjuncts, dis-juncts and conjuncts) and meanings (pro-cess, space, time etc.
We present a named entity recognition and classification system that uses only probabilistic character-level features.
We use average mutual informaLion as global similarity metric to do classification.
The sys-tem is an application of maximum entropy clas-sification for question/answer type prediction and named entity marking.
This paper presents a word segmenta-tion system in France Telecom R&D Beijing, which uses a unified approach to word breaking and OOV identifica-tion.
We describe sign translation using example based machine translation technology.
This paper describes a comparative applica-tion of Grammar Learning by Partition Searchto four different learning tasks: deep parsing,NP identification, flat phrase chunking and NPchunking.
We compare the effect of lexical, temporal, syntactic, semantic, and prosodic features on system performance.
The paper describes a similarity-based model to present the morphological rules for Chinese com-pound nouns.
We present a discriminative, large- margin approach to feature-based matching for word alignment.
The CIAOSENSO WSD system is based on Conceptual Density, WordNet Domains and frequences of WordNet senses.
We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task.
It also facilitates dependency-based evaluation of phrase structure parsers.
This paper presents the user-centered interface of a summarization system for physicians in Bone Marrow Transplan-tation (BMT).
the complexity of language domain and concept inventory).
We introduce CST (cross-document structure theory), a paradigm for multi-document analysis.
A dynamicthreshold using time decay function and spanning window is proposed.
We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases.
Systems to facilitate parsing and structural transfer.
In this paper we present Morphy, an integrated tool for German morphology, part-ofspeech tagging and context-sensitive lemmatization.
The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verb- final clauses.
Using k-NN, na飗e Bayes and centroidbased classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications.
This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG pars-ing system.
First we map verb tokens in sentential contexts to a fixed set of seed verbs using WordNet :: Similarity and Moby抯 Thesaurus.
It uses n-gram mutual information, relative frequency count and parts of speech as the features for compound extraction.
which consist of rules and metarules.
relative clauses.
passivisation.
and questions.
The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences.
We present a method for corpus-based induction of an LFG syntax-semantics interface for frame semantic processing in a computational LFG parsing architecture.
We evaluate the results by applying the inducedframe assignment rules to LFG parser output.'
We use LFG抯 functional representations to distinguish local and non-local role assignments.
Each parser consists of a finite state control that drives the moves of a Bottom-up Embedded Pushdown Automaton.
The domain- specific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication.
Our method of capturing ungrammaticalities involves using malrules (also called 'error productions/.
a tagset containing information about wordclasses.
The maximum entropy classifier is trained to identify and classify the predicates?semantic arguments together.
Lastly we show that patterns of information exchanges in speaker alternation and initiative-taking can be used to characterise three-party dialogues.
This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech.
We propose a statistical dialogue analysis model to determine discourse structures as well as speech acts using maximum entropy model.
The design and implementation of an XML-based corpus environment for multilevel anno-tated multimodal (language) data is described.
subordinators and verbs).
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
Like most existing approaches it utilizes clustering of word co-occurrences.
In SPHINX-II, we incorporated additional dynamic and speaker-normalized features, replaced discrete models with sex-dependent semi-continuous hidden Markov models, augmented within-word triphones with between-word triphones, and extended generalized triphone models to shared- distribution models.
The configuration of SPHINX-II being used for this task includes sex-dependent, semi-continuous, shared- distribution hidden Markov models and left context dependent between-word triphones.
This paper studies the enrichment of Spanish WordNet with synset glosses automatically obtained from the English Word- Net glosses using a phrase-based Statistical Machine Translation system.
This paper presents a framework for the definition of monotonic repair rules on chart items and Lexicalized Tree Grammars.
We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn `missing' grammar rules from an incomplete grammar.
The system uses core technologies such as speaker segmentation, automatic speech recognition, transcription alignment, keyword extraction and speech indexing and retrieval to make spoken communications easy to navigate.
A language-independent method of finite- state surface syntactic parsing and word-disambiguation is discussed.
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
a noun in a noun phrase).
We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference.
mantics of Combinatory Categorial Grammar, including its handling of coordination constructs.
This paper proposes the automatic generation of Fill-in-the-Blank Questions (FBQs) together with testing based on Item Response Theory (IRT) to measure English proficiency.
GermaNet and FrameNet.
We discuss maximum a posteriori estimation of continuous density hidden Markov models (CDHMM).
We present an unsupervised system that exploits linguistic knowledge resources, namely English and German lexical databases and the World Wide Web, to identify English inclusions in German text.
We then apply our method to two complementary tasks: information ordering and extractive summarization.
We investigated the applicability of probabilistic context-free grammars to syllabification and grapheme-to-phoneme conversion.
the language used in official encyclopaedic articles.
We extracted meaningful bigrams using an evaluation function and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers.
We view a modeling problem for n-gram collocations as that for clustering of cohesive words.
This paper reports results on automatically training a Problematic Dialogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain.
directly stored relations.
A semantic net needs proper representation of lexical gaps.
In this paper we describe an approach to coherent sentence ordering for summarizing newspaper articles.
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).
This paper describes improved HMM-based word level alignment models for statistical machine translation.
We investigate several voting- and arbiter- based combination strategies over a diverse pool of unsupervised WSD systems.
In this paper, we compare the rela-tive effects of segment order, segmen-tation and segment contiguity on the retrieval performance of a translation memory system.
This paper presents a method that as-sists in maintaining a rule-based named-entity recognition and classifi-cation system.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We introduce a character-based chunking for unknown word identification in Japanese text.
The method is built upon SVM-based chunking, by use of character n-gram and surrounding context of n-best word segmentation candidates from statistical morphological analysis as features.
The paper deals with generation of natural language text in a dialog system.
The model is based on the Factorial Hidden Markov Model (FHMM) with distributed hidden states representing partof-speech and noun phrase sequences.
We describe a method for interpreting abstract flat syntactic representations, LFG f- structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).
QACIAD (Question Answering Challenge for Information Access Dialogue) is an evaluation framework for measuring interactive question answering (QA) technologies.
We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs).
We present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue.
basic segmentation, named entity recognition, error-driven learner and new word detector.
We represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques.
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical- Functional Grammar (LFG).
The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components.
MaxEnt includes external gazetteers in the system.
off-line) material in foreign languages.
Caseframe parsers employ both semantic and syntactic knowledge.
We describe a corpus-based investigation of proposals in dialogue.
The algorithm combines four original alignment models based on relative corpus frequency, con-textual similarity, weighted string simi-larity and incrementally retrained inflec-tional transduction probabilities.
audio recordings containing spoken text.
Collocational knowledge is necessary for language generation.
We developed automatic corpus-based K-TOBI labeling tools and prediction methods based on several lexicosyntactic linguistic features for decision-tree induction.
The type of local dependencies considered are sequences of part of speech categories for words.
We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank.
We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG.
We name the algorithm minimummean-log-spectral-distance (MMLSD).
We then describe a head-driven chart parser for lexicalized SFG.
These enhancements include function-phrase modeling, between-word coarticulation modeling, and corrective training.
The knowledge sources used were partof-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
EBMT (Example-Based Machine Translation) is proposed.
Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays.
Features include: morphological variants; WordNet synonyms and hypernyms; log-likelihood-based word pairings dy-namically obtained from baseline sen-tence alignments; and formal string features such as word-based edit dis-tance.
The model is an m-component mixture of Ingram models.
We describe SmartMail, a prototype system for automatically identifying action items (tasks) in email messages.
Spelling out means making explicit.
We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences.
We have proposed an incremental trans-lation method in Transfer-Driven Ma-chine Translation (TDMT).
This paper assesses two new approaches to deterministic parsing with respect to the analysis of unbounded dependencies (UDs).
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
all bills only.
We introduce feature terms containing sorts, variables, negation and named disjunction for the specification of feature structures.
and lean declarative impIe.mentation.
LFG f-structures or HPSG feature structures, to dependency triples simple.
In Online Inquiry-Based Learning (OIBL) learners search for information to answer driving questions.
Three N-gram models, a baseline model (Bound-length N-gram) and two new models (Chunk-bound N- gram and Dependency-linked N-gram) are compared.
This paper presents our work on real-izing expressions of doubt appropriately in natural language dialogues.
Non-fiction	11.
Fiction	K. General Fiction	I.
(WSJ) speech corpus.
It takes into account the effects of linguistic features such as tense/aspect, temporal connectives, and discourse structures.
We apply the supervised decision list learn-ing method to Japanese named entity recogni-tion.
This paper describes the Lycos Retriever system, a deployed system for automatically generating coherent topical summaries of popular web query topics.
Core Roles versus Adjuncts).
We provide an XML serialization for intercomponent communication.
A number of operational models are introduced, with web interfaces for lexical databases, DFSA matrices, finite- state phonotactics development, and DATR lexica.
Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in- sequence n-grams automatically.
The second method relaxes strict n-gram matching to skipbigram matching.
Skip-bigram co- occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
We explore unsupervised language model adaptation techniques for Statistical Machine  Translation.
We propose an approach of combining lexical information, web statistics, and inverse search based on Google to backward translate a Chinese named entity (NE) into English.
The classifier presented uses part of speech tagging, cascaded finite state transducers, and simple disambiguation rules.
We apply default inheritance hierarchies to generating the morphology of Hebrew verbs.
The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics.
To disambiguate homograph, several efficient approaches have been proposed such as part-of-speech (POS) n-gram, Bayesian classifier, decision tree, and Bayesian-hybrid approaches.
Among the types of ambiguities handled are prepositional phrases, very compound nominals, adverbials, relative clauses, and preposed prepositional phrases.
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.
a local area network).
This paper presents an approach for processing incomplete and inconsistent knowledge.
The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system.
The context in which a target word occurs is represented by surface lexical features such as unigrams, bigrams, and second order co-occurrences.
valid prefixes.
We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.
Several grammars have been proposed for modeling RNA pseudoknotted structure.
We present ongoing work on prosody predic-tion for speech synthesis.
This paper introduces a grammar formalism specifically designed for syntax-based statistical machine translation.
This paper presents a Unicode based Chinese word segmentor.
at Grenoble.
Achieving this goal requires identification of not only the different topics in the documents but also of the particular flow of these topics.Our approach to content similarity evaluation employs n-grams of lexical chains and measures similarity using the cosine of vectors of n-grams of lexical chains, vectors of tf*idf-weighted keywords, and vectors of unweighted lexical chains (unigrams of lexical chains).
Generation procedure in SEMSYNThis section summarizes the SEMSYN genration procedure.
We learn models of answer type, query content, and answer extraction from clusters of similar questions.
We examine the utility of speech and lexical features for predicting student emotions in computer- human spoken tutoring dialogues.
generate non_grammatical sent_ ences.
This paper designs a novel lexical hub to disambiguate word sense, using both syntagmatic and paradigmatic relations of words.
We describe the use of energy function op-timisation in very shallow syntactic pars-ing.
In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.
the construction of domain- independent lexica.
The computational model, called Augmented Dependency Grammar (ADG), formulates not only the linguistic dependency structure of sentences but also the semantic dependency structure using the extended deep case grammar and field-oriented fact-knowledge based inferences.
This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules.
The phrase-break detector assigns phrase breaks using part-of-speech (POS) information.
We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions.
The NER system uses a hybrid algorithm based on Class-based language model and rule-based knowledge.
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classification (maximum entropy) with linguistic information.
This suite comprises several modules, namely: a sentence chunker, a tokenizer, a POS tagger, featurizers and lemmatizers.
We further show that topic translation with online machine translation resources yields effective CL-SR.
Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification- based grammar (UBG).
1 IntroductionStochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar.
This position paper contrasts rhetorical structuring of propositions with intentional decomposition using communicative acts.
Modules include a syllabification program, a fast tnorphological parser, a lexical database, a phonological knowledge base, transliteration rules, and phonological rules.
We evaluate both syntactic structure and case structure.
We discuss a grammar development process used to generate the trees of the wide-coverage Lexicalized Tree Adjoining Grammar (LTAG) for English of the XTAG Project.
This paper examines the use of an unsupervised statistical model for determining the attachment of ambiguous coordinate phrases (CP) of the form n1 p n2 cc n3.
Collocation-based tagging and bracketing pro- grains have attained promising results.
We present a new chart parsing method for Lambek grammars, inspired by a method for D- Tree grammar parsing.
possible structural and lexical attributes.
some kind of character-stream.
First, the pre-processing modules for tokenisation, sentence splitting, paragraph segmentation, partof-speech tagging, clause chunking and noun phrase extraction are outlined.
Our algorithm generates lists of non- anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.
We also investigated clustering of output documents from term level retrieval.
We evaluate Basilisk on six semantic categories.
We present an overview of TARSQI, a modular system for automatic temporal annotation that adds time expressions, events and temporal relations to news texts.
Theories of discourse structure hypothesize a hierarchical structure of discourse segments, typically tree-structured.
In this paper, we explore prosodic cues to discourse segmentation in human- computer dialogue.
This paper explores the segmentation of tutorial dialogue into cohesive topics.
DTG involve two composition operations called subsertion and sister-adjunction.
Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours.
In this paper, we consider a number of algo-rithms for estimating the parameters of ME mod-els, including iterative scaling, gradient ascent, con-jugate gradient, and variable metric methods.
This paper proposes a new error-driven HM3/1- based text chunk tagger with context-dependent lexicon.
Processing stages include lexical lookup, syntactic parsing, semantic analysis, and pragmatic analysis.
A deductive approach is used to predict vowel and consonant places of articulation.
The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm.
SenseClusters is a freely available word sense discrimination system that takes a purely unsupervised clustering approach.
time, causality.
This paper describes the system MC-WSD presented for the English Lexical Sample task.
Onto these structures, phonological rules are applied such as the "letter梩osound" rules, automatic word stress rules,internal stress hierarchy rules indicating secondary stress,external sandhi rules, phonological focus assignment rules, logical focus assignment rules.
This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora.
We also char-acterize part-of-speech sequences that play a role in detecting non-native speech.
This paper presents a novel method for unsupervised word sense disam-biguation, which combines multiple in-formation sources, including seman-tic relations, large unlabeled corpora, and cross-lingual distributional statis-tics.
This paper presents a human梞achine dialogue model in the field of task梠riented dialogues.
This paper presents the results of an experiment using machine-readable dictionaries (MRDs) and corpora for building concatenative units for text to speech (ITS) systems.
groups of words).
The methods include Linear Discriminant Analysis, Supervised Vector Quantization, Shared Mixture VQ, Deleted Estimation of Context Weights, NMI Estimation Using "N-Best" Alternatives, Cross- Word Triphone Models.
shows suds a system for retrieving a Japanese dictionary.
We compare vectors containing counts of trigrams of part-of-speech (POS) tags in order to obtain an aggregate measure of syntax difference.
Central to this WSD framework is the sense division and semantic relations based on topical analysis of dictionary sense definitions.
LFG and PATR-II.
Most question answering (QA) systems rely on both keyword index and Named Entity (NE) tagging.
Data-Oriented Translation (DOT), based on Data- Oriented Parsing (DOP), is a language-independent MT engine which exploits parsed, aligned bitexts to produce very high quality translations.
This paper describes a Chinese word segmentation system based on unigram language model for resolving segmen-tation ambiguities.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented.
We explore the relationship between question answering and constraint relaxation in spoken dialog systems.
We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.
Collocation map is a sigmoid belief network that can be constructed from bigrams.
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
OF COLING-92.
The present study deals with conflict resolution process in metaphorical interpretation for the noun phrase.
A co- occurrence pattern matrix with semantic categories is built based on these WCP.
1 Concept of collocational analysis770
ARGUMENTATION AND THE SEMANTIC PROGRAM.
For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels).
We introduce the bilingual dual-coding theory as a model for bilingual mental representation.
In this paper, a sememe co-occurrence frequency based WSD method was introduced.
In this paper, we compare and contrast IDF with inverse sentence frequency (ISF) and inverse term frequency (ITF).
We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
The system is built around two separate neural network methodologies: context vectors and self organizing maps.
We have coupled the parser LCFLEX with a lexicon and ontology derived from two lexical resources, Verb- Net for verbs and CoreLex for nouns.
The other components are a selective unigram cache, a conditional bigram cache, and a conventional static trigram.
We investigate the effects of lexicon size and stopwords on Chinese information retrieval using our method of short-word segmentation based on simple language usage rules and statistics.
Association for Computational Linguistics.
Both models combine lexical, syntactic, and prosodic information.
This paper proposes the 揌ierarchical Directed Acyclic Graph (HDAG) Kernel?for structured natural language data.
Most statistical machine translation systems employ a word-based alignment model.
Linear algebraic technique called LSA/SVD is used to find co-relationships of sparse words.
We propose a method of using associative	strength	among	morphemes,morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech.
The model integrates information from a variety of sources, such as word co-occurrence, lexical chains, cue phrases, punctuation, and tense.
Among them are parallel multiple context-free grammars (pmcfg's) and lexical-functional grammars (lfg's).
Next, a method for natural language transfer is presented that integrates translation examples (represented as typed feature structures with source-target indices) with linguistic rules and constraints.
We present a novel disambiguation method for unification-based grammars (UBGs).
We extended language modeling approaches in information retrieval (IR) to combine collaborative filtering (CF) and content-based filtering (CBF).
We define POS blocks to be groups of parts of speech.
We focus on the production of efficient descriptions of objects, actions and events.
We take into account synonymy and hyperonymy.
The basic compo-nents include basic segmentation, factoid recognition, and named entity recognition.
This paper presents an unsupervised topic identification method integrating linguistic and visual information based on Hidden Markov Models (HMMs).
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change.
We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment.
This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver.
We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG).
We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We look at self-triggerability across hyperlinks on the Web.
We adopt a probabilistic parsing strategy to provide a hierarchical lexical analysis of a word, including information such as morphology, stress, syllabification, phonemics and graphemics.
We measure phonetic (un)relatedness between dialects using Levenshtein distance, and classify by clustering distances but also by analysis through multidimensional scaling.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
We discuss shift- reduce parsing of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses.
keywords: Multilingual generation, lexical choice, controlled languages.
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.
This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.
We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
We describe a biographical multi-document summarizer that summarizes information about people described in the news.
This paper proposes an automatic in-terpretation system that integrates free-style sentence translation and parallel text based translation.
This paper introduces an open computational framework for visual perception and grounded language acquisition called Experience-Based Language Acquisition (EBLA).
The algorithms included in this study are Hidden Markov Model, Maximum Entropy, Memory-Based Learning, and Transformation-Based Learning.
This paper investigates the potential for projectinglinguistic annotations including part-of-speech tagsand base noun phrase bracketings from one languageto another via automatically word-aligned parallelcorpora.
These features are derived from algorithms including automatic speech recognition, automatic speech indexing, speaker identification, prosodic and audio feature extraction.
We describe experiments learning quasi-synchronous context-free grammars from bitext.
We discuss a tagging schema and a tagging tool for labeling the rhetorical structure of texts.
This paper presents a semantic model for Chinese garden-path sentences.
This paper presents and analyzes an incremental	algorithm	for	theconstruction of Acyclic Non-deterministic Finite-state Automata (NFA).
This paper describes text meaning represen-tation for Chinese.
Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.
We present a domain-independent topic segmentation algorithm for multi-party speech.
One of the most active and promising areas of statistical machine translation (SMT) research are tree-based SMT approaches.
In this approach we integrate a symbolic se-mantic segmentation parser with a learn-ing dialog act network.
These contain a range of temporal constructions, including time adverbials, progressive aspect and various aspectual classes.
Structural (attachment.)
The method predicts unknown words by recursively extracting common character strings.
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
We investigate generalizations of the allsubtrees "DOP" approach to unsupervised parsing.
theme/rheme and background/kontrast.
Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval.
without inflections or conjugations.
Then a linear- unification parser for English syllables is introduced.
This constrained context-free model is specified by a stochastic context-free prior distribution with N-gram frequency constraints.
We show how idioms can be parsed in lexicalized TAGs.
The HMM-LR algorithm uses a generalized LR parser as a language model and hidden Markov models (HMMs) as phoneme models.
We propose a method for semi-automatic classi-fication of verbs to Levin classes via the seman-tic network of WordNet.
We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.
the Italian wordnet in EuroWordNet (ItalWordNet).
First, extending word similarity measures from direct co-occurrences to co- occurrences of co-occurred words, we compute the word similarities using not co-occurred words but co- occurred clusters.
We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags.
We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support user- directed multidocument summarization.
This paper presents a disambiguation approach for translating non-segmented-Kana into Kanji.
Unsupervised ontology discovery is a key component.
We present a treebank conversion method by which we construct an RMRS bank for HPSG parser evalu-ation from the TIGER Dependency Bank.
These rules are noun sequences with part-of-speech tags.
We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization.
The HKUST word sense disambiguation systems benefit from a new nonlinear Kernel Principal Component Analysis (KPCA) based disambiguation technique.
Transformation-based learning is chosen to predict recognition errors by integrating word confidence scores with linguistic features.
POS-tagging, lemmatisation and a thesaurus-like thematic dictionary.
We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures.
sequences and simplified part-of-speech tem-plates in identifying their syntactic categories.
The language analysis strategy used by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing.
the event profile.
Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness.
In this paper, we explore facets of instructional texts: general prototypical structures, rhetorical structure and natural argumentation.
A rule-based approach uses caseframes and sense classes.
This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.
We present the results of experiments aimed at assigning domains to speech recognition hypotheses (SRH).
This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences.
Graph unification is the most expensive part of unification-based grammar parsing.
This paper describes an attribute grammar specification of the Government-binding theory.
Grammars for parsing have predominantly used generative rewrite rules.
CATEGORY assigns names in hierachies.
We present an integrated architecture for word-level and sentence-level processing in a unification-based paradigm.
In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented.
in a prepositional phrase.
This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, part-of-speech and a small-vocabularycharacter-lists feature for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model.
Our system performs two procedures: Out-of-vocabulary extraction and word segmenta-tion.
We compose three out-of-vocabulary extraction modules: Character-based tag-ging with different classifiers ?maximum entropy, support vector machines, and con-ditional random fields.
We also com-pose three word segmentation modules ?character-based tagging by maximum en-tropy classifier, maximum entropy markov model, and conditional random fields.
It adopts dependency decision making and example-based approaches.
It labels semantic roles of parsed sentences.
We are concerned with dependency- oriented morphosyntactic parsing of running text.
In this paper we discuss cascaded Memory- Based grammatical relations assignment.
Pragmatic.
The paper proposes an information-theory-based method for feature types analysis in probabilistic evaluation modelling for statistical parsing.
of deep cases relations (or thematic relations).
In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci.
Language processing of the corpus texts so far included morpho-syntactic analysis, POS tagging and shallow syntactic parsing.
We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text.
Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling.
This paper describes a prototype for automatically scoring College Board Advanced Placement (AP) Biology essays.'.
This paper describes a new templaterepresentation and generalization method.
This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs).
We propose a lexical organisation for multilingual lexical databases (MLDB).
This paper then applies the graph-structured stack to various natural language parsing methods, including ATN, LA parsing, categorial grammar and principle- based parsing.
-- the problem of diacritics and digraphs.
We present a scheme for efficiently representing a lexicalized tree-adjoining grammar (LTAG).
We describe the CoNLL-2002 shared task: language-independent named entity recogni-tion.
This paper presents an LTAG account for binding of reflexives and reciprocals in English.
In this paper, we propose ellipsis handling method for follow-up questions in Information Access Dialogue (IAD) task of NTCIR QAC3.
We report here the contribution of default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, selectional preferences, and topics or subject fields.
In the GPSG framework.
We show the retrieval examples with the following characteristic features: phrasal expression, long-distance dependency, idiom, synonym, and semantic ambiguity.
This paper presents Archivus, a multi- modal language-enabled meeting browsing and retrieval system.
Approaches are wide and include key wording, statistical analysis, pattern matching, and a method using lexical, syntactic, and semantic filters.
This paper discusses a system for grammat-ically describing and parsing entries from machine-readable dictionary tapes and a lexical data base representation for storing the dictionary information.
The paper also proposes a combinatorial and dynamic organization for lexical systems, which is based on the notion of virtual accumulation and the abstraction levels of lexical sets.Keywords: dictionary resources, lexicalacquisition,	lexical	production,	lexicalaccumulation, computational lexicography.
We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic.
This paper presents LexPhon, a computational framework for modelling segmental aspects of Lexical Phonology (LP).
It features automatic interface generation and self-organization.
The heart of the system is a rule-based top-down DCG-style parser, which uses an LFG oriented grammar organization.
We report results for training and test-ing an automatic classifier to label the in-formation provider抯 utterances in spoken human-computer and human-human dia-logues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.
We introduce a first-order language for semantic underspecification that we call Constraint Language for Lambda-Structures (CLLS).
This paper proposes a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse.
We present a new compositional tense-aspect deindexing mechanism that makes use of tense trees as components of discourse contexts.
This paper studies and evaluates disambiguation strategies for the translation of tense between German and English, using a bilingual corpus of appointment scheduling dialogues.
This paper investigates stacking and voting methods for combining strong classifiers like boosting, SVM, and TBL, on the named-entity recognition task.
We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes.
This paper describes the open-source Phrase-Based Statistical Machine Translation Decoder - Phramer.
problem oriented systems.
domain.
the grammatical morphemes  of the language.
RULES  normalized sequence of categoriesPARSERinput sentence [MORPHOLOGICAL ANALYZER.
We describe the generation of communicative ac-tions in an implemented embodied conversational agent.
This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs).
This paper extends the base noun phrase(BNP) identification into a research on Chinese base phrase identification.
The paper describes a new unification based grammar formalism called Regular Unification Grammar (RUG).
It con-sists of an approximate word match-ing method and an N-best word seg-mentation algorithm using a statistical language model.
We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG.
ASL natural language generation (NLG) is a special form of multimodal NLG that uses multiple linguistic output channels.
A Chinese word segmentation algorithm based on forward maximum matching and word binding force is proposed in this paper.
PARSE GRAPH STRUCTUREThe Parse Graph Structure (PGS) is built by the parser while applying grammar rules.
are described.Keywords: Computational Lexicography, Lexical Knowledge Base, Lexical Semantics.
Two IBL algorithms are utilized: k-Nearest Neighbor (kNN), and Priority Maximum Likelihood (PML) with a modified back-off combination method.
In this paper we present an overview of recent developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic theory of discourse structure.
They are morphological transformation and morpheme Identification.
This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling.
This paper describes an operational se-mantics for DATR theories.
In this paper we present a polynomial time parsing algorithm for Combinatory Categorial Grammar.
The method exploits patterns of non-transitivity in translations across multiple languages.
A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector.
3.3 GB ytes of text.
We consider here the problem of Chinese named entity (NE) identification using statistical language model(LM).
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
This paper presents a generalized unknown morpheme handling method with POSTAG(POStech TAGger) which is a statistical/rule based hybrid POS tagging system.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined.
We present a memory-based learning (MBL) approach to shallow parsing in which POS tagging, chunking, and identification of syntactic relations are formulated as memory-based modules.
We discuss the relevance of our method for linguis-tics and language technology.
This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
phonological rules or metrical systems).
Our environment accepts grammars consisting of binary dependency relations andgrammatical functions.
We first show that non-reentrant unification grammars generate exactly the class of context- free languages.
distinctive features.
This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and T╱Ba-D/Z tree- banks.
Synchronous Tree Adjoining Grammars can be used for Machine Translation.
We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-the- art constituency-based parsers.
He thus formulated aPreferred Argument Structure (PAS) for the preferential structural configurations of arguments.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic para-phrases.
We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
Many linguistse.
This paper presents a new view of Explanation-Based Learning (EBL) of natural language parsing.
We present algorithms for both hard constraints and binary soft constraints.
This demo abstract describes the SmartWeb Ontology-based Annotation system (SOBA).
The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill抯 rule-based part-of-speech tagger.
This paper presents Trace & Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (uG) and ideas of Government & Binding Theory (GB) in an undogmatic way.
We analyze the association of noun-noun and noun-verb pairs in LOB Corpus.
This paper presents a method for inducing transla-tion lexicons based on transduction models of cog-nate pairs via bridge languages.
We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.
This paper presents a method for parsing associative Lambek grammars based on graph- theoretic properties.
Most of the previous Korean noun extraction systems use a morphological analyzer or a Partof-Speech (POS) tagger.
(TSR trees).
We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is defined and basic algorithms for SLTAG are designed.
unscripted) speech.
In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT).
block bigram features.
SEM encodes logical semantics.
This paper describes an algorithm for unifying disjunctive feature structures.
We propose a distribution-based pruning of n-gram backoff language models.
In this paper, we look at comparing high- accuracy context-free parsers with high- accuracy finite-state (shallow) parsers on several shallow parsing tasks.
This paper presents a reestimation algorithm and a best-first parsing (BFP) algorithm for probabilistic dependency grammars (PDC).
This paper presents a unified approach to parsing, in which top-down, bottom- up and left-corner parsers are related to preorder, postorder and inorder tree traversals.
In this paper we present a logical treatment of semi- free word order and bounded discontinuous constituency.
This permits a natural interpretation of implicational universals in terms of theories, subtheories and implicational axioms.
The dictionary is parsed using a head-driven phrase structure grammar of Japanese.
This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a natural language interface to database query ap-plications.
We define noun phrase translation as a subtask of machine translation.
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.
Graph unification remains the most expensive part of unification-based grammar parsing.
This method focuses on semantic and pragmatic constraints such as semantic constraints on cases, modal expressions, verbal semantic attributes and conjunc-tions to determine the deictic reference of Japanese zero pronouns.
the processing objectYenee: to	sentences .
This paper presents an algorithm for text summarization using the the-matic hierarchy of a text.
This paper discusses an approach to incremental learning in natural language processing.
A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors.
In this paper we describe how to hierarchically organize abstract nouns (adjective hyperonyms) in a semantic map mainly using CSM.
First, we introduce the 揹ependency tree path?(DTP).
To resolve the problem, we propose "term distillation", a framework for query term selection in cross-database retrieval.
This study addresses pronominal anaphora resolution, including zero pronouns, in Chinese.
We describe an approach to tagging a monolingual dictionary with linguistic features.
In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus.
statistical lexical head- outward transducers.
We present a corpus-based study of the sequential ordering among premodifiers in noun phrases.
LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification).
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications.
We coerce the English FrameNet into Chinese using a bilingual lexicon, frame context in FrameNet and taxonomy structure in HowNet.
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
This paper explores the extent to which phoneme sequence constraintz can be used to identify word boundaries in continuous speech recognition.
This paper describes the tree-structured maximum mutual information (MMI) encoders used in SSI's Phonetic Engine?to perform large-vocabulary, continuous speech recognition.
These templates represent grammatically correct sentence patterns.
The other is the CKY algorithm for probabilistic context free grammars.
We present the concepts of weighted language, transduction and automaton from algebraic automata theory as a general framework for describing and implementing decoding cascades in speech and language processing.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
Further, we have developed MM-DCG translator to transfer rules in MM-DCG into Prolog predicates.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
relation based sim-ilarity measure and distribution based similarity measure.
A LOGICAL SEMANTICSFOR NONMONOTONIC SORTSMark A.
In this paper, we present a logical basis for NSs and non- monotonically sorted feature structures (NSFSs).
We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic finite automaton.
Natural language parsing requires ex-tensive lexicons containing subcategori-sation information for specific sublan-guages.
This paper extends earlier work on the relation between syntax and intonation in language understanding in Combinatory Categorial Grammar (CCG).
Motion Verbs.
In this paper, we report on data for movement verbs (or motion verbs).
Bilingual Corpus-based Analysis.
This paper discusses the partitive-genitive case alternation of Finnish adpositions.
This paper proposes an efficient example selection method for example-based word sense disambiguation systems.
The first method uses a standard HMM part-of-speech tagger with variable context length.
We address the representation of nouns having complex argument structures like deverbal nominalisations.
This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon.
Thematic knowledge is a basis of semantic interpretation.
Current NER approaches include: dictionary-based, rule-based, or machine learning.
We present discriminative reordering models for phrase-based statistical machine translation.
In this paper we describe EFLUF - an implementation of FLUF.
We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses.
This paper describes using RDF/RDFS/XML to create and navigate a metadata model of relationships among entities in text.
domain-independent, semantic information for question interpretation.
In this paper I outline Type-inheritance Combinatory Categorial Grammar (TCCG), an implemented feature structure based CCG fragment of English.
TCCG combines the fully lexical nature of CCG with the type-inheritance hierarchies and complex feature structures of Head- driven Phrase Structure Grammars (HPSG).
We present a method for compiling grammars into efficient code for head-driven generation in ALE.
Memory-based learning is a form of supervised learning based on similarity-based reasoning.
We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text.
We apply our translit-eration algorithm to the transliteration of names from Arabic into English.
This paper proposes a discriminative HMM (DHMM) with long state dependence (LSDDHMM) to segment and label sequential data.
This paper presents a genetic algorithm based approach to the automatic discovery of finite- state automata (FSAs) from positive data.
We use dependency grammar and employ a stack based shift/ reduce context梔ependent parser as the tagging mechanism.
In this paper, we explore how the taxo-nomic inheritance hierarchy in a seman-tic net can contribute to the resolution of associative anaphoric expressions.
This paper constitutes an investigation into the generative capabilities of two-level phonology with respect to unilevel generative phonological rules.
A hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structures for semantic role labeling (SRL).
This part-ofspeech predictor will be used in a part-of-speech tagger to handle out-of-lexicon words.
This paper offers a provisional mathematical typology of metrical representations.
This paper describes a system for the un-supervised learning of morphological suf-fixes and stems from word lists.
This paper describes discriminative language modeling for a large vocabulary speech recognition task.
Rohrer.
We describe a bidirectional framework for natural language parsing and genera-tion, using a typed feature formalism arid an HPSG-based grammar with a parser arid generator derived from parallel pro-cessing algorithms.
its aggregation and referring expression generation capability).
This paper describes the use of a system of semantic rules to generate noun compounds, vague or polysemous words, and cases of metonymy.
We have implemented an interactive, Web-based, chat-style machine translation system, supporting speech recognition and synthesis, local- or third-party correction of speech recognition and machine translation output, and online learning.
In this paper we describe the Senseval 3 Basque lexical sample task.
morphological derivation and synonymy expansion) in web search strategies.
This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG).
This paper describes an all level approach on statistical natural language translation (SNLT).
It is natural to combine a head-driven HPSG grammar with a head- driven generation algorithm.
OF COLING-92, NANTES, Atm.
information technology test reports and medical finding reports.
We investigate independent and relevant event-based extractive mutli-document summarization approaches.
Two main problems in natural language generation are lexical selection and syntactic structure determination.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
verb-object pairs) from text corpora automatically.
WordNet) to ambiguous words occurring in a syntactic dependency.
We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors' of noun phrases.
We distinguish between context-free repre-sentability and context, free processing.
model and transformation rules.
This paper introduces PhraseNet, a context- sensitive lexical semantic knowledge base system.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.
We perform a linguistic analysis of documents during indexing for information retrieval.
This paper describes machine learning based parsing and question classification for question answering.
ethnologue.com).
PROLOG.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
We propose a signal dependent analysis-synthesis scheme in Sec.
The second experiment investigates a method for unsupervised learning of gender/number/animaticity information.
We use linguistic patterns and HTML text structures to extract text fragments contain-ing term descriptions.
We present BAYESUM (for 揃ayesian summarization?, a model for sentence extraction in query-focused summarization.
Thus, a co-occurrence graph can be constructed by co-occurrence relations in a corpus.
Tagging compound verb groups in an annotated corpus exploiting the verb rules is described.Keywords: compound verb groups, chunking, morphosyntactic tagging, inductive logic pro-gramming
Text metadata.
Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation.
We explore a novel computational approach to identifying 揷onstructions?or 搈ulti-word expressions?
We propose a kana-kanji conversion system with input support based on prediction.
We describe a method for retrieving counts for adjective-noun, noun-noun, and verb- object bigrams from the web by querying a search engine.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics.
This paper describes DialogueView, a tool for annotating dialogues with utter-ance boundaries, speech repairs, speech act tags, and discourse segments.
Triphone analysis is a new correction strategy which combines phonemic transcription with trigram analysis.
Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing.
In this work, we present a new semantic language modeling approach to model news stories in the Topic Detection and Tracking (TDT) task.
We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers.
We describe a probabilistic approach to content selection for meeting summarization.
verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction.
A system under construction is outlined which incorporates morphological checks (using new two-level error rules) over a directed letter graph, tag positional trigrams and partial parking.
This paper presents a method for extracting subcorpora documenting different subcategorization frames for verbs, nouns, and adjectives in the 100 mio.
word British National Corpus.
This paper will focus on our temporal processing algorithm, and in particular on our analysis of narrative progression, rhetorical structure, perfects and temporal expressions.
It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination.
Pos811D.
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.
We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system.
representation methods, hypermedia maps.
1			Back梤eferencing in text		?.
11			Belief structure	.
handwriting).
Next, we describe our more recent work on the CU Move dialog system for in-vehicle route planning and guidance.
Word order is realized by grammatical competition based on linear prece-dence (LP) rules which are based on the discourse-relational features.
We propose a new method for reformatting web documents by extracting semantic structures from web pages.
Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity.
We show how to do this using , -reduction constraints in the constraint language for A-structures (CLLS).
incomplete syntactic analysis.
This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations.
base forms and POS tags.
Context information is produced from rule-based translation such as part-of-speech tags, semantic concept, case relations and so on.
This paper employs different types of information from different levels of text to extract named entities, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache and n-gram model.
"proper name") recognition system built around a maximum entity framework.
Example-based machine translation (EBMT) is based on a bilingual corpus.
We describe an algorithm for recovering non-local dependencies in syntactic dependency structures.
This paper briefly describes our rule-based heuristic analyzer for Finnish nominal and verb forms.
This paper addresses two major problems in closed task of Chinese word segmentation (CWS): tagging sentences interspersed with non-Chinese words, and long named entity (NE) identification.
Three approaches to measuring text similarity are considered: n- gram overlap, Greedy String Tiling, and sentence alignment.
This paper presents a speech understanding component for enabling robust situated human-robot communication.
We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging.
The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
We present the architecture and data model for TEXTRACT, a document analysis framework for text analysis components.
This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism.In parsers for unification-based grammar formalisms, complex phrase types are derived by incremental refinement of the phrase types defined in grammar rules and lexical entries.
This paper explores the effect of improved morphological analysis, particularly context sensitive morphology, on monolingual Arabic Information Retrieval (IR).
a finite-state transducer.
There has recently been considerable interest in the use of lexically-based statistical techniques to resolve prepositional phrase attachments.
This paper describes a method for obtaining the semantic representation for a syntax tree in Systemic Grammar (SG).
In this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) tagging framework.
parsing procedures and semantic head-driven generation.
Dialogue system for 3D virtual environ-ments).
We apply the C4.5 decision tree learner in interpreting Japanese relative clause constructions, based around shallow syntactic and semantic processing.
We propose to score phrase translation pairs for statistical machine translation using term weight based models.
We describe a method for identifying systematic patterns in translation data using part-ofspeech tag sequences.
This paper provides an evaluation of the OntoLearn ontology learning system.
We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms.
This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.
This paper describes new default unification, lenient default unification.
noun phrase (NP) syntax.
We describe an MDL based grammar of a language that contains morphology and lexical categories.
We present an algorithm for collapsing morphological classes (signatures) by using syntactic context.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unification- based shallow-level parser using transformational rules over syntactic patterns.
This paper presents a technique to deal with multiword nominal terminology in a computational Lexical Functional Grammar.
This paper compares two approaches to computational semantics, namely semantic unification in Lexicalized Tree Adjoining Grammars (LTAG) and Lexical Resource Semantics (LRS) in HPSG.
We describe a machine learning system for the recognition of names in biomedical texts.
In this paper we present some novel applications of Explanation-Based Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars.
This paper presents PROVERB a text planner for argumentative texts.
Inter-sentence similarity is estimated by latent semantic analysis (LSA).
These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects.
This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA).
These features include lexical, lexico grammatical and semantic phenomena.
We compared the accuracy of a statistical parser on the LDC Switchboard treebank corpus of transcribed sentence-segmented speech using various combinations of punctuation and sentence-internal prosodic information (duration, pausing, and f0 cues).
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
This paper proposes an effective parsing method for example-based machine translation.
variable-matching without variable substitution.
We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures.
Emdros is a text database engine for linguistic analysis or annotation of text.
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.
idf based method.
In this paper, subclasses of monadic context- free tree grammars (CFTGs) are compared.
We investigate the relation between paragraph boundaries and discourse cues, pronominalization and information structure.
This paper deals with the way temporal connectives affect Temporal Structure as well as Discourse Structure in Narratives.
This paper compares the consistency- based account of agreement phenomena in 'unification-based' grammars with an implication-based account based on a simple feature extension to Lambek Categorial Grammar (LCG).
This paper describes strategies for automatic recognition of unknown variants of known words in a natural language processing system.
We show conclusive results on joint learning and inference of syntactic and semantic representations.
We have constructed the IPA Lexicon of Basic Japanese Nouns (IPAL-BN), which has a hierarchical structure based on the syntactic and semantic proper-ties of nouns.
In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction.
This paper presents a framework for clustering in text-based information retrieval systems.
SCsem is the semantic weighting.
We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.
Industrial applications of a reversible, string-based, unification approach called Humor (High-speed Unification Morphol-ogy) is introduced in the paper.
Dutch subordinate clauses.
Dutch subordinate clauses for non-transformational theories of grammar.
We then examine the effect of integrating pausal and spontaneous speech phe-nomena into syntactic rules for speech recognition, using118 sentences.
A bottom-up generation algorithm for principle-based grammars is proposed.
We present a comparison of two formalisms for representing natural language utterances, namely deep syntactical Tectogrammatical Layer of Functional Generative Description (FGD) and a semantic formalism, MultiNet.
We explore two semantic chunking tasks.
We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural- language processing.
LCP may provide valuable information for resolving anaphora and ellipsis.
For topic identification, we will outline techniques based on stereotypical text structure, cue words, high-frequency indicator phrases, intratext connectivity, and discourse structure centrality.
trigger words and parsing structures.
(HLT-NAACL Workshop on Parallel Texts 2006).
The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution.
Topic representations are integrated in two NLP applications: Information Extraction and Multi- Document Summarization.
We present a language-independent and unsupervised algorithm for the segmentation of words into morphs.
Previous works on question classification are based on complex natural language processing techniques: named entity extractors, parsers, chunkers, etc.
This paper proposes an alignment adaptation	approach	to	improvedomain-specific (in-domain) word alignment.
We present a simple approximation method for turn-ing a Head-Driven Phrase Structure Grammar into a context-free grammar.
The acoustic models make use of dictionary phonetic spellings together with models for phonemes in context.
We describe an implementation of data- driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
WORD- NET DOMAINS).
We consider the translation of European Parliament Speeches.
We introduce a new categorial formal-ism based on intuitionistic linear logic.
We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees.
knowledge and language-specific heuristics.
It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques.
This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE.
This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues.
information technology test reports and medical finding reports.
In the process, we create a word朿ategory co- occurrence matrix, which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses, as well.
We describe a word alignment platform which ensures text pre-processing (tokenization, POS-tagging, lemmatization, chunking, sentence alignment) as required by an accurate word alignment.
We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues.
They are:?Vector-quantized energy-normalized Mel-cepstra?Vector-quantized smoothed 40-ms晅ime derivatives of the Mel-cepstra?Energy?Smoothed 40-ms energy differencesWe use 256-word speaker-independent code- books to vector-quantize the Mel-cepstra and the Melcepstral differences.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
part-of-speech tagged corpora).
The method involves using a bilingual term list to learn source- target surface patterns.
LFG and GPSG are augmented PS- grammars.
We propose astemming method for Mongolian to extractloanwords correctly.
This paper describes PEGASUS, a spoken language interface for on-line air travel planning that we have recently developed.
This paper describes our preliminary research on "attention-sharing" in infants' language acquisition.
This paper outlines a formal computational semantics and pragmatics of the major speech act types.
This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame.
An HHMM-based system ICTCLAS was accomplished.
We introduce a new interactive corpus exploration tool called InfoMagnets.
Using the formalism of generalized phrase structure grammar (GPSG) in an NI, system (e.g.
We present BioGrapher, a biographical QA system that addresses this problem by machine learning algorithms for biography classification.
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language.
We investigate the usefulness of evolutionary al-gorithms in three incarnations of the problem of feature relevance assignment in memory-based language processing (MBLP): feature weight-ing, feature ordering and feature selection.
CTXMATCH performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.
This paper presents Trace dr Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (UG) and ideas of Government dr Binding Theory (on).
in GPSO and HPSG.
This process is called transliteration.
This paper presents a novel language-independent question/answering (Q/A) system based on natural language processing techniques, shallow query understanding, dynamic sliding window techniques, and statistical proximity distribution matching techniques.
Bayesian, decision tree and neural network classifiers) to discover language model errors.
Word category prediction is used to implement an accurate word recognition system.
We present two techniques for combining evidence from dictionary-based and corpus-based translation lexicons, and show that backoff translation outperforms a technique based on merging lexicons.
Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation.
This paper describes the NLMenu System, a menu-based natural language understanding system.
We examine the relationship between the two grammatical formalisms: Tree Adjoining Grammars and Head Grammars.
We present Evita, an application for recognizing events in natural language texts.
It consists of preference based pruning, syntactic based pruning and semantic based pruning.
We present two methods for unsupervised segmentation of words into morpheme-like units.
The method combines morphological and lex-ical processing, bilingual word alignment, and graph-theoretic cluster generation.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
plus petits.
Associated or related terms.
This paper presents an answer selection method based on Support Vector Machines (SVM) for Open-Domain Question Answering (QA).
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
Chinese word segmentation and Part-ofSpeech (POS) tagging have been commonly considered as two separated tasks.
We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features.
We implement, a num-ber of both bag-of-words and word order-sensitive similarity metrics, and test each (wet' character-based and word-based indexing.
We present a method for improving dependency structure analysis of Chi-nese.
This paper describes a chunk-based parser/semantic analyzer used by a language learning model.
Successful proper noun boundary identification within the part of speech tagger is essential for successful categorization.
in content based information retrieval.
This paper describes a first attempt at a sta-tistical model for simultaneous syntactic pars-ing and generalized word-sense disambigua-tion.
This paper describes an approach to translate rarely occurring named entities (NE) by combining phonetic and semantic similarities.
This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT).
Prepositional Phrase is the key issue in structural ambiguity.
Four different attach-ments are told out according to their functionality: noun attachment, verb attachment, sentence-level attachment, and predicate-level attachment.
We present a statistical phrase-based translation model that uses hierarchical phrases?phrases that contain subphrases.
This paper presents a Bayesian model for unsupervised lixtruhnz, of verb selectional preferences.
database tuples).
In this study, the idea of ECOC is applied to memory-based language learning with local (k-nearest neighbor) classifiers.
We report on a series of experiments with probabilistic context-free grammars predicting English and German syllable structure.
This paper compares a number of generative probability models for a wide- coverage Combinatory Categorial Grammar (CCG) parser.
This paper explores the problem of identifying sen-tence boundaries in the transcriptions produced by automatic speech recognition systems.
The features studied are: principal components, independent com-ponents, and non-negative components.
This paper explores the use of lasso for statistical language modeling for text input.
We propose projection models that exploit lexical and syntactic information.
The model uses syntactic patterns and N-grams reflecting the hierarchical discourse structures of dialogues.
We present Minimum Bayes-Risk word alignment for machine translation.
We create five kinds of semantic maps, i.e., semantic maps of abstract nouns organized via (1) adjectives, (2) adjectival nouns, (3) non-adjectival nouns and (4) adjectival and adjectival nouns and a semantic map of adjectives, adjectival nouns and non-adjectival nouns organized via collocated abstract nouns, and compare them with each other to find similarities and differences.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.
9000 Japanese scientific papers.
This paper presents Delphi, the natural language component of the BBN Spoken Language System.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.
In this paper, a memory-based pars-ing method is extended for han-dling compositional structures.
This paper presents a novel training algorithm for a linearly-scored block sequence translation model.
We compare the performances of Decision Tree, Na飗e Bayes, SVM (Support Vector Machine), and ME (Maximum Entropy) classification methods.
The pattern matching capabilities of neural networks can be used to locate syntactic constituents of natural language.
In this paper, we describe a resource-light system for the automatic morphological analysis and tagging of Russian.
We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges.
'Formerly SDC—A Burroughs Company.
(4) Investigation revealed adequate lube oil.
A modular parallel distributed processing architecture for parsing, representing and paraphrasing sentences with multiple hierarchical relative clauses is presented.
In this work we discuss refinements of the stochastic segment model, an alternative to hidden Markov models for representation of the acoustic variability of phonemes.
Results are presented for speaker-independent phoneme classification in continuous speech based on the TIMIT database.
To obtain verb paradigms we extracted left and right bigrams for the 400 most frequent verbs from over 100 million words of text, calculated the Kullback Leibler distance for each pair of verbs for left and right contexts separately, and ran a hierarchical clustering algorithm for each context.
We present the dialogue module of the speech-to-speech translation system VERB-MOBIL.
This paper proposes a segmentation stan-dard for Chinese natural language processing.
Lexical Functional Grammar (LFG) (Kaplan,Bresnan 82] is a powerful formalism for that purpose.
Prolog provides a good tools for the implementation of LFG.
We investigated of the characteristics of in-text causal relations.
We designed causal relation tags.
This paper proposes an efficient linguistic processing strategy for speech recognition and understanding using a dependency structure grammar.
The strategy includes parsing and phrase prediction algorithms.
After speech processing and phrase recognition based on phoneme recognition, the parser extracts the sentence with the best likelihood taking account of the phonetic likelihood of phrase candidates and the linguistic likelihood of the semantic inter-phrase dependency relationships.
A fast parsing algorithm using breadth-first search is also proposed.
In this paper, we focus on exploiting both textual and prosodic features for topic segmentation of Mandarin Chinese.
We then contrast these results with a simple text similarity-based classification scheme.
Finally we build a merged classifier, finding the best effectiveness for systems integrating text and prosodic cues.
We describe an approach to resolving definite descriptions and pronominal anaphora as subcases of a general strategy for presupposition satisfaction.
A first model implements a variant of instance-based learning, in which a weighed similarity metric and a database of prototypical exemplars are used to predict new mappings.
In the second model, grapheme-to-phoneme mappings are looked up in a compressed text-to-speech lexicon (table lookup) enriched with default mappings.
We compare performance and accuracy of these approaches to a connectionist (backpropagation) approach and to the linguistic knowledge-based approach.
This paper proposes a new method for learning a context-sensitive conditional probability context-free grammar from an unlabeled bracketed corpus based on clustering analysis and describes a natural language parsing model which uses a probability-based scoring function of the grammar to rank parses of a sentence.
This result supports the assumption that local contextual statistics obtained from an unlabeled bracketed corpus are effective for learning a useful grammar and parsing.
We investigate techniques to support the answering of opinion-based questions.
We first present the OpQA corpus of opinion questions and answers.
As an initial step towards the development of MPQA systems, we investigate the use of machine learning and rule-based subjectivity and opinion source filters and show that they can be used to guide MPQA systems.
An easy way of translating queries in one language to the other for cross-language information retrieval (IR) is to use a simple bilingual dictionary.
This paper describes three relatively domain-independent capabilities recently added to the Paramax spoken language understanding system: non-monotonic reasoning, implicit reference resolution, and database query paraphrase.
We present a new way to simplify the construction of precise broad-coverage grammars, employing typologically-motivated, customizable extensions to a language-independent core grammar.
We discuss a dual-component linguistic model that attempts to reflect the generation process of the learners.
General Direction for Multilingual Transfers to promote machine translation.
ERRORSAll documents are not suitable for machine translation.
We present a hierarchical phrase-based statistical machine translation in which a target sentence is efficiently generated in left-to-right order.
In this paper, we describe an algorithm for incorporating syntactic features in the alignment process for non-parallel texts with the goal of generating novel paraphrases of existing texts.
Our method uses dynamic programming with alignment decision based on the local syntactic similarity between two sentences.
The paper presents the Constructive Dialogue Model as a new approach to formulate system goals in intelligent di-alogue systems.
This paper describes a novel framework for using scenario knowledge in open- domain Question Answering (Q/A) applications that uses a state-of-the-art textual entailment system (Hickl et al., 2006b) in order to discover textual information relevant to the set of topics associated with a scenario description.
We describe HARC, a system for speech understanding that integrates speech recognition techniques with natural language processing.
We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors.
This paper describes a. method for positioning un-known words in an existing thesaurus by using word-to-word relationships with relation (case) markers extracted from a large corpus.
In this paper, we introduce TextRank ?a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.
We present a simple method for language independent and task independent text categorization learning, based on character-level n-gram language models.
This paper introduces a graph-based algorithm for sequence data labeling, using random walks on graphs encoding label dependencies.
This paper analyzes various issues in building a HMM based multilingual speech recognizer for Indian languages.
A dialogue based Question Answering (QA) system for Railway information in Telugu has been described.
We investigate prototype-driven learning for primarily unsupervised grammar induction.
This paper proposes a new class-based method to estimate the strength of association in word co-occurrence for the purpose of structural disambiguation.
We have applied our method to determining dependency relations in Japanese and prepositional phrase attachments in English.
This paper describes a new finite-state shallow parser.
This paper investigates the automatic identification of aspects of Information Structure (IS) in texts.
We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines.
We show the performance of C4.5, Bagging, and Ripper classifiers on several classes of instances such as nouns and pronouns, only nouns, only pronouns.
This paper presents the current results of an ongoing research project on corpus distribution of prepositions and pronouns within Polish preposition-pronoun contractions.
The results of corpus-based investigations of the distribution of prepositions within preposition-pronoun contractions can be used for grammar-theoretical and lexicographic purposes.
Computational and Theoretical Background2.1	Computational	background.The	experimental	system	of	automaticquestion-answering TIBAQ (Text-and-Inference Based Answering of Questions,	cf.
also D. Hays' notion of automatic encyclopedia).
For example, head-finding rules are used to augment node labels with lexical heads.
One of the approaches that helped recently is the use of latent semantic analysis to capture the semantic fabric of the document and enhance the n-gram model.
In this paper, we explain a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling.
This approach augments each word with its syntactic descriptor in terms of the part-of-speech tag, phrase type or the supertag.
We also present some observations on the effect of the knowledge of content or function word type in language modeling.
Lexical Chains are powerful representations of documents.
In this paper, we propose a greedy Language-Independent algorithm that automatically extracts Lexical Chains from texts.
For that purpose, we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the Pole-Based Overlapping Clustering Algorithm.
This paper proposes a hybrid of handcrafted rules and a machine learning method for chunking Korean.
We apply interpretive rules to clausal patterns and patterns of modification, and concurrently abstract general 損ossibilistic?propositions from the resulting formulas.
Several attempts have been made to apply statistical machine learning approaches, including Support Vector Machines (SVMs) with sophisticated features and kernels.
Using a novel multi-resolution encoding of the question抯 parse tree, we induce a Conditional Random Field (CRF) to identify informer spans with about 85% accuracy.
Syllable-to-word (STW) conversion is important in Chinese phonetic input methods and speech recognition.
This paper describes a noun-verb event-frame (NVEF) word identifier that can be used to solve these problems effectively.
To further expand its coverage, we shall extend the study of NVEF to that of other co-occurrence restrictions such as noun-noun pairs, noun-adjective pairs and verb-adverb pairs.
We describe the application of the Ling- Pipe toolkit to Chinese word segmentation and named entity recognition for the 3rd SIGHAN bakeoff.
We defined 10 Descriptive Answer Type(DAT)s as answer types for descriptive questions.
Our work is theoretically rooted in previous work on information structuring and word order in the Prague School framework as well as on the systemic-functional notion of Theme.
We report on a project to use the lexical aspect features of (a)telicity re-flected in the Lexical Conceptual Structure of the input text to suggest tense and discourse structure in the English translation of a Chinese newspaper corpus.
This paper discusses the role of morphological and syntactic information in the automatic acquisition of semantic classes for Catalan adjectives, using decision trees as a tool for exploratory data analysis.
Unification-based NL parsers that copy argument graphs to prevent their destruction suffer from inefficiency.
The data structures unified are directed acyclic graphs (DAG's), used to encode grammar rules, lexical entries and intermediate parsing structures.
unification is a destructive operation.
Then, we compare the improvement brought to the engine by the adjunction of two different non-interactive spelling correction strategies: a classical one, based on a string-to-string edit distance calculus, and a contextual one, which adds linguistically- motivated features to the string distance module.
This algorithm enables existing corpus-based estimation techniques for stochastic TAGs to be used for language modelling.
We explore how virtual examples (artificially created examples) improve performance of text classification with Support Vector Machines (SVMs).
The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.
This leads to a new type of semantic lexicon (CoRELEx) that supports underspecified semantic tagging through a design based on systematic polysemous classes and a class-based acquisition of lexical knowledge for specific domains.
recognize]: output in order to handle speech recognition errors.
We define two concept-level CMs, which are on content-words and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars.
Negative life events play an important role in triggering depressive episodes.
This framework integrates a cognitive motivated model, Hyperspace Analog to Language (HAL), to represent words as well as combinations of words.
This paper describes experiments in incremental query processing and indexing with the INQUERY information retrieval system on the TIPSTER queries and document collection.
Our method compares two probability distributions over WordNet by measuring the semantic distance of the component nodes, weighted by their probability.
We present an approach to statistical machine translation that combines ideas from phrase-based SMT and traditional grammar-based MT.
Our system incorporates the concept of multi-word translation units into transfer of dependency structure snippets, and models and trains statistical components according to state- of-the-art SMT systems.
Compliant with classical transfer-based MT, target dependency structure snippets are input to a grammar-based generator.
The domain-specific lexical items were obtained from subsections of a synchronous Chinese corpus, LIVAC.
We present Outilex, a generalist linguistic platform for text processing.
We propose a syntax of it-clefts using Tree-Local Multi- Component Tree Adjoining Grammar and a compositional semantics on the proposed syntax using Synchronous Tree Adjoining Grammar.
To deal with the problem, this method employs pattern-based abbreviation rules in addition to text markers and cue words.
The pattern-based rules describe how abbreviations are formed from definitions.
This paper describes the Italian all-words sense disambiguation task for Senseval-3.
This paper describes a system which generates animations for cooking actions in recipes, to help people understand recipes written in Japanese.
We designed and compiled the lexicon of cooking actions required for the animation generation system.
We describe a method for automatic word sense disambiguation using a text corpus and a machine-readable dictionary (MRD).
The method is based on word similarity and context similarity measures.
A proposed "Matrix" method for the representation of the inflectional paradigms of Arabic words is presented.
This paper presents a maximum entropy word alignment algorithm for Arabic- English based on supervised training data.
The main focus of our work is to provide an efficient parser that is practical to use with combining only lexical and part-ofspeech features toward language independent parsing.
Text prediction is a form of interactive machine translation that is well suited to skilled translators.
Besides the widely used lexical-level features, such as word formation pattern, morphological pattern, out-domain POS and semantic trigger, we also explore the name alias phenomenon, the cascaded entity name phenomenon, the use of both a closed dictionary from the training corpus and an open dictionary from the database term list SwissProt and the alias list LocusLink, the abbreviation resolution and in- domain POS using the GENIA corpus.
This paper describes a multi-lingual phrase-based Statistical Machine Translation system accessible by means of a Web page.
This paper deals with translation ambiguity and target polysemy problems together.
We present a novel context pattern induction method for information extraction, specifically named entity extraction.
Using token membership in these extended lists as additional features, we improved the accuracy of a conditional random field-based named entity tagger.
We present CarmelTC, a novel hybrid text classification approach for analyzing essay answers to qualitative physics questions, which builds upon work presented in (Ros磂 et al., 2002a).
We introduce a new technique for using the speech of multiple reference speakers as a basis for speaker adaptation in large vocabulary continuous speech recognition.
Term translation probabilities proved an effective method of semantic smoothing in the language modelling approach to information retrieval tasks.
In this paper, we use Generalized Latent Semantic Analysis to compute semantically motivated term and document vectors.
Our experiments demonstrate that GLSAbased term translation probabilities capture semantic relations between terms and improve performance on document classification.
Johnston 1998 presents an approach in which strategies for mul-timodal integration are stated declaratively using a unification-based grammar that is used by a multi-dimensional chart parser to compose inputs.
In this paper, we present an alternative approach in which multi modal parsing and understanding are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and out-puts their joint interpretation.
Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.
Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.
Each source of information is represented by kernel functions.
We also compare the SVM with KNN on different kernels.
We model the likely contexts of all words in an ASR system vocabulary by performing a lexical co-occurrence analysis using a large corpus of output from the speech system.
We present a divide-and-conquer strategy based on finite state technology for shallow parsing of real-world German texts.
Shallow parsing is supported by suitably configured prepro-cessing, including: morphological and on-line com-pound analysis, efficient POS-filtering, and named entity recognition.
This paper describes a heuristic for morpheme- and morphology-learning based on string edit distance.
A new method of autcmatic lexical disambiguation of big texts is described, using recent proof-theoretical results from the theory of categorial grainier.O.
One of the tasks of the Thesaurus, one of the departments, is to build a database for lexicological research.
As was the case for the morphological analyzer, the syntactic parser is an implementation of a categorial calculus.
ite construction of and philosophy behind the LaMbak categorial parser we use for the disambiguation and syntactic analysis is the topic of this paperl.
This paper describes SMES, an informa-tion extraction core system for real world German text processing.
A parsing architecture is described which uses a permanent store of context-free rule patterns encoded as split composite vectors, and two interacting working memory units.
For example, Waltz and Pollack (1984) have devised a model of word-sense and syntactic disambiguation, and Cottrell (1985) has proposed a neural network style model of parsing.
Conjunctions are particularly difficult to parse in traditional, phrase-based grammars.
This paper explores the use of support Vector Machines (sVMs) for anextended named entity task.
Furthermore we compare the performance of the sVM model to a standard HMM bigram model.
SRV can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training.
We describe an approach to interpreting LFG f-structures (Kaplan & Bresnan, 1982) truth-conditionally as underspeci-tied quasi-logical forms.
We provide a reverse mapping from QLFs to f-structures and establish isomorphic subsets of the QLF and LFG formalism.
In particular I will discuss approaches using stochastic grammars analogous to those used in computational linguistics, both for gene finding and protein family classification.
This paper describes an initial evaluation of systems that answer questions seeking definitions.
In this paper, we propose an implementable characterization of genre suitable for automatic genre identification of web pages.
This paper describes an analysis method which uses heuristic knowledge to find local syntactic structures of Chinese sentences.
We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.
Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.
We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a 揵ag-of-words?kernel.
The aim of this paper is to present a language-neutral, theory-neutral method for annotating sentence- internal temporal relations.
Asearchable	multi-languagedatabase has already been created.
We present an efficient procedure for cost-based abduction, which is based on the idea of using chart parsers as proof procedures.
We define a data model for storing geographic information from multiple sources that enables the efficient production of customizable gazetteers.
This paper presents a method of resolv-ing ambiguity by using a variant of cir-cumscription, prioritized circumscrip-tion.
We also discuss an im-plementation of prioritized circumscrip-tion by a hierarchical logic program-ming (HCIA.))
In this paper, I present a model of the local organization of extended text.
This paper describes an approach for modeling events in text as event intervals and for generating linear orders of event intervals, useful for the summarization of events or as the basis for question answering systems.
There is a type of nominal ellipsis that has been neglected in the study of ellipsis resolution.
We have drawn primarily on explicit and implicit information from machine-readable dictionaries (MRD's) to create a broad coverage lexicon.
Categorial grammar has traditionally used the X-calculus to represent meaning.
We describe an LR parser of parts-ofspeech (and punctuation labels) for Tree Adjoining Grammars (TAGs), that solves table conflicts in a greedy way, with limited amount of backtracking.
Cmcsim-Tutor version 2, a dialogue-based intelligent tutoring system (ITS), is nearly five years old.
We propose to restrain VP-ellipsis resolution by presupposition neutralization.
One is a statistical dependency transduc-tion model using head transducers, the other a case-based transduction model in-volving a lexical similarity measure.
We present a novel method of applying the results of coreference resolution to improve Name Recognition for Chinese.
We consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger.
Finally, we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters.
We present preliminary results concerning robust techniques for resolving bridging definite descriptions.
The described tagger is based on a hidden Markov model and uses tags composed of features such as partof-speech, gentler, etc.
In this paper, we focus on prosody-based topic segmentation of Mandarin Chinese.
Serial verb constructions (SVCs) in Chinese are popular structural ambiguities which make parsing difficult.
This paper describes a novel system for acquiring adjectival subcategorization frames (SCFs) and associated frequency information from English corpus data.
The system incorporates a decision-tree classifier for 30 SCF types which tests for the presence of grammatical relations (GRs) in the output of a robust statistical parser.
It uses a powerful pattern- matching language to classify GRs into frames hierarchically in a way that mirrors inheritance-based lexica.
information retrieval, translation unit discovery, and corpus studies.
This paper describes how to construct a finite-state machine (FSM) approximating a 'unification-based' grammar using a left-corner grammar transform.
This paper presents a new approach to automated table extraction that exploits formatting cues in semi-structured HTML tables, learns lexical variants from training examples and uses a vector space model to deal with non-exact matches among labels.
We propose a novel Co-Training method for statistical parsing.
We tested the clustering and filtering processes on electronic newsgroup discussions, and evaluated their performance by means oftwo experiments: coarse-level clustering and simple information retrieval.
We propose an algorithm labeling semantic classes and for leveraging them to extract is-a relationships using a top-down approach.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Tree- bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.
This paper presents a statistical, learned approach to finding names and other non-recursive entities in text (as per the MUC-6 definition of the NE task), using a variant of the standard hidden Markov model.
Case-based machine translation is a promising approach to resolving problems in rule-based machine translation systems, such as difficulties in control of rules and low adaptability to specific domains.
We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency.
Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance.
The	system's maincomponents are:	a Generalized Phrase StructureGrammar (GPSG); a top-down parser; a logic transducer that outputs a first-order logical representation; and a "disambiguator" that uses sortal information to convert "normal-form" first-order logical expressions into the query language for HIRE, a relational database hosted in the SPHERE system.
This paper describes an unsupervised method for noun compound bracketing which extracts statistics from Web search engines using a x2 measure, a new set of surface features, and paraphrases.
In this paper we propose a generalization of the Stack-based decoding paradigm for Statistical Machine Translation.
Experimental results are also reported for a search algorithm for phrase-based statistical translation models.
Ergodic IIMMs have been successfully used for modeling sentence production.
This paper introduces the N-th order Ergodic Multigram TIMM for language modeling of such languages.
This work represents a first attempt at modeling morphological-syntactic interaction in a generative probabilistic framework to allow for MH parsing.
We show that morphological information selected in tandem with syntactic categories is instrumental for parsing Semitic languages.
We further show that redundant morphological information helps syntactic disambiguation.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
We apply the LOP-CRF to two sequencing tasks.
We describe a probabilistic extraction model that provides mutual benefits to both 搕op-down?relational pattern discovery and 揵ottom-up?relation extraction.
We describe the LDV-COMBO system presented at the Shared Task.
News on electrical bulletin boards con-sist of high density expressions.
In this paper we describe our experiences with a tool for the development and testing of natural language grammars called GTU (German: GrammatikTestumgebumg; grammar test environment).
GTU supports four grammar formalisms under a window-oriented user interface.
This paper deals with multilingual database generation from parallel corpora.
This paper discusses the establishment and implementation of a curriculum for teach-ing NLP.
We propose a novel approach to the identification of biomedical terms in research publications using the Perceptron HMM algorithm.
We present an implemented machine learning system for the automatic detection of nonreferential it in spoken dialog.
The system builds on shallow features extracted from dialog transcripts.
The detection of prosodic characteristics is an important aspect of both speech synthesis and speech recognition.
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
The lexical structures proposed are derived from the analysis of semantic collocations over large text corpora.
This paper describes a method of incremental natural language generation using a parallel marker-passing algorithm for modeling simultaneous interpretation.
real- time speech-to-speech dialog translation system developed at the Center for Machine Translation at Carnegie Mellon University, and publicly demonstrated since March 1989.
We then report on experiments which go a step further, using four-class classifiers based on automated scoring of texts for each dimension of the typology.
We present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections.
Our method combines and extends two previous techniques that were based mostly on manually crafted lexical patterns and WordNet hypernyms.
Then, a method to combine the different models for bilingual lexicon extraction is presented.
Lastly, methods for bilingual terminology extraction and thesaurus enrichment are discussed.
We present the results from a series of experiments aimed at uncovering the discourse structure of man-machine communication in natural language (Wizard of Oz experiments).
We also show how this predictability can be used to aid non-native speakers to determine the countability of English nouns when building a bilingual machine translation lexicon.
This paper explores the text-based emotion prediction problem empirically, using supervised machine learning with the SNoW learning architecture.
The goal is to classify the emotional affinity of sentences in the narrative domain of children抯 fairy tales, for subsequent usage in appropriate expressive rendering of text-to-speech synthesis.
We also discuss results for a tripartite model which covers emotional valence, as well as feature set alternations.
This paper describes a supervised learn-ing method to automatically select froma set of noun phrases, embedding propernames of different semantic classes, theirmost distinctive features.
This classifier is used to esti-mate the probability distribution of anout of vocabulary proper name over atagset.
This probability distribution isitself used to estimate the parameters ofa stochastic part of speech tagger.
This paper presents a hybrid approach to question answering in the clinical domain that combines techniques from summarization and information retrieval.
In our new system, three types of agents: a) domain agents, 13) strat-egy agents, and c) context agents were realized.
Vieira and Poesio (2000) proposed an algorithm for definite description (DD) resolution that incorporates a number of heuristics for detecting discourse- new descriptions.
In this paper, a large class of English declarative sentences, including post-noun-modification by relative clauses, is formalized using a two-level grammar.
In this paper we present an application fostering the integration and interoperability of computational lexicons, focusing on the particular case of mutual linking and cross-lingual enrichment of two wordnets, the ItalWordNet and Sinica BOW lexicons.
We present a novel, data-driven method for integrated shallow and deep parsing.
Mediated by an XML-based multi-layer annotation architecture, we interleave a robust, but accurate stochastic topological field parser of German with a constraint- based HPSG parser.
We here adapt a result on the complexity of ID/LP-grammars to the dependency framework.
We present a novel mechanism for improving reference resolution by using the output of a relation tagger to rescore coreference hypotheses.
There exists strong word association in natural language.
Both the distance-independent(DI) and distancedependent(DD) MI-Trigger-based models are constructed within a window.
This paper describes results from several dozen experimental systems, and draws conclusions about the ability of speech recognition models to represent the relationship among syntax, prosody, and segmental acoustics.
Careful analysis of the relationship between prosody and syntax indicates that syntactic phrase boundaries are the most important cue for prosodic phrase boundary recognition, while part of speech is the most important cue for locating pitch accents, but that neither of these cues is entirely sufficient for either classification task.
We use machine learners trained on a combination of acoustic confidence and pragmatic plausibility features computed from dialogue context to predict the accuracy of incoming n-best recognition hypotheses to a spoken dialogue system.
Our experiment shows that the word similarity-based smoothing can improve the performance by using huge unlabeled data.
The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT).
We present the first model to use word alignments for constraining the space of phrasal alignments searched during Expectation Maximization (EM) training.
We describe a computational formalism that captures structural relationships among participants in a dynamic scenario.
In particular, we investigate feature sets derived from graph representations of sentences and sets of sentences.
We show that a highly connected graph produced by using sentence-level term distances and pointwise mutual information can serve as a source to extract features for novelty detection.
These feature sets allow us to increase the accuracy of an initial novelty classifier which is based on a bagof-word representation and KL divergence.
This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.
In this paper, we present a semiautomatic approach for annotating semantic information in biomedical texts.
To construct BioProp, a semantic role labeling (SRL) system trained on PropBank is used to annotate BioProp.
This paper proposes an English adverb ordering method based on adverb gram-matical functions (subjuncts, adjuncts, dis-juncts and conjuncts) and meanings (pro-cess, space, time etc.
We have recently begun a project on summarizing simple narrative stories.
We present a named entity recognition and classification system that uses only probabilistic character-level features.
We present an integrated approach to speech and natural language processing which uses a single parser to create training for a statistical speech recognition component and for interpreting recognized text.
On the speech recognition side, our innovation is the use of a statistical model combining N-gram and context-free grammars.
This paper presents a unified solution, which is based on the idea of 搑oles tagging?
to the complicated problems of Chinese unknown words recognition.
The result and experiments in our system ICTCLAS shows that our approach based on roles tagging is simple yet effective.Keywords: Chinese unknown words recognition, roles tagging, word segmentation, Viterbi algorithm.
We use average mutual informaLion as global similarity metric to do classification.
Also, we use the resulting classes to do experiments on word class-based language model.
This paper describes a novel method of compiling ranked tagging rules into a deterministic finite-state device called a bimachine.
This paper presents a syntactical method of interpreting pronouns in Polish.
We describe our work in progress on natural language analysis in medical question- answering in the context of a broader medical text-retrieval project.
Spoken dialogue managers have benefited from using stochastic planners such as Markov Decision Processes (MDPs).
We use a Partially Observable Markov Decision Pro-cess (POMDP)-style approach to generate dialogue strategies by inverting the notion of dialogue state; the state represents the user抯 intentions, rather than the system state.
Today, several part-of-speech tagged corpora are readily available for research use.
We present a new approach to stochastic modeling of constraint-based grammars that is based on log-linear models and uses EM for esti-mation from unannotated data.
The techniques are applied to an LFG grammar for German.
Also, a new class-based grammar lexicalization is presented, showing a 10% gain over unlexicalized models.
We present a statistical question answering sys-tem developed for TREC-9 in detail.
The sys-tem is an application of maximum entropy clas-sification for question/answer type prediction and named entity marking.
We explore a hybrid approach for Chinese definitional question answering by combining deep linguistic analysis with surface pattern learning.
This paper presents a word segmenta-tion system in France Telecom R&D Beijing, which uses a unified approach to word breaking and OOV identifica-tion.
We describe sign translation using example based machine translation technology.
We use a user- centered approach in developing an automatic sign translation system.
We have developed a prototype system that can recognize Chinese signs input from a video camera which is a common gadget for a tourist, and translate them into English text or voice stream.KeywordsSign, sign detection, sign recognition, sign translation.
This paper describes a comparative applica-tion of Grammar Learning by Partition Searchto four different learning tasks: deep parsing,NP identification, flat phrase chunking and NPchunking.
It handles irregular, regular and semi-regular morphology through a ro-bust generative model using weighted Levenshtein alignments.
Unsupervised induction of grammatical gender is performed via global modeling of context-window feature agreement.
This paper proposes an automatic method of detecting grammar elements that decrease readability in a Japanese sentence.
In this paper, we describe ontology-based text categorization in which the domain ontologies are automatically acquired through morphological rules and statistical methods.
We present a system that incorporates agent- based technology and natural language generation to address the problem of natural language summarization of live sources of data.
We propose stochas-tic finite-state models for these two subproblems in this paper.
We present a method for learning stochastic finite-state models for lexical choice and lexical reordering that are trained automatically from pairs of source and target utterances.
Punjabi Machine Transliteration (PMT) is a special case of machine transliteration and is a process of converting a word from Shahmukhi (based on Arabic script) to Gurmukhi (derivation of Landa, Shardha and Takri, old scripts of Indian subcontinent), two scripts of Punjabi, irrespective of the type of word.The Punjabi Machine Transliteration System uses transliteration rules (character mappings and dependency rules) for transliteration of Shahmukhi words into Gurmukhi.
We present a perceptron-style discriminative approach to machine translation in which large feature sets can be exploited.
We first discuss several challenges to error-driven discriminative approaches.
Content based navigation is realized by use of automatic speech recognition, information retrieval, information extraction and human computer interaction technology.
In addition to the browsing and querying functionalities, acoustics-based caller ID technology is used to proposes caller names from existing caller acoustic models trained from user feedback.
Specifically, we experiment with three lexical semantic classes of English verbs.
We provide a logical definition of Min-imalist grammars, that are Stabler抯 formalization of Chomsky抯 minimal-ist program.
This paper presents a sentence- based statistical model for Hangeul-Hanja conversion, with word tokenization included as a hidden process.
We present a new architecture for reversible NLP.
Separate parsing and generation grammars are constructed from the underlying application's semantic model and knowledge base.
In this paper we describe the algorithms used in the BBN BYBLOS Continuous Speech Recognition system.
The BYB LOS system uses context-dependent hidden Markov models of phonemes to provide a robust model of phonetic coarticulation.
The two features that have made the largest improvements in recognition accuracy since 1982 were the use of robust context-dependent phonetic models, and the addition of derivative spectral parameters in multiple codebooks.
The BYBLOS systemThe BYBLOS system uses context-dependent hidden Markov models (HMM) of phonemes to provide a robust model of coarticulation [1, 21.
A corpus-based statistical Generalization Tree model is described to achieve rule optimization for the information extraction task.
We present a pattern matching method for compiling a bilingual lexicon of nouns and proper nouns from unaligned, noisy parallel texts of Asian/Indo-European language pairs.
We also show how the results can be used in the compilation of domain-specific noun phrases.
semantic features, selectional restrictions) for complex terms and /or unknown words.
information extraction).
In this paper, we propose to introduce syntactic classes in a lexicalized dependency formalism.
We describe and experimentally evaluate a complete method for the automatic ac-quisition of two-level rules for morphologi-cal analyzers/generators.
In phase one, a mini-mal acyclic finite state automaton (AFSA) is constructed from string edit sequences of the input pairs.
Segmentation of the words into morphemes is achieved through view-ing the AFSA as a directed acyclic graph (DAG) and applying heuristics using prop-erties of the DAG as well as the elemen-tary edit operations.
We introduce the notion of a delimiter edge.
The paper addresses the issue of how to increase adaptivity in response generation for a spoken dialogue system.
We first describe a Java/XML-based generator which produces different realizations of system responses based on agendas specified by the dialogue manager.
This paper describes a Chinese word segmentation system that is based on majority voting among three models: a forward maximum matching model, a conditional random field (CRF) model using maximum subword-based tagging, and a CRF model using minimum subwordbased tagging.
This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).
This paper presents a new open text word sense disambiguation method that combines the use of logical inferences with PageRank-style algorithms applied on graphs extracted from natural language documents.
We also explore and evaluate methods that combine several open-text word sense disambiguation algorithms.
A theory of interlanguage (IL) lexicons is outlined, with emphasis on IL lexical entries, based on the I-IPSG notion of lexical sign.
In this paper, we apply a novel source-channel model to the problem of morphological disambiguation (segmentation into morphemes, lemmatization, and POS tagging) for concatenative, templatic, and inflectional languages.
SEAFACT (Semantic Analysis For the Animation of Cooking Tasks) is a natural language interface to a computer-generated animation system operating in the domain of cooking tasks.
This paper describes the semantic analysis of verbal modifiers on which the SEAFACT implementation is based.
We present a generic template for spoken dialogue systems integrating speech recognition and synthesis with 'higher-level' natural language dialogue modelling components.
This paper focuses on the typology of CJK orthographic variation, provides a brief analysis of the linguistic issues, and discusses why lexical databases should play a central role in the disambiguation process.
In this paper we discuss technical issues arising from the interdependence between tokenisation and XML-based annotation tools, in particular those which use standoff annotation in the form of pointers to word tokens.
We also describe experiments which explore the effects of different granularities of tokenisation on NER tagger performance.
We use patterns of linguistic features (e.g.
These are filtered using plan-based conversational implicatures to eliminate inappropriate ones.
Computational neurolinguistics (CN) is an approach to computational linguistics which includes neurally-motivated constraints in the design of models of natural language processing.
Based on Support Vector Machines (SVMs), a basic segmenter is designed regarding Chinese word segmentation as a problem of character-based tagging.
By applying latent semantic analysis (LSA) to filter extracted hyponymy relations we reduce the rate of error of our initial pattern-based hyponymy extraction by 30%, achieving precision of 58%.
Applying a graph-based model of noun-noun similarity learned automatically from coordination patterns to previously extracted correct hyponymy relations, we achieve roughly a fivefold increase in the number of correct hyponymy relations extracted.
In particular, tree kernels have been used to represent verbal subcategorization frame (SCF) information for predicate argument classification.
classification of unseen verbs.
A proposed "Matrix" method for the representation of the inflectional paradigms of Arabic words is presented.
This paper describes a framework for multilingual inheritance-based lexical representation which al-lows sharing of information across languages at all levels of linguistic description.
The paper fo-cuses on phonology.
This article is concerned with determining the constraints on the selection of appropriate intonation in speech generation in human- machine information seeking dialogues.
We take into consideration factors such as dialogue history, speaker's attitudes, hearer's expectations, and semantic speech functions.
We describe the Spanish-to-English LDVCOMBO system for the Shared Task 2: 揈xploiting Parallel Texts for Statistical Machine Translation?of the ACL-2005 Workshop on 揃uilding and Using Parallel Texts: Data-Driven Machine Translation and Beyond?
Several phrase-based translation models are built out from these alignments.
In this paper we describe the analytic question answering system HITIQA (High-Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts.
Parsing in type logical grammars amounts to theorem proving in a substructural logic.
This paper takes the proof net presentation of Lambek抯 associative calculus as a case study.
We compare the effect of lexical, temporal, syntactic, semantic, and prosodic features on system performance.
This paper proposes the use of uncertainty reduction in machine learning methods such as co-training and bilingual bootstrapping, which are referred to, in a general term, as 慶ollaborative bootstrapping?
It proposes a new measure for representing the degree of uncertainty correlation of the two classifiers in collaborative bootstrapping and uses the measure in analysis of collaborative bootstrapping.
Furthermore, it proposes a new algorithm of collaborative bootstrapping on the basis of uncertainty reduction.
This paper analyzes the requirements for adding a temporal reasoning component to a natural language database query system, and proposes a computational model that satisfies those requirements.
The paper describes a similarity-based model to present the morphological rules for Chinese com-pound nouns.
A technique for reducing a tagset used for n-gram part-of-speech disambiguation is introduced and evaluated in an experiment.
A scalable natural language generation (NLG) system called HYPERBUG 1 embedded in an agent-based, multimodal dialog system is presented.
The few existing approaches have focused on the development of algorithms specific to word domain disambiguation.
In this paper we explore an alternative approach where word domain disambiguation is achieved via word sense disambiguation.
The paper presents a computational theory for resolving Japanese zero anaphora, based on the notion of discourse segment.
We see that the discourse segment reduces the domain of antecedents for zero anaphora and thus leads to their efficient resolution.Also we make crucial use of functional notions such as empathy hierarchy and minimal semantics thesis to resolve reference for zero anaphora [Kuno, 1987].
Multimodal dialogue systems allow users to input information in multiple modalities.
This paper discusses human semantic knowledge and processing in terms of the SCHOLAR system.
The strategy for natural language interpretation presented in this paper implements the dynamics of context change by translating natural language texts into a meaning representation language consisting of (descriptions of) programs, in the spirit of dynamic predicate logic (DPL) [5].
We present a discriminative, large- margin approach to feature-based matching for word alignment.
The structural features provide the information for the correspondences of parts-ofspeech (POS) sequences which are useful in translation.
Discourse chunking is a simple way to segment dialogues according to how dialogue participants raise topics and negotiate them.
The CIAOSENSO WSD system is based on Conceptual Density, WordNet Domains and frequences of WordNet senses.
This paper describes the upvunige-CIAOSENSO WSD system, we participated in the english all-word task with, and its versions used for the english lexical sample and the Word- Net gloss disambiguation tasks.
We use a hybrid between data-driven and knowledge-based mtehods: the basic structure is derived from a set of rules about gesture salience, but the salience weights themselves are learned from a corpus.
We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task.
Based on these results, we present an ECA that uses verbal and nonverbal grounding acts to update dialogue state.
This article presents the Structural Correspondence Specification Environment (SCSE) being implemented at GETA.The SCSE is designed to help linguists to develop, consult and verify the SCS Grammars (SCSG) which specify linguistic models.
This paper addresses two previously unresolved issues in the automatic evaluation of Text Structuring (TS) in Natural Language Generation (NLG).
This paper investigates the use of a language inde-pendent model for named entity recognition based on iterative learning in a co-training fashion, using word-internal and contextual information as inde-pendent evidence sources.
We propose a new approach under the example-based machine translation paradigm.
This paper investigates the feasibility of automated scoring of spoken English proficiency of non-native speakers.
(2) We use classification and regression trees to understand the role of different features and feature classes in the characterization of speaking proficiency by human scorers.
In this article, we apply to natural language parsing and tagging the device of trigger- pair predictors, previously employed exclusively within the field of language modelling for speech recognition.
We describe MSR-MT, a large-scale hybrid machine translation system under development for several language pairs.
Trained on English and Spanish technical prose, a blind evaluation shows that MSR-MT抯 integration of rule-based parsers, example based processing, and statistical techniques produces translations whose quality exceeds that of uncustomized commercial MT systems in this domain.
The phrase acquisition is based on entropy minimization and the feature selection is driven by the entropy reduction principle.
We further demonstrate that the phrase- grammar based n-gram language model significantly outperforms a phrase-based n-gram language model in an end-to-end evaluation of a spoken language application.
The method is based on graph rewriting using memory- based learning, applied to dependency structures.
It also facilitates dependency-based evaluation of phrase structure parsers.
TEXAN is a system of transfers-oriented text analysis.
The transition of this concept of linguistic actions (text acts) to the model of computer analysis is performed by a context-free illocution grammar processing categories of actions and a propositional structure of states of affairs.
This is accomplished by formulating the semantic role labeling as a classification problem of dependency relations into one of several semantic roles.
A dependency tree is created from a constituency parse of an input sentence.
The dependency tree is then linearized into a sequence of dependency relations.
Finally, the features are input to a set of one-versus-all support vector machine (SVM) classifiers to determine the corresponding semantic role label.
We present in this paper the Language Computer抯 system for WMT06 that employs LM- powered reranking on hypotheses generated by phrase-based SMT systems
As part of a cooperative project, we present an innovative approach to auxiliaries and multiple genitive NPs in German.
The approach developed for mul-tiple genitive NPs provides a more ab-stract, language independent representa-tion of genitives associated with noun-nalized verbs.
This paper systematically compares unsupervised word sense discrimination techniques that cluster instances of a target word that occur in raw text using both vector and similarity spaces.
We present a corpus-based study of methods that have been proposed in the linguistics literature for selecting the semantically unmarked term out of a pair of antonymous adjectives.
Temporal information analysis is very important for Chinese Information Process.
We demonstrate how this grammar may be used for efficient incremental parsing, by employing normalisation techniques.
We address the problem of using partially la-belled data, eg large collections were only little data is annotated, for extracting biological en-tities.
We describe and experimentally evalu-ate a system, FeasPar, that learns pars-ing spontaneous speech.
This paper describes the semantic diversity of A no B and a method of semantic analysis for such phrases based on feature unification.
The parsers combine lexical indices such as discourse markers with formatting instruc-tions (HTML tags) for analyzing enumera-tions and associated initializers.
Corpus-based grammar induction relies on us-ing many hand-parsed sentences as training examples.
This paper presents the user-centered interface of a summarization system for physicians in Bone Marrow Transplan-tation (BMT).
It is based on pragmatic proper-ties of Japanese conditionals.
This paper first presents a review of the options available for conveying maps and graphics to visually impaired and blind people.
Tactile perception is serial rather than synoptic.
We also outline an approach for automatically revising attribute value grammars using counter-examples.
This paper examines a particular PROLOG implementation of Discourse Representation theory (DR theory) constructed at the University of Texas.
The implementation also contains a Lexical Functional Grammar parser that provides f-structures; these 1-structures are then translated into the semantic representations posited by DR theory, structures which are known as Discourse Representation Structures (DRSs).
Srini vas	(97)	enriches	traditionalmorpho-syntactic POS tagging withsyntactic	information	by	introducingSupertags.
We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts.
In this paper, we discuss the performance of cross- lingual information extraction systems employing an automatic pattern acquisition module.
Our system employs a set of semantic similarity metrics using the noun portion of WordNet as a knowledge source.
We describe a system that models all levels of the spoken language system using stochastic language models and present experimental results.
This paper describes the Corinna system which integrates a theoretical approach to dialogue modeling with text generation techniques to conduct cooperative dialogues in natural language.
We propose an organization of the lexicon and its interaction with grammar and knowledge that makes extensive use of lexical functions from the Meaning-Text-Theory of Mel'euk.
the complexity of language domain and concept inventory).
The featural model provides statistical evidence for the claim that speakers use non-canonicals to communicate information about discourse structure.
This paper provides an account of the role that the interaction between nominal and temporal reference plays in resolving temporal reference.
We present an empirically grounded method for evaluating content selection in summarization.
We introduce CST (cross-document structure theory), a paradigm for multi-document analysis.
CST takes into account the rhetorical structure of clusters of related textual documents.
This paper describes MIMIC, an adaptive mixed initia-tive spoken dialogue system that provides movie show-time information.
First, it employs initiative-oriented strategy adaptation to automatically adapt response generation strategies based on the cumu-lative effect of information dynamically extracted from user utterances during the dialogue.
Event clustering on streaming news aims to group documents by events automatically.
A dynamicthreshold using time decay function and spanning window is proposed.
The experimental results show that both event words and co-reference chains are useful on event clustering.
In this paper we describe a new technique for parsing free text: a transformational grammar' is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.
CDG represents a sentence抯 grammatical structure as assignments of dependency relations to functional variables associated with each word in the sentence.
In this paper, we describe a statistical CDG (SCDG) parser that performs parsing incrementally and evaluate it on the Wall Street Journal Penn Treebank.
Factors contributing to the SCDG parser抯 performance are analyzed.
We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions.
The concern of this paper is the signalling of segments and relations in written texts.
We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases.
Speech repairs occur often in spontaneous spo-ken dialogues.
We present a framework to detect and correct speech repairs where all rel-evant levels of information, i.e., acoustics, lexis, syntax and semantics can be integrated.
Second a stochastic model.
Finally a lattice parser decides on accepting the repair.
PeriPhrase is a high-level computer language developed by A.L.P.
Systems to facilitate parsing and structural transfer.
The following simple rule forms a noun phrase (NP).!
Suppose that we declared a category NV for marking noun-verb homographs.
We present the multilingual sum-marization functionality for VERB-MOBIL, a speech translation system.
Observed counts vary more than simple models predict, prompting the use of overdispersed models like Gamma-Poisson or Beta-binomial mixtures as robust alternatives.
We propose using zero- inflated models for dealing with this, and evaluate competing models on a Naive Bayes text classification task.
We have proposed a method of ma-chine translation, which acquires trans-lation rules from translation examples using inductive learning, and have eval-uated the method.
In this paper we present Morphy, an integrated tool for German morphology, part-ofspeech tagging and context-sensitive lemmatization.
Syntactic ambiguities can be resolved with a standard statistical part-of-speech tagger.
The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verb- final clauses.
Successive models are evaluated on precision and recall of phrase markup.
This paper proposes a multi-dimensional framework for classifying text documents.
Using k-NN, na飗e Bayes and centroidbased classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications.
We also introduce 106 novel diathesis alternations, created as a side product of constructing the new classes.
We describe two approaches to analyzing and tagging team discourse using Latent Semantic Analysis (LSA) to predict team performance.
This paper describes a system for rapidly retargetable interactive translingual retrieval.
We call this the process of GRAMMATICAL KrourzAmoN2 forced by grammar sharing.
Then what we do is identify sharable packages of common string-types and information structures among independently motivated language-specific grammatical assertaions.
This paper describes FERRET, an interactive question-answering (Q/A) system designed to address the challenges of integrating automatic Q/A applications into real-world environments.
Recent work in Question Answering has focused on web-based systems that extract answers using simple lexicosyntactic patterns.
Among these are parameters based on the variance of the word-frequency distribution (NOCC/EK), and on information theoretical (signal- noise S/N) premises.
We describe a parser used in the CoNLL 2006 Shared Task, 揗ultingual Dependency Parsing.
?The parser first identifies syntactic dependencies and then labels those dependencies using a maximum entropy classifier.
THALES is a software package for plane geometry constructions, supplied with a natural language interface.
A simple class of Context-free Queue Grammars is introduced and discussed.
We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features.
We find, amongst other things, a strong relationship between citation function and sentiment classification.
Experimental results and a comparison to other state—of—theart taggers are reported.Keywords: POS Tagging, Corpus—based modeling, Decision Trees, Ensembles of Classifiers.
The Sydney Language Independent Named En-tity Recogniser and Classifier (SLINERC) is a multi-stage system for the recognition and clas-sification of named entities.
This paper presents cohesion trees (CT) as a data structure for the perspective, hierarchical organization of text corpora.
CTs operate on alternative text representation models taking lexical organization, quantitative text characteristics, and text structure into account.
This paper describes a method to acquire hyponyms for given hypernyms from HTML documents on the WWW.
We present a system for analyzing conversational data.
We show that the common wisdom is wrong for stochastic grammars that use elementary trees instead of context-free rules, such as Stochastic Tree-Substitution Grammars used by Data-Oriented Parsing models.
This paper describes the initial results of an experiment in integrating knowledge-based text processing with real-world reasoning in a question answering system.
We propose an extension to these states which handles some additional cases of multiple ambiguous pronouns.
SRI's approach is to integrate speech and linguistic knowledge into the HMM framework.
This paper describes performance improvements arising from detailed phonological modeling and from the incorporation of cross-word coarticulatory constraints.
We show that it is possible to learn the contexts for linguistic operations which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy.
We cast the problem of learning the contexts for the linguistic operations asclassification tasks, and applystraightforward machine learning techniques, such as decision tree learning.
The training data consist of linguistic features extracted from syntactic and semantic representations produced by a linguistic analysis system.
The target features are extracted from links to surface syntax trees.
Many corpus-based machine translation systems require parallel corpora.
In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus.
A decision tree is used for organizing contextual descriptions of phonological variation.
In this paper, we present and com-pare various single-word based align-ment models for statistical machinetranslation.
We discuss the fiveIBM alignment models, the Hidden-Markov alignment model, smooth-ing techniques and various modifica-tions.
We present different methodsto combine alignments.
This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG pars-ing system.
In this paper we describe results from using a maximum entropy (ME) classifier to identify metaphors.
This paper introduces new definitions of Chinese base phrases and presents a hybrid model to combine Memory-Based Learning method and disambiguation proposal based on lexical information and grammar rules populated from a large corpus for 9 types of Chinese base phrases chunking.
We select the final translation set using the mutual information score and the TF譏DF score.
We address the problem of structural disambiguation in syntactic parsing.
We define a 'three-word probability' for implementing LPR, and a 'length probability' for implementing RAP and ALPP.
Large-scale knowledge-based machine translation requires significant amounts of lexical knowledge in order to map syntactic structures to conceptual structures.
This paper presents a new approach based on Equivalent Pseudowords (EPs) to tackle Word Sense Disambiguation (WSD) in Chinese language.
The experiment verifies the value of EP for unsupervised WSD.
In this paper, we present a method for determining the animacy of English nouns using WordNet and machine learning techniques.
The comma plays an important role in long Chinese sentence segmentation.
This paper proposes a method for classifying commas in Chinese sentences by their context, then segments a long sentence according to the classification results.
We also carried out a pilot study on using this automatically generated bilingual word sense dictionary to choose the best translation candidates and show the first significant evidence that frame semantics are useful for translation disambiguation.
These results demonstrate the great potential for future application of bilingual frame semantics to machine translation tasks.
We present two methods for semiautomatic detection and correction of errors in textual databases.
This paper proposes a two-layered model of dialogue structure for task-oriented dialogues that processes contextual information and disambiguates speech acts.
The final goal is to improve translation quality in a speech-to-speech translation system.
LP grammar formalism separates constraints on immediate dominance front those on linear order.
First we map verb tokens in sentential contexts to a fixed set of seed verbs using WordNet :: Similarity and Moby抯 Thesaurus.
We then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modifier.
Most machine transliteration systems transliterate out of vocabulary (OOV) words through intermediate phonemic mapping.
It uses the linguistic knowledge of possible conjuncts and diphthongs in Bengali and their equivalents in English.
It uses n-gram mutual information, relative frequency count and parts of speech as the features for compound extraction.
The problem is modeled as a two-class classification problem based on the distributional characteristics of n-gram tokens in the compound and the non-compound clusters.
The SAUMER system uses specifications of natural language grammars.
which consist of rules and metarules.
like the correspondence between syntactic and semantic rules, with definite clause grammars (DCGs) (Pereira and Warren.
1980) to create an executable grammar specification.
relative clauses.
passivisation.
and questions.
This paper describes the characteristic features of dependency structures of Japanese spoken language by investigating a spoken dialogue corpus, and proposes a stochastic approach to dependency parsing.
The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences.
Included here are all closed-class morphemes and words--inflections, par-ticles, adpositons, conjunctions, demonstratives, etc.--as well as syntactic constructions, grammatical relations; categorial identities, word order, and intonation.
A scheme for syntax-directed translation that mirrors compositional model-theoretic semantics is discussed.
We present two methods to select the most significant single word trigger pairs.
This paper reports on the SYN-RA (SYNtax-based Reference Annotation) project, an on-going project of annotating German newspaper texts with referential relations.
The project has developed an inventory of anaphoric and coreference relations for German in the context of a unified, XML-based annotation scheme for combining morphological, syntactic, semantic, and anaphoric information.
We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.
We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints.
Finally, we proposed a way to construct a noun case frame dictionary by using examples of "X no Y ."
Our algorithm builds on previous research on corpus-based methods for acquiring extraction patterns and semantic lexicons.
In this paper we sketch an approach for Natural Language parsing.
These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure - a top down approach.Keywords: Example-based parsing, SSTC .
To overcome this limitation, this paper proposes a novel generation method utilizing perceptual groups of objects and n-ary relations among them.
We describe how simple, commonly understood statistical models, such as statistical dependency parsers, probabilistic context-free grammars, and word-to-word translation models, can be effectively combined into a unified bilingual parser that jointly searches for the best English parse, Korean parse, and word alignment, where these hidden structures all constrain each other.
The basic QA system relies on complete sentence parsing, inferences, and semantic representation matching.
The paper deals with the application of NLP technology in e-learning.
We report our research on intelligent platforms for computer- mediated education.
We present a method for corpus-based induction of an LFG syntax-semantics interface for frame semantic processing in a computational LFG parsing architecture.
We show how to model frame semantic annotations in an LFG projection architecture, including special phenomena that involve non-isomorphic mappings between levels.
We evaluate the results by applying the inducedframe assignment rules to LFG parser output.'
We follow this line and explore the potential of deep syntactic analysis for role labelling, choosing Lexical Functional Grammar as underlying syntactic framework.
We aim at a computational interface for frame semantics processing that can be used to (semi-)automatically extend the size of current training corpora for learning stochastic models for role labelling, and ?ultimately ?as a basis for automatic frame assignment in NLP tasks, based on the acquired stochastic models.We discuss advantages of semantic role assignment on the basis of functional syntactic analyses as provided by LFG parsing, and present an LFG syntax-semantics interface for frame semantics, building on a first study in Frank and Erk (2004).
Finally, we apply the acquired frame assignment rules in a computational LFG parsing architecture.The paper is structured as follows.
In Section 3 we discuss advantages of deeper syntactic analysis for a principle-based syntax- semantics interface for semantic role labelling.
Section 4 describes the method we apply to derive frame assignment rules from corpus annotations: we port the frame annotations to a 損arallel?LFG corpus and induce general LFG frame assignment rules, by extracting syntactic descriptions for the frame constituting elements.
We use LFG抯 functional representations to distinguish local and non-local role assignments.
We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords.
We define a set of deterministic bottom-up left to right parsers which analyze a subset of Tree Adjoining Languages.
The LR parsing strategy for Context Free Grammars is extended to Tree Adjoining Grammars (TAGs).
We use a machine, called Bottom-up Embedded Push Down Automaton (BEPDA), that recognizes in a bottom-up fashion the set of Tree Adjoining Languages (and exactly this set).
Each parser consists of a finite state control that drives the moves of a Bottom-up Embedded Pushdown Automaton.
The parsers handle deterministically some context-sensitive Tree Adjoining Languages.In this paper, we informally describe the BEPDA then given a parsing table, we explain the LR parsing algorithm.
We then show how to construct an LR(0) parsing table (no lookahead).
Then, we explain informally the construction of SLR(1) parsing tables for BEPDA.
In this paper Brills rule-based PoS tagger is tested and adapted for Hungarian.
Reordering is currently one of the most important problems in statistical machine translation systems.
This paper presents a novel strategy for dealing with it: statistical machine reordering (SMR).
This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation.
In this interactive presentation, a Chinese named entity and relation identification system is demonstrated.
The domain- specific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication.
A generative statistical model of dependency syntax is proposed based on Tesniere's classical theory.
We investigate lexical paraphrasing in the context of two distinct applications: document retrieval and node identification.
Node identification ?performed in the context of a Bayesian argumentation system ?matches users?Natural Language sentences to nodes in a Bayesian network.
Lexical paraphrases are generated using syntactic, semantic and corpus-based information.
Our evaluation shows that lexical paraphrasing improves retrieval performance for both applications.
This work applies boosted wrapper induction (BWI), a machine learning algorithm for information extraction from semi-structured documents, to the problem of named entity recognition.
The default feature set of BWI is augmented with features based on distributional term clusters induced from a large unlabeled text corpus.
The treebank is built from dictionary definition sentences, and uses an HPSG based Japanese grammar to encode the syntactic and semantic information.
We show how this treebank can be used to extract thesaurus information from definition sentences in a language-neutral way using minimal recursion semantics.
grammatical function, phrase type, and other syntactic traits).
This paper describes the design and application of time-enhanced, finite state models of discourse cues to the automated segmentation of broadcast news.
We describe our analysis of a broadcast news corpus, the design of a discourse cue based story segmentor that builds upon information extraction techniques, and finally its computational implementation and evaluation in the Broadcast News Navigator (BNN) to support video news browsing, retrieval, and summarization.
In previous work, supertag disambiguation has been presented as a robust partial parsing technique.
CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with essay-based discourse elements (e.g., thesis statements).
This system identifies features of sentences based on semantic similarity measures and discourse structure.
Intra-sentential quality is evaluated with rule-based heuristics.
In this paper, we present a statistical approach for dialogue act processing in the dialogue component of the speech-to-speech translation system VERBMOBIL.
Statistics in dialogue processing is used to predict follow-up dialogue acts.
We study parsing of tree adjoining grammars with particular emphasis on the use of shared forests to represent all the parse trees deriving a well-formed string.
The schemes using hg and cfg to represent parses can be seen to underly most of the existing tag parsing algorithms.
We discuss the advantages of lexical-ized tree-adjoining grammar as an al-ternative to lexicalized PCFG for sta-tistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its pars-ing performance.
This paper presents an efficient algorithm for the incremental construction of a minimal acyclic sequential transducer (ST) for a dictionary consisting of a list of input and output strings.
Our method of capturing ungrammaticalities involves using malrules (also called 'error productions/.
It is popular in WSD to use contextual information in training sense tagged data.
This paper reports on word sense disambiguation of English words using static and dynamic sense vectors.
Finally, a word sense of a target word is determined using static and dynamic sense vectors.
In this paper, we propose an Inductive Logic Programming learning method which aims at automatically extracting special Noun-Verb (N-V) pairs from a corpus in order to build up semantic lexicons based on Pustejovsky's Gen-erative Lexicon (GL) principles (Pustejovsky, 1995).
a tagset containing information about wordclasses.
A semantic lexicon based on conceptual graph structures is used to guide text understanding.
We examine their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization.
This paper presents some of the first data visualizations and analysis of distributions for a lexicalized statistical parsing model, in order to better understand their nature.
We explore several distance-based combining methods, as well as a number of distance metrics involving both word and character ngrams.
A maximum entropy classifier is used in our semantic role labeling system, which takes syntactic constituents as the labeling units.
The maximum entropy classifier is trained to identify and classify the predicates?semantic arguments together.
The word sequences are predicted from the attribute using word n-gram model.
The spell of Unknow word is predicted using character n-gram model.
We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and combines them into a coherent predicate- argument output by solving an optimization problem.
We present a new approach to disambiguating syntactically ambiguous words in context, based on Variable Memory Markov (VMM) models.
In a test of a VMM based tagger on the Brown corpus, 95.81% of tokens are correctly classified.
We explore the application of memory- based learning to morphological analysis and part-of-speech tagging of written Arabic, based on data from the Arabic Treebank.
We report on the performance of the morphological analyzer and part-of-speech tagger.
As part of the CUBRICON project, we are developing NL processing technology that incorporates deictic and graphic gestures with simultaneous coordinated NL for both user inputs and system-generated outputs.
Lastly we show that patterns of information exchanges in speaker alternation and initiative-taking can be used to characterise three-party dialogues.
To acquire these noun-verb pairs, we use ASARES, a machine learning technique that automatically infers extraction patterns from examples and counter-examples of realization noun-verb pairs.
We present a neural-network based self-organizing approach that enables vi-sualization of the information retrieval while at the same time improving its precision.
ALLiS 2.0 is a theory refinement system using hierarchical data.
This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech.
In this paper, we describe the pronominal anaphora resolution module of Lucy, a portable English understanding system.
We also propose keyword pair model for the synchronisation between text and speech.
This paper discusses the translation of temporal expressions, in the framework of the machine translation system Rosetta.
In section 3will sketch isomorphic grammars for temporal expressions and illustrate them by some examples.
This paper looks at representing paraphrases using the formalism of Synchronous TAGs; it looks particularly at comparisons with machine translation and the modifications it is necessary to make to Synchronous TAGs for paraphrasing.
We have analyzed definitions from Webster's Seventh New Collegiate Dictionary using Sager's Linguistic String Parser and again using basic UNIX text processing utilities such as grep and awk.
This paper describes a fully implemented system for fusing related news stories into a single comprehensive description of an event.
We describe our experience in developing a discourse-annotated corpus for community-wide use.
We show that this methodology can be used for automatic domain template creation.
We present a supervised machine learning algorithm for metonymy resolution, which exploits the similarity between examples of conventional metonymy.
We show that syntactic head-modifier relations are a high precision feature for metonymy recognition but suffer from data sparseness.
We also compare automatic and manual methods for syntactic feature extraction.
HMM-based models are developed for the alignment of words and phrases in bitext.
There has recently been a revival of interest in Categorial Grammars (CG) among computational linguists.
This algorithm offers a uniform treatment for "spurious" syntactic ambiguities and the "genuine" structural ambiguities which any processor must cope with, by exploiting the associativity of functional composition and the procedural neutrality of the combinatory rules of grammar in a bottom-up, left-to-right parser which delivers all semantically distinct analyses via a novel unification-based extension of chart-parsing.L Combinatory Categorial Grammars"Pure" categorial grammar (CC) is a grammatical notation, equivalent in power to context-free grammars, which puts all syntactic information in the lexicon, via the specification of all grammatical entities as either functions or arguments.
We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases.
The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations.
We also present a model and algorithm for machine translation involving optimal "tiling" of a dependency tree with entries of a costed bilingual lexicon.
We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation.
This paper discusses complex structural changes during transfer within a non-destructive transfer framework.
a small hand-annotated corpus of news messages.
We are carrying out investigations into the application of biophysical and computational models to speech processing.
Described here are studies of the robustness of a speech representation using a biophysical model of the cochlea; experimental results on the representation of speech and complex sounds in the mammalian auditory cortex; and descriptions of computational sequential processing networks capable of recognizing sequences of phonemes.
This paper investigates the problems facing modelling agents?beliefs in Discourse Representation Theory (DRT) and presents a viable solution in the form of a dialogue-based DRT representation of beliefs.
Integrating modelling dialogue interaction into DRT allows modelling agents?beliefs, intentions and mutual beliefs.
We describe the development environment available to linguistic developers in our lab in writing large-scale grammars for multiple languages.
Our discourse-level annotation scheme covers properties of discourse referents (e.g., semantic sort, delimitation, quantification, familiarity status) and anaphoric links (coreference and bridging).
In this paper, we consider exploiting both prosodic and text-based features for topic segmentation of Mandarin Chinese.
We contrast these results with classification using text- based features, exploiting both text similarity and n-gram cues, to achieve accuracies between 77- 95.6%, if silence features are used.
Finally we integrate prosody, text, and silence features using a voting strategy to combine decision tree classifiers for each feature subset individually and all subsets jointly.
We propose a statistical dialogue analysis model to determine discourse structures as well as speech acts using maximum entropy model.
We propose the idea of tagging discourse segment boundaries to represent the structural information of discourse.
Using this representation we can effectively combine speech act analysis and discourse structure analysis in one framework.
We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.
Lexical co-occurrence statistics are becoming widely used in the syntactic analysis of unconstrained text.
For example, the "lexical association" strategy for resolving ambiguous prepositional phrase attachments [Hindle and Rooth.
We developed a fully automated Information Retrieval System which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval.
We investigate the use of a statistical sentence gener-ation technique that recombines words probabilistically in order to create new sentences.
Given a set of event-related sentences, we use an extended version of the Viterbi algorithm which employs dependency relation and bigram proba-bilities to find the most probable sum-mary sentence.
This pa-per presents a reinforcement learning approach for automatically optimizing a dialogue strategy that addresses the technical challenges in applying re-inforcement learning to a working dialogue system with human users.
We first dis-cuss a problem of developing a knowl-edge base by using natural language doc-uments: wrong information in natural language documents.
This paper reports sentence alignment results from 2 corpora, in a 2- pass dictionary based alignment system.
We also propose metrics for measuring directed lexical influence and compare performances.
Keywords: contextual post-processing, defining context, lexical influence, directionality of context
This paper proposes to apply machine learning techniques to the task of combining outputs of multiple LVCSR models.
We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal.
Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure.
The design and implementation of an XML-based corpus environment for multilevel anno-tated multimodal (language) data is described.
subordinators and verbs).
This paper presents a semi-automatic technique for developing broad-coverage finite-state morphological analyzers for any language.
This elicitbuild-test technique compiles lexical and inflectional information elicited from a human into a finite state transducer lexicon and combines this with a sequence of morphographemic rewrite rules that is induced using transformation-based learning from the elicited examples.
Dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides ID information of recognized terms unlike machine learning based approaches.
We also present an approximate string searching method to alleviate the latter problem.
This paper presents a methodology for using the argument structure of sentences, as encoded by the PropBank project, to develop clusters of verbs with similar meaning and usage.
We develop admissible A* search heuristics for synchronous parsing with Inversion Transduction Grammar, and present results both for bitext alignment and for machine translation decoding.
We also combine the dynamic programming hook trick with A* search for decoding.
This paper presents a new language independent model for analysis and generation of word forms based on Finite State Transducers (FSTs).
This processor uses syntactic and semantic informations about words in order to construct a semantic net representing the meaning of the sentences.
This work describes our next step to improve the usability of speechrecognizers---the use of vocabulary-independent (VI) models.
He combines a two-level approach to morpho-graphemics with a unification grammar approach (a modified PATR rule interpreter) to morpho-syntax.
The approach uses topic identification algorithm named FIFA to text classification.
This paper proposes a new approach for Multi-word Expression (MWE)extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis.
Theory of Longest Common Subsequence (LCS) originates from computer science and has been established as affine gap model in Bioinformatics.
We perform this developed LCS technique combined with linguistic criteria in MWE extraction.
We propose a statistical method that finds the maximum-probability seg-mentation of a given text.
We describe and experimentally evaluate an efficient method for automatically de-termining small clause boundaries in spon-taneous speech.
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
Our system incorporates a linguistic parser/generator for LFG, a transfer component for parse reduction operating on packed parse forests, and a maximum-entropy model for stochastic output selection.
So the process consists of finding word stems and than correcting word endings .
This paper describes a technique for parsing dependency grammars using a bottom-up chart parser originally designed for phrase-structure grammars, using typed feature structures as the only data structure.
The first method employs HPSG'S BACK-GROUND feature and a constraint-satisfaction component pipe-lined after the parser.
The second method uses subsorts of referential in-dices, and blocks readings that violate selec-tional restrictions during parsing.
In this paper we present a system for automatically producing multimedia pages of information that draws both from results in data-driven aggregation in information visualization and from results in communicative-goal oriented natural language generation.
We introduce an original data structure and efficient algorithms that learn some families of transformations that are relevant for part-of-speech tagging and phonological rule systems.
We compare five generative graphical models and a neural network, using lexical, syntactic, and semantic features, finding that the latter help achieve high classification accuracy.
Statistical language modeling requires a large corpus for the application domain.
the task of WSD of Word- Net glosses and the task of WSD of English lexical sample.
In this paper, an augmented chart data structure with efficient word lattice parsing scheme in speech recognition applications is proposed.
This paper presents SPQR (Selectional Pat-tern Queries and Responses), a module of the PUNDIT text-processing system designed to facili-tate the acquisition of domain-specific semantic information, and to improve the accuracy and efficiency of the parser.
This paper presents the English valency lexicon EngValLex, built within the Functional Generative Description framework.
In this paper a novel solution to automatic and unsupervised word sense induction (WSI) is introduced.
Like most existing approaches it utilizes clustering of word co-occurrences.
We present an unsupervised approach to Word Sense Disambiguation (WSD).
We automatically acquire English sense examples using an English-Chinese bilingual dictionary, Chinese monolingual corpora and Chinese-English machine translation software.
We then train machine learning classifiers on these sense examples and test them on two gold standard English WSD datasets, one for binary and the other for fine-grained sense identification.
We extend the centering model for the resolution of intra-sentential anaphora and specify how to handle complex sentences.
In SPHINX-II, we incorporated additional dynamic and speaker-normalized features, replaced discrete models with sex-dependent semi-continuous hidden Markov models, augmented within-word triphones with between-word triphones, and extended generalized triphone models to shared- distribution models.
The configuration of SPHINX-II being used for this task includes sex-dependent, semi-continuous, shared- distribution hidden Markov models and left context dependent between-word triphones.
The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer.
A previous paper described a near-optimal admissible Viterbi A* search algorithm for use with noncross-word acoustic models and no-grammar language models [16].
This paper extends this algorithm to include unigram language models and describes a modified version of the algorithm which includes the full (forward) decoder, cross-word acoustic models and longer-span language models.
This paper introduces a system for categorizing un-known words.
This paper presents a method for combining different expressions of the re-sampling approach in a mixture of experts framework.
This paper studies the enrichment of Spanish WordNet with synset glosses automatically obtained from the English Word- Net glosses using a phrase-based Statistical Machine Translation system.
This paper presents a framework for the definition of monotonic repair rules on chart items and Lexicalized Tree Grammars.
These local rules cover ellipsis and common extra-grammatical phenomena such as self-repairs.
We present the psycholinguistically motivated task of predicting human plausibility judgements for verb-role-argument triples and introduce a probabilistic model that solves it.
We report work on effectively incorporating lin-guistic knowledge into grammar induction.
We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn `missing' grammar rules from an incomplete grammar.
The task is to generate sentences with hotel-information from a structured database.
The system uses core technologies such as speaker segmentation, automatic speech recognition, transcription alignment, keyword extraction and speech indexing and retrieval to make spoken communications easy to navigate.
This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.
by the use of graph-structured stacks and packed parse forests (Tomita 1985)).
iii contrast, statistical techniques using lexeme co-occurrence provide a relatively simple mechanism which can imitate semantic filtering in many cases.
Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson抯 product moment correlation coefficient or Spearman抯 rank order correlation coefficient between human scores and automatic scores.
General characteristics of a pragmatic metric for the production evaluation of speech-to-speech translations are dis-cussed.
This work attempts to provide a computational solution, called Word Filtering, to handle those three points prior to parsing.The proposed model of Thai morphological analysis consists of three steps: sentence segmenting, spell checking and word filtering.
We are studying how to extract hierarchical relation on verbs from definition sentences in a Japanese dictionary.
In this paper, we present a statistical language-independent framework for identifying and tracking named, nominal and pronominal references to entities within unrestricted text documents, and chaining them into clusters corresponding to each logical entity present in the text.
In addition, the mention detection model crucially uses feature streams derived from different named entity classifiers.
This paper proposes a knowledge representation model and a logic proving setting with axioms on demand successfully used for recognizing textual entailments.
We describe an approach to Machine Transla-tion of transcribed speech, as found in closed captions.
In particular, we describe components for proper name recognition and input segmentation.
In this paper we present a corpus-based study ofNSUs.
This paper introduces a context-sensitive electronic dictionary that provides translations for any piece of text displayed on a computer screen, without requiring user interaction.
A language independent model for recognition and production of word forms is presented.
In this paper we describe the web and mobile-phone interfaces to our multi- language factoid question answering (QA) system together with a prototype speech interface to our English-language QA system.
Using a statistical, data-driven approach to factoid question answering has allowed us to develop QA systems in five languages in a matter of months.
We have developed a discourse level tagging tool for spoken dialogue corpus using machine learning methods.
In dialogue act tagging, we have implemented a transformation-based learning procedure and resulted in 70% accuracy in open test.
A language-independent method of finite- state surface syntactic parsing and word-disambiguation is discussed.
The role of the morphological analysis in accurate name recognition is discussed.
We also provide evaluations of both morphological analysis and name recognition.
In this paper we also present the impact of modeling the user background on Q/A and discuss the pragmatics pf processing negation in Q/A.
This presentation describes an example- based English-Japanese machine trans- lation system in which an abstract linguistic representation layer is used to extract and store bilingual translation knowledge, transfer patterns between languages, and generate output strings.
The project deals with the evaluation of term and semantic relation extraction from corpora in French.
This expression covers respectively, term extractors, classifiers and semantic relation extractors.
Functional Unification Grammars (FUGs) are popular for natural language applications because the formalism uses very few primitives and is uniform and expressive.
We have implemented an extension of traditional functional unification.
It is based on the notions of typed features and typed constituents.
We show the advantages of this extension in the context of a grammar used for text generation.
Both anaphora resolution and prepositional phrase (PP) attachment are the most frequent ambiguities in natural language processing.
This paper describes the CTB Coreference An-notation Guidelines for annotating pronominal anaphoric expressions in the Penn Chinese Tree-bank.
There are mainly two kinds of statistic- based measures for word extraction: the internal measure and the contextual measure.
This paper suggests refinements for the Distributional Similarity Hypothesis.
Our proposed hypotheses relate the distributional behavior of pairs of words to lexical entailment ?a tighter notion of semantic similarity that is required by many NLP applications.
We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing.
To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel抯 implementation of Collins?lexicalized PCFG model, and on Chiang抯 CFG-based decoder for hierarchical phrase-based translation.
This paper describes a simple pattern- matching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.
This paper presents a technique for discover-ing translationally equivalent texts.
This paper outlines the experimental development of the SWESIL system: a structured lexicon-based word expert system designed to play a pivotal role in the process of Distributed  Language Translation (DLT) which is being developed in the Netherlands.
We describe the design of an MT system that em-ploys transfer rules induced from parsed bitexts and present evaluation results.
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
We propose in this paper a maximum entropy approach for restoring diacritics in a document.
The approach can easily integrate and make effective use of diverse types of information; the model we propose integrates a wide array of lexical, segment- based and part-of-speech tag features.
We develop an efficient way of ex-tracting the compound noun index-ing rules automatically and perform extensive experiments to evaluate our indexing rules.
We report on a study examining the generation of noun phrases within a spoken dialog agent for a navigation domain.
We present MAGEAD, a morphological analyzer and generator for the Arabic language family.
We present a detailed evaluation of MAGEAD.
Four kinds of classifiers were used in our experiments: Na飗e Bayes, Rocchio, k-NN, and SVM.
We exploit the event/result reading distinction, combined with other aspectual information.
We describe a parser for robust and flexible interpretation of user utterances in a multi-modal system for web search in newspaper databases.
Our parser integrates shallow parsing techniques with knowledge-based text retrieval to allow for robust processing and coordination of input modes.
These co-occurrence statistics concern typical noun phrases as they appear in newspaper texts.
Anaphoric reference is an important linguistic phenomenon to understand the discourse structure and content.
The regularity acquired from training was then tested and compared with other approaches in both choosing and resolving anaphora.Keywords: anaphoric reference, semantic roles(case), natural language acquisition, case- based learning.
The reported literatures adopt pure probabilistic approach, or pure rule-based noun phrases grammar to tackle this problem.
In this paper, we apply a probabilistic chunker to deciding the implicit boundaries of constituents and utilize the linguistic knowledge to extract the noun phrases by a finite state mechanism.
The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
a noun in a noun phrase).
This paper describes an automatic method for extracting systematic polysemy from a hierar-chically organized semantic lexicon (WordNet).
We compare the systematic relations extracted by our auto-matic method to manually extracted WordNet cousins.
We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference.
We evaluate three state- of-the-art re-ranking algorithms (MaxEntRank, SVMRank, and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and cross- document inference.
These problems include Topic Classification, Information Retrieval, Extracting Named Entities, and Extracting Relations.128
This paper reports on both context-independent and context-dependent strategies for utter-ance verification that show that the use of dialog context is crucial for intelligent se-lection of which utterances to verify.
This paper proposes a learning method of translation rules from parallel corpora.
This method applies the maximum entropy principle to a probabilistic model of translation rules.
First, we define feature functions which express statistical properties of this model.
mantics of Combinatory Categorial Grammar, including its handling of coordination constructs.
We demonstrate a problem with the stan-dard technique for learning probabilistic decision lists.
We describe an incremental parser which annotates grammatical functions in German on top of a shallow annotation structure consisting of chunks, topological fields and clauses.
We apply our paraphrasing method in the context of machine translation evaluation.
We show how techniques known from generalized LR parsing can be applied to left- corner parsing.
A strong advantage of our presentation is that it makes explicit the role of left-corner parsing in these algorithms.Keywords: Generalized LR parsing, left- corner parsing, chart parsing, hidden left recursion.
As a case study, we show how a prob-abilistic approach can be used to model anaphora resolution in dialogue1.
This paper proposes the automatic generation of Fill-in-the-Blank Questions (FBQs) together with testing based on Item Response Theory (IRT) to measure English proficiency.
This paper describes automatic treatment of multi-word expressions in a morphologically complex flective language ?Estonian.
It focuses on a special type of multi-word expressions ?the verbal multi-word expressions that can function as predicates.
This paper proposes a novel class of PCFG parameterizations that support linguistically reasonable priors over PCFGs.
This paper proposes a method for packing feature structures, which automatically collapses equivalent parts of lexical/phrasal feature structures of HPSG into a single packed feature structure.
Preliminary experiments show that this method can significantly improve a unification speed in parsing.
We describe the design of Comlex Syntax, a computational lexicon providing detailed syntactic information for approximately 38,000 English headwords.
Our work views sentence compression as an optimisation problem.
We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints.
This is based on our experiences with 2 TPI systems, one for generating asthma- information booklets and one for generating smoking-cessation letters.
GermaNet and FrameNet.
We present an efficient multi-level chart parser that was designed for syntactic analysis of closed captions (subtitles) in a real-time Machine Translation (MT) system.
We report progress on adding affect- detection to a program for virtual dramatic improvisation, monitored by a human director.
The project contributes to the application of sentiment and subjectivity analysis to the creation of emotionally believable synthetic agents for interactive narrative environments.
This paper presents an approach of extracting Pinyin names from English text, suggesting translations to these Pinyin using a database of names and their characters with usage probabilities, followed with IR techniques with a corpus as a disambiguation tool to resolve the translation candidates.
In this paper, word sense disambiguation (WSD) ac-curacy achievable by a probabilistic classifier, using very minimal training sets, is investigated.
Our system, named Bayesian Hierarchical Disambiguator (BHD), uses the Internet, arguably the largest corpus in existence, to address the sparse data problem, and uses WordNet's hierarchy for se-mantic contextual features.
We present results on the recognition accuracy of a continuous speech, speaker independent HMM recognition system that incorporates a novel noise reduction algorithm.
In this paper, we describe a machine translation system called PalmTree which uses the "pattern- based" approach as a fundamental framework.
The pure pattern-based translation framework has several issues.
This paper describes the incremental generation of parse tables for the LRtype parsing of Tree Adjoining Languages (TALs).
In this paper, a lazy generation of LRtype parsers for TALs is defined in which parse tables are created by need while parsing.
We then describe an incremental parser generator for TALs which responds to modification of the input grammar by updating parse tables built so far.
The formalism, which we refer to as Partially Linear PATR (PLPATR) manipulates feature structures rather than stacks.
This paper describes a lexicon organized around sys-tematic polysemy: a set of word senses that are related in systematic and predictable ways.
We systematically compare two inductive learning approaches to tagging: MX-POST (based on maximum entropy modeling) and MBT (based on memory-based learning).
In this paper, we propose a method for resolving the syntactic ambiguities of translation examples of bilingual corpora and a method for acquiring lexical knowledge, such as case frames of verbs and attribute sets of nouns.
We discuss maximum a posteriori estimation of continuous density hidden Markov models (CDHMM).
The classical MI,Ereestimation algorithms, namely the forward-backward algorithm and the segmental k-means algorithm, are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities.
We also propose a new probabilistic sentence reduction method based on support vector machine learning.
A new almost-parsing language model incorporating multiple knowledge sources that is based upon the concept of Constraint Dependency Grammars is presented in this paper.
In recent years, statistical approaches on ATR (Automatic Term Recognition) have achieved good results.
We present an unsupervised system that exploits linguistic knowledge resources, namely English and German lexical databases and the World Wide Web, to identify English inclusions in German text.
We then apply our method to two complementary tasks: information ordering and extractive summarization.
Our approach to the problem uses textual similarity and context from other clauses.
We explore the possibility that syntactic information can be used to improve the performance of an HMM-based system for restoring punctuation (specifically, commas) in text.
We explore the use of morphological analysis as preprocessing for protein name tagging.
Our method finds protein names by chunking based on a morpheme, the smallest unit determined by the morphological analysis.
In addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verbs and prepositions, aspectual markers, copular verbs, null arguments, and slot fillers.
Retrieval effectiveness is examined utilizing query weight ratio of these three categories of phrasal terms.
Answer Extraction is performed by recognizing event inter-relationships and by inferring over multiple sentences and texts, using background knowledge.
This paper describes a reestimation method for stochastic language models such as the N-gram model and the Hidden Markov Model(HMM) from ambiguous observations.
We investigated the applicability of probabilistic context-free grammars to syllabification and grapheme-to-phoneme conversion.
The results show that the standard probability model of context-free grammars performs very well in predicting syllable boundaries.
This paper discusses the supervised learning of morphology using stochastic transducers, trained using the Expectation- Maximization (EM) algorithm.
This paper describes an algorithm for propagating verb arguments along lexical chains consisting of WordNet relations.
The algorithm creates verb argument structures using VerbNet syntactic patterns.
We present two measures for compar-ing corpora based on information the-ory statistics such as gain ratio as well as simple term-class frequency counts.
The classic one is the construction of thesaurus.
Lexical features are key to many approaches to sentiment analysis and opinion detection.
A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexicosyntactic patterns.
A semantic concordance is being constructed to use in studies of sense resolution in context (semantic disambiguation).
Some practical uses for semantic concordances are proposed.
Metagrammatical formalisms that combine context-free phrase structure rules and metarules (MPS grammars) allow concise statement of generalizations about the syntax of natural languages.
This paper addresses a dialogue strategy to clarify and constrain the queries for speech-driven document retrieval systems.
This paper addresses the documentation oflarge-scale grammars.'
Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone, e.g.
LFG and PATR-II.
This paper presents a parser for French developed on an unification based categorial grammar (FG) which avoids these problems.
This parser is a bottom-up chart parser augmented with a heuristic eliminating spurious parses.
Automatic extraction of multiword expressions (MWE) presents a tough challenge for the NLP community and corpus linguistics.
We use an English semantic tagger (USAS) developed at Lancaster University to identify multiword units which depict single semantic concepts.
Specifically, we report on combining the BU system based on stochastic segment models and the BBN system based on hidden Markov models.
Previous research has shown that the plausibility of an adjective-noun com-bination is correlated with its corpus co-occurrence frequency.
In this paper, we estimate the co-occurrence frequen-cies of adjective-noun pairs that fail to occur in a 100 million word corpus using smoothing techniques and com-pare them to human plausibility rat-ings.
Both class-based smoothing and distance-weighted averaging yield fre-quency estimates that are significant predictors of rated plausibility, which provides independent evidence for the validity of these smoothing techniques.
Word segmentation in MSR-NLP is an integral part of a sentence analyzer which includes basic segmentation, derivational morphology, named entity recognition, new word identification, word lattice pruning and parsing.
In this paper, we show that time- synchronous one-pass decoding using cross-word triphones and a trigram language model can be implemented using a dynamically built tree-structured network.
We propose a Question-answering (QA) system in Korean that uses a predictive answer indexer.
The predictive answer indexer, first, extracts all answer candidates in a document in indexing time.
One way is to load the system with lexical semantics for nouns or adjectives.
The paper describes a system which uses packed parser output directly to build semantic representations.
This paper is a presentation of a doctoral research in progress focused on a new genre: online encyclopaedias.
the language used in official encyclopaedic articles.
This paper describes the creation of a script in the framework of ontological semantics as the formal representation of the complex event BANKRUPTCY.
We report an empirical study on the role of syntactic features in building a semi- supervised named entity (NE) tagger.
Our study shows that constituency and dependency parsing constraints are both suitable features to extract NEs and train the classifier.
This paper argues that stylistically and pragmatically high-quality spoken language translation requires the transfer of pragmatic information at an abstract level of "utterance strategies".
Previous approach to this problem involves measuring context similarity only based on co-occurring words.
This paper presents a new algorithm using information extraction support in addition to co-occurring words.
Maximum entropy modeling is then used to represent the probability distribution of context similarities based on heterogeneous features.
Statistical annealing is applied to derive the final entity coreference chains by globally fitting the pairwise context similarities.
This paper shows that quantifier scope phenomena can be precisely characterized by a semantic representation constrained by surface constituency, if the distinction between referential and quantificational NPs is properly observed.
This paper describes a statistical methodology for automatically retrieving collocations from POS tagged Korean text using interrupted bigrams.
We devised four statistics, 'frequency', 'randomness', 'condensation', and 'correlation' to account for the more flexible word order properties of Korean collocations.
We extracted meaningful bigrams using an evaluation function and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers.
We view a modeling problem for n-gram collocations as that for clustering of cohesive words.
This paper reports results on automatically training a Problematic Dialogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain.
This paper reports on experiments in classifying the semantic role annotations assigned to prepositional phrases in both the PENN TREEBANK and FRAMENET.
In addition to using traditional word collocations, the experiments incorporate class-based collocations in the form of WordNet hypernyms.
directly stored relations.
A semantic net needs proper representation of lexical gaps.
This article discusses the detection of discourse markers (DM) in dialog transcriptions, by human annotators and by automated means.
Then, several types of features are defined for automatic disambiguation of like: collocations, part-of-speech tags and duration-based features.
This paper addresses the issue of "algorithm vs. representation" for case-based learning of linguistic knowledge.
Next, we present a technique for automating feature set selection for case-based learning of linguistic knowledge.
We apply the linguistic bias approach to feature set selection to the problem of relative pronoun disambiguation and show that the case- based learning algorithm improves as relevant biases are incorporated into the underlying instance representation.
This paper proposes a methodology for the customisation of natural language interfaces to information retrieval applications.
This article presents a method for aligning words between translations, that imposes a compositionality constraint on alignments produced with statistical translation models.
This paper presents a multi-modal feature- based system for extracting salient keywords from transcripts of instructional videos.
Specifically, we propose to extract domain-specific keywords for videos by integrating various cues from linguistic and statistical knowledge, as well as derived sound classes and characteristic visual content types.
We present the approach to generation used in our implemented system for generating and interpreting indirect answers to Yes-No questions in English.
Generation of a discourse plan is performed in two phases: content planning and plan pruning.
In this paper, we propose a statistical approach for clustering of articles us-ing on-line dictionary definitions.
Columbia抯 Newsblaster tracking and summarization system is a robust system that clusters news into events, categorizes events into broad topics and summarizes multiple articles on each event.
In this paper we describe an approach to coherent sentence ordering for summarizing newspaper articles.
Combining the refinement algorithm with topical segmentation and chronological ordering, we address our experiment to test the effectiveness of the proposed method.
The results reveal that the proposed method improves chronological sentence ordering.
We conduct a set of experiments to investigate how noun phrase identification and feature selection can contribute to coreference resolution performance.
We believe that this is the first attempt at an unsupervised approach to Chinese noun phrase coreference resol ution.
We present a loss-based decoding framework for coreference resolution and a greedy algorithm for approximate coreference decoding, in conjunction with Perceptron and logistic regression learning algorithms.
We develop the notion of a word order domain structure, which is linked but structurally dissimilar to the syntactic dependency tree.
A purely functional implementation of LR-parsers is given, together with a simple correctness proof.
It is presented as a generalization of the recursive descent parser.
We propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver, and using Singular Value Decomposition (SVD) to identify the main terms.
This paper reports a pilot study, in which Constraint Grammar inspired rules were learnt using the Progol machine-learning system.
This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system.
Each rule consists of a rule number, a phrase structure rule, and a semantic (logical translation) rule.
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).
The first stage of this SLU utilizes a weighted finite-state transducer (WFST)-based parser, which encodes the regular grammar of concepts to be extracted.
The proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger.
Under the probabilistic WFST framework, the use of N-best hypotheses from the speech recognizer instead of the 1- best can further improve performance requiring only a small additional processing time.
We describe our ongoing work on "Polibox", a web-based text generator producing adap-tive hypertext from a product database, currently one of computational linguistics textbooks.
In this work we propose a translation model for monolingual sentence retrieval.
These kinds of anaphora are pronominal references, surface- count anaphora and one-anaphora.
We only use the following kinds of information: lexical (the lemma of each word), morphologic (person, number, gender) and syntactic.
A recognized effective approach to word segmentation is Longest Matching, a method based on dictionary.
We describe an approach to connected word recognition that allows the use of segmental information through an explicit decomposition of the recognition criterion into classification and segmentation scoring.
This paper describes improved HMM-based word level alignment models for statistical machine translation.
We present a method for using part of speech tag information to improve alignment accu-racy, and an approach to modeling fertility and cor-respondence to the empty word in an HMM align-ment model.
We address the problem of acquiring entire phrases, specifically figurative phrases, through augmenting a phrasal lexicon.
We have designed and implemented a program called RINA which uses demons to implement functional-grammar principles.
In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.
In this project note, we present the main features of lexicalisation strategies deployed by humans in question- answering (QA) tasks.
When a machine learning-based named entity recognition system is employed in a new domain, its performance usually degrades.
This paper examines the benefits of system combination for unsupervised WSD.
We investigate several voting- and arbiter- based combination strategies over a diverse pool of unsupervised WSD systems.
In this paper, we compare the rela-tive effects of segment order, segmen-tation and segment contiguity on the retrieval performance of a translation memory system.
We take a selec-tion of both bag-of-words and segment order-sensitive string comparison meth-ods, and run each over both character-and word-segmented data, in combina-tion with a range of local segment con-tiguity models (in the form of N-grams).
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006).
We describe sources of cross- lingual error and ways to ameliorate them.
We present an annotation scheme for student emotions in tutoring dialogues.
We discuss issues concerning the implementation of emotion prediction and adaptation in the computer tutoring dialogue system we are developing.
We use two well- known techniques, partitioning clustering, k- means and a loss function to create category hierarchy.
k-means is to cluster the given categories in a hierarchy.
This paper presents empirical results in cross-lingual information retrieval using English queries to access Chinese documents (TREC-5 and TREC-6) and Spanish documents (TREC-4).
We explore how a combined probability model of term translation and retrieval can reduce the effect of translation ambiguity.
This paper shows how to induce an N-best translation lexicon from a bilingual text corpus using statistical properties of the corpus together with four external knowledge sources.
We present a novel morphological analysis technique which induces a morphological and syntactic symmetry between two languages with highly asymmetrical morphological structures to improve statistical machine translation qualities.
This paper describes a multi-word expression processor for preprocessing Turkish text for various language engineering applications.
We then summarize the computational setting in which we employ a series of components for tokenization, morphological analysis, and multi-word expression extraction.
In this paper, parsing-as-deduction and constraint pro-gramming are brought together to outline a procedure for the specification of constraint-based chart parsers.
This paper describes a novel undertaking: comparing the relationship between grammatical ambiguity (syncretism) in nouns, as represented in a default inheritance hierarchy, with textual frequency distributions.
Typically, statistical alignment models are based on single-word dependencies.
This approach extends the chunk tags for every problem by a tag-extension function.
In this paper we will present our ongoing work on a plan-based discourse processor developed in the context of the Enthusiast Spanish to English translation system as part of the JANUS multi-lingual speech-tospeech translation system.
Some statistical learning systems are evaluated using measures of distributional similarity.
Here, we investigate the sensi-tivity of entropy-based similarity measures to noise from uninformative smoothing.
The ESPRIT project POLYGLOT aims at developing multi-lingual Speech-to-Text and Text-to-Speech conversion and to integrate this technology in a number of commercially viable prototype applications.
Speech-to-Text conversion is mainly concerned with very large vocabulary isolated word recognition.
The goal of this paper is to integrate the Conceptual Mapping Model with an ontology-based knowledge representation (i.e.
This paper will examine 2000 random examples of 慹conomy?
Finally, the facts are encoded as predicate calculus axioms.
This paper considers several important issues for monolingual and multilingual link detection.
This paper proposes a methodology for building application-specific lexica by using WordNet.
In the past decade, several researchers have started reinvestigating the use of sub-phonetic models for lexical representations within automatic speech recognition systems.
We propose a unified framework in which to treat semantic underspecification and parallelism phenomena in discourse.
The framework employs a constraint language that can express equality and subtree relations between finite trees.
The constraints are solved by context unification.
In this paper we revisit Puste-jovsky's proposal to treat ontologi-cally complex word meaning by so-called dotted pairs.
The Clarity project is devoted to automatic detection and classification of discourse structures in casual, non-task-oriented conversation using shallow, corpus-based methods of analysis.
For the Clarity project, we have tagged speech acts and dialogue games in the Call Home Spanish corpus.
We have done preliminary cross-level experiments on the relationship of word and speech act n-grams to dialogue games.
Knowledge structures called Concept Clustering Knowledge Graphs (CCKGs) are introduced along with a process for their construction from a machine read-able dictionary.
Often in these texts complex linguistic constructions like conjunctions, negations, ellipsis and anaphorae are involved.
The paper shows that incorporating several latent features into the tense classifier boosts the tense classifier抯 performance, and a tense classifier using only the latent features outperforms one using only the surface features.
This paper investigates an isolated setting of the lexical substitution task of replacing words with their synonyms.
We propose to address these ambiguities with predicate argument structures and semantic co-occurrence similarity information, and present encouraging results.
This paper presents a method that as-sists in maintaining a rule-based named-entity recognition and classifi-cation system.
The findings suggest some agendas for computational linguistics.
In this paper, we present and compare various align-ment models for statistical machine translation.
We also compare the im-pact of different alignment models on the translation quality of a statistical machine translation system.
This paper presents a dependency language model (DLM) that captures linguistic constraints via a dependency structure, i.e., a set of probabilistic dependencies that express the relations between headwords of each phrase in a sentence by an acyclic, planar, undirected graph.
First, we incorporate the dependency structure into an n-gram language model to capture long distance word dependency.
Finally, we evaluate the proposed models on a realistic application (Japanese Kana-Kanji conversion).
This paper explores techniques to take advantage of the fundamental difference in structure between hidden Markov models (HMM) and hierarchical hidden Markov models (HHMM).
We present a new approach to HPSG processing: compiling HPSG grammars expressed as type constraints into definite clause programs.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We introduce a character-based chunking for unknown word identification in Japanese text.
The method is built upon SVM-based chunking, by use of character n-gram and surrounding context of n-best word segmentation candidates from statistical morphological analysis as features.
The approach presented bases associations upon thesaurus categories.
This paper describes a corpus-based investigation of anaphora in dialogues, using data from English and Portuguese face- to-face conversations.
Possible applications comprise machine translation, computer-aided language learning, and dialogue systems in general.
The central concept worked out here in some detail is the concept of 'partial isomorphy' between subgrammars.
We also show that, surprisingly, the addition of deep linguistic analysis features to a set of surface level word n-gram features contributes consistently to classification accuracy in this domain.
We present an algorithm, NOMEN, for learning generalized names in text.
An open, extendible multi-dictionary system is introduced in the paper.
This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using natural language learning tech-niques: looking for characteristic statistical "language-signatures" in test corpora.
In this paper, we describe how the LIDAS System (Linguistic Discourse Analysis System), a discourse parser built as an implementation of the Unified Linguistic Discourse Model (U-LDM) uses information from sentential syntax and semantics along with lexical semantic information to build the Open Right Discourse Parse Tree (DPT) that serves as a representation of the structure of the discourse (Polanyi et al., 2004; Thione 2004a,b).
More specifically, we discuss how discourse segmentation, sentence-level discourse parsing, and text-level discourse parsing depend on the relationship between sentential syntax and discourse.
Specific discourse rules that use syntactic information are used to identify possible attachment points and attachment relations for each Basic Discourse Unit to the DPT.
We consider several empirical estimators for probabilistic context-free grammars, and show that the estimated grammars have the so-called consistency property, under the most general conditions.
Our estimators include the widely applied expectation maximization method, used to estimate probabilistic context-free grammars on the basis of unannotated corpora.
In this paper, we will present an efficient method to compute the co-occurrence counts of any pair of substring in a parallel corpus, and an algorithm that make use of these counts to create sub- sentential alignments on such a corpus.
In this paper we present the syntax and inference mechanisms for the language.
We describe a trainable and scalable summarization system which utilizes features derived from information retrieval, information extraction, and NLP techniques and on-line resources.
We also present preliminary results from a task-based evaluation on summarization output usability.
GETARUNS is a symbolic linguistically-based parser written in Prolog Horn clauses which uses a strong deterministic policy by means of a lookahead mechanism and a WFST.
We investigate various contextual effects on text interpretation, and account for them by providing contextual constraints in a logical theory of text interpretation.
This paper proposes a hybrid Chinese named entity recognition model based on multiple features.
Firstly, the proposed Hybrid Model integrates coarse particle feature (POS Model) with fine particle feature (Word Model), so that it can overcome the disadvantages of each other.
In this paper we present methods for reducing the computation time of joint segmentation and recognition of phones using the Stochastic Segment Model (SSM).
The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities.
A hybrid approach to the extraction of habitual collocations and idioms is presented, aiming at a detailed description of collocations and their morphosyntax for natural language generation systems as well as to support learner lexicography.
The paper deals with generation of natural language text in a dialog system.
We present new statistical models for jointly labeling multiple sequences and apply them to the combined task of partof-speech tagging and noun phrase chunking.
The model is based on the Factorial Hidden Markov Model (FHMM) with distributed hidden states representing partof-speech and noun phrase sequences.
Further, we extend this into a novel model, Switching FHMM, to allow for explicit modeling of cross-sequence dependencies based on linguistic knowledge.
This paper briefly describes our system in the third SIGHAN bakeoff on Chinese word segmentation and named entity recognition.
This is done via a word chunking strategy using a context-dependent MutualInformation Independence Model.
Various methods have been proposed for automatic synonym acquisition, as synonyms are one of the most fundamental lexical knowledge.
This study has experimentally investigated the impact of contextual information selection, by extracting three kinds of word relationships from corpora: dependency, sentence co-occurrence, and proximity.
In this paper I report on an investigation into the problem of assigning tones to pitch contours.
We report on a method for compiling decision trees into weighted finite-state transducers.
These rules are compiled into transducers using the weighted rewrite-rule rule-compilation algorithm described in (Mohri and Sproat, 1996).
This paper presents an unsupervised method for assembling semantic knowledge from a part-ofspeech tagged corpus using graph algorithms.
We describe a method for interpreting abstract flat syntactic representations, LFG f- structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).
It provides a model theoretic interpretation and an inferential component which operates directly on underspecified representations for f- structures through the translation images of f-structures as UDRSs.
We propose a conditional random field- based method for supertagging, and apply it to the task of learning new lexical items for HPSG-based precision grammars of English and Japanese.
A `lexicalized' grammar naturally follows from the extended domain of locality of TAGs.A general parsing strategy for `lexicalized' grammars is discussed.
Subsets of these lexicons are being incrementally interfaced to the parser.We finally show how idioms are represented in lexicalized TAGs.
This paper argues for a two-level theory of semantics as opposed to a one-level theory, based on the example of the system of temporal and durational connectives.
QACIAD (Question Answering Challenge for Information Access Dialogue) is an evaluation framework for measuring interactive question answering (QA) technologies.
Modern Question/Answering systems rely on expected answer types for processing questions.
This paper describes a study in which a corpus of spoken Danish annotated with focus and topic tags was used to investigate the relation between information structure and pauses.
We present results of experiments with Elman recurrent neural networks (Elman, 1990) trained on a natural language processing task.
The task was to learn sequences of word categories in a text derived from a primary school reader.
The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.
We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs).
are pervasive in dialogue.
We present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue.
We propose a structured semantic representation, the Lexical Conceptual Paradigm (LOP) which groups nouns into paradigmatic classes exhibiting like behavior.I.
We use this as a principled distinguishing characteristic of polysemous types.
We distinguish two systems that together comprise the meaning of lexical items, the lexical system and the conceptual system.
We limit ourselves in this paper to cases of polysemy involving norninals.We will proceed as follows.
basic segmentation, named entity recognition, error-driven learner and new word detector.
The basic segmentation and named entity recognition, implemented based on conditional random fields, are used to generate initial segmentation results.
Markov logic is a highly expressive language recently introduced to specify the connectivity of a Markov network using first-order logic.
We validate our algorithms on the tasks of citation matching and author disambiguation.
This paper overviews the overall architecture of the project and provides briefs on the three components of this project, namely Urdu Lexicon, English to Urdu Machine Translation System and Urdu Text to Speech System.
This paper studies issues on compiling a bilingual lexicon for technical terms.
As a method of translation estimation for technical terms, we pro-pose a compositional translation esti-mation technique.
Within the context of MUC-7, the SIFT system for extraction of template entities (TE) and template relations (TR) used a novel, integrated syntactic/semantic language model to extract sentence level information, and then synthesized information across sentences using in part a trained model for cross-sentence relations.
We describe a syntax-based algorithm that automatically builds Finite State Automata (word lattices) from semantically equivalent translation sets.
These FSAs are good representations of paraphrases.
The algorithm generates appropriate interpretations for cases of VP ellipsis, pseudo-gapping, bare ellipsis (stripping), and gapping.
A FSA (finite state automaton) based on the discourse grammar determines the possible moves which the dialogue might take.
An extension to classical unification, called graded unification is presented.
We describe a data-driven approach to building interpretable discourse structures for appointment scheduling dialogues.
We represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques.
Automatic book indexing systems are based on the generation of phrase structures capable of reflecting text content.
First, we segment the text with a character-level CRF model.
Then we apply three word-level CRF models to the labeling person names, location names and organization names in the segmentation results, respectively.
Corpus-based sense disambiguation methods, like most other statistical NLP approaches, suffer from the problem of data sparseness.
Using the definition- based conceptual co-occurrence data collected from the relatively small Brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information.
We report on our experiments to automatically identify links between the senses in a machine- readable dictionary.
In particular, we automatically identify instances of zero-affix morphology, and use that information to find specific linkages between senses.
This work has provided insight into the performance of a stochastic tagger.
These new ideas have been preliminarily tested on named entity recognition and PP attachment disambiguation.
This paper presents a connectionist syntactic parser which uses Structure Unification Grammar as its grammatical framework.
To achieve translation technology that is adequate for speech-to-speech translation (S2S), this paper introduces a new attempt named Corpus-Centered Computation, (abbreviated to C3 and pronounced c-cube).
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
In this paper, we study the incorporation of MT models and ASR models using finite-state automata.
We also propose some transducers based on MT models for rescoring the ASR word graphs.
This paper presents a method for the automatic classification of semantic relations in nominalized noun phrases.
The paper presents preliminary results for the semantic classification of the most representative NP patterns using four distinct learning models.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
To demonstrate the benefits of our approach, we consider the problem of parsing Chinese treebank data using only lexical features, that is, without part-of-speech tags or grammatical categories.
We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical- Functional Grammar (LFG).
The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components.
In this paper, we focus on one of the central integration facilities, the XSLT-based Whiteboard Annotation Transformer (WHAT), report on the benefits of XSLT-based NLP component integration, and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture.
It investigates more than 10 classifier combination methods, including second order classifier stacking, over 6 major structurally different base classifiers (enhanced Na飗e Bayes, cosine, Bayes Ratio, decision lists, transformation-based learning and maximum variance boosted mix-ture models).
We describe a statistical approach for modelling and detecting dialogue acts in Instant Messaging dialogue.
The model we developed combines naive Bayes and dialogue-act n-grams to obtain better than 80% accuracy in our tagging experiment.
This paper presents the results of using Roget's International Thesaurus as the taxonomy in a semantic similarity measurement task.
We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages.
This paper presents a hybrid approach for named entity (NE) tagging which combines Maximum Entropy Model (MaxEnt), Hidden Markov Model (HMM) and handcrafted grammatical rules.
MaxEnt includes external gazetteers in the system.
Sub-category generation is also discussed.
With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone.
One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996).
Sentence ranking is a crucial part of generating text summaries.
We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence- based approaches.
The word-based approaches determined sentence rankings based on relative word frequencies (Luhn (1958); Salton & Buckley (1988)).
We present MAGEAn, a morphological analyzer and generator for the Arabic language family.
Five association measures including variants of Dice, Overlap Ratio, Jaccard, and Cosine, as well as Co- Occurrence Double Check (CODC), are presented.
The experiments on link detection of named entities using the strategies of direct association, association matrix and scalar association matrix verify that the double-check frequencies are reliable.
The paper presents a new segmentation-free approach to the Arabic optical character recognition.
We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine- grained classes.
off-line) material in foreign languages.
Our experiments have shown that this method is effective for inferring the POS of unknown words.
Processing in GENIE is category-driven, i.e., grammatical rules are distributed over a part-of-speech hierarchy and, using an inheritance mechanism, are invoked only if appropriate for the category being processed.1- IntroductionThis paper discusses relational-grammar-based generation in the context of JETS, a Japanese-English machine translation (MT) system that is being developed at the IBM Research Tokyo Research Laboratory.To put our work in perspective, we first explain the motivation for basing JETS on relational grammar (RG) and then sketch the processing flow in translation.
Three machine learning methods are compared for this task: decision tree induction, a genetic algorithm, and log-linear regression.
This paper proposes a general probabilistic setting that formalizes a probabilistic notion of textual entailment.
We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting.
We explore here the relationship between protein/gene names and expressions used to characterize protein/gene function.
We achieve 81% accuracy on the task of discourse relation type classification and 70% accuracy on relation identification.
This structure ( named network structure ) involves all possible combinations of syntactically collect phrases.
RASTA (Rhetorical Structure Theory Analyzer), a system for automatic discourse analysis, reliably identifies rhetorical relations present in written discourse by examining information available in syntactic and logical form analyses.
Whereas introspection is a viable strategy for human analysts, a computational discourse analysis system likeRASTA requires explicit methods for identifying discourse relations.
Identifying rhetorical relationsIn the computational discourse analysis literature, there are three strands concerning the identification of rhetorical relations.
Typically simple pattern matching is used to identify cue phrases.
Hence, true spontaneous speech is generated.
Caseframe parsers employ both semantic and syntactic knowledge.
The key problem to be faced when building a HMM-based continuous speech recogniser is maintaining the balance between model complexity and available training data.
This paper describes a method of creating a tied-state continuous speech recognition system using a phonetic decision tree.
We describe a corpus-based investigation of proposals in dialogue.
We will also introduce a web-based interface to MindNet lexicons (MNEX) that is intended to make the data contained within MindNets more accessible for exploration.
The basic model is statistical, but we use broad-coverage rule- based parsers in two ways ?during training for learning rewrite patterns, and at runtime for reordering the source sentences.
We investigate prototype-driven learning for primarily unsupervised sequence modeling.
This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.
Two WSD classifiers are trained from a word sense-tagged corpus: one is a classifier obtained by supervised learning, the other is a classifier using hypernyms extracted from definition sentences in a dictionary.
The algorithm combines four original alignment models based on relative corpus frequency, con-textual similarity, weighted string simi-larity and incrementally retrained inflec-tional transduction probabilities.
This paper describes a method for estimat-ing conditional probability distributions over the parses of "unification-based" grammars which can utilize auxiliary distributions that are estimated by other means.
We show how this can be used to incorporate information about lexical selectional preferences gathered from other sources into Stochastic "Unification-based" Grammars (SUBGs).
We show how the recognition performance of a speech recognition component in a speech retrieval system affects the retrieval effectiveness.
A speech retrieval system facilitates content-based retrieval of speech documents, i.e.
audio recordings containing spoken text.
ATE GrammarsAn ATM grammar is a set of networks formed by labelled states and directed arcs connecting them.
We describe the development of PASTAWeb, a WWW-based interface to the extraction output of PASTA, an IE system that extracts protein structure information from MEDLINE ab-stracts.
Lastly, an ongoing project that integrates POS tagging, parsing, and sense disambiguation in one system is presented.
A series of experiments on speaker-independent phone recognition of continuous speech have been carried out using the recently recorded BREF corpus.
This paper describes a method for asking statistical questions about a large text corpus.
In this paper I propose a Binding rule for the identification of pronoun and anaphor referents in phrase-structure trees, assuming the general framework of the Government-binding theory outlined by Chomsky (1981).
The fragment of the attribute grammar shown here is part of an English grammar and parser being developed in the Prolog and PLNLP languages.
Furthermore; we consider weakly supervised learning technique; CoTraining; to combine labeled data and unlabeled data.Keywords : Korean Named Entity; HMM; Co-Training
We also describe the results of the experiments on learning probabilistic sub-categorization preference from the EDR Japanese bracketed corpus, as well as those on evaluating the performance of subcategorization preference.
The first feature is the partial disambiguation function of the Bi-directional Retriever, which can be used for search request translation in cross-language IR.
The Papillon project is a collaborative project to establish a multilingual dictionary on the Web.
It aims to apply the LINUX cooperative construction paradigm to establish a broad- coverage multilingual dictionary.
This paper discusses the challenges and proposes a solution to performing information retrieval on the Web using Chinese natural language speech query.
It uses the query model to facilitate the extraction of main core semantic string (CSS) from the Chinese natural language speech query.
This paper describes a mix word-pair mix-WP) identifier to resolve homo-nym/segmentation ambiguities as well as perform STW conversion effec-tively for Chinese input.
Collocational knowledge is necessary for language generation.
(ii) High quality translation via word sense disambiguation and accurate word order generation of the target language.
We describe a mechanism which receives as input a segmented argument composed of NL sentences, and generates an inter-pretation.
In this paper, we present a prosody generation architecture based on K-ToBI (Korean Tone and Break Index) representation.ToBI is a multi-tier representation system based on linguistic knowledge to transcribe events in an utterance.
We developed automatic corpus-based K-TOBI labeling tools and prediction methods based on several lexicosyntactic linguistic features for decision-tree induction.
We propose a formal characterization of variation in the syntactic realization of semantic arguments, using hierarchies of syntactic relations and thematic roles, and a mechanism of lexical inheritance to obtain valency frames from individual linking types.
We embed the formalization in the new lexicalized, dependency-based grammar formalism of Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001).
1 IntroductionThis paper deals with the mapping (or linking) of semantic predicate-argument structure to surface syntactic realizations.
We present a formal architecture in the framework of a multi-dimensional, heavily lexicalized, efficiently parsable dependency formalism (Duchier and Debusmann, 2001), which uses lexical inheritance as a means to explicitly model syntactic variation.
We concentrate on variation between prepositional phrases and nominal phrases which realize verbal arguments, and remedy problems that occur with this kind of variation in recent approaches like the HPSG linking architecture proposed by (Davis, 1998).Section 2 presents and analyses some of the problematic data we can model, English dative shift, optional complements and thematic role alternations.
cal Dependency Grammar (TDG) by adding a new representational level (thematic structure additionally to ID structure) to the framework in Section 4.1 and introducing the concept of a valency frame in the TDG inheritance lexicon (Sections 4.2 and 4.3).
Section 5 contrasts our analysis of the dative shift construction with the analysis of thematic role alternations.2 Linguistic DataInsights from corpus studies (e.g.
The type of local dependencies considered are sequences of part of speech categories for words.
We propose a distinction between two kinds of metonymy: "referential" metonymy, in which the referent of an NP is shifted, and "predicative" metonymy, in which the referent of the NP is unchanged and the argument place of the predicate is shifted instead.
We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank.
We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG.
Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser.
This paper presents a disambiguation method in which word senses are determined using a dictionary.
A model-based spectral estimation algorithm is derived that improves the robustness of speech recognition systems to additive noise.
We name the algorithm minimummean-log-spectral-distance (MMLSD).
In this paper, we introduce LiveTree, a core component of LIDAS, the Linguistic Discourse Analysis System for automatic discourse parsing with the Unified Linguistic Discourse Model (U-LDM) (X et al, 2004).
Elementary logic (i.e.
Traditional intensional logics (i.e.
We present an approach to the incremental ac-crual of lexical information for unknown words that is constraint, based and compatible with standard unification-based grammars.
We show how morphological information, especially in-flectional class, is successfully acquired using a type-based HPSG-like analysis.
We present an implemented unification-based parser for relational grammars developed within the stratified feature grammar (SFG) framework, which generalizes Kasper-Rounds logic to handle relational grammar analyses.
We first introduce the key aspects of SFG and a lexicalized, graph-based variant of the framework suitable for implementing relational grammars.
We then describe a head-driven chart parser for lexicalized SFG.
The basic parsing operation is essentially ordinary feature-structure unification augmented with an operation of label unification to build the stratified features characteristic of SFG.
Named entity (NE) recognition is a task in which proper nouns and nu-merical information in a document are detected and classified into categories such as person, organization, location, and date.
NE recognition plays an es-sential role in information extraction systems and question answering sys-tems.
This paper describes recent improvements in the SPHINX Speech Recognition System.
These enhancements include function-phrase modeling, between-word coarticulation modeling, and corrective training.
We provide a constraint based computational model of linear precedence as employed in the HPSG grammar formalism.
In this paper, we present a logic-based computational model for movement theory in Government and Binding Theory.
A news article is first segmented into clauses, then into words by a Viterbi-based word identification system.
We also adopt Kupiec's concept of word equivalence classes in the tagger.
The method exploits a definition of N2 in a dictionary.
Parts of speech are found through a tagger, and related neighboring words are identified by a phrase extractor operating on the tagged text.
To suggest possible senses, each heuristic draws on semantic relations extracted from a Webster's dictionary and the semantic thesaurus WordNet.
We propose a general model for joint inference in correlated natural language processing tasks when fully annotated training data is not available, and apply this model to the dual tasks of word sense disambiguation and verb subcategorization frame determination.
This paper presents the XTAG system, a grammar development tool based on the Tree Adjoining Grammar (TAG) formalism that includes a wide-coverage syntactic grammar for English.
As part of the DARPA Spoken Language System program, we recently initiated an effort in spoken language understanding.
This paper describes our early experience with the development of the MIT VOYAGER spoken language system.
The performance of semantic parsing can be potentially improved by using discriminative reranking, which explores arbitrary global features.
In this paper, we investigate discriminative reranking upon a baseline semantic parser, SCISSOR, where the composition of meaning representations is guided by syntax.
We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics.
We created a data collection for research, development and evaluation of a method for automatically answering why-questions (why-QA) The resulting collection comprises 395 why-questions.
We developed a question analysis method for why-questions, based on syntactic categorization and answer type determination.
This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated mono-lingual corpora.
We combine vari-ous clues such as cognates, similar context, preservation of word similar-ity, and word frequency.
We describe two new strategies to automatic bracketing of parallel corpora, with particular application to languages where prior grammar resources are scarce: (1) coarse bilingual grammars, and (2) unsupervised training of such grammars via EM (expectation-maximization).
This paper deals with the reference choices involved in the generation of argumentative text.
We describe GLP, a chart parser that will be used as a SYNTAX module of the Erlangen Speech Understanding System.
words.
Besides morpho-syntactic, recency, and semantic features based on existing lexical knowledge resources, our algorithm obtains additional semantic knowledge from the Web.
We present an integrated strategy for ordering information, combining constraints from chronological order of events and cohesion.
We describe an implemented system for robust domain-independent syntactic parsing of English, using a unification-based grammar of part-ofspeech and punctuation labels coupled with a probabilistic LR parser.
Here, we discuss the development of RACE ?Retrospective Analysis of Communications Events ?a test-bed prototype to investigate issues relating to multi-modal multi-party discourse.
We show how a general grammar may be automatically adapted for fast parsing of utterances from a specific domain by means of constituent pruning and grammar specialization based on explanation-based learning.
Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.
Valiant showed that Boolean matrix multiplication (BMM) can be used for CFG parsing.
In this paper, we propose a new method, combining the transliterated string segmentation module with phoneme- based and grapheme-based transliteration modules in order to enhance the back?transliterations of Japanese words.
The paper describes two equivalent grammatical formalisms.
In this paper are described experiments on un-supervised learning of the domain lexicon and relevant phrase fragments from a dialog cor-pus.
Suggested approach is based on using do-main independent words for chunking and us-ing semantical predictional power of such words for clustering and automatic extraction phrase fragments relevant to dialog topics.
We propose re-segmenting the ASR hypotheses as well as applying post- classification to improve the performance.
We use acoustic, phonetic, language model, NER and other scores as confidence measure.
We present here a simple two-stage method for extracting complex relations between named entities in text.
We participated in the SENSEVAL-3 English lexical sample task and multilingual lexical sample task.
The knowledge sources used were partof-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
Lexicalized Tree Adjoining Grammars have proved useful for NLP.
This paper describes a non-statistical approach for semantic annotation of documents by analysing their syntax and by using semantic/syntactic behav-iour patterns described in VerbNet.
We describe an automatic process for learning word units in Japanese.
Our method applies a compound-finding algorithm, previously used to find word sequences in English, to learning syllable sequences in Japanese.
We used SemCat as training data to investigate name classification techniques.
We generated a statistical language model and probabilistic context- free grammars for gene and protein name classification, and compared the results with the new model.
This paper presents a new framework that allows direct orthographical mapping (DOM) between two different languages, through a joint source-channel model, also called n-gram transliteration model (TM).
With the n-gram TM model, we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary.
An inheritance network.Bohrow and Winograd's KR1.
In this paper, I outline several fully intensional semantics for intensional semantic networks by discussing the relations between a semanttc-network "language- I. and several candidates for I. .
We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).
This paper will propose a new objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.
We describe an approach to improve the bilingual cooccurrence dictionary that is used for word alignment, and evaluate the improved dictionary using a version of the Competitive Linking algorithm.
We have experimented with using a statistical word-alignment algorithm to derive word association pairs (French-English) that complement an existing multipurpose bilingual dictionary.
EBMT (Example-Based Machine Translation) is proposed.
This paper explains the basic idea of EBMT, illustrates the experiment in detail, explains the bread applicability of EBMT to several difficult translation problems for RBMT and discusses the advantages of integrating EBMT with RBMT.
This paper mainly presents a Turkish sentence generator for producing the actual text from its semantic description.
We use a functional linguistic theory called Systemic-Functional Grammar (SFG) to represent the linguistic resources, and FUF text generation system as a software tool to perform them.
In this paper, we present the systemic-functional representation and realization of simple Turkish sentences.
In this paper we present an algorithm for automated inversion of a PROLOG-coded unification parser into an efficient unification generator, using the collections of minimal sets of essential arguments (MSEA) for predicates.
Our approach distinguishes two orthogonal yet mutu-ally constraining structures: a syntactic dependency tree and a topological de-pendency tree.
We present a classifier-based parser that produces constituent trees in linear time.
The parser uses a basic bottom-up shift- reduce algorithm, but employs a classifier to determine parser actions instead of a grammar.
SenseClusters is a freely available system that clusters similar contexts.
There are three measures based on clustering criterion functions, and another on the Gap Statistic.
We present an environment for multimodal visualization of geometrical constructions, including both graphical and textual realizations.
We describe a segmentation component that utilizes minimal syntactic knowledge to produce a lattice of word candidates for a broad coverage Japanese NL parser.
This paper describes an implementation to compute positional ngram statistics (i.e.
Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays.
Positional ngrams are ordered sequences of words that represent continuous or discontinuous substrings of a corpus.
In particular, the positional ngram model has shown successful results for the extraction of discontinuous collocations from large corpora.
Features include: morphological variants; WordNet synonyms and hypernyms; log-likelihood-based word pairings dy-namically obtained from baseline sen-tence alignments; and formal string features such as word-based edit dis-tance.
The model is an m-component mixture of Ingram models.
Next, we analyze common approaches to two subproblems: term overlap for answer sentence identification, and answer typing for short answer extraction.
We present a syntax-based constraint for word alignment, known as the cohesion constraint.
We describe SmartMail, a prototype system for automatically identifying action items (tasks) in email messages.
We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others).
Spelling out means making explicit.
First, it discusses how to perform comprehension under Optimality Theory grammars consisting of finite-state constraints.
Information Extraction (IE) is the task of extracting knowledge from unstructured text.
Hidden Markov models (HMMs) are powerful statistical models that have found successful applications in Information Extraction (IE).
In order to retrieve extraction- relevant segments from documents, we introduce a method to use HMMs to model and retrieve segments.
This paper presents our method of incorporating character clustering based on mutual information into Decision- Tree Dictionary-less morphological analysis.
Today there is a relatively large body of work on automatic acquisition of lexicosyntactical preferences (subcategorization) from corpora.
One experiment is described in (Carroll et al., 1998) ?they use subcategorization probabilities for ranking trees generated by unification-based phrasal grammar.
This paper reported our work on annotating Chinese texts with information structures derived from HowNet.
An information structure consists of two components: HowNet definitions and dependency relations.
This work is part of a multi-sentential approach to Chinese text understanding.
Efficient algorithms for generation in this framework take a semantics-driven strategy.
Therefore Lambek Theorem Proving is a natural candidate for a 'uniform' architecture for natural language parsing and generation.Keywords: generation algorithm; natural language generation; theorem proving; bidirectionality; categorial grammar.
It has been hypothesized that Tree Adjoining Grammar (TAG) is particularly well suited for sentence generation.
We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences.
The algorithm exploits robust lexical, syntactic, and semantic knowledge sources.
Two different classes of metonymies are inferred by using (1) lexico-semantic connections between concepts or (2) morphological cues and logical formulae defining lexical concepts.
In both cases the derivation of metonymic paths is based on approximations of sortal constraints retrieved from Word- Net.This novel method of inferring coercions validates the related knowledge through coreference links.
We have proposed an incremental trans-lation method in Transfer-Driven Ma-chine Translation (TDMT).
The paper compares use of presentation features to word features, and the combination thereof, using Na飗e Bayes, C4.5 and SVM classifiers.
Example-based parsing has already been proposed in literature.
Link Grammar based parsing has been considered as the underlying parsing scheme for this work.
This paper assesses two new approaches to deterministic parsing with respect to the analysis of unbounded dependencies (UDs).
We examine the predictions made by a LR(1) deterministic parser and the Lexicat deterministic parser concerning the analysis of UDs.
We show how the language, composed of orthographic mks, word formation rules, and paradigm inheritance, can be compiled into a run-time data stricture for efficient morphological analysis and generation with a dynamic secondary storage lexicon.
We introduce inversion-invariant transduction grammars which serve as generative models for parallel bilingual sentences with weak order constraints.
Focusing on transduction grammars for bracketing, we formulate a normal form, and a stochastic version amenable to a maximum-likelihood bracketing algorithm.
Unification-based grammar formalisms use structures containing sets of features to describe linguistic objects.
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
all bills only.
We introduce feature terms containing sorts, variables, negation and named disjunction for the specification of feature structures.
We define context-unique feature descriptions, a relational, constraint-based representation language and give a normalization procedure that allows to test consistency of feature terms.
In its most robust configuration, the system uses only a general lexicon, a local morphosyntactic parser and a dictionary of synonyms.
We show that treating ANNs as a post-process to partially recognized speech from an HMM pre-processor is superior to using ANNs alone or to hybrid systems applying ANNs before HMM processing.
We will use task domain constraints provided by particular application packages on personal computers to create constrained natural language understanding.
Furthermore we will implement interactive voice and text response mechanisms such as dialog boxes and speech synthesis to respond to the users input.
We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
We use a language-universal rule-based algorithm to find a good set of parameters, and then train the parameter weights using EM.
In this paper, we present our work on the detection of question- answer pairs in an email conversation for the task of email summarization.
We show that various features based on the structure of email- threads can be used to improve upon lexical similarity of discourse segments for question- answer pairing.
LEXTER is a software package for extracting terminology.
This paper describes the mechanisms used by the UNITRAN machine translation system for mapping an underlying lexical- conceptual structure to a syntactic structure (and vice versa), and it shows how these mechanisms coupled with a set of general linking routines solve the problem of thematic divergence in machine translation.
A Natural Language Generation system produces text using as input semantic data.
and lean declarative impIe.mentation.
This paper presents a method for improving the performance of a link detection system by using a variety of similarity measures and using source-pair specific statistical information.
The techniques investigated were SVMs, voting, and decision trees, each of which makes use of similarity and statistical information differently.
Decoding algorithm is a crucial part in statistical machine translation.
To deal with organization names, keywords, prefix, word association and parts-of-speech are applied.
Shalt2 is a knowledge-based machine translation system with a symmetric architecture.
The grammar rules, mapping rules between syntactic and conceptual (semantic) representations, and transfer rules for conceptual paraphrasing are all bi-directional knowledge sources used by both a parser and a generator.
We conducted an experiment using automatic summarization evaluation data developed for Text Summarization Challenge 3 (TSC-3).
An NE transliteration model is presented and iteratively trained using named entity pairs extracted from a bilingual dictionary.
We propose a syntactic filter for identifying non-coreferential pronoun-NP pairs within a sentence.
In particular, the fitter formulates constraints on pronominal anaphora in terms of the head-argument structures provided by Slot Grammar syntactic representations rather than the configurational tree relations, particularly c-command, on which the binding theory relies.
We first used a method based on cross-language information retrieval (CLIR) to align the Japanese and English articles and then used a method based on dynamic programming (DP) matching to align the Japanese and English sentences in these articles.
The measure for article alignment uses similarities in sentences aligned by DP matching and that for sentence alignment uses similarities in articles aligned by CLIR.
This paper describes the RenTAL system, which enables sharing resources in LTAG and HPSG formalisms by a method of grammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar.
We study the formal and linguistic properties of a class of parenthesis-free categorial grammars derived from those of Ades and Steed- man by varying the set of reduction rules.
We characterize the reduction rules capable of generating context-sensitive languages as those having a partial combination rule and a combination rule in the reverse direction.
We describe here a method for hierarchically organ-ising discourse markers.
of simple translations.
Most current definitional question answering systems apply one-size-fits-all lexicosyntactic patterns to identify definitions.
By analyzing a large set of online definitions, this study shows that the semantic types of definienda constrain both lexical semantics and lexicosyntactic patterns of the definientia.
incorporates semantic-typedependent lexicosyntactic patterns (e.g., 揟ERM locates ...?
We will demonstrate SconeEdit, a new tool for exploring and editing knowledge bases (KBs) that leverages interaction with domain texts.
This paper presents an analysis of purpose clauses in the context of instruction understanding.
We compare the asymptotic time complexity of left-to-right and bidirectional parsing techniques for bilexical context-free grammars, a grammar formal-ism that is an abstraction of language models used in several state-of-the-art real-world parsers.
The TiGer DB is a dependency bank derived from the TiGer Treebank containing predicate- argument relations and several grammatical features which can be considered as semantically meaningful.
LFG f-structures or HPSG feature structures, to dependency triples simple.
This position paper outlines approaches to diagram summarization that can augment the many well-developed techniques of text summarization.
We describe the advances in raster image vectorization and parsing needed to produce corpora for diagram summarization.
In Online Inquiry-Based Learning (OIBL) learners search for information to answer driving questions.
This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.
Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.
We present two efficient search algorithms for real-time spoken language systems.
The first called the Word-Dependent N-Best algorithm is an improved algorithm for finding the top N sentence hypotheses.
The second algorithm 4 a fast match scheme for continuous speech recognition called the Forward-Backward Search.
The main objective of this paper is to examine various N- gram models of generating translation units for bilingual lexicon extraction.
Three N-gram models, a baseline model (Bound-length N-gram) and two new models (Chunk-bound N- gram and Dependency-linked N-gram) are compared.
This paper describes how to automatically extract grounding features and segment a dialogue into discourse units, once the dialogue has been annotated with the DRI backward- and forward-looking tags.
Preliminary experiments concerning the resolution of Japanese pronouns in spoken-language dialogs result in a success rate of 80.6%.
This paper presents our work on real-izing expressions of doubt appropriately in natural language dialogues.
This paper describes our ongoing research project on text simplification for congenitally deaf people.
In this paper, we discuss the issues we should address to realize text simplification and report on the present results in three different aspects of this task: readability assessment, paraphrase representation and post-transfer error detection.
A simple method for categorizing texts into pre-determined text genre categories using the statistical standard technique of discriminant analysis is demonstrated with application to the Brown corpus.
Press: reportage	13.
Non-fiction	11.
Learned3.
Fiction	K. General Fiction	I.
Mystery	M. Science	N. Adv.
then use discriminant analysis, a. technique from descriptive statistics.
(WSJ) speech corpus.
All Gaussians have diagonal covariance matrices.2.
We discuss such 搒trapping?methods in general, and exhibit a particular method for strapping word- sense classifiers for ambiguous words.
We report work1 in progress on adding affect-detection to an existing program for virtual dramatic improvisation, monitored by a human director.
in users?text input, by means of pattern-matching, robust parsing and some semantic analysis.
In this paper we propose and evaluate a technique to perform semi-supervised learning for Text Categorization.
In this paper, a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a Chinese multiple-clause sentence.
It takes into account the effects of linguistic features such as tense/aspect, temporal connectives, and discourse structures.
A set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution.
We present a bottom-up approach to arranging sentences extracted for multi-document summarization.
To capture the association and order of two textual segments (eg, sentences), we define four criteria, chronology, topical-closeness, precedence, and succession.
This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts.
The paper proposes an architecture for advanced NLG systems that handle narratives.
Domain modelling and meta-knowledge modelling for a narratological structurer are exemplified.
This paper proposes a model using associative processors (APs) for real-time spoken language translation.
We have already proposed a model, TDMT (Transfer-Driven Machine Trans-lation), that translates a sentence utilizing ex-amples effectively and performs accurate struc-tural disambiguation and target word selection.
This paper describes a novel approach to inducing lexico-structural transfer rules from parsed bi-texts using syntactic pattern matching, statistical co- occurrence and error-driven filtering.
In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.
The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.
Two experiments on opinion/modality classification confirm that subtree features are important.
causal connectives, may be categorised into different discourse domains.
The tool is designed to be used in three applications: generating training data for machine learning of co-reference relations, evaluating theories of referring expression generation and resolution in texts, and developing theories for understanding reference in dialogs.
We address the issue of judging the significance of rare events as it typically arises in statistical natural- language processing.
We first define a general approach to the problem, and we empirically compare results obtained using log-likelihood-ratios and Fisher抯 exact test, applied to measuring strength of bilingual word associations.
Tins paper focuses on the issue of named entity chunking in Japanese named entity recognition.
We apply the supervised decision list learn-ing method to Japanese named entity recogni-tion.
We also investigate and incorporate sev-eral pained-entity noun phrase chunking tech-niques and experimentally evaluate and com-pare their performance.
The three corpus-based statistical sense resolution methods studied here attempt to infer the correct sense of a polysemous word by using knowledge about patterns of word co- occurrences.
The techniques were based on Bayesian decision theory, neural networks, and content vectors as used in information retrieval.
This paper presents an algorithm for learning the probabilities of optional phonological rules from corpora.
This paper presents comparative experimental results on four techniques of language model adaptation, including a maximum a posteriori (MAP) method and three discriminative training methods, the boosting algorithm, the average perceptron and the minimum sample risk method, on the task of Japanese Kana-Kanji conversion.
In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.
This paper describes the Lycos Retriever system, a deployed system for automatically generating coherent topical summaries of popular web query topics.
The METAL machine translation project incorporates two methods of structural transfer - direct transfer and transfer by grammar.
This paper describes our initial efforts at porting the VOYAGER spoken language system to Japanese.
The paper describes an algorithm that employs English and French text taggers to associate noun phrases in an aligned bilingual corpus.
The taggers provide part-of-speech categories which are used by finite-state recognizers to extract simple noun phrases for both languages.
Our experiments were conducted using three original translation models and two types of neural nets (single-layer and multi- layer perceptrons) for the confidence estimation task.
We use machine learning techniques to find the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references.
We find that using first mention, utterance distance, and lexical distance computed using either Google or WordNet results in an accuracy significantly higher than obtained in previous experiments.
The whole process is performed in a unification-based framework.
Using alignment techniques from phrase- based statistical machine translation, we show how paraphrases in one language can be identified using a phrase in another language as a pivot.
We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities, and show how it can be refined to take contextual information into account.
We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments, and contrast the quality with paraphrases extracted from automatic alignments.
We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification).
Core Roles versus Adjuncts).
This paper presents a novel approach to combining different word alignments.
We view word alignment as a pattern classification problem, where alignment combination is treated as a classifier ensemble, and alignment links are adorned with linguistic features.
This paper presents the strategy and design of a highly efficient semiautomatic method for labelling the semantic features of common nouns, using semantic relationships between words, and based on the information extracted from an electronic monolingual dictionary.
ILLICO combines two principles: modularity in the representation of knowledge defined at the different levels of language processing, and sentence composition using partial synthesis and guided composition.
Our approach was to develop a model of natural language generation from semantics, and train the model using maximum likelihood and smoothing.
To address this issue we have created a robust, semantic parser as a single finite-state machine (FSM).
We describe a method for evaluating a grammar checking application with hand–bracketed parses.
We introduce two measures to make dialogs for fixing recognition errors.
With performance above 97% accuracy for newspaper text, part of speech (POS) tagging might be considered a solved problem.
Previous studies have shown that allowing the parser to resolve POS tag ambiguity does not improve performance.
However, for grammar formalisms which use more fine-grained grammatical categories, for example TAG and CCG, tagging accuracy is much lower.
We extend this multi- tagging approach to the POS level to overcome errors introduced by automatically assigned POS tags.
Although POS tagging accuracy seems high, maintaining some POS tag ambiguity in the language processing pipeline results in more accurate CCG supertagging.
This paper presents the results of a study on the semantic constraints imposed on lexical choice by certain contextual indicators.
Based on this correlation, we have developed a technique for automatic lexical choice of descriptions of entities in text generation.
In this paper, we introduce a new parser, called SXLFG, based on the Lexical- Functional Grammars formalism (LFG).
Words unknown to the lexicon present a substantial problem to part-of-speech tagging.
In this paper we present a technique for fully unsupervised statistical acquisition of rules which guess possible partsof-speech for unknown words.
Three complementary sets of word-guessing rules are induced from the lexicon and a raw corpus: prefix morphological rules, suffix morphological rules and ending-guessing rules.
We provide an XML serialization for intercomponent communication.
A the-saurus navigator having novel functions such as term clustering, thesaurus overview, and zooming-in is proposed.
We propose a computational model of visually-grounded spatial language understanding, based on a study of how people verbally describe objects in visual scenes.
We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework.
superordinate -hyponym relation, synonym relation) is one of the most important problems for thesaurus construction.
This paper presents an active-learning word selection strategy that is mindful of human limitations.
In this paper we describe a coreference resolution method that employs a classification and a clusterization phase.
This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice, thereby improving the naturalness of synthetic speech in a spoken language dialogue system.
The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized.
This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.
A number of operational models are introduced, with web interfaces for lexical databases, DFSA matrices, finite- state phonotactics development, and DATR lexica.
We instead propose to acquire it automatically via a novel multiple-pass algorithm employing multiple-sequence alignment, a technique commonly used in bioinformatics.
In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank.
This report summarizes Me system and its performance on the MUG-4 task.
In this paper, we evaluate a vari-ety of knowledge sources and super-vised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
The learning al-gorithms evaluated include Support Vec-tor Machines (SVM), Naive Bayes, Ad-aBoost, and decision tree algorithms.
We describe a three-tiered approach for evaluation of spoken dialogue systems.
I estimate a confidence value of each SCF using corpus-based statistics, and then perform clustering of SCF confidence- value vectors for words to capture co- occurrence tendency among SCFs in the lexicon.
I apply my method to SCFs acquired from corpora using lexicons of two large-scale lexicalized grammars.
Corpus-based Natural Language Processing (NLP) tasks for such popular languages as English, French, etc.
This POS-tagger made use of the Transformation-Based Learning (or TBL) method to bootstrap the POS-annotation results of the English POS-tagger by exploiting the POS-information of the corresponding Vietnamese words via their word- alignments in EVC.
Then, we directly project POSannotations from English side to Vietnamese via available word alignments.
single event, multiple events and declared event(s).
Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in- sequence n-grams automatically.
The second method relaxes strict n-gram matching to skipbigram matching.
Skip-bigram is any pair of words in their sentence order.
Skip-bigram co- occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
This paper introduces a method for the semi-automatic generation of grammar test items by applying Natural Language Processing (NLP) techniques.
We describe the possible integration of topic signatures with ontologies and its evaluatonon an automated text summarization system.
This paper describes a rule-learning approach towards Chinese prosodic phrase prediction for TTS systems.
We explore unsupervised language model adaptation techniques for Statistical Machine  Translation.
In this paper we approach word sense disambiguation and information extraction as a unified tagging problem.
We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model.
This paper advocates an account of the different imperfective readings in terms of pragmatic principles and inferential heuristics based on, and supplied by, a semantic skeleton consisting of a 憇electional theory?of aspect.
Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English.
We propose an approach of combining lexical information, web statistics, and inverse search based on Google to backward translate a Chinese named entity (NE) into English.
The paper discusses how compositional semantics is implemented in the Verb-mobil speech-to-speech translation sys-tem using LUD, a description language for underspecified discourse representa-tion structures.
In this paper, we explore the use of structured content as semantic constraints for enhancing the performance of traditional term-based document retrieval in special domains.
First, we describe a method for automatic extraction of semantic content in the form of attribute-value (AV) pairs from natural language texts based on domain models constructed from a semi- structured web resource.
Sentence retrieval plays a very important role in question answering system.
In this paper, we present a novel cluster-based language model for sentence retrieval in Chinese question answering which is motivated in part by sentence clustering and language model.
Sentence clustering is used to group sentences into clusters.
The POSBIOTM/W1 is a workbench for machine-learning oriented biomedical text mining system.
These paraphrases are generated using WordNet and part-of-speech information to propose synonyms for the content words in the queries.
Statistical information, obtained from a corpus, is then used to rank the paraphrases.
This paper describes a system for constructing conceptual graph representation of text by using a combination of existing linguistic resources (VerbNet and WordNet).
We use a two- step approach, by firstly identifying the semantic roles in a sentence, and then using these roles, together with semi-automatically compiled domain-specific knowledge to construct the conceptual graph representation.
We describe the outline of Text Summarization Challenge 2 (TSC2 hereafter), a sequel text summarization evaluation conducted as one of the tasks at the NTCIR Workshop 3.
One long-term goal of this research is the development of predictive models of natural modality integration to guide the design of emerging multimodal architectures.Keywordsmultimodal interaction, integration and synchronization, speech and pen input, dynamic interactive maps, spatial location information, predictive modeling
Based on this analy-sis, we propose a variety of new morphological un-known-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German.
In this paper we describe an approach to constraint based syntactic theories in terms of finite tree automata.
We achieve this by using the intertranslatability of formulae of MSO logic and tree automata and the embedding of MSO logic into a constraint logic programming scheme.
In	this paper,	the	addition of	part-of-speech	ambiguity to	adeterministic parser written in Prolog is described.
Second, acquire a phonetic transcription of the new word.
In this paper we present the results of a preliminary study that employs a novel approach to the problem of acquiring the orthographic transcription through the use of an n-gram language model of english spelling and a quad-letter labeling of acoustic models that when taken together potentially produce an acoustic to spelling transcription of any spoken input.
Word posterior probabilities are a common approach for confidence estimation in automatic speech recognition and machine translation.
We will generalize this idea and introduce n-gram posterior probabilities and show how these can be used to improve translation quality.
This article describes the construction of a morphological, syntactic and semantic analyzer to operate a high-grade search engine for Hebrew texts.
This paper discusses an innovative approach to the computer assisted scoring of student responses in WebLAS (web-based language assessment system)- a language assessment system delivered entirely over the web.
In this paper, we explain a rapid development method of multimodal dialogue sys-tem using MIML (Multimodal Interaction Markup Language), which defines dialogue patterns between human and various types of interactive agents.
This paper discusses how a two-level knowledge representation model for machine translation integrates aspectual information with lexical-semantic information by means of parameterization.
Verbal and compositional lexical aspect provide the underlying temporal structure of events.
We show that it is possible to represent lexical aspect—both verbal and compositional—on a large scale, using Lexical Conceptual Structure (LCS) representations of verbs in the classes cataloged by Levin (1993).
In this paper we describe a Disambiguation Module which analyzes the content of dictionary definitions, in particular, definitions of the form "to VERB with NP".
The paper describes a language-independent approach for semiautomatic extension of lexical-semantic word nets and evaluates the method on CoreNet, the Korean version of word net.
This paper proposes a novel method to extract paraphrases of Japanese noun phrases from a set of documents.
In this paper we examine some issues pertaining to the task of selection in text planning.
Root based clusters can substitute dictionaries in indexing for IR.
This paper extends our previous work on committee-based sample selection for probabilistic classifiers.
We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging.
To realize our goal, we focus on expressions associated with time-slot (time-associated words).
We therefore use a semi-supervised learning method, the Na飗e Bayes classifier backed up with the Expectation Maximization algorithm, in order to iteratively extract time-associated words while improving the classifier.
This paper describes classification of typed student utterances within AutoTutor, an intelligent tutoring system.
The classifier presented uses part of speech tagging, cascaded finite state transducers, and simple disambiguation rules.
The Probabilistic Context-Free Grammar (PCFG) model is widely used for parsing natural languages, including Modern Chinese.
Now in this paper, we move on to the PCFG parsing of Classical Chinese texts.
We present an algorithm that learns the cross-language correspondence between affixes and letter sequences.
This paper explores the relationship between the translation quality and the retrieval effectiveness in Machine Translation (MT) based Cross-Language Information Retrieval (CLIR).
We apply default inheritance hierarchies to generating the morphology of Hebrew verbs.
in speech recognition, POS tagging and machine translation, but its justification is rarely questioned.
In this paper we investigate the applicability of Co-training to train classifiers that predict emotions in spoken dialogues.
- actionorganizationcivil fan namenationality mil itary	?assassin ccomando		terroris	...extremist			murderer narcoterror 1st rebel			robber			... subversive			1 la	attr桬	terrorist	guerri		Tat				... terrorist			.... thief		(KB EdANALYZEXTEND(Quit)Figure 3: Hierarchy EditorFlUSER: (none)癕AIN: (none)120000ES Find: 癰iKnowledge Base Graph Tool(Grow)(Done)(Prune)(Options)(Grow All)(Prune All)(Redisplay)(Ent ire Graph)ID: C offammo(Add Vocabulary)Children: Coff(Recalculate Graph)INLET: Interactive Natural Language Engineering ToolMOI111?11111111(Quit)RULE EDITOR: professional (1 of 1)IMMO ISEIMB MUD	Elli013 0 0 =II IMRule modes: CatonNode(s): .
Functional unification (FU) grammar is a general linguistic formalism based on the merging of feature-sets.
The left-corner transform removes left-recursion from (probabilistic) context-free grammars and uni-fication grammars, permitting simple top-down parsing techniques to be used.
An HMM-based Single Character Recovery (SCR) Model is proposed in this paper to extract a large set of 揳tomic abbreviation pairs?from a large text corpus.
This paper proposes an approach to improve statistical word alignment with a rule-based translation system.
This approach first uses IBM statistical translation model to perform alignment in both directions (source to target and target to source), and then uses the translation information in the rule-based machine translation system to improve the statistical word alignment.
We propose that logic (enhanced to encode probability information) is a good way of characterizing semantic interpretation.
In support of this we give a fragment of an axiomatization for word-sense disambiguation, noun- phrase (and verb) reference, and case disambiguation.
We describe an inference engine (Frail3) which actually takes this axiomatization and uses it to drive the semantic interpretation process.
Bilingual sentence pairs are first aligned in syntactic structure by combining English parse trees with a statistical bilingual language model.
It is implemented using oneversus-all support vector machine (SVM) classifiers and a number of feature extractors at several linguistic levels.
Two kinds of semantic variants can be found in traditional terminologies : strict synonymy links and fuzzier relations like see-also.
He validated an important part of the detected links as synonymy.
We present two different approaches to the modeling of extraposition, both based on machine learned decision tree classifiers.
The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics.
In this paper, we present a fully automated extraction system, named IntEx, to identify gene and protein interactions in biomedical text.
Our approach is based on first splitting complex sentences into simple clausal structures made up of syntactic roles.
Then, tagging biological entities with the help of biomedical and linguistic ontologies.
To disambiguate homograph, several efficient approaches have been proposed such as part-of-speech (POS) n-gram, Bayesian classifier, decision tree, and Bayesian-hybrid approaches.
This paper discusses the application of a previously reported theory of explanation rhetoric (Maybury, 1988b) to the task of explaining constraint violations in a hybrid rule/frame based system for resource allocation (Dawson et al, 1987).
We begin by exploring theoretical and practical issues with phrasal SMT, several of which are addressed by syntax-based SMT.
The learning component of our method is based on algorithms for reinforcement learning, such as dynamic programming and Q-learning.
We illustrate our method with a dialogue agent named ELVIS (EmaiL Voice Interactive System), that supports access to email over the phone.
Among the types of ambiguities handled are prepositional phrases, very compound nominals, adverbials, relative clauses, and preposed prepositional phrases.
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.
We thus propose a Senseval-3 lexical sam-ple activity where the training data is collected via Open Mind Word Expert.
We propose a range of deep lexical acquisition methods which make use of morphological, syntactic and ontological language resources to model word similarity and bootstrap from a seed lexicon.
In this paper, we present a machine learning system for identifying non-referential it.
We identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model.
These considerations make syntactic focusing a more accurate predictor of the interpretation of one-anaphoric noun phrases without decreasing the accuracy for definite pronouns.
This paper presents a framework and a system for implementing, comparing and analyzing parsers for some classes of Constraint-Based Grammars.
a set of morphemes with associated features.
a local area network).
The regularity of named entities is used to learn names and to extract named entities.
Novel aspects of our system include multimodal ambiguity resolution, modular ontology- driven artifact manipulation, and a meeting browser for use during and after meetings.
This paper presents an approach for processing incomplete and inconsistent knowledge.
To help developing a localization oriented EBMT system, an automatic machine translation	evaluation	method	isimplemented which adopts edit distance, cosine correlation and Dice coefficient as criteria.
In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers.
This work introduces a methodology for designing dialogue managers in spoken dialogue systems for restricted domains.
This paper describes the framework of a Korean phonological knowledge base system using the unification- based grammar formalism : Korean Phonology Structure Grammar (KPSG).
The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system.
The Korean syllable structure has two types: one is the type of consonant and vowel goup(CV type : : ga), and the other is the type of consonant, vowel and consonant group(CVC type 4 : gak).
1 Korean syllable structure3.
Korean Phonology Structure GrammarAs mentioned above, KLSG[5] is a new grammar theory for the Korean language and follows a unification-based grammar such as GPSG[6] and HPSG[7].
The phonological feature system in ICPSGAll the Korean phonological categories of KPSG are presented by the sets of feature and they consist of feature and their values.
This paper presents preliminary experiments in the use of translation equivalences to disambiguate prepositions or case suffixes.
The context in which a target word occurs is represented by surface lexical features such as unigrams, bigrams, and second order co-occurrences.
We apply the log-linear model to automatically restore missing articles based on features of the noun phrase.
We first show that the model yields competitive results in article generation.
We use HBMs as word models conditioned on both DAs and (hidden) DA- segments.
These we will call IC Classes.
(1) a. Configurations: sisterhood, c-command, m-command, ±maximal projection ...b. Lexical features: ±N, ±V, ±Funct, ±c-selected, ±Strong Agrc.
These features are compiled into context-free rules in our parser.
valid prefixes.
We present an approach to parallel natural language parsing which is based on a con-current, object-oriented model of computa-tion.
This paper presents our experiments in applying Latent Semantic Analysis (LSA) to dialogue act classification.
We employ both LSA proper and LSA augmented in two ways.
Unlexicalized probabilistic context-free parsing is a general and flexible approach that sometimes reaches competitive results in multilingual dependency parsing even if a minimum of language-specific information is supplied.
We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.
The paper presents a new model for context- dependent interpretation of linguistic expressions about spatial proximity between objects in a natural scene.
This paper proposes an approach to automated ontology alignment and domain ontology extraction from two knowledge bases.
First, WordNet and HowNet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus.
This paper presents the results of applying transformation-based learning (TBL) to the problem of semantic role labeling.
Each approach is based on a particular model for the patterns to be acquired, such as a predicate-argument structure or a dependency chain.
In this paper, we compare the prior models and introduce a new model, the Subtree model, based on arbitrary sub- trees of dependency trees.
Speech-to-speech translation can be ap-proached using finite state models and several ideas borrowed from automatic speech recognition.
A 搒erial architecture?would use the Hidden Markov and the language models for recognizing input utterance and the transducer for finding the transla-tion.
In this paper, a hybrid disambiguation method for the prepositional phrase (PP) attachment and interpretation problem is presented.1 The data needed, semantic PP interpretation rules and an annotated corpus, is described first.
We consider the problem of question- focused sentence retrieval from complex news articles describing multi-event stories published over time.
To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.
This paper addresses an important problem in Example-Based Machine Translation (EBMT), namely how to measure similarity between a sentence fragment and a set of stored examples.
Training classifiers to perform text categorization on abstracts is one way to accomplish this task.
We present a method for improving text classification for biological journal abstracts by generating additional text features using the knowledge represented in a biological concept hierarchy (the Gene Ontology).
This paper describes the input specification language of the WAG Sentence Generation system.
We present two methods for improving performance of person name recognizers for email: email- specific structural features and a recall- enhancing method which exploits name repetition across multiple documents.
We describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton.
We create a word-trie, transform it into a minimal DFA, then identify hubs.
The surface forms of applicable morphophonemic transformations are then derived using finite state machines.
This investigation proposes an approach to modeling the discourse of spoken dialogue using semantic dependency graphs.
Text Summarization Evaluation (SUMMAC) has established definitively that automatic text summarization is very effective in relevance assessment tasks.
The normalized representation can be used to detect paraphrases in texts.
Normalization and paraphrase detection tasks are built on top of a robust analyzer for English and are exclusively achieved using symbolic methods.
This paper presents an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts.
We present a new composite similarity metricthat combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units.
Several grammars have been proposed for modeling RNA pseudoknotted structure.
In this paper, we focus on multiple context-free grammars (MCFGs), which are natural extension of context-free grammars and can represent pseudoknots, and extend a specific subclass of MCFGs to a probabilistic model called SMCFG.
We present ongoing work on prosody predic-tion for speech synthesis.
The predic-tion is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neigh-bour algorithm or an analogy-based approach.
This paper presents a new method for translating a term-list by using a corpus in the target language.
We discuss a computer implementation of these representations using the Semantic Network Processing System (SNePS) and an ATN parser-generator with a question-answering capability.
We first define a word similarity measure based on the distributional pattern of words.
The similarity measure allows us to construct a thesaurus using a parsed corpus.
We propose a new framework to mine key phrase translations from web corpora.
We retrieve mixed- language web pages based on the expanded queries.
Finally, we extract the key phrase translation from the second- round returned web page snippets with phonetic, semantic and frequency- distance features.
This paper introduces a grammar formalism specifically designed for syntax-based statistical machine translation.
A Dependency Insertion Grammars (DIG) is a generative grammar formalism that captures word order phenomena within the dependency representation.
Synchronous Dependency Insertion Grammars (SDIG) is the synchronous version of DIG which aims at capturing structural divergences across the languages.
We then introduce a probabilistic extension of SDIG.
We finally evaluated our current implementation of a simplified version of SDIG for syntax based statistical machine translation.
For example, with a gene name dictionary, wecan identify the gene names contained in an article.
We propose a corpus-based method (Biber,1993; Nagao,1993; Smadja,1993) which generates Noun Classifier Associations (NCA) to overcome the problems in classifier assignment and semantic construction of noun phrase.
The NCA is created statistically from a large corpus and recomposed under concept hierarchy constraints and frequency of occurrences.Keywords: Thai language, classifier, corpus-based method, Noun Classifier Associations (NCA)
This paper presents a Unicode based Chinese word segmentor.
Analysis grammar of Japanese in the Mu-project Is presented.
The proposed tool supports semi-automatic tagging.
This paper sketches some basic features of the SYNPHONICS account of the computational modelling of incremental language production with the example of the generation of passive sentences.
We differentiate between two possible kinds of stimuli within the generation process that trigger the formation of passive sentences: a Formulator-external stimulus and a Formulator-internal one.
This paper presents a multi-layered Question Answering (Q.A.)
capabilities with the possibility of processing complex questions.
This paper discusses aspects of the planning of explanatory texts for logic based systems.
It presents a method for deriving Natural Language text plans from Natural Deduction-based structures.
at Grenoble.
We propose a paradigm for concurrent natural language generation.
In order to represent grammar rules distributively, we adopt categorial unification grammar (CUG) where each category owns its functional typo.
We augment typed lambda calculus with several new combinators, to make the order of A-conversions free for partial / local processing.
Th.e concurrent calculus is modeled with Chemical Abstract Machine.
This paper describes an extension to the hidden Markov model for part-of-speech tagging using second-order approximations for both contextual and lexical probabilities.
Achieving this goal requires identification of not only the different topics in the documents but also of the particular flow of these topics.Our approach to content similarity evaluation employs n-grams of lexical chains and measures similarity using the cosine of vectors of n-grams of lexical chains, vectors of tf*idf-weighted keywords, and vectors of unweighted lexical chains (unigrams of lexical chains).
Our results show that n-grams of unordered lexical chains of length four or more are particularly useful for the recognition of content similarity.
This paper addresses the task of extract-ing opinions from a given document collection.
We also find that partitioning the data may help memory-based learning.
In this research, naive, syntax-based disambiguation is attempted by assigning each word a part-of-speech tag and by enriching the 慴ag-ofwords?data representation often used for document clustering with synonyms and hypernyms from WordNet.
In particular, we present a corpus-based approach for finding base NPs by matching part-ofspeech tag sequences.
In this paper, a language tagging template named POC-NLW (position of a character within an n-length word) is presented.
In this method, the basic word segmentation is based on n-gram language model, and a Hidden Markov tagger based on the POC-NLW template is used to implement the out-of-vocabulary (OOV) word identification.
We present a variant of 'FAGG, called synchronous TAGs, which characterize correspondences between languages.
Various lexical and syntactic fea-tures are derived from parse trees and used to derive statistical clas-sifiers from hand-annotated training data.
A series of comparative experiments show the resulting models outperform uniform memory Markov models in a part-of-speech tagging task.
We exhibit a linear-time chart parsing algorithm with a low grammar constant.
Generation procedure in SEMSYNThis section summarizes the SEMSYN genration procedure.
We propose a knowledge-poor method that exploits the sentencial context of words for extracting similarity relations between them as well as semantic in nature word clusters.
Distributional similarity is a useful notion in estimating the probabilities of rare joint events.
Here, we examine the tradeoffs between model size and prediction accuracy for cluster-based and nearest neighbors distributional models of unseen events.
The paper describes a method to process recursive noun phrases with finite-state cascades.
A coarse match between voiced segments of speech and voiced segments of the phonetic spelling of the utterance is executed by dynamic programming as for approximate string matching.
Issues to be considered include comparative inflections, left recursion and other forms of nesting, extraposition of comparative complements, ellipsis, the wh element "how", and the translation of normalized parse trees into logical form.
An experimental system for dialogue structure analysis based on a new type plan recognition model for spoken dialogues has been implemented.
Anticipating the availability of large question- answer datasets, we propose a principled, data- driven Instance-Based approach to Question Answering.
We learn models of answer type, query content, and answer extraction from clusters of similar questions.
In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model.
Over the past few years, HNC has developed a neural network based, vector space approach to text retrieval.
In this paper, we argue how the richer vocabulary for lexical semantics proposed in Pustejovsky's "Generative Lexicon" theory allows one to explore the role of lexical information in such cases, and therefore sheds more light on the distinction between lexical inferences, which follow from defaults associated with lexical items and rules of composition, and pragmatic inferences, which depend on reasoning with respect to the context of the utterance.
GOD (General Ontology Discovery) is an unsupervised system to extract semantic relations among domain specific entities and concepts from texts.
This paper describes acquisition of English surface case frames from it corpus, based on a gradual knowledge acquisition approach.
In this paper I outline a normal form system for a sequent formulation of the product-free associative Lambek Calculus.
We propose an apparently minor extension to Kay's (1985) notation for describing directed acyclic graphs (DAGs).
This paper introduces a methodology to analyse and resolve cases of coreference in dialogues in English and Portuguese.
Traditional grammars classify words according to generic syntactic functions or morphological characteristics.
Correlational Grammar is an attempt in that direction.
It exemplifies this approach in two ares of grammar: predicative adjoctives and transitive verbs.
Speech recognition errors are inevitable in a speech dialog system.
We present an overview of the underlying dimensions that were used in describing the semantics and pragmatics of the Dutch subordinating conjunctions.
Traditionally, word sense disambiguation (WSD) involves a different context model for each individual word.
A context clustering scheme is developed within the Bayesian framework.
To achieve this task, we advocate that Segmented Discourse Representation Theory (SDRT) is a most expressive discourse framework.
This paper investigates adapting a lexicalized probabilistic context-free grammar (PCFG) to a novel domain, using maximum a posteriori (MAP) estimation.
MAP adaptation can also be based on either supervised or unsupervised adaptation data.
This paper deals with aspects of the resolution of deictic and elliptic expressions that are related to gestures.
The first approach uses click free mouse gestures for deictic pointing, while manipulative gestures are performed by using mouse button events as is usual in graphic interfaces.
Syntactic knowledge is important for pronoun resolution.
In this paper, we present a method that automatically constructs a Named Entity (NE) tagged corpus from the web to be used for learning of Named Entity Recognition systems.
We use an NE list and an web search engine to collect web documents which contain the NE instances.
This paper reports on work carried out to de-velop a spelling and grammar corrector for Dan-ish, addressing in particular the issue of how a form of shallow parsing is combined with er-ror detection and correction for the treatment of context-dependent spelling errors.
This has particularly been a problem with WordNet which is widely used for word sense disambiguation (WSD).
We examine the utility of speech and lexical features for predicting student emotions in computer- human spoken tutoring dialogues.
We then extract acoustic-prosodic features from the speech signal, and lexical items from the transcribed or recognized speech.
We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions.
The central questions are: How useful is information about part-of-speech frequency for text categorisation?
In this paper we introduce an empirical approach to the semantic interpretation of superlative adjectives.
We present a corpus annotated for superlatives and propose an interpretation algorithm that uses a wide-coverage parser and produces semantic representations.
Va...rious devices for the improvement of phrase structure grammars (PSG) have been suggested recently.
generate non_grammatical sent_ ences.
In this paper, we present Espresso, a weakly-supervised, general-purpose, and accurate algorithm for harvesting semantic relations.
We present the computational, linguistic and ergonomic aspects of the mock-up, and discuss them in the perspective of building an operational prototype in the Inture.KeywordsInteractive MT, DBMT for monolingual author, Interactive disambiguation, Production of disambiguation dialogues, Distributed architecture, Whiteboard approach
We present an approach for tagging verb sense that combines a domain-independent method based on subcategorization and alternations with a domain-dependent method utilizing statistically extracted verb clusters.
We present the technique of Virtual Annotation as a specialization of Predictive Annotation for answering definitional What is questions.
Virtual Annotation uses a combination of knowledge-based techniques using an ontology, and statistical techniques using a large corpus to achieve high precision.KeywordsQuestion-Answering, Information Retrieval, Ontologies
The method is used within a text-to-speech system to help generate pronunciations of unknown words.
Based on this approach, we develop a client-side personalized web search agent PAIR (Personalized Assistant for Information Retrieval), which supports both English and Chinese.
This empirical study attempts to find answers to the question of how a natural language (henceforth NL) system could resolve attachment of prepositional phrases (henceforth PPs) by examining naturally occurring PP attachments in typed dialogue.
We then proceed to show how to define a SSTC with a Structural Correspondence Static Grammar (SCSC), and which constraints to put on the rules of the SCSG to get a "natural" SSTC.linguistic	descriptors,	discontinuousconstituents, discontinuous phrase structure grammars, structured string-tree correspondences, structural correspondence static grammarsAbrevia,..ioes: DPSG, MT, NI., SSTC, STOP.Ordered trees, annotated with simple labels or complex "cecorations" (property lists), are widely used for representing natural language (NL) utterances.
In the last part, we show how to define a SSTC with a Structural Correspondence Static Grammar (SCSS), and which constraints to put on the rules of the SCSG to get a "natural" SSTC.I.
Two new parsing algorithms for context-free phrase structure grammars are presented which perform a bounded amount of processing per word per analysis path, independently of sentence length.
This paper designs a novel lexical hub to disambiguate word sense, using both syntagmatic and paradigmatic relations of words.
It only employs the semantic network of WordNet to calculate word similarity, and the Edinburgh Association Thesaurus (EAT) to transform contextual space for computing syntagmatic and other domain relations with the target word.
Frequency-based approaches with and without dictionary are proposed to extract formulation rules of named entities for individual languages, and transformation rules for mapping among languages.
An application of the results on cross language information retrieval is also shown.
In Chinese, zero anaphors occur frequently.
Syntactic tagging of words in a running text for detection of anaphora requires specification of argument structure for verbal elements.
We describe the use of energy function op-timisation in very shallow syntactic pars-ing.
The rules are con-textual constraints for resolving syntactic ambiguities expressed as alternative tags, and the statistical language model consists of corpus-based n-grams of syntactic tags.
In this paper a sentence compression tool is described.
We propose a path-based transfer model for machine translation.
A semi-automatic procedure of linguistic knowledge acquisition is proposed, which combines corpus-based techniques with the conventional rule-based approach.
We show that for context-sensitive spelling correction the Web Corpus results are better than using a search engine.
This paper presents an evaluation of indirect anaphor resolution which considers as lexical resource the semantic tagging provided by the PALAVRAS parser.
This paper proposes an unsupervised learning model for classifying named entities.
This paper investigates several augmented mixture models that are competitive alternatives to standard Bayesian models and prove to be very suitable to word sense disambiguation and related classifica-tion tasks.
This paper explores the determinants of adjective-noun plausibility by using correlation analysis to compare judgements elicited from human subjects with five corpus-based variables: co-occurrence frequency of the adjective-noun pair, noun frequency, conditional probability of the noun given the adjective, the log-likelihood ratio, and Resnik's (1993) selectional association measure.
Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.
In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.
Our experiments show that log-linear models significantly outperform IBM translation models.
The last stage constructs the surface string using knowledge about syntax, morphology, and style.
A simple approach to address the semantic class of the particle verb is introduced.
Three models (and several variants) are trained and tested: an n-gram language model, a discriminative maximum entropy model using structural information (and incorporating the language model as a separate feature), and finally an SVM ranker trained on the same feature set.
In this paper we report on several issues arising out of a first attempt to annotate task-oriented spo-ken dialog for rhetorical structure using Rhetorical Structure Theory.
The purpose of this paper is to compare different ways of adopting reason-maintenance techniques in incremental parsing (and interpretation).
A reason- maintenance system supports incremental formation and revision of beliefs.
Moreover, an assumption-based reason-maintenance system (ATMS) can be used to support efficient comparisons of (competing) interpretations.
Automatic recognition of Arabic dialectal speech is a challenging task because Arabic dialects are essentially spoken varieties.
At first glance, generalized phrase structure grammar (GPSG) appears to be a blessing on two counts.
metarules and the theory of syntactic features) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG.
This paper examines efficient predictive broad- coverage parsing without dynamic programming.
In contrast to bottom-up methods, depth-first top-down parsing produces partial parses that are fully connected trees spanning the entire left context, from which any kind of non-local dependency or partial semantic interpretation can in principle be read.
We contrast two predictive parsing approaches, top- down and left-corner parsing, and find both to be viable.
the construction of domain- independent lexica.
We compute the consensus alignment using a multi-sequence alignment algorithm used for DNA sequence alignment.
We present an application of this technique to bootstrap bilingual data for the general domain of instant messaging.
We train hierarchical statistical translation models on the bootstrapped bilingual data and show that the resulting statistical translation model outperforms each individual off-the-shelf translation system.
We introduce total rank distance and scaled total rank distance, we prove that they are metrics and investigate their max and expected values.
We will concentrate on multiword expressions (MWE), particularly on multiword nouns, (i) illustrating their most relevant morphological features, and (ii) pointing out the methods and techniques adopted to generate the inflected forms from lemmas.
This paper describes a fully-automated real- time broadcast news video and audio processing system.
The system combines speech recognition, machine translation, and cross- lingual information retrieval components to enable real-time alerting from live English and Arabic news sources.
The computational model, called Augmented Dependency Grammar (ADG), formulates not only the linguistic dependency structure of sentences but also the semantic dependency structure using the extended deep case grammar and field-oriented fact-knowledge based inferences.
Legato based on the ADG framework, constructs semantic dependency structure of Japanese input sentences by feature-oriented dependency grammar rules as main control information for syntactic analysis, and by semantic inference mechanism on a object fields' fact knowledge base.
The second component, Crescendo, extracts a conceptual structure about facts from the semantic dependency structure through logicalinterpretation on the language-particular semantic dependency using knowledge based inferences.Input SentenceIPMorphological AnalysisWord List IP Dependency structure AnalysisSemantic Dependency / 11, Structure 	)Inference EngineThesaurus Knowledge BaseLegato:Crescendo: 'Conceptual StructureAnalysisConceptual DependencyStructure Fig.
1	VENUS Analysis Module198
This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules.
The method consists of mainly four modules including morpheme normalization, phrase-break detection, morpheme to phoneme conversion and phoneme connectivity check.The morpheme normalization is to replace non-Korean symbols into standard Korean graphemes.
The phrase-break detector assigns phrase breaks using part-of-speech (POS) information.
In the morpheme-to-phoneme conversion module, each morpheme in the phrase is converted into phonetic patterns by looking up the morpheme phonetic pattern dictionary which contains candidate phonological changes in boundaries of the morphemes.
In this paper we tackle this problem by learning rules that generalize the state-based strategy.
This paper describes a practical method of automatic simultaneous interpretation utilizing an example-based incremental transfer mechanism.
We primarily show how incremental translation is achieved in the context of an example-based framework.
Finally, we propose a scheme for automatic simultaneous interpretation exploiting this example-based incremental translation mechanism.
This paper examines language similarity in messages over time in an online community of adolescents from around the world using three computational measures: Spearman抯 Correlation Coefficient, Zipping and Latent Semantic Analysis.
This paper describes to what extent deep processing may benefit from shallow processing techniques and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad梒overage unification梑ased grammar of Spanish.
We have collected, transcribed and analyzed over 8 hours of human-human interactive problem solving dialogue in the air travel planning domain, including traveler-agent dialogues and the more constrained agent-airline dialogues.
In this paper, we present a learning approach for coreference resolution of noun phrases in unrestricted text.
Listen to Ramesh.
In this paper we consider the problem of assigning a probability distribution to alternative sets of coreference relationships among entity descriptions.
We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions.
In this paper, we propose a method that exploits full parsing information by representing it as features of argument classification models and as constraints in integer linear learning programs.
In addition, to take advantage of SVM-based and Maximum Entropy-based argument classification models, we incorporate their scoring matrices, and use the combined matrix in the above-mentioned integer linear programs.
This paper presents two word segmentation (WS) systems and a named entity recognition (NER) system in France Telecom R&D Beijing.
The one system of WS is for open tracks based on n- gram language model and another one is for closed tracks based on maximum entropy approach.
The NER system uses a hybrid algorithm based on Class-based language model and rule-based knowledge.
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classification (maximum entropy) with linguistic information.
Minimal Recursion Semantics (MRS) is the standard formalism used in large-scale HPSG grammars to model underspecified semantics.
We have established a phonotactic language model as the solution to spoken language identification (LID).
A voice tokenizer converts a spoken document into a text-like document of acoustic tokens.
We apply latent semantic analysis to the vectors, in the same way that it is applied in information retrieval, in order to capture salient phonotactics present in spoken documents.
This work explores computing distributional similarity between sub-parses, i.e., fragments of a parse tree, as an extension to general lexical distributional similarity techniques.
In the same way that lexical distributional similarity is used to estimate lexical semantic similarity, we propose using distributional similarity between sub- parses to estimate the semantic similarity of phrases.
We present some novel machine learning techniques for the identification of subcategorization informa-tion for verbs in Czech.
We show how the learning algorithm can be used to dis-cover previously unknown subcategorization frames from the Czech Prague Dependency Treebank.
In this paper, we present a novel global reordering model that can be incorporated into standard phrase-based statistical machine translation.
To cope with sparseness, we use N-best phrase alignments and bilingual phrase clustering, and investigate a variety of combinations of conditioning factors.
The two current approaches to language generation, template-based and rule-based (linguistic) NLG, have limitations when applied to spoken dialogue systems, in part because they were developed for text generation.
In this paper, we propose a new corpus-based approach to natural language generation, specifically designed for spoken dialogue systems.
We describe an approach to surface generation designed for a "pragmatics-based" dialogue system.
We propose a generalized bootstrapping algorithm in which categories are described by relevant seed features.
The paper discusses three different kinds of syntactic ill-formedness: ellipsis, conjunctions, and actual syntactic errors.
In this paper we propose a trainable method for extracting Chinese entity names and their relations.
We view the entire problem as series of classification problems and employ memory-based learning (MBL) to resolve them.
Rather than using length-based or translation-based criterion, a part-of-speech-based criterion is proposed.
We investigate the utility of an algo-rithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexi-cons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.
We achieve this through an extension of the method of lexical triggers to the cross-language problem, and by developing a likelihood- based adaptation scheme for combining a trigger model with an -gram model.
We describe the application of such language models for automatic speech recognition.
We also compare our cross-lingual adaptation scheme to monolingual language model adaptation, and to an alternate method for exploiting cross-lingual cues, via cross- lingual information retrieval and machine translation, proposed elsewhere.
We propose a method for dealing with semantic complexities occurring in information retrieval systems on the basis of linguistic observations.
In this paper we present LX-Suite, a set of tools for the shallow processing of Portuguese.
This suite comprises several modules, namely: a sentence chunker, a tokenizer, a POS tagger, featurizers and lemmatizers.
We investigate the impact of the precision/recall trade-off of information extraction on the performance of an offline corpus-based question answering (QA) system.
We analyze subcategorization frequencies from four different corpora: psychological sentence production data (Connine et al.
We conclude that verb sense and discourse type play an important role in the frequencies observed in different experimental and corpus based sources of verb subcategorization frequencies.
Speech recognition problems are a reality in current spoken dialogue systems.
We apply Chi Square (x2) analysis to a corpus of speech-based computer tutoring dialogues to discover these dependencies both within and across turns.
This paper studies the computational complexity of disambiguation under probabilistic tree-grammars as in (Bod, 1992; Schabes and Waters, 1993).
We present an approach to named entity recognition that uses support vector machines to capture transition probabilities in a lattice.
Proposed approaches can be classified into table lookup, linguistic, combinatorial and rule-based techniques.This paper proposes a new approach to enhance a rule-based Arabic stemmer.
In this paper, we propose a novel Cooperative Model for natural language understanding in a dialogue system.
We build this based on both Finite State Model (FSM) and Statistical Learning Model (SLM).
This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.
A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model.
We identify lexical semantic information auto-matically by exploiting the consistent correspondences between surface syntactic cues and lexical meaning.
The algorithm successfully combines temporal occurrence similarity across dates in news cor-pora, wide and local cross-language context similarity, weighted Levenshtein distance, relative frequency and burstiness similarity measures.
These characteristics allow distinguishing wiki-based systems from citation and more traditional text-based networks augmented by hyper- links.
We further show that topic translation with online machine translation resources yields effective CL-SR.
In this paper, we tackle the problem of vocabulary selection, language modeling and pruning for inflective languages.
In this paper, we propose a method for constructing bigram LR tables by way of incorporating bigram constraints into an LR table.
Using a bigram LR table, it is possible for a GLR parser to make use of both bigram and CFG constraints in natural language processing.Applying bigram LR tables to our GLR method has the following advantages:(1) Language models utilizing bigram LR tables have lower perplexity than simple bigram language models, since local constraints (bigram) and global constraints (CFG) are combined in a single bigram LR table.
In this paper, we present an evaluation of the Link Grammar parser on a corpus consisting of sentences describing protein-protein interactions.
We introduce the notion of an interaction subgraph, which is the subgraph of a dependency graph expressing a protein-protein interaction.
In this paper, we propose an approach for content determination and surface generation of answers in a question-answering system on the web.
The DLSI-UA team is currently working on several word sense disambiguation approaches, both supervised and unsupervised.
This paper presents a view of different system results for Word Sense Disambiguation in different tasks of SENSEVAL-3.
In this paper we present a corpus-based study of NSUs.
We propose a mixed language query disambiguation approach by using co-occurrence information from monolingual data only.
Two novel features for disambiguation, namely contextual word voting and 1-best contextual word, are introduced and compared to a baseline feature, the nearest neighbor.
Within the complex domain of HPSG parse selection, we show that ideas from ensemble learning can help further reduce the cost of annotation.
We describe how diathesis alternation patterns can be used to make coarse sense distinctions for Chinese verbs as a necessary step in annotating the predicate-structure of Chinese verbs.
We then discuss the representation scheme we use to label the semantic arguments and adjuncts of the predicates.
From 1000 newly treebanked Korean sentences we generate a deterministic shift-reduce parser.
We present a strictly lexical parsing model where all the parameters are based on the words.
Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification- based grammar (UBG).
1 IntroductionStochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar.
These grammars can incorporate virtually all kinds of linguistically important constraints (including non-local and non-context-free constraints), and are equipped with a statistically sound framework for estimation and learning.Abney (1997) pointed out that the non-contextfree dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of log- linear models for defining probability distributions over the parses of a unification grammar.
This paper presents an unsupervised batch learner for the quantity-insensitive stress systems described in Gordon (2002).
This position paper contrasts rhetorical structuring of propositions with intentional decomposition using communicative acts.
We det.cribe a set of modules that together make up a grapheme-to-phoneme conversion system for Dutch.
Modules include a syllabification program, a fast tnorphological parser, a lexical database, a phonological knowledge base, transliteration rules, and phonological rules.
In particular we discuss a natural language generation system that is composed of SPoT, a trainable sentence planner, and FERGUS, a stochastic surface realizer.
We present an integrated probabilistic model for Japanese syntactic and case structure analysis.
This model selects the syntactic and case structure that has the highest generative probability.
We evaluate both syntactic structure and case structure.
We discuss a grammar development process used to generate the trees of the wide-coverage Lexicalized Tree Adjoining Grammar (LTAG) for English of the XTAG Project.
This paper shows how lexical choice during text generation depends on linguistic context.
We also discuss the limits of bigram statistical knowledge.
This paper investigates two approaches to speech segmentation based on different heuristics: the utterance-boundary strategy, and the predictability strategy.
This paper is concerned with the summarization of spontaneous conversations.
Previous work has focused on textual features extracted from transcripts.
This paper describes Vi-xfst, a visual interface and a development environment, for developing finite state language processing applications using the Xerox Finite State Tool, xfst.
The verb subcategorization frame information plays a major role of disambiguations in many NLP applications.
The discourse entities are used in intra- and extra-sentential pronoun resolution in BBN Janus.
The MTTK alignment toolkit for statistical machine translation can be used for word, phrase, and sentence alignment of parallel documents.
This paper examines the use of an unsupervised statistical model for determining the attachment of ambiguous coordinate phrases (CP) of the form n1 p n2 cc n3.
Attention on constraint-based grammar formalisms such as Head-driven Phrase Structure Grammar (liPsG) has focussed on syntax and semantics to the exclusion of phonology.
This paper investigates the incorporation of a non-procedural theory of phonology into HPSG, based on the 'one-lever model of Bird & Ellison (1992).
Prosodic domains, which limit the applicability of phonological constraints, are expressed in a prosodic type hierarchy modelled on imso's lexical type hierarchy.
We explore the differences in verb subcategorization frequencies across several corpora in an effort to obtain stable cross corpus subcategorization probabilities for use in norming psychological experiments.
In this paper an artificial life approach to the explanation of the shape of vowel systems is presented.
mildly context-sensitive grammars while keeping a polynomial parse time behavior.
Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues.
We describe our use of an existing re-source, the Mouse Anatomical Nomen-clature, to improve a symbolic interface to anatomically-indexed gene expression data.
We address the problem of automatically constructing a thesaurus by clustering words based on corpus data.
We view this problem as that of estimating a joint distribution over the Cartesian product of a partition of a set of nouns and a partition of a set of verbs, and propose a learning algorithm based on the Min-imum Description Length (MDL) Prin-ciple for such estimation.
We also evaluated the method by conduct-ing pp-attachment disambiguation ex-periments using an automatically con-structed thesaurus.
We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality?
This results in best-first processing of the idiomatic analysis.
Two models are discussed fot the lexical representation of idioms.
The connectionist model has an important advantage over the continuation class model: the conventionality principle follows naturally from the architecture of the connectionist model.Keywords: idiom processing, ambiguity resulution, two- level morphology, connectionism.
In the view presented here, lexical ambiguity resolution is an integral part of the same procedure that creates the semantic interpretation of a sentence itself.Keywords: lexical ambiguity, lexical semantics, cativositionality, lexical organization.
Collocation-based tagging and bracketing pro- grains have attained promising results.
OF COLING-92.
We present a new chart parsing method for Lambek grammars, inspired by a method for D- Tree grammar parsing.
for text classification problems in order to apply it to word sense disambiguation (WSD) problems.
In experiments, we solved 50 noun WSD problems in the Japanese Dictionary Task in SENSEVAL2.
We present the first application of the head-driven statistical parsing model of Collins (1999) as a simultaneous language model and parser for large- vocabulary speech recognition.
The parser uses structural and lexical dependencies not considered by n- gram models, conditioning recognition on more linguistically-grounded relationships.
This approach to text planning can be conveniently implemented as a Functional Unification Grammar.
We report on the Parallel Grammar (ParGram) project which uses the XLE parser and grammar development platform for six languages: English, French, German, Japanese, Norwegian, and Urdu.1
We then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels.
The results also showed that the proposed method is effective in understanding misrecognition speech sentences and in improving speech translation results.
Answer Extraction (AE) over texts from highly restricted domains.
We focus on the problem of building large repositories of lexical conceptual structure (LCS) representations for verbs in multi-ple languages.
We are currently using these lexicons in an operational foreign language tutoring and machine translation.
We revisit the idea of history-based parsing, and present a history-based parsing framework that strives to be simple, general, and flexible.
We derive the rhetorical structures of texts by means of two new, surface-form-based algorithms: one that identifies discourse usages of cue phrases and breaks sentences into clauses, and one that produces valid rhetorical structure trees for unrestricted natural language texts.
Finally an LFGbased multilingual parser simulating parsing strategies with ambiguous sentences.
In this paper, we address the issue of generating multilingual computational semantic lexicons from analysis lexicons, showing the necessity of relying on a conceptual lexicon.
The goal in this work is to develop a new language modeling approach based on randomly grown Decision Trees (DTs) and apply it to automatic speech recognition.
We study our RF approach in the context of -gram type language modeling.
This paper proposes a new paradigm for sentiment analysis: translation from text documents to a set of sentiment units.
We developed a high-precision sentiment analysis system at a low development cost, by making use of an existing transfer-based machine translation engine.
We investigate information retrieval between English and French, incorporating both translations directions into both document translation and query translation-based information retrieval, as well as into hybrid systems.
We develop a model of preposition definitions in a machine-readable dictionary using the theory of labeled directed graphs and analyze the resulting digraphs to determine a primitive set of preposition senses.
We propose a generalization of Categorial Grammar in which lexical categories are defined by means of recursive constraints.
possible structural and lexical attributes.
Our approach is language-, structure-, and domain-independent because it only requires some shallow syntactic analysis (e.g., a POS-tagger and a phrase chunker).
This paper describes a novel approach for the automatic generation and evaluation of a trivial dialogue phrases database.
Using Dana Scott's domain theory, we elucidate the nature of the feature systems used in augmented phrase-structure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar, lexical functional grammar and PATRII, and provide a denotational semantics for a simple grammar formalism.
This paper describes algorithms and software developed to characterise and detect generic intelligent language-like features in an input signal, using Natural Language Learning techniques: looking for characteristic statistical "language-signatures" in test corpora.
some kind of character-stream.
Our language-detection algorithm for symbolic input uses a number of statistical clues: data compression ratio, "chunking" to find character bit-length and boundaries, and matching against a Zipfian type-token distribution for "letters" and "words".
Our current research goal is to apply Natural Language Learning techniques to the identification of "higher-level" grammatical and semantic structure in a linguistic signal.
We apply a complexity theoretic notion of feasible learnability called "polynomial learnability" to the evaluation of grammatical formalisms for linguistic description.
This process created a data set that could be used to train a simple Hidden Markov Model (HMM) entity tagger.
In this paper we describe a new technique for parsing free text: a transformational grammar' is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.
This paper describes LINGUA - an architecture for text processing in Bulgarian.
First, the pre-processing modules for tokenisation, sentence splitting, paragraph segmentation, partof-speech tagging, clause chunking and noun phrase extraction are outlined.
1 IntroductionPrevious CoNLL shared tasks focused on NP chunking (1999), general chunking (2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005).
This shared task on full (dependency) parsing is the logical next step.
This paper presents a novel algorithm for efficiently generating paraphrases from disjunctive logical forms.
The algorithm is couched in the framework of Combinatory Categorial Grammar (CCG) and has been implemented as an extension to the OpenCCG surface realizer.
This paper describes a system for seg-menting Chinese text into words us-ing the MBDP-1 algorithm.
Coreference resolution involves finding antecedents for anaphoric discourse entities, such as definite noun phrases.
We have developed a corpus-based algorithm for automatically identifying definite noun phrases that are non-anaphoric, which has the potential to improve the efficiency and accuracy of coreference resolution systems.
Our algorithm generates lists of non- anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.
We explored supervised machine-learning systems using Support Vector Machines to automatically classify images into six representative categories based on text, image, and the fusion of both.
In this paper, we present a method to han-dle complex sentences with the centering theory and describe our framework that, identifies the antecedents of zero pro-nouns in naturally occurring Japanese discourses.
The lightweight procedures in IdentiFinder are SGML recognition, hidden Markov models, finite state pattern recognition, and SGML output.By heavyweight processing, we mean procedures that depend on global evidence and involve deeper understanding.
We present a trainable model for identify-ing sentence boundaries in raw text.
in machine translation).
This paper describes an example-based correction component for Japanese word segmentation and part of speech labelling (AMED), and a way of combining it with a pre-existing rule-based Japanese morphological analyzer and a probabilistic part of speech tagger.Statistical algorithms rely on frequency of phenomena or events in corpora; however, low frequency events are often inadequately represented.
Here we report on an example- based technique used in finding word segments and their part of speech in Japanese text.
Probabilistic models have been effective in resolving prepositional phrase attachment ambiguity, but sparse data remains a significant problem.
Three thesauruses are compared on this task: two existing generic thesauruses and a new specialist PP thesaurus tailored for this problem.
We also compare three smoothing techniques for prepositional phrases.
We investigated both English and Chinese ad-hoc information retrieval (IR).
We also investigated clustering of output documents from term level retrieval.
Best results were obtained by combining retrievals of bigram and short-word with character representation.
Browman and Goldstein have developed a general model of the timing of articulatory gestures.
This paper describes a transformation-based learning approach to disfluency detection in speech transcripts using primarily lexical features.
This paper proposes a system of mapping classes of syntactic structures as instruments for automatic text understanding.
The system illustrated in Japanese consists of a set of verb classes and information on mapping them together with noun phrases, tense and aspect.
This paper describes a bootstrapping algorithm called Basilisk that learns high- quality semantic lexicons for multiple categories.
Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts.
We evaluate Basilisk on six semantic categories.
The purpose of the study is to develop an integrated knowledge management system for the domains of genome and nano-technology, in which terminology-based literature mining, knowledge acquisition, knowledge structuring, and knowledge retrieval are combined.
In order to treat lexical issues systematically in transfer-based MT systems, we introduce the concept of bilingual-sings which are defined by pairs of equivalent monolingual signs.
In this paper, we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems.
We first apply the training strategy of combining active learning and self-training (Tur et al., 2005) for topic classifier.
Also, we propose a practical method for bootstrapping the topic-dependent semantic classifiers from a small amount of labeled sentences.
This paper investigates the correlation between acoustic confidence scores as returned by speech recognizers with recognition quality.
In this paper we propose an integration of a selforganizing map and semantic networks from WordNet for a text classification task using the new Reuters news corpus.
The Hypernym relation in WordNet supplements the neural model in classification.
We present an overview of TARSQI, a modular system for automatic temporal annotation that adds time expressions, events and temporal relations to news texts.
The paper describes how the information about the semantic interpretation of PPs is represented in the lexicon and in PP interpretation rules and how this information is used during semantic analysis.
Moreover, we report on experiments that evaluate the impact of using this information about PP interpretation on the CLEF question answering task.
We present an approach to natural language understanding based on a computable grammar of constructions.
A grammar is a set of constructions.
We show an application of our methodology to the cultural heritage domain, using the CIDOC CRM core ontology.
To enrich the CIDOC, we use available resources such as the AAT art and architecture glossary, WordNet, the Dmoz taxonomy for named entities, and others.
To determine proper suffixes for a given adjective-verb pair, we have examined the verbal features involved in the theory of Lexical Conceptual Structure.
We make preliminary comparisons with an analytical constraint-based approach to modeling loanword formation.An intended application for the NN parser, was to devise an English-Japanese proper or place-name translator, which would map an English phoneme sequence into Katakana(e.g.
We investigated automatic action item detection from transcripts of multi-party meetings.
We provide two different methods for bounding search when parsing with freer word-order languages.
In addition, we improve the word alignment results by combining the results of the two semi-supervised boosting methods.
Multimodal grammars provide an expressive formalism for multimodal integration and understanding.
This paper gives an overview of our work on statistical machine translation of spoken dialogues, in particular in the framework of the V~RBMoBiL project.
AN APPLICATION OF COMPUTER TECHNIQUES TO ANALYSISOF THE VERB PHRASE IN HINDI AND ENGLISH:A Preliminary ReportDr.
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).
An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.
We present a novel controlled natural language interface to temporal databases, based on translating nat-ural language questions into SQL/Temporal, a temporal database query language.
The semantic analysis is done using a novel theory of the semantics of tempo-ral questions, focusing on the role of temporal preposition phrases rather than the more tradi-tional focus on tense and aspect.
The method allows a user to explore a model of syntax-based statistical machine translation (MT), to understand the model抯 strengths and weaknesses, and to compare it to other MT systems.
The model is improved further by the use of class-based modifier- head bigrams constructed using semantic classes automatically extracted from a corpus.
We also present preliminary results obtained with a word prediction model integrating compound and simple word prediction.
Fourteen indicators that measure the frequency of lexico-syntactic phenomena linguistically related to aspectual class are applied to aspectual classification.
We automatically annotate training text with positive and negative examples of fact extractions and train Rote, Naive Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages.
A novel cross-field bootstrapping method leverages data interdependencies to yield improved performance.
Theories of discourse structure hypothesize a hierarchical structure of discourse segments, typically tree-structured.
In this paper, we explore prosodic cues to discourse segmentation in human- computer dialogue.
Using data drawn from 60 hours of interactions with a voice-only conversational spoken language system, we identify pitch and intensity features that signal segment boundaries.
We introduce a new model of selectional preference induction.
CABs pose conceptual constraints on the formation of complex RefOs in general.
A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guess- Mg heuristic, a most-frequent heuristic, and a co-occurrence heuristic.
This technique generalizes and exemplifies a new and original use of an existing concept of "proper guides" recently proposed in literature for controlling top-down left-to-right (TDLR) execution in logic programs.
Perhaps the best known example of this is using the IDLIZ derivation for left-recursive rules.
This paper describes a hybrid model that combines a rule-based model with two statistical models for the task of POS guessing of Chinese unknown words.
In this paper I elaborate a model of competence for corpus-based machine translation (CBMT) along the lines of the representations used in the transla-tion system.
This paper explores the segmentation of tutorial dialogue into cohesive topics.
A latent semantic space was created using conversations from human to human tutoring transcripts, allowing cohesion between utterances to be measured using vector similarity.
A novel moving window technique using orthonormal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task.
DTG involve two composition operations called subsertion and sister-adjunction.
This paper is concerned with the identification of two semantically close categories ?temporal locating adverbials and time-denoting expressions.
This paper presents experiments performed on lexical knowledge acquisition in the form of verbal argumental information.
The system obtains the data from raw corpora after the application of a partial parser and statistical filters.
This work presents a model for learning inference	procedures	for	storycomprehension	through	inductivegeneralization	and	reinforcementlearning, based on classified examples.
In this paper, we outline a theory of referential accessibility called Veins Theory (VT).
To do this, we jointly perform parsing and semantic role labeling, using a probabilistic SRL system to rerank the results of a probabilistic parser.
In this paper, we describe our hybrid approach to two key NLP technologies: biomedical named entity recognition (Bio-NER) and (Bio-SRL).
In Bio-NER, our system successfully integrates linguistic features into the CRF framework.
In addition, we employ web lexicons and template-based post-processing to further boost its performance.
We only annotate the predicate-argument structures (PAS抯) of thirty frequently used biomedical verbs (predicates) and their corresponding arguments.
Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) machine- learning model.
Grammaticality is a continuum phenomenon with many dimensions.
We refer the anomalous terms used in such context as network informal language (NIL) expres-sions.
We propose to study NIL expressions with a NIL corpus and investigate techniques in processing NIL ex-pressions.
Two methods for Chinese NIL ex-pression recognition are designed in NILER system.
We define a new learning task, minimum average lookahead grammar induction, with strong potential implications for incremental parsing in NLP and cognitive models.
Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours.
In this paper, we consider a number of algo-rithms for estimating the parameters of ME mod-els, including iterative scaling, gradient ascent, con-jugate gradient, and variable metric methods.
Application-specific data are collected with the help of Wizard-of-Oz techniques.
This paper proposes a new error-driven HM3/1- based text chunk tagger with context-dependent lexicon.
Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry.
This implements an approach to portable grammar-based language modelling in which all models are derived from a single linguistically motivated unification grammar.
In this article we report on a double-blind experiment with a surface- oriented morphosyntactic grammatical representation used in a large-scale English parser.
ATRS1 is an automatic transcription reconstruction system that can produce near-literal transcriptions with almost no human labor.
Processing stages include lexical lookup, syntactic parsing, semantic analysis, and pragmatic analysis.
In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
In oursystem, the resolution of anaphoric ambiguities is done uniquely by the semantic analyzer.
We have proposed a technique for labelling ambiguities in texts and in dialogue transcriptions, and experimented it on multilingual data.
This paper shows that inference rules with temporal constraints can be acquired by using verb-verb co-occurrences in Japanese coordinated sentences and verb-noun co- occurrences.
Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.
We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views.
In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.
This paper accompanies a demo of the GoDiS sys-tem.
An assumption shared by many theories of discourse is that discourse structure constrains anaphora resolution (cf.
The aim of this paper is (i) to show that this assumption also applies to multiple VP ellipsis (VPE), (ii) to argue that other levels of linguistic information (such as syntax and semantics) interact with discourse structure in determining multiple VPE acceptability and (iii) to make these intuitions precise by providing a unification-based account of multiple VPE resolution.
A deductive approach is used to predict vowel and consonant places of articulation.
(PDG), and proposes the 揋raph Branch Algorithm?for computing the optimum dependency tree from a DF.
In this paper we report results of a supervised machine-learning approach to Chinese word segmentation.
This paper describes Multi-Modal-Method, a design method for building grammar-based multi modal systems.
We introduce a first-order version of Categorial Grammar, based on the idea of encoding syntactic types as definite clauses.
Our approach to encoding types-as definite clauses presupposes a modification of standard Horn logic syntax to allow internal implications in definite clauses.
This paper describes a hybrid proposal to combine n-grams and Stochastic Context-Free Grammars (SCFGs) for language modeling.
A classical n-gram model is used to capture the local relations between words, while a stochas-tic grammatical model is considered to repre-sent the long-term relations between syntactical structures.
In order to define this grammatical model, winch will be used on large-vocabulary complex tasks, a category-based SCFG and a probabilistic model of word distribution in the categories have been proposed.
This paper describes a heuristic algorithm capable of automatically assigning a label to each of the senses in a machine readable dictionary (MRD) for the purpose of acquiring a computational-semantic lexicon for treatment of lexical ambiguity.
We also describe an implementation of the algorithm for labeling definition sentences in Longman Dictionary of Contemporary English (LDOCE).
We introduce a purely applicative language (PAL) as an intermediate representation and an object-oriented computation mechanism for its interpretation.
It exploits a vector-space model developed in information retrieval research.
The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm.
We present an authoring system for logical forms encoded as conceptual graphs (CG).
This paper discusses approaches for visualizing the affective content of documents and describes an interactive capability for exploring emotion in a large document collection.
The UNL module of ETAP-3 naturally combine's the two major approaches accepted in machine translation: the transfer-based approach and the iiiterlingua approach.
The identification of genes in biomedical text typically consists of two stages: identifying gene mentions and normalization of gene names.
The system identifies human gene synonyms from online databases to generate an extensive synonym lexicon.
SenseClusters is a freely available word sense discrimination system that takes a purely unsupervised clustering approach.
This paper explores the relationships between a computational theory of temporal representation (as developed by James Allen) and a formal linguistic theory of tense (as developed by Norbert Hornstein) and aspect.
time, causality.
Assuming an ordered representation of the predicate-argument structure, this work proposes a Combinatory Categorial Grammar formulation of relating surface case cues to categories and types for correctly placing the arguments in the predicate- argument structure.
This is achieved by treating case markers as type shifters.
The classification method used is linear discriminatory analysis, based on a learning sample.
This paper describes the system MC-WSD presented for the English Lexical Sample task.
We present two translation systems experimented for the shared-task of 揥orkshop on Statistical Machine Translation,?a phrase-based model and a hierarchical phrase-based model.
Experiments showed that the hierarchical phrase- based model performed very comparable to the phrase-based model.
We also report a phrase/rule extraction technique differentiating tokenization of corpora.
We describe a basic Geo-coding service encompassing a geo-parsing tool and integrated digital gazetteer service.
Figure 1: The geo-coding process
This paper presents a class of dependency-based formal grammars (FODG) which can be parametrized by two different but similar measures of nonprojectivity.
Various feature descriptions are being employed in constrained-based grammar formalisms.
We incorporate context into the search for an optimal alignment in two complementary ways: learning rules for matching paragraphs using topic structure and further refining the matching through local alignment to find good sentence pairs.
We will elaborate on our architecture and the experimental results.Keywords: answer set driven IR, attribute- based classification, automatic knowledge base construction,.
Structural information is provided by such hierarchical prosodic constituents as Syllable (S), Metrical Foot (MF), Phonological Word (PW), Intonational Group (IG).
Onto these structures, phonological rules are applied such as the "letter梩osound" rules, automatic word stress rules,internal stress hierarchy rules indicating secondary stress,external sandhi rules, phonological focus assignment rules, logical focus assignment rules.
We present a supervised learning approach to identification of anaphoric and non-anaphoric noun phrases and show how such information can be incorporated into a coreference resolution system.
We describe an approach to semiautomatic lexicon development from machine readable dictionaries with specific reference to verbal diatheses, envisaging ways in which the results obtained can be used to guide word classification in the construction of dictionary databases.
We describe the MFCDCN algorithm, an environment-independent extension of the efficient SDCN and FCDCN algorithms developed previously.
lb deal with long-distance dependencies, Applicative Universal Grammar (AUG) proposes a new type of categorial rules, called superposition rules.
We are developing an Intelligent Network News Reader which extracts news articles for users.
This paper proposes a method to extract rules for anaphora resolution of Japanese zero pronouns from aligned sentence pairs.
Then resolution rules for Japanese zero pronouns are automatically extracted using the pairs of Japanese zero pronouns and translation equivalents of their antecedents in English and equivalent word/phrase pairs which were extracted from the aligned sentence pairs, based on the syntactic and semantic structure of the Japanese sentence.
In this paper, we will introduce the anaphoric component of the Mimo formalism.
In Mimo, the translation of anaphoric relations is compositional.
We present two methods for learning the struc-ture of personal names from unlabeled data.
This paper introduces a novel Support Vector Machines (SVMs) based voting algorithm for reranking, which provides a way to solve the sequential models indirectly.
In this paper' I describe the use of Danish pronouns and deictics in dialogues.
This paper describes an approach to using se-mantic representations for learning information extraction (IE) rules by a type-oriented induc-tive logic programming (11,1)) system.
Knowledge-based interlingual machine translation systems produce semantically accurate translations, but typically require massive knowledge acquisition.
We describe our experiences building spo-ken language interfaces to four demon-stration applications all involving 2- or 3-D spatial displays or gestural interac-tions: an air combat command and control simulation, an immersive VR tactical sce-nario viewer, a map-based air strike sim-ulation tool with cartographic database, and a speech/gesture controller for mobile robots.
We present a linear-time, bidirectional subsump-tion test for typed feature structures and demonstrate that (a) subsumption- and equivalence-based packing is applicable to large HPSG grammars and (b) average parse complexity can be greatly reduced in bottom-up chart parsing with comprehensive HPSG implementations.
In this paper we present an algorithm to anchor floating quantifiers in Japanese, a language in which quantificational nouns and numeral- classifier combinations can appear separated from the noun phrase they quantify.
The algorithm differentiates degree and event modifiers from nouns that quantify noun phrases.
In this paper we present a novel feature- enriched approach that learns to detect the conversation focus of threaded discussions by combining NLP analysis and IR techniques.
Using the graph-based algorithm HITS, we integrate different features such as lexical similarity, poster trustworthiness, and speech act analysis of human conversations with feature- oriented link generation functions.
This paper describes the framework for a new abstraction method that utilizes event-units written in sentences.
This report outlines the principles of autodirective beamforming for acoustic arrays, and it describes two experimental implementations.
gz.
Relationships are formulated as discourse plans.
We present a method for automatically identifying verbal participation in diathesis alternations.
This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora.
While classical hybrid systems manually define local part-ofspeech patterns that lead to the identification of well-known multiword units (mainly compound nouns), our solution automatically identifies relevant syntactical patterns from the corpus.
We present a view of DATR as a language for defining certain kinds of partial functions by cases.
We show that this event identification task and a related task, identifying the semantic class of these events, can both be formulated as classification problems in a word-chunking paradigm.
Information extraction systems incorporate multiple stages of linguistic analysis.
We demonstrate this by using the results of coreference analysis and relation extraction to reduce the errors produced by a Chinese name tagger.
They are expressed in terms of special binary relations on trees called command relations.
Pre-processing includes garbage text removal and question segmentation.
We describe the grammar formalism used and report a parsing experiment which compared eight parsing strategies within the framework of chart parsing.
We take advantage of the intrinsic graphical structure of an ontology for representing a context.
This paper investigates the use of sentential pronouns in English and Norwegian.
In this paper I describe research devoted to developing techniques for building sublanguage lexicons via syntactic and statistical corpus analysis coupled with analytic techniques based on the tenets of a generative lexicon.
In this paper, we show that naive Bayes classification can be used to iden-tify non-native utterances of English.
We also char-acterize part-of-speech sequences that play a role in detecting non-native speech.
This paper introduces a new statistical approach to partitioning text automatically into coherent segments.
We present a method that takes as input a syntactic parse forest with associated constraint- based semantic construction rules and directly builds a packed semantic structure.
Inverse Document Frequency (IDF) is a popular measure of a word's importance.
In order to facilitate the use of a dictionary for language production and language learning we propose the construction of a new network-based electronic dictionary along the lines of Zock (2002).
This paper presents a novel method for unsupervised word sense disam-biguation, which combines multiple in-formation sources, including seman-tic relations, large unlabeled corpora, and cross-lingual distributional statis-tics.
A system for the automatic segmentation of German words into morphs was developed.
The main linguistic knowledge sources used by the system are a word syntax and a morph dictionary.
A means for converting the feature dependencies into a unification grammar is described wherein feature structures are projected on to unlabeled words.
an early corpus from the Sundial project in spoken language dialogue systems development.
At our institute a speech understanding and dialog system is developed.
This paper presents a human梞achine dialogue model in the field of task梠riented dialogues.
The paper proposes a multimodal interface for a real sales database application.
This paper presents the results of an experiment using machine-readable dictionaries (MRDs) and corpora for building concatenative units for text to speech (ITS) systems.
Theoretical questions concerning the nature of phonemic data in dictionaries are raised; phonemic dictionary data is viewed as a representative corpus over which to extract n- gram phonemic frequencies in the language.
A methodology is defined to compute phonemic n-grams for incorporation into a TTS system.
This paper explores the contribution of a broad range of syntactic features to WSD: grammatical relations coded as the presence of adjuncts/arguments in isolation or as subcategorization frames, and instantiated grammatical relations between words.
We present a novel approach for applying the Inside-Outside Algorithm to a packed parse forest produced by a unification- based parser.
In this paper we discuss a mechanism for modifying context in a tutorial dialogue.
In the paper, we outline several types of PMMs and detail a particular PMM in a sample dialogue situation.
groups of words).
In this paper we study different improvements to the standard phrase-based translation system.
(ACL Workshop on Parallel Texts 2005).
We describe the WHY2-ATLAS intelligent tutoring system for qualitative physics that interacts with students via natural language dialogue.
We focus on the issue of analyzing and responding to multi- sentential explanations.
Different from previous approaches, LEILA uses a deep syntactic analysis.
The system currently translates air travel planning queries from English to Swedish.
The methods include Linear Discriminant Analysis, Supervised Vector Quantization, Shared Mixture VQ, Deleted Estimation of Context Weights, NMI Estimation Using "N-Best" Alternatives, Cross- Word Triphone Models.
In this paper, we investigate cross language information retrieval (CUR) for Chinese and Japanese texts utilizing the Han characters ?common ideographs used in writing Chinese, Japanese and Korean (CJK) languages.
We discuss the importance of Han character semantics in document indexing and retrieval of the ideographic languages.
This paper presents a plan-based model of dialogue that combines world, linguistic, and contextual knowledge in order to recognize complex communicative actions such as expressing doubt.
We present a novel new word extraction method from Japanese texts based on expected word frequencies.
First, we compute expected word frequencies from Japanese texts using a robust stochastic N-best word segmenter.
One of the aims of automatic ex-- traction is to produce a thesaurus.
Developed extraction programs analyze the definition sentence in LDOCE with a pattern matching based algorithm.
shows suds a system for retrieving a Japanese dictionary.
We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
This paper describes a system for managing dialogue in a natural language interface.
The dialogue manager integrates information about segment types and moves into a hierarchical dialogue tree.
In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system, a language tutoring application for deaf learners of written English.
We describe a generation-oriented workbench for the Performance Grammar (PG) formalism, highlighting the treatment of certain word order and movement constraints in Dutch and German.
significant power to an NL system.
This paper presents an approach to pragmatic modeling in which metaplans are used to model that level of discourse structure for problem-solving discourse of the sort arising in NL interfaces to expert systems or databases.The discourse setting modeled by metaplans in this work is expert-assisted problem-solving.
In this paper, we outline the development of a system that automatically constructs ontologies by extracting knowledge from dictionary definition sentences using Robust Minimal Recursion Semantics (RMRS).
In this paper we present a family of kernel functions, named Syntagmatic Kernels, which can be used to model syntagmatic relations.
We evaluated the syntagmatic kernel on two standard Word Sense Disambiguation tasks (i.e.
In this paper we describe how infor-mation extraction technology has been used to build a summarisation system in the domain of occupational health and safety.
The core of the applica-tion is based on named entity recog-nition using pattern-action semantic grammar rules.
This paper describes applications of stochastic and symbolic NLP methods to treebank annotation.
Both the English and French parts of the corpus are analysed with a POS tagger and a robust parser.
We explore the use of restricted dialogue contexts in reinforcement learning (RL) of effective dialogue strategies for information seeking spoken dialogue systems (e.g.
We compare vectors containing counts of trigrams of part-of-speech (POS) tags in order to obtain an aggregate measure of syntax difference.
In all cases, the problems referred to a binary document classification.
The Segmentation Problem.
We show how it is possible to perform the analogical anal-ysis and generation of sentences, using a tree-bank and approximate pattern-matching.
In this paper, we introduce a generative probabilistic optical character recognition (OCR) model that describes an end-to-end process in the noisy channel framework, progressing from generation of true text through its transformation into the noisy output of an OCR system.
This paper proposes an Optimality Theory (Prince & Smolensky, 1993) [OT]-based generator of the Interlanguage [1111 syllabification of Korean speakers of English.
However, in order to treat some features of Korean accented English such as vowel epenthesis, segment modification (stop voicing, devoicing, nasalization, etc.
I will also use the ALIGN family of constraints to treat the Korean coda neutralization phenomena effectively.
We advance previous work by adopting a Maximum Entropy approach and by using Viterbi search to find the highest probability tag sequence for a given sentence.
Further we examine the use of syntactic pattern based re-ranking to further increase performance.
The corpus was created using heuristic extraction techniques in conjunction with an SVM-based classi-fier to select likely sentence-level para-phrases from a large corpus of topic-clustered news data.
Our system combines two existing robust components: the WU S-II natural language understanding system and the SPOKESMAN generation system.
Finally, we look at the role of paraphrasing in a cooperative dialog system.
We consider here the task of linear thematic segmentation of text documents, by using features based on word distributions in the text.
Central to this WSD framework is the sense division and semantic relations based on topical analysis of dictionary sense definitions.
The process begins with an initial disambiguation step using an MRDderived knowledge base.
We also evaluated the new light-stemming algorithm within the context of information retrieval, comparing its performance with other stemming algorithms.
This paper deals with the automatic translation of route descriptions into graphic sketches.
This paper describes an application of active learning methods to the classification of phone strings recognized using unsupervised phonotactic models.
Topic-driven Discourse Structure is formalized which identifies mainly non-human zero pronouns as a by-product.
This paper presents a study aimed at evaluating the feasibility of an NLP-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neo-nates.
In this paper, we prove the decidability of the generation problem for those unification grammars which are based on context- free phrase structure rule skeletons, like e.g.
LFG and PATR-II.
Using encyclopedia resources and text information resources on the Web, we focus on the method of constructing domain knowledge base through technologies in natural language text analysis and machine learning.
This paper describes the Sentence Planner (se) in the HealthDoc project, which is concerned with the production of customized patient- education material from a source encoded in terms of plans.
We present an engine for text adventures ?computer games with which the player interacts using natural language.
Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit.
We propose a new method of classifying documents into categories.
We define for each category a finite mixture model based on soft clustering of words.
We consider the problem of parsing non-recursive context-free grammars, i.e., context-free grammars that generate finite languages.
Personal MT (PMT) is a new concept in dialogue based MT (DBMT) , which we are currently studying and prototyping in the LIMA project Ideally, a PMT system should run on PCs and be usable by everybody.
The paper briefly presents some of them (HyperText, distributed architecture, guided language, hybrid transfer/interlingua, the goes on to study in more detail the structure of the dialogue with the writer and the place of speech synthesis [1].KeyworPersonal Machine Translation, dialogue-based Machine Translation, iVian-Machine Dialogue:, Ambiguity Resolution, Speech Synthesis.lt rod twtionA first classificatio of MAT (Machine Aided Translation) systems is by user.
This paper presents the WordFrame model, a noise- robust supervised algorithm capable of inducing morphological analyses for languages which exhibit prefixation, suffixation, and internal vowel shifts.
They can access Page-X via modem over a standard ASCII terminal.
Data-driven grammatical function tag assignment has been studied for English using the Penn-II Treebank data.
We use three machine-learning methods to assign Cast3LB function tags to sentences parsed with Bikel抯 parser trained on the Cast3LB treebank.
In atask-based evaluation we generate LFG functional-structures from the functiontag-enriched trees.
Most question answering (QA) systems rely on both keyword index and Named Entity (NE) tagging.
speech recognition results.
We present the distilling method and guidelines for distillation.
This paper reports on an ongoing project that combines NLP with semantic web technologies to support a content-based storage and retrieval of medical pathology reports.
In this paper we discuss issues related to speeding up parsing with wide-coverage unification grammars.
As an alternative, we describe an optimisation technique that combines ambiguity packing at the constituent structure level with pruning based on local features.
We describe how a surface generator can produce complex sentences when given these features in input.
We use state-of-the-art NLP techniques to perform the linguistic annotation using xML-based tools and a combination of rule- based and statistical methods.
We focus here on the predictive capacity of tense and aspect features for a classifier.
We propose an algorithm to resolve anaphors, tackling mainly the problem of intrasentential antecedents.
Data-Oriented Translation (DOT), based on Data- Oriented Parsing (DOP), is a language-independent MT engine which exploits parsed, aligned bitexts to produce very high quality translations.
This paper proposes a method for assigning gestures to text based on lexical and syntactic information.
Two well-known phenomena in the area of pronoun binding are considered: Indirect binding of pronouns by indefinite NPs ("donkey sentences") and surface- syntactic constraints on binding ("weak cross-over").
We apply this method to English partof-speech tagging and Japanese morphological analysis, and show that the method performs well.
This paper describes a method for lan-guage independent extractive summariza-tion that relies on iterative graph-based ranking algorithms.
Moreover, we show how a meta-summarizer relying on a layered appli-cation of techniques for single-document summarization can be turned into an ef-fective method for multi-document sum-marization.
This paper compares two systems for computational morphological analysis of Dutch.
This paper describes a Chinese word segmentation system based on unigram language model for resolving segmen-tation ambiguities.
and 63 lexical entry templates (assigned to parts of speech (POSs) ).
A finite-state method, based on leftmost longest-match replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes.
The system consists of three specialized pattern-based tagging modules, a high-precision co-reference resolution module, and a configurable template generation module.
In this paper we propose and investigate Ontology Population from Textual Mentions (OPTM), a sub-task of Ontology Population from text where we assume that mentions for several kinds of entities (e.g.
This paper shows how a hypernym-hyponym based lexi-con for Swedish can be created directly from a news paper corpus.
An algorithm is presented for building partial hierarchi-cal structures from non domain-specific texts.
A new, flexible inference method for Horn logic program is proposed.
Chart- like parsing and semantic-head-driven generation emerge from this method.
This paper present a new method for parsing English sentences.
The parser called LUTE-EJ parser is combined with case analysis and ATNG-based analysis.
LUTE-EJ parser has two interesting mechanical characteristics.
This parser's features are (1)extracting a case filler, basically as a noun phrase, by ATNGbased analysis, including recursive case analysis, and (2)mixing syntactic and semantic analysis by using case frames in case analysis.
This paper presents results from experiments in automatic classification of animacy for Norwegian nouns using decision-tree classifiers.
The method makes use of relative frequency measures for linguistically motivated morphosyntactic features extracted from an automatically annotated corpus of Norwegian.
This paper presents a linguistic model for language understanding and describes its application to an experimental machine translation system called LUTE.
The language understanding model is an interactive model between the memory structure and a text.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
Simulations using artificial language stimuli demonstrate the capacity of the model to learn suffixation, prefixation, infixation, circumfixation, mutation, template, and deletion rules.
This paper describes recent MADCOW activities.
In this paper we compare two competing approaches to part-of-speech tagging, statistical and constraint-based disambiguation, using French as our test language.
We demonstrate the open-source LKB system which has been used to teach the fundamentals of constraint-based grammar development to several groups of students.
We present a semantic and pragmatic account of the anaphoric properties of past and perfect that improves on previous work by integrating discourse structure, aspectual type, surface structure and commonsense knowledge.
A novel aspect of our account is that we distinguish between two kinds of temporal intervals in the interpretation of temporal operators — discourse reference intervals and event intervals.
This distinction makes it possible to develop an analogy between centering and temporal centering, which operates on discourse reference intervals.
Our temporal property- sharing principle is a defeasible inference rule on the logical form.
This paper addresses the question whether metaphors can be represented in Word- Nets.
We present the lexical-semantic net for German "GermaNet" which integrates conceptual ontological information with lexical semantics, within and across word classes.
GermaNet includes a new treatment of regular polysemy, artificial concepts and of particle verbs.
An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented.
The model is a piecewise nonlinear transformation applied to the noisy speech feature.
The transformation is a set of multidimensional linear least-squares filters whose outputs are combined using a conditional Gaussian model.
This paper represents a status report on the MIT ATIS system.
This paper presents a lexicon-based approach to syntactic analysis, Lexicase, and applies it to a lexicon-driven computational parsing system.
The basic descriptive mechanism in a Lexicase grammar is lexical features.
Syntactic tree structures are representaed as networks of pairwise dependency relationships among the words in a sentence.
Section 2 describes the way in which grammatical information can be presented as s set of generalizations about classes of lexical items represented in a dependency-type tree format.
The rules of lexica se grammar proper are lexical rules, rules that express relations among lexical items and among features within lexical entries.
)Figure 1 lists the rule types in a lexicase grammar and their interrelationships.
We present a method for induction of concise and accurate probabilistic context- free grammars for efficient use in early stages of a multi-stage parsing technique.
The results provide direct evidence demonstrating that both grapheme frequency and grapheme entropy influence performance on pseudoword naming.
We discuss the implications of those findings for current models of phonological coding in visual word recognition.
This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the con-struction of large-scale knowledge sources.
We describe two experiments: one which ignored word-sense distinctions, re-sulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy.
Unification-based theories of grammar al-low to integrate different levels of linguis-tic descriptions in the common framework of typed feature structures.
We present an approach to a modular use of codescrip-tions on the syntactic and semantic level.
In the paper we describe the partitioning of grammatical information for the parsers and present results about the performance.
This paper proposes architecture of multilingual news summarizer, including monolingual and multilingual clustering, similarity measure among meaningful units, and presentation of summarization results.
We explore the relationship between question answering and constraint relaxation in spoken dialog systems.
In particular, we describe methods for dealing with the results of database queries in information- seeking dialogs.
We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.
We propose a support vector learning-based method employing target language corpus and bilingual dictionary data, and evaluate it over a English Japanese machine translation task.
We use IE patterns learned from the MUC-4 training set as anchors to identify domain-specific web pages and then learn new IE patterns from them.
A specialized transition networkmechanism, the interruptable transition network (ITN) is used to perform the last of three stages in a multiprocessor syntactic parser.
The most common statistic is n-grams measuring word cooccurrences in texts.
Collocation map is a sigmoid belief network that can be constructed from bigrams.
We compared the conditional probabilities and mutual information computed from bigrams and Collocation map.
Hearst [1991] suggests using syntactic information and partof-speech tagging to aid in the disambiguation.
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.
We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extra- sentential context.
More and more researchers have recognized the potential value of the parallel corpus in the research on Machine Translation and Machine Aided Translation.
An iterative algorithm based on degree of word association is proposed to identify the multiword units for Chinese and English.
This paper presents a computational model of how conversational participants collaborate in making referring expressions.
We use the primitive actions s-refer and s-attr.
In this part, we use linguistic and statistical methods to produce keywords from a stream of data.
We use the same extraction module based on linguistic and add a knowledge based system to deduce implicit keywords.
OF COLING-92.
In this paper, we present a parallel context sensitive graph rewriting formalism for a dependency-oriented generation grammar.
information extraction, text sum-marisation, document generation, machine translation, and second language learning).
We put forward a statistical language model that resolves these problems, does POS tagging, and can be used as the language model of a speech recognizer.
This paper proposes a new dialogue control method for spoken dialogue systems.
For speaker-independent speech recognition, speaker variation is one of the major error sources.
In this paper, a speaker-independent normalization network is constructed such that speaker variation effects can be minimized.
A codeword-dependent neural network is associated with each speaker cluster.
We will report on one of the two tasks in the IREX (Information Retrieval and Extraction Exercise) project, an evaluation-based project for Information Retrieval and Information Ex-traction in Japanese (Sekine and Isahara, 2000) (IREX Committee, 1999).
In this paper, the Named Entity (NE) task is reported.
The present study deals with conflict resolution process in metaphorical interpretation for the noun phrase.
By using production system couped with contex free parser (ELINGOL), theworking system called META桽IM is constructed to analyze the noun phrase metaphor.
We present an algorithm which creates a German CCGbank by translating the syntax graphs in the German Tiger corpus into CCG derivation trees.
, This paper proposes a new disambiguation method for Japanese text input.
This method evaluates candidate sentences by measuring the number of Word Co-occurrence Patterns (WCP) included in the candidate sentences.
An automatic WCP extraction method is also developed.
A co- occurrence pattern matrix with semantic categories is built based on these WCP.
Using this matrix, the mean number of candidate sentences in Kana-to-Kanji translation is reduced to about 1/10 of those from existing morphological methods.1桰ntroductionFor keyboard input of Japanese, Kana-to-kanji translation method [Kawada,79] [Makino80] [A be861 is the most popular technique.
For this purpose, Dependency Localization Analysis (DLA) is used.
This identifies the word pairs having a definite dependency relationship using syntactic analysis and some heuristic rules.This paper will first describe collocational analysis, a new concept in Ka,na-to-Kanji translation, then the compilation of WCP dictionary, next the translation algorithm and finally translation experimental results.2.
are examples of WCP.
1 Concept of collocational analysis770
We report a comparative study of two methods for estimating word co- occurrence frequencies required by word similarity measures.
A lexicon-grammar is constituted of the elementary sentences ofa language.
Instead of considering words as basic syntactic unitsto which grammatical information is attached, we use simplesentences (subject-verb-objects) as dictionary entries.
N raises a question,- the lexicon-grammar of support verbs.
We present the structure of the lexicon-grammar built for French and we discuss its algorithmic implications for parsing.275
We end by proposing extensions to lexical correction and to some syntactic errors.
We present a new approach to topological parsing of German which is corpus-based and built on a simple model of probabilistic CFG parsing.
Besides the practical aspect of developing a robust and accurate topological parser for hybrid shallow and deep NLP, we investigate to what extent topological structures can be handled by context-free probabilistic models.
We discuss experiments with systematic variants of a topological treebank grammar, which yield competitive results.1
This paper outlines our strategy for dealing with spontaneous spoken input in a speech recognition system.
ARGUMENTATION AND THE SEMANTIC PROGRAM.
In this paper, we propose to model access to heterogeneous databases, by interpreting natural language queries into queries in formal languages such as SQL, OQL, and CPL by accounting for various language-specific constructions including join relations, path expressions, and object bindings with domain resources and a common lexicon, in a combinatory categorial grammar framework. '
Research in discourse processing has identified two representational requirements for discourse planning systems.
This paper describes a novel framework for interactive question-answering (Q/A) based on predictive questioning.
In this paper we discuss the use of multi- layered tagsets for dialogue acts, in the context of dialogue understanding for multiparty meeting recording and retrieval applications.
We then define MALTUS, a new tagset based on the ICSI-MR and Switchboard tagsets, which satisfies these requirements.
We present some experiments using MALTUS which attempt to compare the merits of integrated versus multi-level classifiers for the detection of dialogue acts.
In this paper, we introduce a new data representation format for language processing, the syntactic and semantic graphs (SSGs), and show its use for call classification in spoken dialog systems.
For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels).
At the core of our new structured language model is a fast context-sensitive and lexicalized PLCG parsing algorithm that uses dynamic programming.
In contrast to knowledge- rich sentence aggregation approaches explored in the past, this approach exploits statistical parsing and robust coreference detection.
We introduce the bilingual dual-coding theory as a model for bilingual mental representation.
Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation.
The Lambek categorial grammar is one representative of the grammar family under consideration.
Word sense disambiguation (WSD) is a difficult problem in natural language processing.
In this paper, a sememe co-occurrence frequency based WSD method was introduced.
In this method, Hownet was used as our information source , and a co-occurrence frequency database of sememes was constructed and then used for WSD.
The experimental result showed that this method is successful.Keywordsword sense disambiguation, Hownet, sememe, co-occurrence
SenseClusters is a freely available system that identifies similar contexts in text.
In this paper, we compare and contrast IDF with inverse sentence frequency (ISF) and inverse term frequency (ITF).
We propose a mapping between prosodic phenomena and semantico-pragmatic effects based upon the hypothesis that intonation conveys information about the intentional as well as the attentional structure of discourse.
In particular, we are investigating the problem of intonational assignment in synthetic speech.
This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken rlialogue agents.
Grammar Association is a technique for Machine Translation and Language Understanding introduced in 1993 by Vidal, Pieraccini and Levin.
This paper deals with the automatic translation of prepositions, which are highly polysemous.
Following cognitive principles of spatial conceptualization, we design, a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from a semantic sort hierarchy.
IntroductionThis paper deals with a general phenomenon of (machine) translation.
We design a semantic interpretation process for spatial relations in which our translation system uses semantic features derived from the semantic sort hierarchy.
This paper presents a multi-neuro tagger that uses variable lengths of contexts and weighted inputs (with information gains) for part of speech tagging.
We present a novel hybrid approach for Word Sense Disambiguation (WSD) which makes use of a relational formalism to represent instances and background knowledge.
It is built using Inductive Logic Programming techniques to combine evidence coming from both sources during the learning process, producing a rule -based WSD model.
We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
In translation, we apply source sentence reordering on word level and use a reordering automaton as input.
We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints.
We further add weights to the reordering automata.
We propose an algorithm which ranks MWEs by their compositionality relative to a semantic field taxonomy based on the Lancaster English semantic lexicon (Piao et al., 2005a).
The semantic information provided by the lexicon is used for measuring the semantic distance between a MWE and its constituent words.
We compared the output of our tool with human judgments using Spearman抯 rank-order correlation coefficient.
This experiment demonstrates that a semantic lexicon can assist in MWE compositionality measurement in addition to statistical algorithms.
We propose an ontology-based framework for linguistic annotation of written texts.
In this paper we present hidden Markov models for Korean part-of-speech tagging, which consider Ko-rean characteristics such as high agglutinativity, word-spacing, and high lexical correlativity.
We describe a hybrid SNN/HMM system that combines the speed and performance of our HMM system with the segmental modeling capabilities of SNNs.
The system is built around two separate neural network methodologies: context vectors and self organizing maps.
We present first results using paraphrase as well as textual entailment data to test the language universal constraint posited by Wu抯 (1995, 1997) Inversion Transduction Grammar (ITG) hypothesis.
In machine translation, parsing of long English sentences still causes some problems, whereas for short sentences a good machine translation system usually can generate readable translations.
We describe work in progress on a corpus-based tutoring system for education in traditional and formal grammar.
This paper concerns the discourse understanding process in spoken dialogue systems.
The present paper proposes a method by which to translate outputs of a robust HPSG parser into semantic representations of Typed Dynamic Logic (TDL), a dynamic plural semantics defined in typed lambda calculus.
We also discuss the impact of using publicly available manually annotated verb data to improve the SRL accuracy of nouns, exploiting a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure.
Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed insignificant improvement.
In this paper, we address the issue of syntagmatic expressions from a computational lexical semantic perspective.
In particular, we focus on the place of lexical and semantic restricted co-occurrences.
From a processing viewpoint, we show how to generate/analyze syntagmatic expressions by using an efficient constraint- based processor, well fitted for a knowledge-driven approach.
This paper presents a constraint logic programming language cu-Prolog and introduces a simple Japanese parser based on Japanese Phrase Structure Grammar (JPSG) as a suitable application of cu-Prolog.cu-Prolog adopts constraint unification instead of the normal Prolog unification.
Such a clause is called Constraint Added Horn Clause (CAHC).
It enables a natural implementation of JPSG and other unification based grammar formalisms.From this April, Fujitsu Corporation 1 IntroductionProlog is frequently used in implementing natural language parsers or generators based on unification based grammars.
The models we use are structures built from intervals of time, events and individuals.
We present a classification-based word prediction model based on IGTREE, a decision-tree induction algorithm with favorable scaling abilities and a functional equivalence to n-gram models with back- off smoothing.
This paper addresses the problem of acquiring lexical semantic relationships, applied to the lexical entailment relation.
Our main contribution is a novel conceptual integration between the two distinct acquisition paradigms for lexical relations ?the pattern- based and the distributional similarity approaches.
Supervised machine learning was applied to monitor the performance of the rule-based method, using Memory Based Learning (MBL).
We have coupled the parser LCFLEX with a lexicon and ontology derived from two lexical resources, Verb- Net for verbs and CoreLex for nouns.
This suggests that current statistical parsing methods are sufficiently general to produce accurate shallow semantic annotation.
We describe our latest attempt at adaptive language modeling.
The other components are a selective unigram cache, a conditional bigram cache, and a conventional static trigram.
We introduce a novel search algorithm for statistical machine translation based on dynamic programming (DP).
During the search process two statistical knowledge sources are combined: a translation model and a bigram language model.
We present experimental results on the Verbmobil task.
Accurately representing synonymy using distributional similarity requires large volumes of data to reliably represent infrequent words.
This paper discusses relationships among word pronunciations.
The analogical processes involve techniques for segmenting and matching word spellings, and for mapping spelling to sound in known words.
Lexifanis is the first working tool for Modern Greek Language.
The goal of this work is recognizing opinionated and evaluative (subjective) language in text.
This paper focuses on disambiguating potentially subjective expressions in context, based on the density of other clues in the surrounding text.
We present a machine learning based method for identifying patients diagnosed with congestive heart failure and other related conditions by automatically classifying clinical notes.
The aim of this paper is to investigate how much the effectiveness of a Question Answering (QA) system was affected by the performance of Machine Translation (MT) based question translation.
We investigate the effects of lexicon size and stopwords on Chinese information retrieval using our method of short-word segmentation based on simple language usage rules and statistics.
We present a constraint-based case frame lexicon architecture for bi-directional mapping between a syntactic case frame and a semantic frame.
In this paper, we evaluate the results of the Antwerp University word sense dis-ambiguation system in the English all words task of SENSEVAL-2.
Association for Computational Linguistics.
We compare and contrast two different models for detecting sentence-like units in continuous speech.
The first approach uses hidden Markov sequence models based on N-grams and maximum likelihood estimation, and employs model interpolation to combine different representations of the data.
The second approach models the posterior probabilities of the target classes; it is discriminative and integrates multiple knowledge sources in the maximum entropy (maxent) framework.
Both models combine lexical, syntactic, and prosodic information.
In this paper we compare two interlin-gua representations for speech transla-tion.
In this paper, we present an unsupervised methodology for propagating lexical co- occurrence vectors into an ontology such as WordNet.
We use to specify schemata annotated rules and the LFG uniqueness, completeness and coherence principles.
This paper describes the analysis component of the language processing system PLAIN from the viewpoint of unification grammars.
The principles of Dependency Unification Grammar (DUG) are discussed.
A unification-based parsing procedure is part of the formalism.
Correspondingly, three types of attributes are grouped together in each DRL-term: a lexeme, a syntagmatic role and a complex morpho-syntactic category.
In this paper, we describe our efforts to create a robust, large-scale lexical-semantic resource for the recognition and classification of expressions of commonsense psychology in English Text.
The algorithm which is used for deciding satisliability of a feature description is based on a restricted deductive closure construction for sets of literals (atomic formulas and negated atomic formulas).
The deductive closure construction is the direct proof-theoretic correlate of the congruence closure algorithm (cf.
The verb-noun sequence in Chinese often creates ambiguities in parsing.
Finally, we show how to translate Chinese norninals within a knowledge-based framework.
SYSTRAN抯 Chinese word segmentation is one important component of its Chinese-English machine translation system.
The Chinese word segmentation module uses a rule-based approach, based on a large dictionary and fine-grained linguistic rules.
Considering some differences between PPAs and other kinds of anaphors, such as personal or demonstrative pronouns, we define three knowledge sources (KSs) for PPA resolution: surface patterns (taking in account factors such as syntactic parallelism), possessive relationship rules and sentence centering.
We describe two "semantically-oriented" dependency-structure formalisms, 11-forms and S-forms.
Stochastic categorial grammars (SCGs) are introduced as a more appropriate formalism for statistical language learners to estimate than stochastic context free grammars.
This paper presents recent natural language work on HARC, the BBN Spoken Language System.
The HARC system incorporates the Byblos system [6] as its speech recognition component and the natural language system Delphi, which consists of a bottom-up parser paired with an integrated syntax/semantics unification grammar, a discourse module, and a database question-answering backend.
This paper presents a general computational method for automated inversion of a unification-based parser for natural language into an efficient generator.
In this paper the functional uncertainty machinery in LFG is compared with the treatment of long distance dependencies in TAG.
Lexico-semantic pattern matching, with rules that combine lexical analysis with ordering and semantic categories, is a good method for this form of analysis.
The explored approaches include using Model-1 conditional probability, a boosting strategy for lexicon probabilities based on importance sampling, applying Parts of Speech to discriminate English words and incorporating information of English base noun phrase.
This paper proposes the 揌ierarchical Directed Acyclic Graph (HDAG) Kernel?for structured natural language data.
We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a kernel function.
This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring.
This paper investigates interactions between collocational properties and methods for organizing them into features for machine learning.
This paper describes a highly-portable multilingual question answering system on multiple relational databases.
We apply semantic category and pattern-based grammars, into natural language interfaces to relational databases.
Lexico-semantic pattern (LSP) and multi-level grammars achieve portability of languages, domains, and DBMSs.
MATES/EK is a transfer-based system and it has several subsystems that can be used to support other MT-developments.
(Kim, 1992)?Augmented Context Free Grammars for English Syntactic Analysis : We developed a set of augmented context free grammar rules for general English syntactic analysis and the analyzer is implemented using Tomita Lit parsing algorithm (Tomita, 1987).
Applications that can benefit from such an annotated corpus include information extraction (e.g., normalizing temporal references for database entry), question answering (answering 搘hen?questions), summarization (temporally ordering information), machine translation (translating and normalizing temporal references), and information visualization (viewing event chronologies).KeywordsAnnotation, temporal information, semantics, ISO-8601.
XMLencodingNon-XML EncodingFigure 3.
Using standard methods and formats established at LADL, and adopted by several European research teams to construct large- coverage electronic dictionaries and grammars, we elaborated for Portuguese a set of lexical resources, that were implemented in INTEX We descnbe the main features of such linguistic data, refer to their maintenance and extension, and give different examples of automatic text parsing based on those dictionaries and grammarsKeywords Text parsing, large-coverage dictionaries, computational lexicons; word tagging, information retneval.
This paper describes how a language-planning system can produce natural-language referring expressions that satisfy multiple goals.
Most statistical machine translation systems employ a word-based alignment model.
Linear algebraic technique called LSA/SVD is used to find co-relationships of sparse words.
Three variant estimation methods are sug-gested and they are evaluated for estimating unseen noun-verb co-occurrence probability.
Tied-mixture (or semi-continuous) distributions are an important tool for acoustic modeling, used in many high- performance speech recognition systems today.
Additionally, we describe an extension of tied mixtures to segment-level distributions.
The designed Chinese morphological analyzer contains three major functions, 1) to segment a word into a sequence of morphemes, 2) to tag the part-of-speech of those morphemes, and 3) to identify the morpho-syntactic relation between morphemes.
We propose a method of using associative	strength	among	morphemes,morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech.
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.
In this paper we define two intermediate models of textual entailment, which correspond to lexical and lexical-syntactic levels of representation.
We present three ways in which a natural language generator that produces textual descriptions of objects from symbolic information can exploit OWL ontologies, using M-PIRO抯 multilingual generation system as a concrete example.
We propose a novel method to predict the inter- paragraph discourse structure of text, i.e.
Our method combines a clustering algorithm with a model of segment 搑elatedness?acquired in a machine learning step.
The model integrates information from a variety of sources, such as word co-occurrence, lexical chains, cue phrases, punctuation, and tense.
Our method outperforms an approach that relies on word co-occurrence alone.
Improvements included use of a Hidden Markov Model (HMM) statistical classifier to identify the likely linguistic provenance of a surname, and application of language-specific rules to generate plausible spelling variations of names.
We focus here on the results obtained by starting with morphological analysis and proceeding to a grammatical (part-of-speech) tagging.In the proposed system, the vocalic ambiguity is detected by means of a double dictionary of voweled and non-voweled forms.
Currently, information architects create meta- data category hierarchies manually.
Among them are parallel multiple context-free grammars (pmcfg's) and lexical-functional grammars (lfg's).
Finite state translation systems (fts') were introduced as a computational model of transformational grammars.
In this paper, three subclasses of lfg's called nc-lfg's, dc-lfg's and fc-lfg's are introduced and the generative capacities of the above mentioned grammatical formalisms are investigated.
This algorithm relies on a combination of smoothing and linear segmentation together with the notion of word start groups.
For example, using the contextual words, instead of contextual parts of speech, enhances the prediction power for tagging parts of speech.
The generation of words in speech involves a number of processing stages.
In the computational model, lexical concepts figure in a semantic, spreading-activation network.The lexical concept is input to a process called lexical selection.
Lemmas are syntactic words.
In this paper, we will discuss one aspect of this type of work: designing the so-called Vietools to detect and correct spelling of Vietnamese texts by using a spelling database based on TELEX code.
We describe a new method for the representation of NLP structures within reranking approaches.
We make use of a conditional log杔inear model, with hidden variables representing the assignment of lexical items to word clusters or word senses.
Next, a method for natural language transfer is presented that integrates translation examples (represented as typed feature structures with source-target indices) with linguistic rules and constraints.
We present a novel disambiguation method for unification-based grammars (UBGs).
In this article, we present a statistical approach to machine translation that is based on Data-Oriented Parsing: Data-Oriented Translation (DOT).
Here we propose a method allowing a purely semantic-based analysis of sequences of semantic units.
This paper describes an algorithm for the resolution of discourse deictic anaphors, which constitute a large percentage of anaphors in spoken dialogues.
In this paper, we address the word alignment problem for statistical machine translation.
This paper describes a morphological analyzer which, when parsing a word, uses two sets of rules: rules describing the syntax of words, and rules describing facts about orthography.
This paper describes a general approach to the design of natural language interfaces that has evolved during the development of DATALOG, an English database query system based on Cascaded ATN grammar.
In this paper we present the results of the combination of stochastic and rule-based disambiguation methods applied to Basque languagel .
The methods we have used in disambiguation are Constraint Grammar formalism and an H1VIM based tagger developed within the MULTEXT project.
We show that the phrase-based translation engine we implemented benefits from Tree-Phrases.
We extended language modeling approaches in information retrieval (IR) to combine collaborative filtering (CF) and content-based filtering (CBF).
We first introduce a multinomial model that combines CF and CBF in a language modeling framework.
We then generalize the model to another multinomial model that approximates the Polya distribution.
We present a constraint-based morphological disambiguation system in which individual constraints vote on matching morphological parses, and disambiguation of all the tokens in a sentence is performed at the end by selecting parses that receive the highest votes.
We address these claims empirically in an important application domain for machine learning — text categorization.
The past decade has witnessed exciting work in the field of Statistical Machine Translation (SMT).
We propose Hidden Markov models with unsupervised training for extractive summarization.
Extractive summarization selects salient sentences from documents to be included in a summary.
Our method incorporates unsupervised training with clustering, into a probabilistic framework.
Text cohesion is modeled by the transition probabilities of an HMM, and term distribution is modeled by the emission probabilities.
Parameter training is carried out by the segmental K-means (SKM) algorithm.
We ex-tract a set of lexical, syntactic and onto-logical features and the corresponding noun-classifier pairs from a corpus and then train SVMs to assign classifers to nouns.
The cluster n-gram model is a variant of the n-gram model in which similar words are classified in the same cluster.
The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion.
The approach is based on the "distributional hypothesis".
We investigate the connection between part of speech (POS) distribution and content in language.
We define POS blocks to be groups of parts of speech.
For many years, statistical machine translation relied on generative models to provide bilingual word alignments.
The purpose of this paper is to automatically create multilingual translation lexicons with regional variations.
We focus on the production of efficient descriptions of objects, actions and events.
We present in this paper a series of induced methods to assign domain tags to WordNet entries.
Next we further examine the similarity between common lexical taxonomy and the semantic hierarchy of WordNet.
We propose this as the first step of wordnet expansion into a bona fide semantic network linked to real-world knowledge.0.
Introduction1WordNet is a lexicon comprising of nouns, verbs, adjectives and adverbs.
We describe noun phrases composed of a proper noun and/or a description of a human occupation.
We take into account synonymy and hyperonymy.
We are pursuing lexical acquisition through the syntactic relationships of words in medical corpora.
We have designed a dependency grammar parser that learns through a transformational-based algorithm.
Further work will evaluate the usefulness of this parse for lexical acquisition.
This paper proposes a new method for word translation disambiguation using a machine learning technique called 態ilingual Bootstrapping?
An efficient context-free parsing algorithm is presented that can parse sentences with unknown parts of unknown length.
This paper presents a method for augmenting taxonomies with domain information using a simple combination of three existing lexical similarity metrics.
We propose three schemes, that is, utterance unit, discourse segment and discourse markers.
The extracted relations are used for query expansion in IR.
A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
In this paper, we evaluate an approach to automatically acquire sense- tagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.
Our investigation reveals that this method of acquiring sense-tagged data is promising.
We consider here the problem of Base Noun Phrase translation.
In one method, we employ an ensemble of Na飗e Bayesian Classifiers constructed with the EM Algorithm.
In the other method, we use TF-IDF vectors also constructed with the EM Algorithm.
We study the computational complexity of the parsing problem of a variant of Lambek Categorial Grammar that we call semidirectional.
We show that the parsing problem for semidirectional Lambek Grammar is NP-complete by a reduction of the 3- Partition problem.Key words: computational complexity, Lambek Categorial Grammar
We describe a classifier which determines the rhetorical status of sentences in texts from a corpus of judgments of the UK House of Lords.
The basic compo-nents include basic segmentation, factoid recognition, and named entity recognition.
The postprocessors include merging of ad-joining words, morphologically derived word recognition, and new word identi-fication.
This paper presents an unsupervised topic identification method integrating linguistic and visual information based on Hidden Markov Models (HMMs).
For an input sentence, syntactic constituent structure parses are generated by a Charniak parser and a Collins parser.
Semantic role labels are assigned to the constituents of each parse using Support Vector Machine classifiers.
It demonstrates the need, under particular assumptions, for more access to full text articles and for the use of Part-of-Speech tagging.
An investigation into the use of Bayesian learning of the parameters of a multivariate Gaussian mixture density has been carried out.
In a continuous density hidden Markov model (CDHMM) framework, Bayesian learning serves as a unified approach for parameter smoothing, speaker adaptation, speaker clustering, and corrective training.
Our approach is to use Bayesian learning to incorporate prior knowledge into the CDHMM training process in the form of prior densities of the HMM parameters.
This paper describes the results of some experiments using a new approach to information access that combines techniques from natural language process-ing and knowledge representation with a penalty-based technique for relevance estimation and passage retrieval.
We introduce a new pointwise-prediction single-classifier method that predicts trigrams of class labels on the basis of windowed input sequences, and uses a simple voting mechanism to decide on the labels in the final output sequence.
We apply the method to maximum-entropy, sparse- winnow, and memory-based classifiers using three different sentence-level chunking tasks, and show that the method is able to boost generalization performance in most experiments, attaining error reductions of up to 51 %.
We compare and combine the method with two known alternative methods to combat near-sightedness, viz.
a feedback-loop method and a stacking method, using the memory-based classifier.
We report results for this small test corpus on a variety of experiments involving automatic speech recognition and named entity tagging.
The lexicons for Knowledge-Based Machine Translation systems require knowledge intensive morphological, syntactic and semantic information.
This system is currently being used in the ESTRATO machine translation project at the Center for Machine Translation.
We propose that while better document matching leads to better parallel sentence extraction, better sentence matching also leads to better document matching.
Using this evaluation method, we compare two semantic extraction techniques which produce similar word lists, one using syntactic context of words , and the other using windows of heuristically tagged words.
Part of speech taggers based on Hidden Markov Models rely on a series of hypotheses which make certain errors inevitable.
Their lexical and syntactic counts are computed on the basis of the lexical and syntactic counts of their constituents, using impurity functions.
The first approach uses a Finite State Gra.mmar (i SC) to pro-duce noun and verb groups while the second uses a Supertagging model to produce de-pendency linkages.
We discuss the impact of these two input representations on the shn-plification process.
We describe the metaphoneme inventory defined for Dutch, English and German, comparing the results for vowels and consonants.
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.
To support summarization of automatically transcribed meetings, we introduce a classifier to recognize agreement or disagreement utterances, utilizing both word-based and prosodic cues.
We present a general model for PP attachment resolution and NP analysis in French.
Each component draws on one or more sets of data, including a lexicon, a broad-coverage grammar of English, semantic verb decompositions, rules mapping between syntactic and semantic constituents, and a domain model.This paper discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit.
)In the Information Retrieval community.
1991) and (Church and Hanks 1990) have applied robust grammars and statistical techniques over large corpora to extract interesting noun phrases and subject-verb, verb-object pairs.
(Hearst 1992) has shown that certain lexical-syntactic templates can reliably extract hyponym relations froM text.
Components of LEI include a language analyzer, a geograph-ical reasoner, an object-oriented geographic knowledge base derived from US Geological Survey digital maps with user input, and a graphical user interface.
We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial contributions.
We present an experiment for designing a logic based QA system, WEBCOOP, that integrates knowledge representation and advanced reasoning procedures to generate cooperative responses to natural language queries on the web.
We present a probabilistic bilingual capitalization model for capitalizing machine translation outputs using conditional random fields.
We extend existing methods for automatic sentence boundary detection by leveraging multiple recognizer hypotheses in order to provide robustness to speech recognition errors.
We present in this paper a method for achieving in an integrated way two tasks of topic analysis: segmentation and link detection.
This method combines word repetition and the lexical cohesion stated by a collocation network to compensate for the respective weaknesses of the two approaches.
This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change.
We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment.
This paper proposes an approach to full parsing suitable for Information Extraction from texts.
It was implemented in the IE module of FACILE, a EU project for multilingual text classification and IE.
Word dependency is important in parsing technology.
This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver.
We expect this method is also applicable to phrase structure parsers.
We describe a history-based generative parsing model which uses a k-nearest neighbour (k-NN) technique to estimate the model抯 parameters.
An algorithm is described that computes finite-state approximations for context-free grammars and equivalent augmented phrase-structure grammar formalisms.
We refer to such interfaces as Speech-In List-Out or SILO interfaces.
We propose a metric based on stochastic iterative string alignment (SIA), which aims to combine the strengths of both approaches.
This paper describes an efficient parallel system for processing Typed Feature Structures (TFSs) on shared-memory parallel machines.
We call the system Parallel Substrate for TFS (PSTFS).
The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers.
In this paper we describe the analytic question answering system HITIQA (High- Quality Interactive Question Answering) which has been developed over the last 2 years as an advanced research tool for information analysts.
We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG).
We extend XDG to lexicalize dependency subgraphs, and show how to compile them into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver.
This paper describes a novel progressive training algorithm that selects features from virtually unlimited feature spaces for conditional maximum entropy (CME) modeling.
This paper discusses the extension of ViewGen, a program for belief ascription, to the area of intensional object identification with applications to battle environments, and its combination in a overall system with MGR, a Model-Generative Reasoning system, and PREMO a semantics-based parser for robust parsing of noisy message data.ViewGen represents the beliefs of agents as explicit, partitioned proposition-sets known as environments.
We attempt to evaluate two different approaches to anaphoric processing in discourse by comparing the accuracy and coverage of two published algorithms for finding the co-specifiers of pronouns in naturally occurring texts and dialogues.
In this paper, we propose a new context- dependent SMT model that is tightly coupled with a language model.
We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.
These models enable word- alignment process to leverage topical contents of document-pairs.
This paper discusses the consequences of allowing discontinuous constituents in syntactic representions and phrase-structure rules, and the resulting complications for a standard parser of phrase-structure grammar.It is argued, first, that discontinuous constituents seem inevitable in a phrase-structure grammar which is acceptable from a semantic point of view.
The notions .of linear precedence and adjacency are reexamined, and the concept of "n-place adjacency sequence" is introduced.Finally, the resulting form ofphrase-structure grammar, called "Discontinuous Phrase-Structure Grammar", is shown to be parsable by an algorithm for context-free parsing with relatively minor adaptations.
Natural language systems based on Categorial Unification Grammar (CUG) have mainly employed bottom- up parsing algorithms for processing.
We show how these summaries can be factored into a set of document features, and how an implemented content planner uses the topicality document feature to create indicative multidocument query-based summaries.
Task based evaluation shows a 26% F-measure improvement in named entity recognition when using truecasing.
We describe a method for annotating spoken dialog corpora using both automatic and manual annotation.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
It is based on word-document co- occurrence statistics in the training corpus and a dimensionality reduction technique.
We present here an approach called Syntactically Enhanced LSA (SELSA) which generalizes LSA by considering a word along with its syntactic neighborhood given by the part-of-speech tag of its preceding word, as a unit of knowledge representation.
This paper discusses the application of Unification Categorial Grammar (UCG) to the framework of Isomorphic Grammars for Machine Translation pioneered by Landsbergen.
In experiments on a natural language information retrieval system that retrieves images based on textual captions, we show that syntactic complexity actually aids retrieval.
Like previous work in this area, our approach exploits orthographic regularities in a search for possible morphological segmentation points.
The semantic web (SW) vision is one in which rich, ontology-based semantic markup will become widely available.
AquaLog novel ontology-based relation similarity service makes sense of user queries.
We present a computational model of contextual facilitation based on word co-occurrence vectors, and empirically validate the model through simulation of three representative types of context manipulation: single word priming, multiple-priming and contextual constraint.
These features were extracted from a 23M word WSJ corpus based on part-of-speech tags and phrasal chunks alone.
In this paper, we discuss the phenomenon of logical polysemy in natural language as addressed by Generative Lexicon Theory.
We discuss generally the role of type and sortal coercion operations in the semantics, and specifically the conditions on the application of coercion in aspectual predicates and other contexts.
The paper presents a morpho-lexical environment, designed for the management of root- oriented natural language dictionaries.
We present a small set of attachment heuristics for postnominal PPs occurring in full-text articles related to enzymes.
We look at self-triggerability across hyperlinks on the Web.
We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.
This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component.
We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.
This paper describes a semi-automatic method for associating a Japanese lexicon with a semantic concept taxonomy called an ontology, using a Japanese-English bilingual dictionary as a "bridge".
The ontology supports semantic processing in a knowledge-based machine translation system by providing a set of language-neutral symbols and semantic information.
Evaluations using a large-scale test collection on Japanese- English and different weighting schemes of SMART retrieval system confirmed the effectiveness of the proposed combination of two-stages comparable corpora and linguistics-based pruning on Cross- Language Information Retrieval.Keywords: Cross-Language Information Retrieval, Comparable corpora, Translation, Disambiguation, Part-of-Speech.
In order to examine the domain dependence of parsing, in this paper, we report 1) Comparison of struc-ture distributions across domains; 2) Ex-amples of domain specific structures; and 3) Parsing experiment using some domain dependent grammars.
In this paper, we describe a reversible letter-to-sound/soundto-letter generation system based on an approach which combines a rule-based formalism with data-driven techniques.
We adopt a probabilistic parsing strategy to provide a hierarchical lexical analysis of a word, including information such as morphology, stress, syllabification, phonemics and graphemics.
This paper proposes a structurally based method for computing the relative scope of such modifiers, based on their order, type, and syntactic complexity.
This project measures and classifies language variation.
We measure phonetic (un)relatedness between dialects using Levenshtein distance, and classify by clustering distances but also by analysis through multidimensional scaling.
We present results of our work in relevancy visualization, news visualization, world events visualization and sensor/battlefield visualization to enhance user interaction in information access and exploitation tasks.
The Situated Constructional Interpretation Model is one of these attempts.
We present an analysis of Dutch cross serial dependencies in Head-driven Phrase Structure Grammar UP&S(1994)]).
The speech synthesis group at the Computer- Based Education Research Laboratory (CERL) of the University of Illinois at Urbana-Champaign is developing a diphone speech synthesis system based on pitch-adaptive short-time Fourier transforms.
The UTTER (for "Unmarked Text Transcription by Expert Rule") system maps English text onto a phoneme string, which is then used as an input to the diphone speech synthesis system.
This paper analyzes the functionality of different distance metrics that can be used in a bottom-up unsupervised algorithm for automatic word categorization.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
We discuss shift- reduce parsing of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses.
This paper focuses on anaphora interpreted as referring to entities of type event and action.
Finite-state transducers give efficient representations of many Natural Language phenomena.
A relatively self-contained subtask of natural language generation is sentence realization: the process of generating a grammatically correct sentence from an abstract semantic / logical representation.
Using basic machine learning algorithms as Naive Bayes and K-Nearest Neighbors, we have developed a real-time system which generates questions on English grammar and vocabulary from on-line news articles.
This paper describes ongoing research on the lexicalisation problem in a multilingual generation framework.
keywords: Multilingual generation, lexical choice, controlled languages.
The model is then used to classify the unknown Named Entities in the test set.
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.
The parser uses bit-vector operations to parallelise the basic parsing operations.
We outline a general architecture for word learning, in which structural alignment coordinates this contextual information in order to restrict the possible interpretations of unknown words.
The idea behind our method is to utilize certain layout structures and linguistic pattern.
In our splitting method, we generate candidates for sentence splitting based on N-grams, and select the best one by measuring sentence similarity.
We show how categorial deduction can be implemented in higher-order (linear) logic programming, thereby realising parsing as deduction for the associative and non-associative Lambek calculi.
We aim to show here how such unfolding allows compilation into programs executable by a version of SLD resolution, implementing categorial deduction in dynamic linear clauses.
We aim to indicate here how higher-order logic programming can provide for such a need.After reviewing the "standard" approach, via sequent proof normalisation, we outline the relevant features of (linear) logic programming and explain compilation and execution for associative and non-associative calculi in terms of groupoid and binary relational interpretations of categorial connectives.
We go on to briefly mention multi- modal calculi for the binary connectives.The parsing problem is usually construed as the recovery of structural descriptions assigned to strings by a grammar.
This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.
We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.
This paper describes the current state of work on unification-based semantic interpretation in HARC (for Hear and Recognize Confinous speech) the BBN Spoken Language System.
It presents the implementation of an integrated syntax/semantics grammar written in a unification formalism similar to Definite Clause Grammar.
This paper discusses an approach to modeling monolingual and bilingual dictionaries in the description logic species of the OWL Web Ontology Language (OWL DL).
In this paper we provide a quantitative evaluation of information automatically extracted from machine readable dictionaries.
In this paper, we construct a biomedical semantic role labeling (SRL) system that can be used to facilitate relation extraction.
We only annotate the predicate-argument structures (PAS抯) of thirty frequently used biomedical predicates and their corresponding arguments.
Second, we use our proposition bank to train a biomedical SRL system, which uses a maximum entropy (ME) model.
Thirdly, we automatically generate argument-type templates which can be used to improve classification of biomedical argument types.
The discussion focusses on the generation of complex Boolean descriptions and sentence aggregation.
This article outlines a quantitative method for segmenting texts into thematically coherent units.
This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words.
This paper proposes a practical approach employing n-gram models and error-correction rules for Thai key prediction and Thai-English language identification.
The paper also proposes rule-reduction algorithm applying mutual information to reduce the error-correction rules.
In the new Head-driven Phrase Structure Grammar (HPSG) language processing system that is currently under development at Hewlett-Packard Laboratories, the Montagovian semantics of the earlier GPSG system (see IGawron et al.
The paper describes a numerical scoring system used in Slot Grammar for ambiguity resolution, which not only ranks parses but also contributes to parsing efficiency through a parse space pruning algorithm.
We propose the use of multilingual corpora in the automatic classification of verbs.
We extend the work of (Merlo and Stevenson, 2001), in which statistics over simple syntactic features extracted from textual corpora were used to train an automatic classifier for three lexical semantic classes of English verbs.
The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus.
We report in this paper on an experiment on auto-matic extraction of a Tree Adjoining Grammar from the WSJ corpus of the Penn Treebank.
This paper will describe the phrasal knowledge base of FLUSH and its use in TRUMP.II.
First, intentional context is represented and inferred from user actions using probabilistic context free grammars.
This paper presents a corpus study of evaluative and speculative language.
This study yields knowledge needed to design effective machine learning systems for identifying subjective language.
The present paper reports the ongoing research on corpus representativeness.
We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
We describe a biographical multi-document summarizer that summarizes information about people described in the news.
We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.
And	the application of thecharacteristics to automatic content analysis is dicsussed.
This paper presents a language model and its application to sentence structure manipulations for various natural language applications including human-computer communications.
We present a derivation of the alignment template model for statistical machine translation and an implementation of the model using weighted finite state transducers.
We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers.
The approach is based on the statistical machine transliteration model to exploit the phonetic similarities between English words and corresponding Chinese transliterations.
For a given proper noun in English, the proposed method extracts the corresponding transliterated word from the aligned text in Chinese.
This paper' describes a method for chart parsing Lambek grammars.
The proposed method divides the documents into sentences, and categorizes each sentence using keyword lists of each category and sentence similarity measure.
Anaphora in multi-modal dialogues have different aspects compared to the anaphora in language-only dialogues.
In this paper, we define two kinds of anaphora: screen anaphora and referring anaphora, and propose two general methods to resolve these anaphora.
This paper investigates how to extend coverage of a domain independent lexicon tailored for natural language understanding.
We introduce two algorithms for adding lexical entries from VERBNET to the lexicon of the TRIPS spoken dialogue system.
This paper explores the use of a character segment based character correction model, language modeling, and shallow morphology for Arabic OCR error correction.
We introduce a stochastic grammatical channel model for machine translation, that synthesizes several desirable characteristics of both statistical and grammatical machine translation.
The model employs only (1) a translation lexicon, (2) a context-free grammar for the target language, and (3) a bigram language model.
Here, we view word alignment as matrix factorisation.
We then propose an algorithm for orthogonal non-negative matrix factorisation, based on a probabilistic model of the alignment data, and apply it to word alignment.
In this paper we analyze the types of errors produced by a tagger, distinguishing name classification and various types of name identification errors.
We present a joint inference model to improve Chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline: name structure parsing, cross-document coreference, semantic relation extraction and event extraction.
This paper describes a dialog based QA system, Dialog Navigator, which can answer questions based on large text knowledge base.
Another feature of the system is that it retrieves relevant texts precisely, using question types, synonymous expression dictionary, and modifier-head relations in Japanese sentences.
In the paper we focus on the notion of importance from a computational standpoint, and we propose a procedural, rule-based approach toimportance evaluation.
This paper proposes an automatic in-terpretation system that integrates free-style sentence translation and parallel text based translation.
Free-style sentence translation accepts natural language sen-tences and translates them by machine translation.
We developed a prototype of an au-tomatic interpretation system for Japanese overseas travelers with parallel text based translation using 9206 parallel bilingual sentences prepared in task-oriented man-ner.
This paper introduces an open computational framework for visual perception and grounded language acquisition called Experience-Based Language Acquisition (EBLA).
We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.
This paper' describes a flaw ral language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best," parse of a sentence according to a given grammar.
We present a broad coverage Japanese grammar written in the HPSG formalism with MRS semantics.
It is connected to a POS tagging and word segmentation tool.
The algorithms included in this study are Hidden Markov Model, Maximum Entropy, Memory-Based Learning, and Transformation-Based Learning.
This paper investigates the potential for projectinglinguistic annotations including part-of-speech tagsand base noun phrase bracketings from one languageto another via automatically word-aligned parallelcorpora.
This paper focuses on subject shift and presents a method for extracting key paragraphs from documents that discuss the same event.
Our aim in this paper is to identify genre-independent factors that influence the decision to pronominalize.
One isa Probabilistic Context-Free Grammar (PCFG)approach, the other is a classification-basedMemory-Based Learning (MBL) approach.
This paper reports our on-going efforts to exploit multiple features derived from an audio stream using source material such as broadcast news, teleconferences, and meetings.
These features are derived from algorithms including automatic speech recognition, automatic speech indexing, speaker identification, prosodic and audio feature extraction.
Finally, we discuss our approach to semantic, morphological, phonetic query expansion to improve audio retrieval performance and to access cross-lingual data.
This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline.
Dictionaries and word translation models are used by a variety of systems, especially in machine translation.
This paper presents a new approach to bitext correspondence problem (BCP) of noisy bilingual corpora based on image processing (IP) techniques.
Therefore, BCPs, including sentence and word alignment, can benefit from a wealth of effective, well established EP techniques, including convolution-based filters, texture analysis and Hough transform.
This paper describes a new program, PlotAlign that produces a word-level bitext map for noisy or non-literal bitext, based on these techniques.Keywords: alignment, bilingual corpus,image processing
Sentence understanding builds a model of the state of the world described, through the application of several knowledge modules: (i) LFG parsing, (ii) syntactic disambiguation based on lexical entry semantic components, (iii) assembly of semantic components and instantiation of domain entities, and (iv) construction of a world model through activation of common sense and domain knowledge.
Chinese word segmentation and POS tagging are two key techniques in many applications in Chinese information processing.
CSeg&Tag1.0, a Chinese word segmenter and POS tagger which unifies these two procedures into one model, is introduced in this paper.
In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus.
We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers.
Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text.
During decoding, we use a block unigram model and a word-based trigram language model.
The blocks are further filtered using unigram-count selection criteria.
We study a method for incorporating noun-class information, in the context of learning to resolve Prepositional Phrase Attachment (PPA) disambiguation.
A method for automatic plot analysis of narrative texts that uses components of both traditional symbolic analysis of natural language and statistical machine-learning is presented for the story rewriting task.
Many syntactic models in machine translation are channels that transform one tree into another, or synchronous grammars that generate trees in parallel.
We present a new model of the translation process: quasi-synchronous grammar (QG).
Given a source-language parse tree T1, a QG defines a monolingual grammar that generates translations of T1.
We describe experiments learning quasi-synchronous context-free grammars from bitext.
We place synchronous tree-adjoining grammars and tree transducers in the single overarching framework of bimorphisms, continuing the unification of synchronous grammars and tree transducers initiated by Shieber (2004).
In particular, the tree relations definable by synchronous tree-substitution grammars (STSG) were shown to be just those definable by linear complete bimorphisms, thereby providing for the first time a clear relationship between synchronous grammars and tree transducers.In this work, we show how the bimorphism framework can be used to capture a more powerful formalism, synchronous tree-adjoining grammars, providing a further uniting of the various and disparate formalisms.After some preliminaries (Section 1), we begin by recalling the definition of tree-adjoining grammars and synchronous tree-adjoining grammars (Section 2).
We turn then to a set of known results relating context-free languages, tree homomorphisms, tree automata, and tree transducers to extend them for the tree-adjoining languages (Section 3), presenting these in terms of restricted kinds of functional programs over trees, using a simple grammatical notation for describing the programs.
This paper reports on an investigation into representing tone unit boundaries (pauses) as well as words in a corpus of spoken English.
This paper describes PaTrans - a fully automatic production MT system de-signed for producing raw translations of patent texts from English into Dan-ish.
In this paper, we describe improved alignment models for statistical machine translation.
The statistical translation approach uses two types of information: a translation model and a language model.
The language model used is a bigram or general m-gram model.
The translation model is decomposed into a lexical and an alignment model.
The first approach is based on dependencies between single words, the second approach explicitly takes shallow phrase structures into account, using two different alignment levels: a phrase level alignment between phrases and a word level alignment between single words.
We discuss a tagging schema and a tagging tool for labeling the rhetorical structure of texts.
We extract a set of heuristic principles from a corpus-based sample and formulate them as probabilistic Horn clauses.
We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset, and run two different machine learning algorithms: SLIPPER, a rule-based learning algorithm, and TiMBL, a memory-based system.
The second model uses a reranking approach to add arbitrary global features of parse trees to the morphological model.
Phonologists employ a variety of contextual descriptors, based on factors such as stress and syllable boundaries, to explain phonological variation.
In order to incorporate a wide variety of factors in the creation of pronunciation networks, we used data-derived context trees, which possess properties useful for pronunciation network creation.
5000 parallel sentences of Tamil and English data.
It uses dynamic program-ming to efficiently compare weighted aver-ages of sets of adjacent scored component translations.
Our method combines shallow linguistic processing with machine learning to extract phrasal units that are representative of email content.
This paper presents a semantic model for Chinese garden-path sentences.
The second is a Markov model of syntax and is based on syntactic categories (tags) associated with words.
We present a new approach for mapping natural language sentences to their formal meaning representations using stringkernel-based classifiers.
Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers.
We investigate why weights from generative models underperform heuristic estimates in phrase- based machine translation.
This paper describes a Verb Phrase Ellipsis (VPE) detection system, built for robustness, accuracy and domain independence.
We investigate the use of machine learning in combination with feature engineering techniques to explore human multi- modal clarification strategies and the use of those strategies for dialogue systems.
move+ed).
Unknown word recognition is an important problem in Chinese word segmentation systems.
This paper presents and analyzes an incremental	algorithm	for	theconstruction of Acyclic Non-deterministic Finite-state Automata (NFA).
Focusing on bottom-up chart generation, we describe how the notions of chart algorithms relate to the knowledge base and Rete network of production systems.
We draw on experience gained in two research projects on natural language generation (NLG), one involving surface realization, the other involving both a content determination task (referring expression generation) and surface realization.
This paper proposes the application of finite-state approximation techniques on a unification-based grammar of word formation for a language like German.
This paper describes an implemented mechanism for handling bound anaphora, disjoint reference, and pronominal reference.
In this paper, we adapt the new approach of contrast classifiers for semi-supervised learning.
This paper shows that a class of Combinatory Categorial Grammars (CCGs) augmented with a linguistically-motivated form of type raising involving variables is weakly equivalent to the standard CCGs not involving variables.
The German joint research project Verb-mobil (VM) aims at the development of a speech to speech translation sys-tem.
In this paper we discuss the use of cascaded finite state transducers for machine translation.
The system uses uses syntactic information from Penn Treebank parse trees.
We have evaluated the appropriateness of sentence concatenations in summaries by using evaluation measures used for evaluating word concatenations in summaries through word extraction.
We investigate that claim by adopting a simple MT- based paraphrasing technique and evaluating QA system performance on paraphrased questions.
In this paper, we contrast two language model adaptation approaches: MAP estimation and the perceptron algorithm.
This paper presents AsdeCopas, a module designed to interface syntax and semantics.
We present a MT system that applies graph unification in transfer from English to Finnish.
The transfer system is responsible for generating target language phrase structure.
The lexicon system consists of separate transfer and monolingual lexicons and a common lexicon of language independent definitions.Keywords: unification, machine translation, transfer, bilingual lexicon
This paper describes text meaning represen-tation for Chinese.
It integrates lexical, textual and world knowledge into a single hierarchical frame-work.
Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.
The IBM flight information system is a speech based conversational system which allows users to create multi-leg airline travel itineraries based on live flight availability information.
This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best" parse of a sentence.
We present a domain-independent topic segmentation algorithm for multi-party speech.
Our feature-based algorithm combines knowledge about content using a text-based algorithm as a feature and about form using linguistic and acoustic cues about topic shifts extracted from speech.
The embedded text-based algorithm builds on lexical cohesion and has performance comparable to state-of-the-art algorithms based on lexical information.
This paper proposes a system of 97 verbal semantic attributes for Japanese verbs which considers both dynamic characteristics and the relationship of verbs to cases.
In this paper, we propose a topic detection method using a dialogue history for a speech translator.
The method uses a k-nearest neighbor method for the algorithm, automatically clusters target topics into smaller topics grouped by similarity, and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases.
We describe a language learner that extracts distributional information from a corpus annotated with parts of speech and is able to use this extracted information to accurately parse short sentences.
We define a more general class of unification grammars, which admits x-bar grammars while preserving the desirable properties of offline parsable grammars.Consider a unification grammar based on term unification.
The parsing problem for offline parsable grammars is solvable.
This in turn affects the accuracy of word sense disambiguation (WSD) systems trained and applied on different domains.
We propose a formal model that estimates the lexical similarity between sources and potential destinations of hyperlinks.
In this paper, we present a new phrase break prediction architecture that; integrates proba-bilistic approach with decision-tree based error correction.
This paper proposes an automatic method of reading proper names with multiple pronunciations.
We describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model.
In this paper, we focus on two distinctive features of the K3 system: plan-based anaphora resolution and handling vagueness in spatial ex-pressions.
In this paper, we first describe how we have customized our data-driven multilingual discourse module within our text understanding system for different languages and for a particular NLP application by utilizing hierarchically organized discourse KB's.
One of the most active and promising areas of statistical machine translation (SMT) research are tree-based SMT approaches.
Tree-based SMT has the potential to overcome the weaknesses of early SMT architectures which (a) do not handle long-distance dependencies well, and (b) are underconstrained in that they allow too much flexibility in word reordering.In this tutorial, we will review the various possible approaches to tree-based SMT, ranging from the original Inversion Transduction Grammar (ITG) models to later models such as alignment templates, dependency models, tree-to-string models, tree-to-tree models, and also probabilistic EBMT models.
This paper describes a method of pro-cessing unknown words in a HPSG-based dialogue system, with acquisi-tion of lexical semantics via clarifica-tion questions answered by the user.
Successful examples include subword models with many smoothing techniques.
We propose to model subphonetic events with Markov states and treat the state in phonetic hidden Markov models as our basic sub- phonetic unit ?senone.
In this approach we integrate a symbolic se-mantic segmentation parser with a learn-ing dialog act network.
The paper describes a particular approach to multi- engine machine translation (MEMT), where we make use of voted language models to selectively combine translation outputs from multiple off-theshelf MT systems.
We describe two choices for the representation of lexical items and two choices for the representation lexical relations.
We describe how these comparison strategies are used within the PEBA-II hypertext generation system to generate descriptions of animals.
We describe a computational system which parses discourses consisting of sequences of simple sentences.
These contain a range of temporal constructions, including time adverbials, progressive aspect and various aspectual classes.
We illustrate how we use sublexical units for handling simple productive derivational morphology and more interesting cases such as causativization, etc., which change verb valency.
We are aiming to acquire named entity (NE) translation knowledge from nonparallel, content-aligned corpora, by utilizing NE extraction techniques.
Structural (attachment.)
We have proposed a method of word segmen-tation for non-segmented language using Induc-tive Learning.
The method predicts unknown words by recursively extracting common character strings.
This paper reports the present results of a research on unsupervised Persian morpheme discovery.
We utilized a Minimum Description Length (MDL) based algorithm with some improvements and applied it to Persian corpus.
^ The method uses a k-nearest neighbor method for the algorithm, automatically clusters target topics into smaller topics grouped by similarity, and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases.
CRFs are log-linear, allowing the incorporation of arbitrary features into the model.
To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist.
In this paper, we investigate the problem of automatically predicting segment boundaries in spoken multiparty dialogue.
We first apply approaches that have been proposed for predicting top-level topic shifts to the problem of identifying subtopic boundaries.
Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks: (1) for predicting subtopic boundaries, the lexical cohesion-based approach alone can achieve competitive results, (2) for predicting top-level boundaries, the machine learning approach that combines lexical-cohesion and conversational features performs best, and (3) conversational cues, such as cue phrases and overlapping speech, are better indicators for the top- level prediction task.
This paper examines the role that summaries can play in document retrieval.
We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts.
We explored a simple, fast and effective learning algorithm, the uneven margins Perceptron, for Chinese word segmen-tation.
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
We investigate generalizations of the allsubtrees "DOP" approach to unsupervised parsing.
Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
This paper describes a semi-automatic method of inducing underspecified semantic classes from WordNet verbs and nouns.
An underspecified semantic class is an abstract semantic class which encodes systematic polysemy: a set of word senses that are related in systematic and predictable ways.
We show the usefulness of the induced classes in the semantic interpretations and contextual inferences of real-word texts by applying them to the predicate-argument structures in Brown corpus.
We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction.
Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words.
We describe two systems for distributional grammar induction which operate on such principles, using part-of-speech tags as the contextual features.
We present a framework for the integrated analysis of the textual and prosodic characteristics of information structure in the Switchboard corpus of conversational English.
theme/rheme and background/kontrast.
This paper presents a character-based model of automatic sense determination for Chinese compounds.
The model adopts a sense approximation approach using synonymous compounds retrieved by measuring similarity of semantic template in compounding.
In this position paper we describe the scopes of two schools in lexical semantics, which we call syntax-driven lexical semantics and ontology-driven lexical semantics, respectively.
In this paper we propose an integrated knowledge management system in which terminology-based knowledge acquisition, knowledge integration, and XML-based knowledge retrieval are combined using tag information and ontology management tools.
Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval.
We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains.
In this paper a method to compile unification grammars into speech recognition packages is presented, and in particular, rules are specified to transfer the compositional semantics stated in unification grammars into speech recognition grammars.
The method was tested on a medium-sized unification grammar for English using Nuance speech recognition software on a corpus of 131 utterances of 12 different speakers.
Introductionin the field of natural language analysis, Unification Grammars are a main research topic.
Presently, Unification is defined as extension of context-free grammars.Knowing the formalism of Tree Adjoining Grammars (in the following called TA Gs in short), which is closely related to context-free grammars (in the following abbreviated CFG), the idea arises to replace the context-free grammar in a Unification Grammar by a TAG.
A TAG is a tree generation system.
Then we analyse the results of this data collection and estimate the efficiency of the collected list for the resolution of morphosyntactic and semantic ambiguity in a corpus.
This paper describes a model for a lexical knowledge base (LKB).
Thus, we propose a model for an LKB focusing on dictionary knowledge such as that obtained from machine-readable dictionaries.When an LKB is given a key from a user, it accesses the stored knowledge associated with that key.
They include machine translation systems, style analyzers, personal databases, an electronic book [Weyer82], and an electronic encyclopedia [VVeyer85].
We call an access unit of stored dictionary knowledge a lexical knowledge unit (LKU).
Headwords in machine-readable dictionaries are usually standardized; i.e.
without inflections or conjugations.
This paper presents a hybrid rule-based and statistical model for morpho-syntactic annotation of German, a highly ambiguous inflectional language.
Its dependency-based shallow parsing approach provides significant robustness in the face of language learners?ungrammatical compositions.
The lexicon now plays a central role in our implementation of a Head-driven Phrase Structure Grammar (HPSG), given the massive relocation into the lexicon of linguistic information that was carried by the phrase structure rules in the old GPSG system.
SemFrame generates FrameNet-like frames, complete with semantic roles and evoking lexical units.
The semantic relations are detected by checking selectional constraints.
The ability to represent cross-serial dependencies is one of the central features of Tree Adjoining Grammar (TAG).
This paper describes a prototype mul-timodal spoken natural language dia-log system for capturing a comman-der's expectations about planned mil-itary actions.
To provide practical spelling checkers on micro-computers, good compression algorithms are essential.
Experiments with multilingual dictionaries show a significant reduction rate attributable to our logic compression alone and even better results when using our method in conjunction with existing methods.KEY WORDS: Spelling checkers, Multilinguism, Compression, Dictionary, Finite-state machines.
In this paper, we look into extending standard dialogue move taxonomies for the genre of tutorial dialogues.
We suggest a way of investigating tutorial dialogue phenomena robustly.
Each TCT describes a translation example (a pair of bilingual sentences).
Past work of generating referring expressions mainly utilized attributes of objects and binary relations between objects.
To overcome this limitation, this paper proposes a method utilizing the perceptual groups of objects and n-ary relations among them.
We present a dialogue manager for 揅all for Fire?training dialogues.
In this paper, using a similar syntactic analysis for wh pied-piping as in Han (2002) and further developed in Kallmeyer and Scheffler (2004), I propose a compositional semantics for relative clauses based on Synchronous Tree Adjoining Grammar.
In this paper	unification and	transductionmechanisms are applied in a new approach to phonological parsing.
Then a linear- unification parser for English syllables is introduced.
This parser takes phonetic input in Lie form of feature bundles and uses phonological rules represented by networks of transduction relations together with unification, and an iterative finite-state process to produce phonemic output with marled syllable boundaries.
parsers and transducers) for the interpretation of such networks.
A new language model incorporating both N-gram and context-free ideas is proposed.
This constrained context-free model is specified by a stochastic context-free prior distribution with N-gram frequency constraints.
The resulting distribution is a Markov random field.
This paper discusses automatic text summarization based on GDA.
In order to calculate the importance score of a text element, the algorithm uses spreading activation on an intradocument network which connects text elements via thematic, rhetorical, and coreferential relations.
We show how idioms can be parsed in lexicalized TAGs.
We thus consider idioms of different syntactic categories : NP, S, adverbials, compound prepositions... in both English and French.In lexicalized TAGs, the same grammar is used for idioms as for 'free' sentences.
We also show how regular 'transformations' are taken into account by the parser.Topics: Parsing, Idioms.
This paper presents the motivating goals, acoustic data design, text processing steps, lexicons, and testing paradigms incorporated into the multi-faceted WSJ CSR Corpus.
This paper describes an accurate and efficient algorithm for very-large-vocabulary continuous speech recognition based on an HMM-LR algorithm.
The HMM-LR algorithm uses a generalized LR parser as a language model and hidden Markov models (HMMs) as phoneme models.
To cope with the problem of background noise, an HMM composition technique which combines a noise- source HMM and a clean phoneme HMM into a noise-added phoneme HMM was investigated and incorporated into the system.
A brief description of the bilingual dictionary is given, followed by descriptions of grammar rules representation and the main processes involved in translation.SYSTEM CONFIGURATIONECTST consists of a translation program, a bilingual dictionary and a rule-data base.
The analysis phase involves syntactic and semantic parsing, which are accomplished through linguistic models and case frame.
In this paper, we present a named entity recognition system in the biomedical domain, called PowerBioNE.
We demonstrate an implemented disambiguator for a certain class of three-clause sentences based on our theory.
This paper describes the scalability and portability of a Belief Network (BN)-based mixed initiative dialog model across application domains.
We have also enhanced our dialog model with the capability of discourse context inheritance.
In this paper, we describe a system that applies maximum entropy (ME) models to the task of named entity recognition (NER).
We propose a method for semi-automatic classi-fication of verbs to Levin classes via the seman-tic network of WordNet.
We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.
This paper describes our actual and ongoing work in supporting semi-automatic ontology ac-quisition from a corporate intranet of an in-surance company.
GermaNet, WordNet).
the Italian wordnet in EuroWordNet (ItalWordNet).
This paper presents a system for unsupervised verb sense disambiguation using small corpus and a machine-readable dictionary (MRD) in Korean.
First, extending word similarity measures from direct co-occurrences to co- occurrences of co-occurred words, we compute the word similarities using not co-occurred words but co- occurred clusters.
We describe novel aspects of a new natural language generator called Nitrogen.
In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple sliding- window method and maximum entropy classifiers for phrase recognition in each level of chunking.
In this paper we develop an automatic classifier for a very large set of labels, the WordNet synsets.
An input string is parsed by combining subtrees from the corpus.
shotat (hl, 13) .
We introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial and final contributions.
Further analysis reveals that using dialogue exchanges versus dialogue contributions improves topic segmentation quality.
In this paper we investigate an application of feature clustering for word sense disambiguation, and propose a semi- supervised feature clustering algorithm.
This paper addresses the problems of movement transformation in Prolog-based bottom-up parsing system.
Three principles of Government-Binding theory are employed to deal with these problems.
For the treatment of this phenomenon, we cannot just write down the rules:sentence --> noun-phrase,verb-phrase.verb-phrase --> transitive-verb,noun-phrase.verb-phrase --> transitive-verb.verb-phrase --> intransitive-verb.This is because many ungrammatical sentences will be accepted.
A Government-Binding based Logic GrammarFormalism2.1 The specifications of grammar formalismThe Government-Binding based Logic Grammar (GBLG)formalism is specified informally as follows:(1) the general grammar rules -(a) c (Arg) --> ci(Argi ),c2(Arg2),...,cn(Argn).
We present a graph-based semi-supervised learning algorithm to address the sentiment analysis task of rating inference.
This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be effectively carried out.
We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags.
We then applied an automatic algorithm to detect errors in the dialogues.
Our algorithm extends the information-bottleneck soft clustering method for a suitable setting consisting of several datasets.
We term this task cross-dataset (CD) clustering.In this article we demonstrate CD clustering through detecting corresponding themes across three different religions.
In this paper we present a brief look at some of the knowledge-based processes used in generating referring expressions in the natural language advisory system WISBER.
We apply a gene and protein name tagger trained on Medline abstracts (ABGene) to a randomly selected set of full text journal articles in the biomedical domain.
This paper presents a corpus-based method to assign grammatical subject/object relations to ambiguous German constructs.
We also report results of a subjective listening experiment evaluating an implementation of this system for text-to-speech synthesis purposes.
This paper will introduce the tasks of tokenization, normalization before introducing BAccHANT, a system built for bioscience text normalization.
Casting tokenization / normalization as a problem of punctuation classification motivates using machine learning methods in the implementation of this system.
Word alignment plays a crucial role in statistical machine translation.
We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support user- directed multidocument summarization.
We present a statistical shift-reduce parser that bridges the gap between deterministic and probabilistic parsers.
The method is based on explanation-based learning EBL.
This paper proposes a method for extracting key paragraph for multi-document summarization based on distinction between a topic and an event.
A topic and an event are identified using a simple criterion called domain dependency of words.
bus information system, an experimental system designed to study the use of spoken dialogue interfaces by non-native speakers.
This paper presents a disambiguation approach for translating non-segmented-Kana into Kanji.
The focus of this article is the integra-tion of two different perspectives on lexi-cal semantics: Discourse Representation Theory's (DRT) inferentially motivated approach and Semantic Emphasis The-ory's (SET) lexical field based view.
We present a new statistical language model based on a combination of individual word language models.
In particular, we describe the integration of a high-level HPSG parsing system with different high-performance shallow components, ranging from named entity recognition to chunk parsing and shallow clause recognition.
We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars.
Leximancer is a software system for performing conceptual analysis of text data in a largely language independent manner.
The system is modelled on Content Analysis and provides unsupervised and supervised analysis using seeded concept classifiers.
Unsupervised ontology discovery is a key component.
This paper applies machine learning techniques to acquiring aspects of the meaning of discourse markers.
causal, temporal or additive).
We investigate methods that add syntactically motivated features to a statistical machine translation system in a reranking framework.
The goal is to analyze whether shallow parsing techniques help in identifying ungrammatical hypotheses.
We show that improvements are possible by utilizing supertagging, lightweight dependency analysis, a link grammar parser and a maximum-entropy based chunk parser.
We create two bilingual pronunciation dictionaries for the language pairs German-Dutch and German- English.
The data is used for automatically learning phonological similarities between the two language pairs via EM- based clustering.
We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments.
While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns.
In this paper, a novel kernel-based method is presented for the problem of relation extraction between named entities from Chinese texts.
By em-ploying the Voted Perceptron and Sup-port Vector Machine (SVM) kernel ma-chines with the IED kernel as the clas-sifiers, we tested the method by extract-ing person-affiliation relation from Chi-nese texts.
We present a treebank conversion method by which we construct an RMRS bank for HPSG parser evalu-ation from the TIGER Dependency Bank.
We present the semantics construction mechanism, and focus on some spe-cial phenomena.
The model can be classified as a Maximum Entropy model and simultaneously uses many contextual "features" to predict the PUS tag.
These rules are noun sequences with part-of-speech tags.
In this paper we investigate the adaptations of users when they encounter recognition errors in interactions with a voice-in/voice-out spoken language system.
Extracting sentences that contain important information from a document is a form of text summarization.
This paper proposes a method of sentence extraction based on Support Vector Machines (SVMs).
This paper studies a strategy for identifying and using multi-word expressions in Statistical Machine Translation.
We have developed a model based on a hierarchy of plans and metaplans that accounts for the clarification subdialogues while maintaining the advantages of the plan-based approach.I.
Quantified mass noun phrases is one such part.
The decision procedure is based on a tableau calculus.
We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization.
Semantic similarity measures have focused on individual word senses.
We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers concepts from text.
We present an algorithm for simultaneously constructing both the syntax and semantics of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG).
We present this architecture and the synchro-nization issues we encountered in building a truly distributed, agent-based dialogue architecture.
The HKUST word sense disambiguation systems benefit from a new nonlinear Kernel Principal Component Analysis (KPCA) based disambiguation technique.
The present article demostrates the implementation of a psycholinguistic model of second language learners' interlanguage in an Intelligent Computer Assisted Language Learning (ICALL) system for studying second language acquisition.
The algorithm uses a trellis-based structure as opposed to the binary branching tree structure used by the I/O algorithm.
1973] for the procedural embedding of knowledge.
We introduce stereotypes as an actor version of a frame theory.
We also present an inductive learning approach to the automatic discovery of lexical and semantic constraints necessary in the disambiguation of causal relations that are then used in question answering.
This work presents the data model we adopted for annotating coreference.
Our data model includes different levels of annotation, such as part-of-speech, syntax and discourse.
113y checking lexical cohesion between the current word and lexical chains in the order of the salience, in tandem with generation of lexical chains, we realize incremental word sense disattr, biguation based on contextual information that lexical chains,reveal.
A novel technique for automatic thesaurus construction is proposed.
It is based on the complementary use of two tools: (1) a Term Extraction tool that acquires term candidates from tagged corpora through a shallow grammar of noun phrases, and (2) a Term Clustering tool that groups syntactic variants (insertions).
Natural language processing systems need large lexicons containing explicit information about lexical-semantic relationships, selection restrictions, and verb categories.
This paper describes methods for finding taxonomy and set-membership relationships, recognizing nouns that ordinarily represent human beings, and identifying active and stative verbs and adjectives.
We present an implemented concept-to-speech (CTS) system that offers original proposals for certain couplings of dialogue computation with prosodic computation.
We introduce a MetaGrammar, which allows us to automatically generate, from a single and compact MetaGrammar hierarchy, parallel Lexical Functional Grammars (LFG) and Tree-Adjoining Grammars (TAG) for French and for English: the grammar writer specifies in compact manner syntactic properties that are potentially framework-, and to some extent language-independent (such as subcategorization, valency alternations and realization of syntactic functions), from which grammars for several frameworks and languages are automatically generated offline.1
Automatic Multi-Document summarization is still hard to realize.
Based on the observation that recognition errors may result in ungrammatical sentences, especially in dictation application where an acceptable level of accuracy of generated documents is indispensable, we propose to incorporate two kinds of linguistic features into error detection: lexical features of words, and syntactic features from a robust lexicalized parser.
Transformation-based learning is chosen to predict recognition errors by integrating word confidence scores with linguistic features.
It proposes that two widely used kinds of relations, lexical dependen-cies and structural relations, have complementary disambiguation ca-pabilities.
Linearization-based HPSG theories are widely used for analyzing languages with relatively free constituent order.
This paper introduces the Generalized ID/LP (GIDLP) grammar format, which supports a direct encoding of such theories, and discusses key aspects of a parser that makes use of the dominance, precedence, and linearization domain information explicitly encoded in this grammar format.
This paper describes Acorn, a sentence planner and surface realizer for dialogue systems.
We are using a computational learning sys-tem that is composed of a Universal Grammar with associated parameters, and a learning al-gorithm, following the Principles and Parame-ters Theory.
The Universal Grammar is imple-mented as a Unification-Based Generalised Cat-egorial Grammar, embedded in a default inher-itance network of lexical types.
We present a simple, but surprisingly effective, method of self-training a two- phase parser-reranker system using readily available unlabeled data.
We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker.
We identify several cues based on acoustic and prosodic analysis of repairs in a corpus of spontaneous speech, and propose methods for exploiting these cues to detect and correct repairs.
We quantify the degree to which gazetteers, web resources, encyclopedia, web documents and web-based query expansion can help Question Answering in general and specific question types in particular.
This paper describes a system which uses a decision tree to find and classify names in Japanese texts.
The decision tree uses part-of-speech, character type, and special dictionary information to determine the probability that a particular type of name opens or closes at a given position in the text.
One is a phonemic based transcription of sounds for acoustic modelling in Automatic Speech Recognizers and for Text to Speech synthesizer, using ASCII based symbols, rather than International Phonetic Alphabet symbols.
This paper proposes a new term weighting method for summarizing documents retrieved by IR system.
This paper proposes using sentence-rankbased and content-based measures for evaluating extract summaries, and compares these with recall- based evaluation measures.
This paper presents in implemented theory for quantifying noun phrases in clauses containing copular verbs (e.g.. 'be' and 'become.).
We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language.
A monotone phrasal decoder generates contextual replacements.
This paper describes a dependency based tagging scheme for creating tree banks for Indian languages.
It is based on Paninian grammatical model.
Frequently, such methods are based on Centering Theory, which deals with the resolution of anaphoric pronouns.
This paper presents a maximum entropy-based named entity recognizer (NER).
I'This work was supported by CEC Telernatics Applications Programme project LEI4111 "SPARKLE: Shallow PARsing and Knowledge extraction for Language Engineering".
The parser uses information about co-occurence and domination of linguistic elements as well as concept hierarchies in the domain to construct a parse tree and, subsequently, a derivation in quasi- logical form.
This paper evaluates the effect of using different lexical and syntactic features both individually and in combination.
This paper introduces a supersonic Korean morphological analyzer named MACH.
The connectivity between two sentences is measured based on correference between a pronoun and a preceding (pro)noun, and on lexical cohesion of lexical items.
This paper explores the usefulness of a technique from software engineering, code instrumentation, for the development of large-scale natural language grammars.
The first is a baseline condition using only training data available from NIST on CD-ROM and a word-based statistical bi-gram grammar developed at MIT/Lincoln.
In the second condition, we added training data from speakers collected at BBN and used a 4-gram class grammar.
The Microsoft Research translation system is a syntactically informed phrasal SMT system that uses a phrase translation model based on dependency treelets and a global reordering model based on the source dependency tree.
In this paper a new, tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition is proposed.
The Multi-Class Composite N-gram main-tains an accurate word prediction ca-pability and reliability for sparse data with a compact model size based on multiple word clusters, called Multi-Classes.
Fur-thermore, by introducing higher order word N-grams through the grouping of frequent word successions, Multi-Class N-grams are extended to Multi-Class Composite N-grams.
POS-tagging, lemmatisation and a thesaurus-like thematic dictionary.
(MILE).
We present a Chinese word seg-mentation system submitted to the closed track of Sighan bakeoff 2005.
In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on "most similar" words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model.
In this paper we present the results for building a grapheme-based speech recognition system for Thai.
In this paper, a treatment of Czech phonological rules in two-level morphology approach is described.
In this work, we apply a clustering technique to integrate the contents of items into the item-based collaborative filtering framework.
A hierarchical hidden Markov model (HHMM) based approach of product named entity recognition (NER) from Chinese free text is presented in this pa-per.
This paper presents a method for word• sense disambiguation and coherence understanding of prepositional relations.
We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures.
This paper proposes a machine learning based question classification method using a kernel function, Hierarchical Directed Acyclic Graph (HDAG) Kernel.
This paper describes a method for retrieving patterns of words and expressions frequently used in a specific domain and building a dictio-nary for machine translation(MT).
The method uses an untagged text corpus in retrieving word.
sequences and simplified part-of-speech tem-plates in identifying their syntactic categories.
The paper presents experimental results for ap-plying the words and expressions to a pattern-based machine translation system.
Multimodal Functional Unification Grammar (MUG) is a unification-based formalism that uses rules to generate content that is coordinated across several communication modes.
This paper describes a prototype of a multi- modal railway information system that was built by extending an existing speech-only system.
The language analysis strategy used by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing.
We applied Conditional Random Fields (CRFs) to the tasks of Amharic word segmentation and POS tagging using a small annotated corpus of 1000 words.
We present results of two methods for assessing the event profile of news articles as a function of verb type.
the event profile.
Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents.
The proposed method makes use of two kinds of word correspondences in aligning bilingual texts.
One is a bilingual dictionary of general use.
Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness.
This paper proposes a method for retrieving 搈eaning-equivalent sentences?to overcome these two problems.
The paper describes how monotonic reference resolution and scoping can be carried out using a revised Quasi Logical Form (QLF) representation.
The correct attachment of prepositional phrases (PPs) is a central disambiguation problem in parsing natural languages.
This paper describes the usage of XML for representing cross-language phrase alignments in parallel treebanks.
We describe a method for obtaining subject-dependent word sets relative to some (subject) domain.
This paper presents a new web mining scheme for parallel data acquisition.
Based on the Document Object Model (DOM), a web page is represented as a DOM tree.
N e implemented this idea as an unsupervised tokenization of Japanese with extended Hidden-Markov-Models (HNIMs), where hidden n-gram probabilities (i.e., state transition probabilities) are affected by co-occurring words in the English part.
The translation part of PIVOT is the rule-based system and adopts the interlingua method.
OF COLING-92, NANTES, Auo.
In this paper, we explore facets of instructional texts: general prototypical structures, rhetorical structure and natural argumentation.
We study the problem of learning to recognise objects in the context of autonomous agents.
First, the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure.
Here we address the problem of mapping phrase meanings into their conceptual representations.
They are formed based on finite state machine specifications thus resulting in a fast grouper.
We describe a corpus-based induction algorithm for probabilistic context-free grammars.
The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside- Outside algorithm.
This paper presents a new bootstrapping approach to named entity (NE) classification.
We utilize meta-patterns of high- frequency words and content words in order to discover pattern candidates.
Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets.
This paper presents a hybrid model for restoring an elided entry word for en-cyclopedia QA system.
A rule-based approach uses caseframes and sense classes.
In this paper, we present a term detection approach that discovers, structures, and infers conceptual relationships between terms for French.
We describe a new approach to syntactic generation with Head-Driven Phrase Structure Grammars (HPSG) that uses an extensive off-line preprocessing step.
Direct generation algorithms apply the phrase-structure rules (schemata) of the grammar on-line which is an computationally expensive step.
This process is known as 'compiling HPSG to TAG' and derives a Lexicalized Tree-Adjoining Grammar (LTAG).
In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.
This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.
Constraint Grammar framework divides parsing into two different modules: morphological disambiguation and determination of syntactic functions.
Document indexing and representation of term-document relations are very important for document clustering and retrieval.
In this paper, we combine a graph-based dimensionality reduction method with a corpus-based association measure within the Generalized Latent Semantic Analysis framework.
We evaluate the graph-based GLSA on the document clustering task.
In this paper two different approaches to the resolution of lexical ambiguities are applied to a multi-domain corpus of speech recognition output produced from spontaneous utterances in a spoken dialogue system.
We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.
Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation.
This paper presents a network formalism for representing the meaning of noun phrases occurring in the context of intensional verbs such as seek and want.
This paper focuses on the automated processing of temporal information in written texts, more specifically on relations between events introduced by verbs in finite clauses.
This paper focuses on the transformation of grammar checking technology into a learning environment for second language writing.
Our starting point is a grammar checker for Swedish, called Granska.
Then we use lin-guistic patterns and HTML structures to ex-tract text fragments describing the term.
We present the results of experiments aimed at assigning domains to speech recognition hypotheses (SRH).
The methods rely on high-level linguistic representations of SRHs as sets of ontological concepts.
In this paper we examine the issues that arise from the annotation of the discourse connectives for the Chinese Discourse Treebank Project.
We then examine the types of syntactic units that can be arguments to the discourse connectives.
This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences.
Graph unification is the most expensive part of unification-based grammar parsing.
This research is aimed at the problem of disambiguating toponyms (place names) in terms of a classification derived by merging information from two publicly available gazetteers.
We present a usage consultation tool based on Internet searching.
GRADE allows a grammar writer to control the process of a machine translation.
This paper presents a novel approach to extracting phrase-level answers in a question answering system.
This approach uses structural support provided by an integrated Natural Language Processing (NLP) and Information Extraction (IE) system.
Phrase-level answer extraction is modelled by comparing the structural similarity involving the question-phrase and the candidate answer-phrase.There are two types of structural support.
The second type involves generic grammatical relationships such as V-S (verb-subject), V-O (verb-object).Preliminary experimental results show an improvement in both precision and recall in extracting phrase-level answers, compared with a baseline system which only uses Named Entity constraints.
Throughout, we use feature cooccurence restrictions as illustration and linguistic motivation.
Word- Net) is here proposed.
A methodology aiming to support semantic bootstrapping in a NLP application is defined.
We describe a method for augmenting unification-based deep parsing with statistical methods.
We extend and adapt the Bikel parser, which uses head-driven lexical statistics, to dialogue.
Methodological problems	in Montague Grammar arediscussed.
sentencedenotations.Turner[161 extended intensional logic in the sense88
Attentional focus is simulated in both models to select relevant subnetworks for Bayesian propagation.
This paper presents an enhanced model of plan-based dialogue understanding.
We call these features shared domain plan constraints.
We exploit and extend the Generative Lexicon Theory to develop a formal description of adnominal constituents in a lexicon which can deal with linguistic phenomena found in Japanese adnominal constituents.
We classify the problematic behavior into "static disambiguation" and "dynamic disambiguation" tasks.
Using the similarities, classes are built using hierarchical agglomerative clustering.
We compare two feature sets: one with lexical features only, and another with a mixture of lexical and semantic features.
This paper describes an attribute grammar specification of the Government-binding theory.
The paper focuses on the description of the attribution rules responsible for determining antecedent-trace relations in phrase-structure trees, and on some theoretical implications of those rules for the GB model.
We select word format as target linguistic feature and propose an HMMbased approach to this issue.
This paper concerns the design of cooperative response generation (CRG) systems, NLQA systems that are able to produce integrated cooperative responses.
This paper presents a unification-based approach to Japanese honorifics based on a version of HPSG (Head-driven Phrase Structure Grammar)l11121.
Utterance parsing is based on lexical specifications of each lexical item, including honorifics, and a few general PSG rules using a parser capable of unifying cyclic feature structures.
The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser.
The task of template filling is cast as constrained parsing using the SLM.
We propose a generic paraphrase-based approach for Relation Extraction (RE), aiming at a dual goal: obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervised configuration for RE.
Our findings reveal a high potential for unsupervised paraphrase acquisition.
In recent years there is much interest in word cooccurrence relations, such as n-grams, verb- object combinations, or cooccurrence within a limited context.
This paper presents a deterministic parsing algorithm for projective dependency grammar.
This position paper sketches the author's research in six areas related to speech translation: interactive disambiguation; system architecture; the interface between speech recognition and analysis; the use of natural pauses for segmenting utterances; dialogue acts; and the tracking of lexical co-occurrences.
A coin puter-based course addressing the topic of applying Natural Language resources and techniques to Information Retrieval is presented.
This paper talks about the deciding practical sense boundary of homonymous words.
We are interested in the problem of modeling and evaluating spoken language systems in the context of human-machine dialogs.
Spoken dialog corpora allow for a multidimensional analysis of speech recognition and language understanding models of dialog systems.
This paper investigates different ways for encoding dialogues into multidimensional structures and different clustering methods.
This paper outlines a high-level language FUNDPL for expressing functional structures for parsing dependency constraints.
Grammars for parsing have predominantly used generative rewrite rules.
This paper describes a high-level language FUNDPL (FUNctional DPL) we have designed on top of DPL.
CATEGORY assigns names in hierachies.
This paper presents a knowledge-based method for measuring the semantic- similarity of texts.
In this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.
We also report on an experimental result of case structure analysis using the constructed case frame dictionary.
This paper discusses three different but related large-scale computational methods for the transformation of machine readable dictionaries (MRDs) into machine tractable dictionaries, i.e., MRDs converted into a format usable for natural language processing tasks.
The MRD used is The Longman Dictionary of Contemporary English.
We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area.
The solution lies in a grammar design in which lexicalized grammar rules defined in terms of semantic categories and syntactic rules defined in terms of part-of-speech are utilized together.
We present an integrated architecture for word-level and sentence-level processing in a unification-based paradigm.
The core of the system is a CLP implementation of a unification engine for feature structures supporting relational values.
In this framework an IIPSG-style grammar is implemented.
Word-level processing uses X2MoRF, a morphological component based on an extended version of two-level morphology.
In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented.
We propose a detection method for orthographic variants caused by transliteration in a large corpus.
One is string similarity based on edit distance.
The other is contextual similarity by a vector space model.
This paper proposes a method of automatic back transliteration of proper nouns, in which a Japanese transliterated-word is restored to the original English word.
We confirmed the effectiveness of using the target English context by an experiment of personal-name back transliteration.
Chinese NE (Named Entity) recognition is a difficult problem because of the uncertainty in word segmentation and flexibility in language structure.
We then treat the process of selecting the best possible NEs as a multi-agent negotiation problem.
In tins paper, temporal expressions and the context for disambiguation which is called local context are represented using lexical data extracted from corpus and the finite state transducer.
We train a decision tree inducer (CART) and a memory-based classifier (MBL) on predicting prosodic pitch accents and breaks in Dutch text, on the basis of shallow, easy-to-compute features.
We design and test a sentence comparison method using the framework of Robust Minimal Recursion Semantics which allows us to utilise the deep parse information produced by Jacy, a Japanese HPSG based parser and the lexical information available in our ontology.
Our method was used for both paraphrase detection and also for answer sentence selection for question answering.
This paper describes a domain-independent, machine-learning based approach to temporally anchoring and ordering events in news.
We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences.
We have developed an effective probabilistic classifier for document classification by introducing the concept of the differential document vectors and DLSI (differential latent semantics index) spaces.
A simple posteriori calculation using the intra- and extra-document statistics demonstrates the advantage of the DLSI space-based probabilistic classifier over the popularly used LSI space-based classifier in classification performance.
This paper introduces a new type of grammar learning algorithm, inspired by string edit dis-tance (Wagner and Fischer, 1974).
This paper describes a modular connectionist model of the acquisition of receptive inflectional morphology.
In its simplest version, the network consists of separate simple recurrent subnetworks for root and inflection identification; both networks take the phone sequence as inputs.
In a more elaborate version of the model, the network learns to use separate hidden-layer modules to solve the separate tasks of root and inflection identification.
We describe the CoNLL-2003 shared task: language-independent named entity recognition.
This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
We argue that probabilistic generative grammars are demonstrably a more psychologically realistic model of phonological competence than standard generative phonology or Optimality Theory.
in a prepositional phrase.
This paper represents initial work on corpus methods for acquiring lexical/semantic pattern lexicons for text understanding.
In this paper, we propose an optimized strategy, called Bottom-Up Filtering, for parsing GPSGs.
This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, part-of-speech and a small-vocabularycharacter-lists feature for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model.
We look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic relatedness plays a role in discourse structure.
After description of the tag system used, we show the results of four experi-ments using a simple probabilistic model to tag Czech texts (unigram, two bigram experiments, and a trigram one).
The experiments use the source channel model and maxi-mum likelihood training on a Czech hand-tagged corpus and on tagged Wall Street Journal (WSJ) from the LDC collection.
In order to compare two different approaches to text tagging ?statistical and rule-based ?we modified Eric Brill's rule-based part of speech tag-ger and carried out two more experiments on the Czech data, obtaining similar results in terms of the error rate.
Our system performs two procedures: Out-of-vocabulary extraction and word segmenta-tion.
We compose three out-of-vocabulary extraction modules: Character-based tag-ging with different classifiers ?maximum entropy, support vector machines, and con-ditional random fields.
We also com-pose three word segmentation modules ?character-based tagging by maximum en-tropy classifier, maximum entropy markov model, and conditional random fields.
We describe the development of a gen-erator for German built by reusing and adapting existing linguistic data and software.
In the context of lexicalized grammars, we propose general methods for lexical disambiguation based on polarization and abstraction of grammatical formalisms.
We describe the architecture of the ILEX system, which supports opportunistic text generation.
This paper presents a glue language account of how negative polarity items (e.g.
Spoken language understanding is a critical component of automated customer service applications.
It adopts dependency decision making and example-based approaches.
憁anner?and roles for nominalmodifiers.
The design of role assignment algorithm is based on the different decision features, such as head-argument/modifier, case makers, sentence structures etc.
It labels semantic roles of parsed sentences.
We describe a resource-based method of morphological annotation of written Korean text.
We show that morphological annotation of Korean text can be performed directly with a lexicon of words and without morpho-logical rules.
We are concerned with dependency- oriented morphosyntactic parsing of running text.
Using SHARDS ~?a semantically-based HPSG approach to the res-olution of dialogue fragments ?~ wewill show how to generate full paraphrases for fragments in dialogue.
This paper shows how higher levels of generalization can be introduced into unification grammars by exploiting methods for typing grammatical objects.
We describe here an algorithm for detecting subject boundaries within text based on a statistical lexical similarity measure.
We also sketch a theoretical foundation for unification- based semantic interpretation, and compare the unification-based approach with more conventional techniques based on the lambda calculus.
Named entity recognition is a fundamental task in biological relationship mining.
This paperemploys protein collocates extracted from a biological corpus to enhance the performance of protein name recognizers.
This paper presents a method to implicitly resolve ambiguities using dynamic incremental clustering in Korean-to-English cross-language information retrieval.
We then present machine learn-ing results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone.
The syntactic processing model posits four modules, recovering phrase structure, long-distance dependencies, coreference, and thematic structure.
In this paper we discuss cascaded Memory- Based grammatical relations assignment.
In this paper we view Lexicalized Tree Adjoining Grammars as the compilation of a more abstract and modular layer of linguistic description : the metagrammar (MG).
MG provides a hierarchical representation of lexicosyntactic descriptions and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions.
This paper describes an automatic, context-sensitive, word-error correction system based on statistical language modeling (SLM) as applied to optical character recognition (OCR) post- processing.
The system exploits information from multiple sources, including letter n-grams, character confusion probabilities, and word-bigram probabilities.
Letter n-grams are used to index the words in the lexicon.
Given a sentence to be corrected, the system decomposes each string in the sentence into letter n-grams and retrieves word candidates from the lexicon by comparing string n-grams with lexicon-entry n-grams.
Finally, the word-bigram model and Viterbi algorithm are used to determine the best scoring word sequence for the sentence.
In terms of both speed and mem-ory consumption, graph unification remains the most expensive com-ponent of unification-based gram-mar parsing.
This paper describes automatic tech-niques for mapping 9611 entries in a database of English verbs to Word-Net senses.
Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes;(2) word sense probabilities, from frequency counts in a tagged corpus;(3) semantic similarity of WordNet senses for verbs within the same class;(4) probabilistic correlations between WordNet data and attributes of the verb classes.
In this paper we propose a unified framework for automatic evaluation of NLP applications using N-gram co-occurrence statistics.
The course work was organized around four substantial programming assignments in which the students implemented the important parts of several core tools, including language models (for speech reranking), a maximum entropy classifier, a part-of-speech tagger, a PCFG parser, and a word-alignment system.
A computational approach to metonymy and metaphor is proposed that distinguishes between them, literalness, and anomaly.
This paper presents example-based machine translation (MT) based on syntactic transfer, which selects the best translation by using models of statistical machine translation.
In order to incorporate this kind of long distance context dependency, this paper proposes a new MI-Ngram modeling approach.
The MI-Ngram model consists of two components: an ngram model and an MI model.
It is found that MI-Ngram modeling has much better performance than ngram modeling.
In the meanwhile, evaluation on Chinese word segmentation shows that about 35 percent of errors can be corrected by using the MI-Trigram model compared with the trigram model.
Pragmatic.
This paper describes a hybrid Chinese word segmenter that is being developed as part of a larger Chinese unknown word resolution system.
The segmenter consists of two components: a tagging component that uses the transforma-tion-based learning algorithm to tag each character with its position in a word, and a merging component that transforms a tagged character sequence into a word-segmented sentence.
This paper outlines the linguistic semantic cornmitments underlying an application which automatically constructs depictions of verbal spatial descriptions.
The paper proposes an information-theory-based method for feature types analysis in probabilistic evaluation modelling for statistical parsing.
We demonstrate that exploiting the contextual information in the messages can noticeably improve email-act classification.
of deep cases relations (or thematic relations).
The query engine uses symbolic case-based reasoning techniques, while the generation module integrates canned text, templates, and grammar rules to produce texts and hypertexts in a simple way.
The original Rosetta grammar formalism, called M-gramrnars, was a computational variant of Montague grammar.
The result of the M-Parser algorithm is a syntactic derivation tree which reflects the history of the analysis process.
The M-Generator algorithm generates a set of S-trees by bottom-up application of M-rules, the names of which are mentioned in a syntactic derivation tree.
Next, each semantic derivation tree is mapped onto a set of S-trees of the target language.
The new formalism was called controlled M-grammars.
For the analysis part we use an ITPSG-based (Pollard and Sag 87) syntax and semantics that is further developed for German.
85.
We introduce a new approach to create concept-based text representations, and apply it to a standard text categorization collection.
This paper describes a statistical model for extraction of events at the sentence level, or "semantic tagging", typically the first level of processing in Information Extraction systems.
solving a task requiring data retrieval.
Our approach is the reverse- engineer text categorization to supply mappings from ordinary language vocabulary to specialist vocabulary by constructing maximum likelihood mappings between words and phrases and classification schemes.
We apply BioAR to the protein names in the biological interactions as extracted by our biomedical information extraction system, or BioIE, in order to construct protein pathways automatically.
This paper describes efforts to improve an automatic tagging system which identifies and classifies discourse markers in Chinese texts by applying machine learning (ML) to the disambiguation of discourse markers, as an integral part of automatic text summarization via rhetorical structure.
Encouraging results are reported.Keywords: discourse marker, Chinese corpus, rhetorical relation, automatic tagging, machine learning
We consider a logicist approach to natural language understanding based on the translation of a quasi-logical form into a temporal logic, explicitly constructed for the representation of action and change, and the subsequent reasoning about this semantic structure in the context of a background knowledge theory using automated theorem proving techniques.
The approach is substantiated through a proof-ofconcept question answering system implementation that uses a head-driven phrase structure grammar developed in the Linguistic Knowledge Builder to construct minimal recursion semantics structures which are translated into a Temporal Action Logic where both the SNARK automated theorem prover and the Allegro Prolog logic programming environment can be used for reasoning through an interchangeable compilation into first-order logic or logic programs respectively.
This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser.
This paper explores the automatic construction of a multilingual Lexical Knowledge Base from pre-existing lexical resources.
We present a new and robust approach for linking already existing lexical/semantic hierarchies.
The Smooth Injective Map Recognizer (SIMR) algorithm presented here is a generic pattern recognition algorithm that is particularly well-suited to mapping bitext correspondence.
This paper introduces a new approach to morpho-syntactic analysis through Humor 99 (High-speed Unification Morphology), a reversible and unification-based morphological analyzer which has already been integrated with a variety of industrial applications.
Occurence patterns of words in documents can be expressed as binary vectors.
We present minimally supervised methods for training and testing geographic name disambiguation (GND) systems.
In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci.
For example, we use the form "X of Y" for estimating referents of demonstrative adjectives.
We describe a re- estimation process which uses the accumulated counts of hypernyms of the alternative senses in order to redistribute the count.
This paper presents the results of automatically inducing a Combinatory Categorial Grammar (CCG) lexicon from a Turkish dependency treebank.
This paper introduces a new paradigmatic similarity measure and presents a minimally supervised learning approach combining effective se-lection and weighting methods based on paradigmatic and contextual similarity measures populated from large quanti-ties of inexpensive raw text data.
Language processing of the corpus texts so far included morpho-syntactic analysis, POS tagging and shallow syntactic parsing.
The main application of name searching has been name matching in a database of names.
This paper investigates semantic and pragmatic presupposition in Discourse Representation Theory (DRT) and enhances the pragmatic perspective of presupposition in DRT.
We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text.
One method is adapted from POS tagging, the other is based on finite state transducers.We show experimental results for letter e on the French version of the Medical Subject Headings thesaurus.
Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling.
We describe a program for assigning correct stress contours to nominals in English.
We have also investigated the related issue of parsing complex nominals in English.
This paper presents a system that detects various types of disfluencies and other structural information with cues obtained from lexical and prosodic information sources.
Specifically, combinations of decision trees and language models are used to predict sentence ends and interruption points and, given these events, transformation- based learning is used to detect edit disfluencies and conversational fillers.
We evaluate the inequality ME model using text categorization datasets.
We address the text-to-text generation problem of sentence-level paraphrasing ?a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing.
Our approach applies multiple-sequence alignment to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by word lattice pairs and automatically determines how to apply these patterns to rewrite new sentences.
This paper describes a prototype for automatically scoring College Board Advanced Placement (AP) Biology essays.'.
One hundred training essays were used to build an example-based lexicon and concept grammars.
Thus the proper mode of representation for discourse particles in a system coincides with the framework of cooperative question-answering.
?Edison Invented the telegraph.
discourse particles, onto subsets of DR.
We present a novel sentence reduction system for automatically removing extraneous phrases from sentences that are extracted from a document for summarization purpose.
This paper proposes a method for generating a logicalconstraint-based internal representation from a unification grammar formalism with disjunctive information.
We present our work on open-domain multi-document summarization in the framework of Web search.
We present a task-based extrinsic evaluation of the quality of the produced multi-document summaries.
We describe our initial investigations into generating textual summaries of spatiotemporal data with the help of a prototype Natural Language Generation (NLG) system that produces pollen forecasts for Scotland.
We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT shared- task.
This paper describes a new templaterepresentation and generalization method.
Combing a semantic diction-ary, it uses multiple semantic codes to represent a paraphrase template.
This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs).
We compare the performance of the learned PCFG grammars and log linear models over the same features.
We propose a lexical organisation for multilingual lexical databases (MLDB).
We also present our current work in defining and prototyping a specialised system for the management of acception-based MUM.Keywords: multilingual lexical database, acception, linguistic structure.
This paper describes an alternative translation model based on a text chunk under the framework of statistical machine translation.
The translation model suggested here first performs chunking.
This paper deals with the discovery, representation, and use of lexical rules (LRs) during large-scale semi-automatic computational lexicon acquisition.
One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.
In this paper we describe a method to determine bilingual word classes suitable for statistical machine translation.
We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
This paper then applies the graph-structured stack to various natural language parsing methods, including ATN, LA parsing, categorial grammar and principle- based parsing.
The paper gives an overview of repair sequences used in Estonian spoken information dialogues.
The lexicon is defined in terms of a lexicalized Tree Adjoining Grammar, which is subsequently mapped to a FS representation.
We present results using Parsli on an application that creates 3D-images from typed input.
-- the problem of diacritics and digraphs.
We present a scheme for efficiently representing a lexicalized tree-adjoining grammar (LTAG).
Furthermore, the lexical and syntactic rules can be used to derive new elementary trees from the default structures specified in the hierarchical lexicon.In the envisaged scheme, the use of a hierarchical lexicon and of lexical and syntactic rules for lexicalized tree-adjoining grammars will capture important linguistic generalizations and also allows for a space efficient representation of the grammar.
We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus.
We propose a solution to the annotation bottleneck for statistical parsing, by exploiting the lexicalized nature of Combinatory Categorial Grammar (CCG).
The parsing model uses predicate-argument dependencies for training, which are derived from sequences of CCG lexical categories rather than full derivations.
This paper presents an approach to identifying conjuncts of coordinate conjunctions appearing in text which has been labelled with syntactic and semantic tags.
We describe the CoNLL-2002 shared task: language-independent named entity recogni-tion.
This paper presents an LTAG account for binding of reflexives and reciprocals in English.
These include a QA Typology with answer patterns, WordNet, information about typical numerical answer ranges, and semantic relations identified by a robust parser, to filter out likely-looking but wrong candidate answers.
We present a new grammatical formalism called Constraint Dependency Grammar (CDG) in which every grammatical rule is given as a constraint on wordto-word modifications.
CDG parsing is formalized as a constraint satisfaction problem over a finite domain so that efficient constraint-propagation algorithms can be employed to reduce structural ambiguity without generating individual parse trees.
In this paper, we propose ellipsis handling method for follow-up questions in Information Access Dialogue (IAD) task of NTCIR QAC3.
We report here the contribution of default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, selectional preferences, and topics or subject fields.
This paper presents baseline unsupervised techniques for performing word alignment based on geometric and word edit distances as well as supervised fusion of the results of these techniques using the nearest neighbor rule.
This paper describes a word and phrase alignment approach based on a dependency analysis of French/English parallel corpora, referred to as alignment by 搒yntax-based propagation.
This paper shows the effectiveness of the external resources (bilingual dictionary and word list) for Chinese word segmentations.
We show how adjunction allows us to clexicalize' a CFG freely.We then show how a `lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for `lexicalized' grammars is discussed.
Zellidja grant.
lexical rules in LFG, used also by HPSG, or Gross 1984's lexicon- grammar).
We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases.
FERGUS uses twodistinct stochastic models, a tree model which refersto a grammar, and a linear language model.
We useautomatic grammar extraction techniques to extractgrammars from different-sized tree banks, and thenuse these extracted grammars to train the tree mod-els.
Major components of the structure are discourse relations and "rhetorical types" (pragmatic objects similar to speech acts) to be discovered via paraphrase relations.
As illustration, we list the discourse relations and rhetorical types needed to generate a paragraph-length discourse studied by Mann and Thompson.
The SENSEVAL-3 task to perform word-sense disambiguation of WordNet glosses was designed to encourage development of technology to make use of standard lexical resources.
This paper describes a new method for aligning real bilingual texts using sentence pair location information.
In the GPSG framework.
In word sense disambiguation, a system attempts to determine the sense of a word from contextual features.
We call our system SNOOD (Hop-kins APL Inductive Retargetable Named Entity Tagger) .
This paper	presents	a	declarative,	dependencyconstraint model for parsing an inflectional free word order language, like Finnish.
In this paper we demonstrate that speech recognition can be effectively applied to information retrieval (IR) applications.
Termed "Semantic Co-occurrence Filtering" this enables the system to simultaneously disambiguate word hypotheses and find relevant text for retrieval.
The system is built by integrating standard IR and speech recognition techniques.
The paper is concerned with automatic classification of new lexical items into synonymic sets on the basis of their co- occurrence data obtained from a corpus.
Specifically, the following components of the system are described: the syntactic analyzer, based on a Procedural Systemic Grammar, the semantic analyzer relying on the Conceptual Dependency Theory, and the dictionary.
We offer a semantics and pragmatics of the pluperfect in narrative discourse.
In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result.
These probabilities are estimated using statistical decision tree models.
This paper reports on two contributions to large vocabulary continuous speech recognition.
In this paper we present word sense disambiguation (WSD) experiments on ten highly polysemous verbs in Chinese, where significant performance improvements are achieved using rich linguistic features.
For a few verbs, semantic role information actually hurt WSD performance.
Our character-based best match retrieval method can retrieve translation examples similar to the given input.
We show the retrieval examples with the following characteristic features: phrasal expression, long-distance dependency, idiom, synonym, and semantic ambiguity.
This paper presents Archivus, a multi- modal language-enabled meeting browsing and retrieval system.
We present two semi-supervised learning techniques to improve a state-of-the-art multi-lingual name tagger.
We describe effective measures to automatically select documents and sentences.
We compare two wide-coverage lexicalized grammars of English, LEXSYS and XTAG, finding that the two grammars exploit EDOL in different ways.
We present a novel approach to parsing phrase grammars based on Eric Brill's notion of rule sequences.
Our information extraction prototype, Alembic, is in fact based on a pipeline of rule sequence processors thatrun the gamut from part-of-speech tagging, to phrase identification, to sentence parsing, to inference (Aberdeeen et al.
The morphological rules arc followed by contextual transformations: these rules inspect lexical context to relabel lexemes that are ambiguous with respect to part-of-speech.
This paper describes a prototype news analysis system which classifies and indexes news stories in real time.
Approaches are wide and include key wording, statistical analysis, pattern matching, and a method using lexical, syntactic, and semantic filters.
1988/ does describe a strictly pattern matching approach to news categorization.)
Several example stories and their indexes are also provided.3 0 The Architecture of NASNAS consists of four major subsystems, viz., a stream filter, a lexical scanner, a parser, and a semantic processor or filter working sequentially as listed.
This paper presents a statistical method for finger-printing text.
Our method exploits the characteristic distribution of word trigrams, and measures to determine similarity are based on set theoretic principles.
Compound noun phrases provide typical examples.
Traditionally, supervised machine learning approaches adopt the single- candidate model.
By contrast, our approach adopts a twin-candidate learning model.
The impact of word alignment on MT quality is investigated, using a phrase-based MT system.
This paper discusses a system for grammat-ically describing and parsing entries from machine-readable dictionary tapes and a lexical data base representation for storing the dictionary information.
We describe the design and implementation of the dialogue management module in a voice operated car-driver information system.
This paper reports on ongoing worka CALL system to facilitate foreign lan-guage learning: GLOSSER-RuG.
Follow-ing a brief introduction to the project, the paper describes the architecture of GLOSSER-RuG.
This paper introduces to the finite-state calculus a family of directed replace operators.
Other useful applications of directed replacement include tokenization and filtering of text streams.
This paper proposes a statistical, treeto-tree model for producing translations.
Two main contributions are as follows: (1) a method for the extraction of syntactic structures with alignment information from a parallel corpus of translations, and (2) use of a discriminative, feature- based model for prediction of these target- language syntactic structures梬hich we call aligned extended projections, or AEPs.
This paper presents an approach for detecting semantic relations in noun phrases.
A learning algorithm, called semantic scattering, is used to automatically label complex nominals, genitives and adjectival noun phrases with the corresponding semantic relation.
In this paper, the search engine Intuition is described.
In order to realize multi-document summarization focused by multiple questions, we propose a method to calculate sentence importance using scores produced by a Question-Answering engine in response to multiple questions.
We also describe an integration of it into a generic multi-document summarization system.
This paper presents our work on accumulation of lexical sets which includes acquisition of dictionary resources and production of new lexical sets from this.
The paper also proposes a combinatorial and dynamic organization for lexical systems, which is based on the notion of virtual accumulation and the abstraction levels of lexical sets.Keywords: dictionary resources, lexicalacquisition,	lexical	production,	lexicalaccumulation, computational lexicography.
Unlike conventional methods that use optical character recognition (OCR), we con-vert document images into word shape tokens, a shape-based representation of words.
We report the impact of speech recognition errors on speech act identification and discuss how standard control mechanisms can participate to robustness by assisting the user in repairing the consequences of speech recognition errors.
This paper addresses the problem for a fairly general form of Combinatory Categorial Grammar, by means of an efficient, correct, and easy to implement normal-form parsing technique.
Second, we develop both a two-step approach which combines a trace tagger with a state-of-the-art lexicalized parser and a one-step approach which finds non- local dependencies while parsing.
The first method involves generating a set of (unimodal) PELs for a given speaker by clustering the hypothetical frames found in the spectral models for that speaker, and then constructing speaker-dependent PEL sequences to represent each PIC.
We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic.
Additionally, we explore the utility of punctuation in helping parsing and extraction of subcategorization cues.
Our experiments show that punctuation is of little help in parsing spoken language and extracting sub- categorization cues from spoken language.
This paper describes the English朒indi Multilingual lexical sample task in SENSEVAL?.
A method for generating a machine translation (MT) dictionary from parallel texts is described.
We report on a head-driven way to generate a language- specific representation for a language-independent conceptual structure.
This paper presents LexPhon, a computational framework for modelling segmental aspects of Lexical Phonology (LP).
This paper describes a text mining technique to aid an Ontology Engineer to identify the important concepts in a Domain Ontology.
We present a novel approach to the problem of overfitting in the training of stochastic mod-els for selecting parses generated by attribute-valued grammars.
We also developed an alignment algorithm of graphemes and phonemes for both ordinary text and OCR out-put.
We show, by experiment, that the combination of the grapheme-phoneme tuple ngram model and the grapheme-phoneme alignment algorithm significantly improve character recognition accuracy if both grapheme and phoneme representations are given.
It is embedded to the C-value approach for automatic term recognition (ATR), in the form of weights constructed from statistical characteristics of the context words of the candidate string.
The usefulness of a statistical approach suggested by Church and Hanks (1989) is evaluated for the extraction of verb-noun ( V-N) collocations from German text corpora.
We present precision and recall results for V-N collocations with support verbs and discuss the consequences for further work on the extraction of collocations from German corpora.
This paper describes an n-gram based reinforcement approach to the closed track of word segmentation in the third Chinese word segmentation bakeoff.
Character n-gram features of unigram, bigram, and trigram are extracted from the training corpus and its frequencies are counted.
Definite and loose segmentation are performed simply based on the bigram and trigram statistics.
We describe a distributed, modular architecture for platform independent natural language systems.
It features automatic interface generation and self-organization.
We present a constancy rate principle governing language generation.
This paper discusses interactions between negative concord and restructuring/clause union in Palestinian Arabic.
Analysis of a subset of tickets, guided by sublanguage theory, identified linguistic patterns, which were translated into rule-based algorithms for automatic identification of tickets?discourse structure.
Anaphora resolution for dialogues is a difficult problem because of the several kinds of complex anaphoric references generally present in dialogic discourses.
In this paper, we describe a system for anaphora resolution in multi-person dialogues.
In our system, we propose a new technique based on the use of anaphora chains to enable resolution of a large variety of anaphors, including plural anaphora and cataphora.
Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based methods to the output of an Automatic Speech Recognition (ASR) system.
In this paper, we propose a hybrid approach to chunking Chinese base noun phrases (base NPs), which combines SVM (Support Vector Machine) model and CRF (Conditional Random Field) model.
This paper presents the integration of a large-scale, reusable lexicon for generation with the FUF/SURGE unification-based syntactic realizer.
Summarization and Question Answering need precise linguistic information with a much higher coverage than what is being offered by currently available statistically based systems.
The heart of the system is a rule-based top-down DCG-style parser, which uses an LFG oriented grammar organization.
Most natural language processing tasks require lexical semantic information.
This paper describes an acquisition method which makes use of fixed correspondences between derivational affixes and lexical semantic information.
We begin by proposing a typology of paraphrases.
This paper discusses an approach to planning the content of instructional texts.
In this paper we describe a way to discover Named Entities by using the distribution of words in news articles.
This paper describes an hierarchical approach to WordNet sense distinctions that provides different types of automatic Word Sense Disambiguation (WSD) systems, which perform at varying levels of accuracy.
Finally, we discuss how lexical selection is influenced by thematic (focus) information in the input.
The objective of this paper is to present a new dimension of Game Theoretic Semantics (GTS) using the idea of the coordination problem game to explain the semantics of metaphor.
Research in Named Entity extraction is no exception.
We report results for training and test-ing an automatic classifier to label the in-formation provider抯 utterances in spoken human-computer and human-human dia-logues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.
In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue.
This paper focuses on the use of Finite State morphological analysis (FST) resources for Irish and Part of Speech (POS) taggers for German.
Latent Semantic Analysis (LSA) is a statistical Natural Language Processing (NLP) technique for inferring meaning from a text.
We introduce a first-order language for semantic underspecification that we call Constraint Language for Lambda-Structures (CLLS).
CLLS features constraints for dominance, lambda binding, parallelism, and anaphoric links.
In this paper, we apply this technique to build micro- planning rules for preventative expressions in instructional text.
We call sentences like (2) semantically disambiguatable garden path sentences (SDGPs).
We address the problem of clustering words (or constructing a thesaurus) based on co-occurrence data, and using the acquired word classes to improve the accuracy of syntactic disambiguation.
We view this problem as that of estimating a joint probability distribution specifying the joint probabilities of word pairs, such as noun verb pairs.
We propose an efficient algorithm based on the Minimum Description Length (MDL) principle for estimating such a probability distribution.
This paper proposes a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse.
We investigate the extension of basic feature logic with subsumption (or matching) constraints, based on a weak notion of subsumption.
This paper presents a way to compute rela-tive social status of the individuals involved in Korean dialogue.
Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results.
This paper explores the role of information retrieval in answering 搑elationship?questions, a new class complex information needs formally introduced in TREC 2005.
The paper describes GEMS, a system for Generating and Expressing the Meaning of Sentences, focussing on the generation task, i.e.
GEMS is lexically distributed.
In this paper we consider the problem of analysing sentence-level discourse structure.
We introduce discourse chunking (i.e., the identification of intra-sentential nucleus and satellite spans) as an alternative to full-scale discourse parsing.
We also demonstrate how discourse chunking can be successfully applied to a sentence compression task.
We present a new compositional tense-aspect deindexing mechanism that makes use of tense trees as components of discourse contexts.
In this paper, we address the problem of dealing with a large collection of data and propose a method for text classification which manipulates data using two well-known machine learning techniques, Naive Bayes(NB) and Support Vector Machines(SVMs).
We present an extension of the classic A* search procedure to tabular PCFG parsing.
The strategic lazy incremental copy graph unification method is a combination of two methods for unifying feature structures.
This paper describes a method to automatically create and maintain gazetteers for Named Entity Recognition (NER).
Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).
This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).
This paper studies and evaluates disambiguation strategies for the translation of tense between German and English, using a bilingual corpus of appointment scheduling dialogues.
It describes a scheme to detect complex verb predicates based on verb form subcategorization and grammatical knowledge.
We introduce BLANC, a family of dynamic, trainable evaluation metrics for machine translation.
Towards this end, we discuss ACS (all common skipngrams), a practical algorithm with trainable parameters that estimates reference- candidate translation overlap by computing a weighted sum of all common skipngrams in polynomial time.
We have found that it is possible to identify many of these interesting co-occurrence relations by computing simple summary statistics over millions of words of text.
This paper describes the roles of disjunctions and inheritance in the use of feature structures and their formal semantics.
We illustrate the approach for the acquisition of lexical information for several classes of nomi n als.Keywords: Knowledge Acquisition, Information Retrieval, Lexical Semantics.
This paper investigates stacking and voting methods for combining strong classifiers like boosting, SVM, and TBL, on the named-entity recognition task.
Finally, we combine the sense proximity estimation with a classification of semantic relations between senses.
Most recent research in trainable part of speech taggers has explored stochastic tagging.
In this paper, we describe a number of extensions to this rule-based tagger.
First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express.
We present a procedure which, given a cfg and phrase-structure semantics for a source language and a cfg and phrase-structure semantics for a target language, will (usually) produce the finite set of ttee-replacement rules for tne translation, if the translation exists.
We present an LFG-DOP parser which uses fragments from LFG-annotated sentences to parse new sentences.
We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes.
We have implemented a system for generating and applying corpus-based patterns.
The method is based on a structured probabilistic model of the domain, and unsupervised learning is performed with the EM algorithm.
This paper proposes an input-splitting method for translating spoken-language which includes many long or ill-formed expressions.
In this study we propose a method of segment-ing a sentence.
This paper describes a system for un-supervised learning of morphological af-fixes from texts or word lists.
The system is composed of a generative probability model and a search algorithm.
This paper describes NJFun, a real-time spoken dia-logue system -Out-provides users with information about things to do in New Jersey.
We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.
This paper describes a statistics-based Chinese parser, which parses the Chinese sentences with correct segmentation and POS tagging information through the following processing stages: 1) to predict constituent boundaries, 2) to match open and close brackets and produce syntactic trees, 3) to disambiguate and choose the best parse tree.
of Computing Tech., CAS in the ACL- SIGHAN-sponsored First International Chinese Word Segmentation Bake- off.
The authors introduce the unified HHMM-based frame of our Chinese lexical analyzer ICTCLAS and explain the operation of the six tracks.
To perform this task, we build decision tree classifiers that use a combination of simple speech features (speech lengths and spoken keywords) extracted from the participants?speech in meetings.
We present a method for recognizing semantic role arguments using a kernel on weighted marked ordered labeled trees (the WMOLT kernel).
We propose a plan-based approach for responding to user queries in a collaborative environment.
We present a novel application of NLP and text mining to the analysis of financial documents.
In this paper, we introduce a new approach to lexical or-ganization that leads to more compact and flexible lexicons.
We show how a data- driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.
Our dictation system uses character-trigram probabilities as a source model obtained from a text database consisting of both ICanji and Kana, and generates Kanji-and-Kana sequences directly from input speech.
An anaphor resolution algorithm is pre-sented which relies on a combination of strategies for narrowing down and select-ing from antecedent sets for reflexive pro-nouns, nonreflexive pronouns, and com-mon T101MS.
Applications to speech-to-document alignment and more generally to meeting processing and retrieval are finally discussed.
This paper describes the open-source Phrase-Based Statistical Machine Translation Decoder - Phramer.
This paper describes a heuristic approach to automatically identifying which senses of a machine- readable dictionary (MRD) headword are semantically related versus those which correspond to fundamentally different senses of the word.
It provides a large amount of semantic information such as semantic hierarchy and collocation features for 66,539 Chinese words and their English counterparts.
Its POS and semantic classification represent the latest progress in Chinese linguistics and language engineering.
The paper introduces the outline of SKCC, and indicates that it is effective for word sense disambiguation in MT applications and is likely to be important for general Chinese language processing.Key words: Semantic knowledge-base, lexical semantic, computational lexicography, word sense disambiguation ^WSD^ , Chinese language processing
We describe the IG (In-teraction Grammar) formalism, an extension of DT-D's which permits powerful linguistic manipulations, and show its application to the production of multi-lingual versions of a certain class of pharmaceutical documents.
This paper introduces SENSELEARNER ?a minimally supervised sense tagger that attempts to disambiguate all content words in a text using the senses from WordNet.
We propose a question answering system which uses an encyclopedia as a knowledge base.
Then linguistic patterns and HTML structures are used to extract text fragments describing the term.
Covering ambiguity is one of the two basic types of ambiguities in Chinese word segmentation.
We select 90 frequent cases of covering ambiguities as the target.
State of the art in statistical machine translation is currently represented by phrase- based models, which typically incorporate a large number of probabilities of phrase-pairs and word n-grams.
In this work, we investigate data compression methods for efficiently encoding n-gram and phrase-pair probabilities, that are usually encoded in 32-bit floating point numbers.
This paper also proposes an objective method of classifying these con-structs using a large amount of linguistic data.
The feasibility of this was verified with a self-organizing semantic map based on a neural net-work model.
We have participated in three open tracks of Chinese word segmentation and named entity recognition tasks of SIGHAN Bakeoff3.
Our named entity recognizer achieved the highest F measure for MSRA, and word segmenter achieved the medium F measure for MSRA.
We find effective combining of the external multi-knowledge is crucial to improve performance of word segmentation and named entity recognition.
Finally, I outline a theory of lexical semantics embodying a richer notion of compositionality, termed cocomposition, which aims to spread the semantic load more evenly throughout the lexicon.
We investigate the lexical and syntactic flexibility of a class of idiomatic expressions.
We develop measures that draw on such linguistic properties, and demonstrate that these statistical, corpus-based measures can be successfully used for distinguishing idiomatic combinations from non-idiomatic ones.
This paper introduces the new grammar formalism of Extensible Dependency Grammar (XDG), and emphasizes the benefits of its methodology of explaining complex phenomena by interaction of simple principles on multiple dimensions of linguistic description.
problem oriented systems.
The approach to knowledge representation taken in a multi-modal multi-domain dialogue system - SMARTKOM - is presented.
Prepositions and verbs are defined as semantic functions that explicate spatial relations among noun images.
domain.
system to analyze Hungarian texts using a morphological.
MORPHOLOGICAL ANALYSISThe first phase is the morphological analysis of word forms.
SIOMS from the archiphonemes of the lexicon.
the grammatical morphemes  of the language.
RULES  normalized sequence of categoriesPARSERinput sentence [MORPHOLOGICAL ANALYZER.
Third, we propose two models (BOTW and BOF) which use domain knowl-edge as textual features for text catego-rization.
We describe the generation of communicative ac-tions in an implemented embodied conversational agent.
User modeling is an important components of dialog systems.
Most previous approaches are rule-based methods.
This paper analyses the intonation of polar questions extracted from a corpus of task- oriented dialogues in the Bari variety of Italian.
This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs).
This paper extends the base noun phrase(BNP) identification into a research on Chinese base phrase identification.
This paper discusses word choice for natural language generation.
The paper describes a new unification based grammar formalism called Regular Unification Grammar (RUG).
The RUG formalismRUG constitutes a unification based grammar formalism /Shieber86/.
This paper describes the formalism for incorporating emerging linguistic theory in a joint model of the acoustic/prosody/concept relationships.
We present a novel approach to the unsupervised detection of affixes, that is, to extract a set of salient prefixes and suffixes from an unlabeled corpus of a language.
In this paper we compare the use of machine translation (MT) to the commonly used dictionary term lookup (DTL) method for Reuter news article alignment in English and Japanese.
This paper presents Japanese morphological analysis based on conditional random fields (CRFs).
First, flexible feature designs for hierarchical tagsets become possible.
We have developed a Text Structurer module which recognizes text-level structure for use within a larger information retrieval system to delineate the discourse-level organization of each document's contents.
An algorithm for the morphological decomposition of words into morphemes is presented.
The application area is information retrieval, and the purpose is to find morphologically related terms to a given search term.
First, the parsing framework is presented, then several linguistic decisions are discussed: morpheme selection and segmentation, morpheme classes, morpheme grammar, allomorph handling, etc.
A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates.
Bagging and boosting, two effective machine learn-ing techniques, are applied to natural language pars-ing.
An automatic treebank conversion method is proposed in this paper to convert a treebank into another treebank.
It con-sists of an approximate word match-ing method and an N-best word seg-mentation algorithm using a statistical language model.
We present an implemented compilation algorithm that translates HPSG into lexicalized feature-based TAG, relating concepts of the two theories.
We propose to handle this by breaking up the morhosyntactic tags into inflectional groups, each of which contains the inflectional features for each (interme(hiate) derived form.
Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflection groups in a trigram model.
The basic mechanism of CLaRK for linguistic pro-cessing of text corpora is the cascade regular gram-mar processor.
In this paper, we present the design of a lexical resource focusing on German verb phrase idioms.
Dictionary-based searching is useful for retrieving biological information in gene units.
In our laboratory, we have developed a gene name dictionary:GENA and a family name dictionary.
The effect of our gene/protein/family recognition method on protein-interaction and protein-function ex-
We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG.
This paper presents a technique for sentence genera-tion.
In this paper we present an approach to structure learning in the area of web documents.
This is done in order to approach the goal of webgenre tagging in the area of web corpus linguistics.
Much previous research on topic tracking use machine learning techniques.
This paper presents an empirical analysis of prosodic phenomena (intonation and timing) in 'common ground units' (Nakatani & Traum 1999).
ASL natural language generation (NLG) is a special form of multimodal NLG that uses multiple linguistic output channels.
In this paper, we identify semantic, pragmatic and syntactic features that are required to support a motivated choice of German temporal subordinating conjunctions and prepositions during text production.
A Chinese word segmentation algorithm based on forward maximum matching and word binding force is proposed in this paper.
This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction.
The proposed method builds an explicit error model for word pronunciations.
By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction.
The system makes use of partial and full syntactic information and converts the task into a sequential BIO-tagging.
Building on a state-of-the-art set of features, a binary classifier for each label is trained using AdaBoost with fixed depth decision trees.
This paper presents an HMM-based chunk tagger for Hindi.
Contextual information is incorporated into the chunk tags in the form of part-of-speech (POS) information.
We use logical inference techniques for recognising textual entailment.
This paper discusses one of the problems of machine translation, namely the translation of idioms.
In this paper, we discuss how a paraphrase maw be used as a heuristic device, viz.
We describe an experimental instruction swaths in mathematics incorporating this, feature.
We present the new multilingual version of the Columbia Newsblaster news summarization system.
In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.
In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web.
We start with an unlexicalized PCFG as a baseline model, which is enriched to the level of Collins?Model 2 by adding lexicalization and subcategorization.
Using data from WordNet, we generate 6 types of vocabulary questions.
This paper describes an approach to extract the aspectual information of Japanese verb phrases from a monolingual corpus.
We introduce an 搊racle?score, based on the probability distribution of unigrams in human summaries.
The paper presents a concise description of the LFG-ParserGenerator developed at the EWH in Koblenz.
We then demonstrate a three-way connection between relative frequency of similar words, a concept of distributional gnerality and the semantic relation of hyponymy.
We describe and present evaluation results for Talk抧扵ravel, a spoken dialogue language system for making air travel plans over the telephone.
We present an unsupervised approach to recognizing discourse relations of CON-TRAST, EXPLANATION-EVIDENCE, CONDITION and ELABORATION that hold between arbitrary spans of texts.
A new approach to bottom-up parsing that extends Augmented Context-Free Grammar to a Process Grammar is formally presented.
A Process Grammar (PG) defines a set of rules suited for bottom-up parsing and conceived as processes that are applied by a PG Processor.
The aim of this paper is to introduce a new approach to bottom-up parsing starting from a well known and based framework - parallel bottom-up parsing in immediate constituent analysis, where all possible parses are considered - making use of an Augmented Phrase-Structure Grammar (APSG).
PARSE GRAPH STRUCTUREThe Parse Graph Structure (PGS) is built by the parser while applying grammar rules.
are described.Keywords: Computational Lexicography, Lexical Knowledge Base, Lexical Semantics.
This paper presents a method of automatically constructing information extraction patterns on predicate-argument structures (PASs) obtained by full parsing from a smaller training corpus.
The results demonstrated that our method improved accuracy compared to a machine learning approach using surface word/part-of-speech patterns.
Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality.
In particular, we describe an algorithm to obtain the semantic dependencies on a TAG parse forest and construct a target derivation forest with isomorphic or locally non-isomorphic dependencies in 0(n7) time.
Complexity theoretic notion of feasible learnability called "polynomial learnability" to the evaluation of grammatical formalisms for linguistic description.
Specifically, we present a novel, nontrivial constraint on grammacs called "k-locality", which enables a rich class of mildly context sensitive grammars called Ranked Node Rewriting Grammars (RNR( ) to be feasibly learnable.
We describe a parallel implementation of a chart parser for a shared-memory multipro-cessor.
We extend the existing corpus-based measures for identifying LVCs between verb-object pairs in English, by proposing using new features that use mutual information and assess other syntactic properties.
This paper demonstrates two methods to improve the performance of instance- based learning (IBL) algorithms for the problem of Semantic Role Labeling (SRL).
Two IBL algorithms are utilized: k-Nearest Neighbor (kNN), and Priority Maximum Likelihood (PML) with a modified back-off combination method.
In this paper we exhibit a novel approach to the problems of topic and speaker identification that makes use of a large vocabulary continuous speech recognizer.
Our method is based on stochastic incremental parsing, and it incrementally evaluates the validity for each partial parse tree on a wordby-word basis.
In our method, incremental parser returns partial parse trees at the point where the validity for the partial parse tree becomes greater than a threshold.
Then we will show how syntactic decisions interact with an intonation grammar.
We shall also introduce two functional notions: STRUCTURE REVERSIBILITY vs. FUNCTIONAL REVERSIBILITY in Italian.
In this paper, we will present the referent identification component of XTRA, a system for a natural-language access to expert systems.
Lexical rules are used in constraint-based grammar formalisms such as Head-Driven Phrase Structure Grammar (IIPSG) (Pollard and Sag 1994) to ex-press generalizations among lexical en-tries.
In this paper, we present a parser based on a stochastic structured language model (SLM) with aflexible history reference mechanism.
An SLM is an alternative to an n-gram model as a language model for a speech recognizer.
Thus SLMs are expected to play an important part in spoken language understanding systems.
We introduce a flexible history reference mechanism called an ACT (arboreal context tree; an extension of the context tree to tree-shaped histories) and describe a parser based on anSLM with ACTs.
This paper describes how a machine- learning named entity recognizer (NER) on upper case text can be improved by using a mixed case NER and some unlabeled text.
We present algorithms for constructing automata from elementary structures, merging and minimising them, and string recognition and parse recovery with the resulting grammar.
?fhis paper proposes a new method for Chinese language corpus processing.
Unlike the past researches, our approach has following charactericsties : it blends segmentation with tagging and integrates rule-based approach with statistics-based one in grammatical disambiguation.
In this article we describe research on the development of large dictionaries for natural language processing.
We describe the process of restructuring the information in the dictionary and our use of the Longman grammar code system to construct dictionary entries for the PATR-H parsing system and our use of the Longman word definitions for automated word sense classification.
in this paper, we present a summarization system for spontaneous dialogues which consists of a. novel multi-stage architecture.
In this paper we present an overview of recent developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic theory of discourse structure.
We give a novel approach to the problem of discourse segmentation based on discourse semantics and sketch a limited but robust approach to symbolic discourse parsing based on syntactic, semantic and lexical rules.
Pronunciation-by-analogy (PbA) is an emer-ging technique for text-phoneme conversion based on a psychological model of read-ing aloud.
This paper explores the impact of certain basic implementational choices on the performance of various PbA mod-els.
We present a technique which complements Hidden Markov Models by incorporating some lexicalized states representing syntactically uncommon words.
A method is presented for segmenting text into subtopic areas.
The lexical cohesion relations of reiteration and collocation are used to identify related words.
These relations are automatically located using a combination of three linguistic features: word repetition, collocation and relation weights.
Prepositional Phrase-attachment is a common source of ambiguity in natural language.
We propose to solve the PP-attachment ambiguity with a Support Vector Machines learning model that uses complex syntactic and semantic features as well as unsupervised information obtained from the World Wide Web.
This paper presents an algorithm for extracting invertible probabilistic translation grammars from bilingual aligned and linguistically bracketed text.
This paper proposes a Japanese/English cross- language information retrieval (CUR) system targeting technical documents.
To counter the first problem, we use a compound word translation method, which uses a bilingual dictionary for base words and collocational statistics to resolve translation ambiguity.
For the second problem, we propose a transliteration method, which identifies phonetic equivalents in the target language.
One model with some experimental support uses distributional statistics of sound sequence predictability (Saffran et al.
We briefly describe a two-way speech-tospeech English-Farsi translation system prototype developed for use in doctor- patient interactions.
This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al.
The Chinese character-based maximum entropy model, which switches the word segmentation task to a classification task, is adopted in system developing.
This paper describes the technology and an experiment of subcategorization acquisition for Chinese verbs.
A crucial problem in topic recognition is how to identify topic continuation.
We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation.
Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora.
We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.
This paper describes an innovative evaluation regimen developed for the text meaning representations (TMRs) produced by the Ontological Semantic (OntoSem) general	purpose	syntactic-semanticanalyzer.
The overall architecture of the OntoSem semantic analyzer.
We address the problem of automati-cally acquiring case frame patterns (se-lectional patterns) from large corpus data..
We view the problem of learning case frame patterns as that of learning a multi-dimensional discrete joint distribution, where random vari-ables represent case slots.
This	paper	describes	a	syllable-basedcomputational model for the Korean morphology.
They are morphological transformation and morpheme Identification.
Two-level model and syllable--based formalism focussed on the problem of morphological transformation(13ear88, Cahi90, Kosk83i.
Analysis candidates are generated as a reverse process of word formation rules: morpheme isolation and morphological transformation.
In this paper, I describe SAGE and its components in the context of event descriptions.
This paper uses human verb associations as the basis for an investigation of verb properties, focusing on semantic verb relations and prominent nominal features.
Second, a statistical grammar is used for determining the conceptual roles of the noun responses.
We present prominent syntax- semantic roles and evidence for the usefulness of co-occurrence information in distributional verb descriptions.
The SAMMIE' system is an in-car multi-modal dialogue system for an MP3 application.
A new type of stochastic grammars is introduced for investigation: weakly restricted stochastic grammars.
In this paper, we present a unigram segmentation model for statistical machine translation where the segmentation units are blocks: pairs of phrases without internal structure.
We develop a framework for formaliz-ing semantic construction within gram-mars expressed in typed feature struc-ture logics, including HPSG.
A model of Japanese honorific expressions in situation semantics is proposed.
This paper describes informally an algorithm for the generation from under- and overspecified feature structures.
A new model for statistical translation is presented.
We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers.
First, we investigate how well the addressee of a dialogue act can be predicted based on gaze, utterance and conversational context features.
We present a study of the mappings from semantic content to syntactic ex-pression with the aim of isolating the precise locus and role of pragmatic infor-mation in the generation process.
The study reveals how multilin-gual NLG can be informed by language-specific principles for syntactic choice.
We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.
Our formulation uses a factorization analogous to the standard dynamic programs for parsing.
In this paper we present results on developing robust natural language interfaces by combining shallow and partial interpretation with dialogue management.
The system uses a structural trans-fer approach in translating the domain of IBM computer manuals.
We present a machine learning approach to the problem of extracting roots of Hebrew words.
This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling.
We report two applications of this approach: PP-attachment and POStagging.
This paper argues for the development of parallel treebanks.
This paper describes the symbolic and statistical hybrid approaches to solutions of problems of the previous English-to-Korean machine translation system in terms of the improvement of translation quality.
In this paper, a Generalized Probabilistic Semantic Model (GPSM) is proposed for preference computation.
An effective semantic tagging procedure is proposed for tagging semantic features.
A semantic score function is derived based on a score function, which integrates lexical, syntactic and semantic preference under a uniform formulation.
bilingual dictionaries).
We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system.
We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.
Recent results have established that there is a family of languages that is exactly the class of languages generated by three independently developed grammar formalisms: Tree Adjoining Grammars, Head Grammars, and Linear Indexed Grammars.
We discuss the structural descriptions produced by Combinatory Categorial Grammars and compare them to those of grammar formalisms in the class of Linear Context-Free Rewriting Systems.
In this paper we propose a small set of lexical conceptual relations which allow to encode adjectives in computational relational lexica in a principled and integrated way.
Language understanding work at Paramax focuses on applying general-purpose language understanding technology to spoken language understanding, text understanding, and document processing, integrating language understanding with speech recognition, knowledge-based information retrieval and image understanding.
SPLAT (Sentence Plan Language Authoring Tool) is an authoring tool intended to facilitate the creation of sentence-plan specifications for the Penman natural language generation system.
SPLAT uses an example- based approach in the form of sentence-plan templates to aid the user in creating and maintaining sentence plans.
In this paper we apply conditional random fields (CRFs) to the semantic role labelling task.
We define a random field over the structure of each sentence抯 syntactic parse tree.
They used root-based clusters to substitute for dictionaries in indexing for information retrieval.
This paper presents a new approach to partial parsing of context-free structures.
This mechanism is implemented in an interactive argumentation sys-tem.
We show on the task of non-anaphoric it identification how to overcome these handicaps with the Bayesian Network (BN) formalism.
We view the QA problem as a classification problem and as a re- ranking problem.
Our method integrates two probability parameters to perform zero pronoun detection and resolution in a single framework.
This paper presents the LiLFeS system, an efficient feature-structure description language for HPSG.
The core engine of LiLFeS is an Abstract Machine for Attribute-Value Logics, proposed by Carpenter and Qu.
The paper discusses two implementations of the LiLFeS.
This paper describes an operational se-mantics for DATR theories.
The seman-tics is presented as a set of inference rules that axiomatises the evaluation relation-ship for DATR expressions.
In this paper we present a polynomial time parsing algorithm for Combinatory Categorial Grammar.
The recognition phase extends the CKY algorithm for CFG.
This paper presents a detailed study of the integration of knowledge from both dependency parses and hierarchical word ontologies into a maximum-entropy-based tagging model that simultaneously labels words with both syntax and semantics.
We propose a novel method for inducing monolingual semantic hierarchies and sense clusters from numerous foreign-language-to-English bilingual dictionaries.
The method exploits patterns of non-transitivity in translations across multiple languages.
We then propose a monolingual synonymy measure derived from this aggregate resource, which is used to derive multilinguallymotivated sense hierarchies for monolingual English words, with potential applications in word sense classification, lexicography and statistical machine translation.
In this paper we investigate the incorporation of Tree Adjoining Grammars (TAG) into the systemic framework.
A formalism is presented for lexical specification in unification-based grammars which exploits defeasible multiple inheritance to express regularity, sub- regularity, and exceptions in classifying the properties of words.
Variants of these using feature weighting by entropy reduction were systematically compared, as was the representation of diphthongs (as one symbol or two).
We describe a new algorithm for compiling rewrite rules into FSTs.
We present a phonological probabilistic context-free grammar, which describes the word and syl-lable structure of German words.
Both grammars are evaluated qualitatively showing that probabilistic context-free grammars can contribute linguistic knowledge to phonology.
Generalizing from efforts parsing natural language sentences using the grammar formalism, Dependency Grammar (DG) has been emulated by a context-free grammar (CFG) constrained by grammatical function annotation.
This paper describes an experimental implementation of this approach using unification to realize grammatical function constraints imposed on a dependency structure backbone emulated by a context-free grammar.
is expanding the repertoire of commercial user interfaces by incorporating multimodal techniques combining traditional point and click interfaces with speech recognition, speech synthesis, and gesture recognition.
This article is devoted to the problem of quantifying noun groups in German.
In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.
In this paper, we present a method for stochastic finite-state ma-chine translation that is trained automatically from pairs of source and target utterances.
We have embedded the Japanese-English translation system in a call routing task of unconstrained speech utterances.
A word-based morphological analyzer and a dictionary for recognizing inflected forms of French words have been built by adapting the UDICT system.
This work lays the groundwork for doing French derivational morphology and morphology for other languages.
We applied an automatic lexical acquisition technique over parsed texts to identify semantically similar words.
After that, we made use of this lexical knowledge to resolve coreferent definite descriptions where the head-noun of the anaphor is different from the head-noun of its antecedent, which we call indirect anaphora.
Several parsers have been proposed that utilize the all-subtrees representation (e.g., tree kernel and data oriented parsing).
A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector.
We start by defining a classification schema for adjectives based on their syntactic and semantic properties.
Tree-based approaches to alignment model translation as a sequence of probabilistic operations transforming the syntactic parse tree of a sentence in one language into that of the other.
This paper will focus on the semantic representation of verbs in computer systems and its impact on lexical selection problems in machine translation (MT).
Coverage: The project adopted a corpus-based approach.
The goal of the on-going project described in this paper is evaluation of the utility of Latent Semantic Analysis (LSA) for unsupervised word sense discrimination.
Our results indicate that we can generate 抔ood?summaries even when using only acoustic/prosodic information, which points toward the possibility of text-independent summarization for spoken documents.
It consists of a statistical OCR model, an approximate word matching method using character shape similarity, and a word segmentation algorithm using a statistical language model.
3.3 GB ytes of text.
We present TISC, a multilingual, language- independent and context-sensitive spelling checking and correction system designed to facilitate the automatic removal of non- word spelling errors in large corpora.
Its lexicon is derived from raw text corpora, without supervision, and contains word unigrams and word bigrams.
A second, related theme is the use of speech and text-image recognition to retrieve arbitrary, user-specified information from documents with signal content This paper discusses three research initiatives at PARC that exemplify these themes: a text-image editor[1], a wordspotter for voice editing and indexing [12], and a decoding framework for scanned-document content retrieval[4].
1 The discussion focuses on key concepts embodied in the research that enable novel signal-based document processing functionality.
We present a systematic comparison of memory-based learning (MBL) and support vector machines (SVM) for inducing classifiers for deterministic dependency parsing, using data from Chinese, English and Swedish, together with a variety of different feature models.
The results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models.
In this paper, we propose a tree annotation tool using a parser in order to build a treebank.
We present an unsupervised learning strategy for word sense disambiguation (WSD) that exploits multiple linguistic resources including a parallel corpus, a bilingual machine readable dictionary, and a thesaurus.
The approach is based on Class Based Sense Definition Model (CBSDM) that generates the glosses and translations for a class of word senses.
We consider here the problem of Chinese named entity (NE) identification using statistical language model(LM).
In this research, word segmentation and NE identification have been integrated into a unified framework that consists of several class-based language models.
Our experiments further demonstrate the improvement after seamlessly integrating with linguistic heuristic information, cache-based model and NE abbreviation identification.
This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.
Fine- grained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.
<li> tag for HTML.
This paper describes experiments in Machine Learning for text classification using a new representation of text based on WordNet hypernyms.
Six binary classification tasks of varying difficulty are defined, and the Ripper system is used to produce discrimination rules for each task using the new hypernym density representation.
Rules are also produced with thecommonly used bag-of-words representation,incorporating no knowledge from WordNet.
This paper describes the use of statistical analyses of untagged corpora to detect similarities and differences in the meaning of words in text.
The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling.
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
Most of errors in Korean morphological analysis and POS (Part-of-Speech) tagging are caused by unknown morphemes.
This paper presents a generalized unknown morpheme handling method with POSTAG(POStech TAGger) which is a statistical/rule based hybrid POS tagging system.
The generalized unknown morpheme guessing is based on a combination of a morpheme pattern dictionary which encodes general lexical patterns of Korean morphemes with a posteriori syllable tri-gram estimation.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined.
The characteristics of SLTAG are unique and novel since it is lexically sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars).Then, two basic algorithms for SLTAG are introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outsidelike iterative algorithm for estimating the parameters of a SLTAG given a training corpus.Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars.
We present a memory-based learning (MBL) approach to shallow parsing in which POS tagging, chunking, and identification of syntactic relations are formulated as memory-based modules.
This paper investigates the syntax of extraposition in the HPSG framework.
We present a system for computing similarity between pairs of words.
Our tests focus on the identification of cognates ?words of common origin in related languages.
The methodology is centered on the use of a dependency grammar based parser.
We present a syntax-based statistical translation model.
We describe a case study in the ap-plication of symbolic machine learning techniques for the discovery of linguis-tic rules and categories.
A supervised rule induction algorithm is used to learn to predict the correct diminutive suffix given the phonological representation of Dutch nouns.
We discuss the relevance of our method for linguis-tics and language technology.
A new algorithm is presented for estimating the parameters of a stochastic context-free grammar (SCFC) from ordinary unparsed text.
The trellis is a generalization of that used by the Baum-Welch algorithm which is used for estimating hidden stochastic regular grammars.
This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input.
We present an approach to pronunciation modeling in which the evolution of multiple linguistic feature streams is explicitly represented.
We have implemented a flexible feature-based pronunciation model using dynamic Bayesian networks.
In this paper, we describe our approach and report on a pilot experiment using phonetic transcriptions of utterances from the Switchboard corpus.
The DLSI method first narrows down the search space of a sought-after patent document by content search and the template matching technique then pins down the documents by exploiting the words-based template matching scheme by syntactic search.
The former feature provides a good basis for testing techniques of collocation detection.
We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling.
In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and language- independent heuristics to find links between single words and multiword units in sentence-aligned parallel texts.
So we developed a system, SEGAPSITH, that acquires it automatically from text segments by using an unsupervised and incremental clustering method.
We assume that the first step towards content driven synthetic prosody generation (Concept-to-speech) is invariably to determine the perceptually relevant prosodic features.
In previous work, we have developed hidden Markov model (HMM) and maximum entropy (Maxent) classifiers that integrate textual and prosodic knowledge sources for detecting sentence boundaries.
We present a statistical word feature, the Word Relation Matrix, which can be used to find translated pairs of words and terms from non-parallel corpora, across language groups.
Online dictionary entries are used as seed words to generate Word Relation Matrices for the unknown words according to correlation measures.
Word Relation Matrices are then mapped across the corpora to find translation pairs.
Event detection and recognition is a complex task consisting of multiple sub-tasks of varying difficulty.
We present a prototype natural-language problem-solving application for a financial services call center, developed as part of the Amiti閟 multilingual human-computer dialogue project.
Discourse structure here refers to informational relations that hold between sentences in a discourse (cf.
This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
Standard bigram probability estimation techniques are extended to calculate probabilities of dependencies between pairs of words.
In this paper, we describe clarification dialogues as one method to deal with incomplete or inconsistent information.
We present a task-level evaluation of the French to English version of MedSLT, a medium-vocabulary unidirectional controlled language medical speech translation system designed for doctor-patient diagnosis interviews.
In this paper, we report results from using machine learn-ing to train and test a nominal-expression generator on a set of 393 nominal descriptions from the CO-CONUT corpus of task-oriented de-sign dialogues.
This paper proposes an unsupervised training approach to resolving overlapping ambiguities in Chinese word segmentation.
We present an ensemble of adapted Na飗e Bayesian classifiers that can be trained using an unlabelled Chinese text corpus.
In this work, we investigate the use of the Hidden Markov Model with Latent Dirichlet Allocation (HMM-LDA) to obtain syntactic state and semantic topic assignments to word instances in the training corpus.
From these context-dependent labels, we construct style and topic models that better model the target document, and extend the traditional bag-of-words topic models to n-grams.
phonological rules or metrical systems).
In this paper, we describe temporal information retrieval in distributed search engines.
In our system, Trigram model with smoothing algorithm is the core module in word segmentation, and Maximum Entropy model is the basic model in Named Entity Recognition task.
Our environment accepts grammars consisting of binary dependency relations andgrammatical functions.
We apply our algorithm on the problem of sense-disambiguated noun hyponym acquisition, where we combine the predictions of hypernym and coordinate term classifiers with the knowledge in a preexisting semantic taxonomy (WordNet 2.1).
Traditional vector-based models use wordco-occurrence counts from large corporato represent lexical meaning.
In this pa-per we present a novel approach for con-structing semantic spaces that takes syn-tactic relations into account.
We introducea formalisation for this class of modelsand evaluate their adequacy on two mod-elling tasks: semantic priming and auto-matic discrimination of lexical relations.
This paper proposes a method for incrementally translating English spoken language into Japanese.
To resolve the problem of generating a grammatically incorrect sentence, our method uses dependency structures and Japanese dependency constraints to determine the word order of a translation.
This paper describes some preliminary results about Word Domain Disambigua-tion, a variant of Word Sense Disam-biguation where words in a text are tagged with a domain label in place of a sense label.
The paper presents a constraint based semantic formalism for HPSG.
It clusters sequences of tags together based on local distributional information, and selects clusters that satisfy a novel mutual information criterion.
This paper compares different bilexical tree-based models for bilingual alignment.
We present a novel algorithm for Japanese dependency analysis.
In this paper we propose a methodology for investigating the relationship between architectures of natural language generation (NLG) systems and stylistic properties of texts.
The system that we have been developing is a hybrid system with rule-based, example-based, and statistical components.
Resolving anaphora is an important step in the identification of named entities such as genes and proteins in biomedical scientific articles.
This paper addresses the problem of iden-tifying likely topics of texts by their posi-tion in the text.
It describes the automated training and evaluation of an Optimal Posi-tion Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure.
This article approaches syntactical analysis of Portuguese language based upon a lbrinalism called Tree Adjoining Giammars ('/A Gs) 1.10S111 851.
Speech research focuses on narrowband algorithm development and uses human based intelligibility to evaluate success.
In this paper, we present a clustering experiment directed at the acquisition of semantic classes for adjectives in Catalan, using only shallow distributional features.We define a broad-coverage classification for adjectives based on Ontological Semantics.
We describe an approach to text planning that uses the XSLT template-processing engine to create logical forms for an external surface realizer.
This paper addresses the problem of automatically selecting the best among outputs from multiple machine translation (MT) systems.
This paper presents work on using Bayesian networks for the dialogue act recognition module of a dialogue sys-tem for Dutch dialogues.
Random Indexing is a vector space technique that provides an efficient and scalable approximation to distributional similarity problems.
This paper presents a method for auto-matically recognizing local cohesion be-tween utterances, which is one of the dis-course structures in task-oriented spoken dialogues.
In this paper we focus on speech act type-based local cohesion.
We present two methods of interpo-lating the plausibility of local cohesion based on surface information on utter-ances.
Based on our survey, we then used the rhythm feature in a practical shallow parsing task by using rhythm as a statistical feature to augment a PCFG model.
This paper presents the adaptation and customization of two lexical resources: Brill tagger, Brill (1992), and EuroWordNet, Vossen et al.
A new statistical method called 揵ilingual chunking?for structure alignment is proposed.
The alignment is finished through a simultaneous bilingual chunking algorithm.
We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks.
The platform incorporates a lexicon, a morphological analyzer/generator, and a DCG parser/generator that translates Turkish sentences to predicate logic formulas, and a knowledge base framework.
We describe and evaluate a method for performing backwards transliterations by machine.
The paper presents an NLU system developed in the context of the Geometry Explanation Tutor.
The system combines unification-based syntactic processing with description logics based semantics to achieve the necessary accuracy level.
This paper presents a Chinese word segmentation system that uses improved source- channel models of Chinese sentence generation.
Chinese words are defined as one of the following four types: lexicon words, morphologically derived words, factoids, and named entities.
Our system provides a unified approach to the four fundamental features of word-level Chinese language processing: (1) word segmentation, (2) morphological analysis, (3) factoid detection, and (4) named entity recognition.
In general, the recognition problem is undecidable for unification grammars.
We first show that non-reentrant unification grammars generate exactly the class of context- free languages.
Sentences are translated into production rules using the methodology of lexical-functional grammar.
text comprehension, this paper presents a recent advance in multilingual knowledge- based machine translation (KBMT).
A pilot implementation of our KBMT architecture using functional grammars and entity-oriented semantics demonstrates the feasibility of the new approach.1
This article examines this task within the context of a sub-sentential translation-memory system, i.e.
This paper describes various types of semantic ellipsis and underspecification in natural language, and the ways in which the meaning of semantically elided elements is reconstructed in the Ontological Semantics (OntoSem) text processing environment.
We present a novel representation of parse trees as lists of paths (leafprojection paths) from leaves to the top level of the tree.
We define tree kernels via string kernels on projection paths and explore their performance in the context of parse disambiguation.
We define a new feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes.
In this paper, we investigate the practical applicability of Co-Training for the task of building a classifier for reference resolution.
Both rhetorical structure and punctuation have been helpful in discourse processing.
Based on a corpus annotation project, this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts: Colon, Dash, Ellipsis, Exclamation Mark, Question Mark, and Semicolon.
In this ppr, w dsrib a first prototyp of a pttrn-bsd nlyzr dvlopd in th ontxt of sph-to-sph trnsltion projt using pivot-bsd pproh (th pivot is lld IF).
Th nlyzr pplis "phrs spotting" mhnism on th output of th sph rognition modul.
We present an algorithm that automatically learns context constraints using statistical decision trees.
We then use the acquired constraints in a flexible PUS tagger.
Objectives: To explore the phenomenon of adjectival modification in biomedical discourse across two genres: the biomedical literature and patient records.
Our algorithm constructs a simple statistical word-to-word translation model on the fly during alignment.
In this paper we show that an account for coordination can be constructed us-ing the derivation structures in a lexical-ized Tree Adjoining Grammar (LTAG).
This paper investigates bootstrapping for statistical parsers to reduce their reliance on manually annotated training data.
We present an unsupervised learning algorithm that mines large text corpora for patterns that express implicit semantic relations.
The algorithm is empirically evaluated on two tasks, solving multiple-choice SAT word analogy questions and classifying semantic relations in noun-modifier pairs.
From the literature on probabilistic context-free grammars (CFGs), we know precisely the conditions which ensure that consistency is true for a given CFG.
This paper looks at treebank-training of generators, an alternative method for building statistical models for NLG from raw corpora, and two different ways of using treebank-trained models during generation.
distinctive features.
This paper describes how we use the arrows properties from the 5P Paradigm to generate a dependency structure from a surface analysis.
This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and T╱Ba-D/Z tree- banks.
Experiments with the Stanford parser, which uses a factored PCFG and dependency model, show that, contrary to previous claims for other parsers, lexicalization of PCFG models boosts parsing performance for both treebanks.
This paper describes the treatment of nominal compounds in a transfer based machine translation system; it presents a new approach for resolving ambiguities in compound segmentation and constituent structure selection using a combination of linguistic rules and statistical data.
OF COL1NG-92, NANTEs, AUG. 23-28, 1992
This paper describes a learning method that exploits unlabeled data to tackle data sparseness problem.
Synchronous Tree Adjoining Grammars can be used for Machine Translation.
I present a mechanism to translate scrambled Korean sentences into English by combining the concepts of Multi-Component TAGs (MC-TAGs) and Synchronous TAGs (STAG s) .by looking in the transfer lexicon.
formalism such as Tree Adjoining Grammar.
This paper describes the syntactic rules which are applied in the Japanese speech recognition module of a speech-to-speech translation system.
The grammar-formalism generating the new class - the DI-grammars - cover unbound dependencies in a rather natural way.
It will be shown that, apart from DI-grammars, DI-languages can equivalently be characterized by a special type of automata - DI-automata.
For biomedical information extraction, most systems use syntactic patterns on verbs (anchor verbs) and their arguments.
We propose to use predicate-argument structures (PASs), which are outputs of a full parser, to obtain verbs and their arguments.
In this paper, we evaluated PAS method by comparing it to a method using part of speech (POSs) pattern matching.
In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications.
The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ("uniform lexico-structuralprocessing").
When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task.
This paper investigates the usefulness of a large medical database (the Unified Medical Language System) for the translation of dialogues between doctors and patients using a statistical machine translation system.
An algorithm is proposed to determine antecedents for VP ellipsis.
We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-the- art constituency-based parsers.
This paper examines the generation problem for a certain linguistically relevant subclass of LFG gram-mars.
We use the co杘ccurrence information along with the WordNet definitions to build gloss vectors corresponding to each concept in Word- Net.
The method we have used in this paper for language and encoding identification uses pruned character n-grams, alone as well augmented with word n-grams.
We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords.
Unification of disjunctive feature descriptions is important for efficient unification-based parsing.
This paper presents constraint projection, a new method for unification of disjunctive feature structures represented by logical constraints.
We evaluated the topic signatures on a WSD task, where we trained a second-order vector co- occurrence algorithm on standard WSD datasets, with promising results.
This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.
Unlike most previous systems, the parser produces labeled dependency graphs, using as arc labels a combination of bracket labels and grammatical role labels taken from the Penn Treebank II annotation scheme.
Semi-supervised learning addresses the problem of utilizing unlabeled data along with supervised labeled data, to build better classifiers.
The approach has been used to supplement a maximum entropy model for semi-supervised training of the ACE Relation Detection and Characterization (RDC) task.
We sketch algorithms allowing the numerical parameters of our probabilistic interpretations of HPSG to be estimated from corpora.
This paper outlines research on processing strategies being developed for a language understanding system, designed to interpret the structure of arguments.
In this paper, we describe how to use such constraints for parsing ID/LP grammars and propose an implementation in Prolog III.Keywords : constraints, syntax, ID/LP formalism, bottom-up altering, Prolog III
Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation.
Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter.
Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries.
employs a unified statistical process to map from words to semantic structures.
The architecture uses a natural language processing module to extract entities, dependencies and simple semantic relationships from texts, and then feeds these features into a probabilistic reasoning module which combines the semantic relationships extracted by the NLP module to form new semantic relationships.
Every phrase structure language has a description using a regular grammar and a context free semantics.
For every description D with an gnrestricted grammar and context sensitive semantics there is a description D' using a context free grammar and context free semantics such that L(D) = L(D').
We prove the following results: Eveiy computable translation is definable as a syntax-controlled translation.
For a syntax-controlled translation which produces a
In this paper, we present a method for the semantic tagging of word chunks extracted from a written transcription of conversations.
Our purpose is to automatically annotate parts of texts with concepts from a SAR ontology.
Our approach combines two knowledge sources a SAR ontology and the Wordsmyth dictionary- thesaurus, and it uses a similarity measure for the classification.
This paper describes a principled approach for analyzing relations between constituent words of compound nouns whose heads are deverbal nouns.
Our approach is based on the classification of deverbal nouns by their lexical conceptual structure (LCS) and the classifica-tion of nouns in general (to appear in the mod-ifier position) vis-a-vis a few core LCS types (of head deverbal nouns).
A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
This purely syntax-based similarity measure shows remarkably plausible semanticrelations.2.
This paper presents a Constraint Grammar- inspired machine learner and parser, Ling?Pars, that assigns dependencies to morphologically annotated treebanks in a function- centred way.
The system not only bases attachment probabilities for PoS, case, mood, lemma on those features' function probabilities, but also uses topological features like function/PoS n-grams, barrier tags and daughter-sequences.
This paper proposes that sentence analysis should be treated as defeasible reasoning, and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige, which is a formalization of defeasible reasoning, that includes arguments and defeat rules that capture defensibility.
This model is a classification and coding system of medical procedures.
We derive a measure based on an extension of multinomial na飗e Bayes classification that combines multiple language models to estimate the most likely grade level for a given passage.
In this paper, we present asimilarity-based framework to model the taskof backward transliteration, and provide alearning algorithm to automatically acquirephonetic similarities from a corpus.
The learning algorithm is based on Widdrooww-Hoffrule with some	modifications.
Backwardtransliteration is more challenging than forwardtransliteration.
We mainly focus onbackward transliteration here.In this paper, we propose a similarity-basedframework to model the task of backwardHuo4-ge2-hua2-zi 1(CChhinnese)Hoguwaatsu(Jappaneese))Harry Potter(EEnngglishh)Haa1-li4-bboo1-te4(CChhinnese)Hard Pottaa(J^appannese)Hogwarts(EEnngglishh)
To avoid grammar coverage problems we use a fully-connected first-order statistical class grammar.
We report in this paper the observation of one tokenization per source.
He thus formulated aPreferred Argument Structure (PAS) for the preferential structural configurations of arguments.
In this paper, we provide an account of how to generate sentences with coordination constructions from clause-sized semantic representations.
These include the use of partof-speech tags to guide the generalization, Named Entity categories inside the patterns, an edit-distance-based pattern generalization algorithm, and a pattern accuracy calculation procedure based on evaluating the patterns on several test corpora.
In this paper we investigate the impact of morphological features on the task of au-tomatically extending a dictionary.
We used a boosting clas-sifier to compare the performance of mod-els that use different sets of features.
We present a statistical model of Japanese unknown words consisting of a set of length and spelling models classified by the character types that constitute a word.
Both word segmentation accuracy and part of speech tagging accuracy are improved by the proposed model.
The annotation in-formation consists of speech, tran-scription delimited by slash units, prosodic, part of speech, dialogue acts and dialogue segmentation.
This paper presents F-PATR, a generalization of the PATR-II unification-based formalism, which incorporates relational constraints expressed as user-defined functions.
It is designed particularly for unification- based formalisms implemented in functional programming environments such as Lisp.
Tomita's	parsing	algorithm[Tomita 86], which adapted the LR parsing algorithm to context free grammars, makes use of a breadth-first strategy to handle LH table conflicts.
Thus we obtain a parallel generalized LR parser implemented in GHC.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic para-phrases.
We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
The model combines full and partial parsing techniques to reach full grammar coverage on unseen data.
sys-tem.
Many linguistse.
Candide uses methods of information theory and statistics to develop a probability model of the translation process.
We report on our work to build a discourse parser (SemDP) that uses semantic features of sentences.
We use an Inductive Logic Programming (ILP) System to exploit rich verb semantics of clauses to induce rules for discourse parsing.
In this work, we present three information extraction methods, one based on hand-crafted rules, one based on maximum entropy tagging, and one based on probabilistic trans-ducer induction.
A broad-coverage dependency parser is first used to extract the lexical relations from the definitions.
We propose a method 揑nteractive Paraphrasing?which enables users to interactively paraphrase words in a document by their definitions, making use of syntactic annotation and word sense annotation.
Our strategy is to apply grammatical constraints at the phrase level and to use semantic rather than lexical grammars.
We associate phrases by frame-based semantics.
This paper proposes a method for iden-tifying probable real words among out-of-vocabulary (OOV) words in text.
The identification of real words is done based on entropy of probability of char-acter trigrams as well as the morpho-logical rules of English.
It also gener-ates possible parts-of-speech (POS) of the identified real words on the basis of lexical formation rules and word end-ings.
Several classification techniques are tested, including text classification using support vector machines, language-model based retrieval, and cross-language information retrieval techniques, with the latter having the highest success rate.
As a result, SNAP employs an extended marker- passing model and a dynamically modifiable network model.
We have discovered that the SNAP provides milliseconds or microseconds performance on several important applications such as the memory-based parsing and translation, classification-based parsing, and VLKB search.
We first identify semantic arguments, and then assign semantic roles to the identified semantic arguments.
We present CARPANTA, an e-mail summarization system that applies a knowledge intensive approach to obtain highly coherent summaries.
This paper describes a series of cepstral-based compensation procedures that render the SPHINX-II system more robust with respect to acoustical environment.
Back-off N-gram language models[11] are an effective class of word based stochastic language model.
The first part of this paper describes our experiences using the back-off language models in our time-synchronous decoder CSR.
The second part of this paper describes our experiences with our prototype stack decoder CSR using no grammar, the word-pair grammar, and N-gram back-off language models.N-GRAM BACK-OFF LANGUAGEMODELSN-gram language models[2, 101 are an attractive method for estimating the probability of the sentence W by successively estimating the probability of the next word in the sentence:P(W) wt_i)where N is the order of the model.
We present work on the automatic generation of short indicative-informative abstracts of scientific and technical articles.
Our work combines intentional and social accounts of discourse, unifying theories of speech act production, interpretation, and the repair of misunderstandings.
In this paper we present a means of defining morphonological phenomena in an inheritance based lexicon.
SPUD simultaneously constructs the semantics and syntax of a sentence using a Lexicalized Tree Adjoining Grammar (LTAG).
We represent questions as frequency weighted vectors of salient terms.
We compare our approcah to related work that uses relatively complex syntactic/semantic processing to create features and a sparse network of linear units to classify questions.
Recent studies in word sense induction are based on clustering global co-occurrence vectors, i.e.
We describe and evaluate hidden understanding models, a statistical learning approach to natural language understanding.
We discuss the properties of the system components and report results on the translation of spoken dialogues in the VERBMOBIL project.
We describe a system which performs word sense disambiguation on all content words in free text by combining different knowledge sources: semantic preferences, dictionary definitions and subject/domain codes along with part-of-speech tags, optimised by means of a learning algorithm.
We describe a proposal for an extensible, component- based software architecture for natural language engineering applications.
This paper presents a new view of Explanation-Based Learning (EBL) of natural language parsing.
(Bod, 1995, Charniak, 1996, Sekine and Grishman, 1995).The present method consists of an EBL algorithm for learning partial-parsers, and a parsing algorithm which combines partial- parsers with existing "full-parsers".
The learned partial-parsers, implementable as Cascades of Finite State Transducers (CFSTs), recognize and combine constituents efficiently, prohibiting spurious overgeneration.
The technique is demonstrated on the task of learning word and letter bigram pairs from text.
The BYBLOS continuous speech recognition system is applied to on-line cursive handwriting recognition.
The text mining system first identifies protein names in the text using a trained Conditional Random Field (CRF) and then identifies interactions through a filtered co-citation analysis.
We also report two new strategies for mining interactions, either by finding explicit statements of interactions in the text using learned pattern-based rules or a Support-Vector Machine using a string kernel.
Statistical machine translation (SMT) is currently one of the hot spots in natural language processing.
Word clustering is important for automatic thesaurus construction, text classification, and word sense disambiguation.
This paper proposes an unsupervised algorithm for word clustering based on a word similarity measure by web counts.
By calculating the similarity of words, a word co- occurrence graph is obtained.
We demonstrate COSMA, a fully im-plemented German language server for ex-isting appointment scheduling agent sys-tems.
There are at least two kinds of semantic information: lexical semantics for words and phrases and structural semantics for phrases and sentences.We have built a Japanese corpus of over three million words with both lexical and structural semantic information.
Transfer-Driven Machine Translation (TDMT) is presented as a method which drives the translation processes according to the nature of the input.
TDMT effectively utilizes an example-based framework for transfer and analysis knowledge.
In this paper we describe a QA architecture where questions are analyzed and candidate answers generated by 1) identifying predicate argument structures and semantic frames from the input and 2) performing structured probabilistic inference using the extracted relations in the context of a domain and scenario model.
A novel aspect of our system is a scalable and expressive representation of actions and events based on Coordinated Probabilistic Relational Models (CPRM).
In this paper we report on the ability of the implemented system to perform several forms of probabilistic and temporal inferences to extract answers to complex questions.
In order to extract the best- matched documents from several parallel corpora, we propose ranking individual documents by using a length-normalized Okapi-based similarity score between them and the target corpus.
Several non-speech-recognition based techniques were employed.
We address these requirements by a hybrid word/phoneme search in lattices, and a supporting indexing scheme.
We will introduce the ranking criterion, a unified hybrid posterior-lattice representation, and the indexing algorithm for hybrid lattices.
We describe finite-state constraint relaxation, a method for applying global constraints, expressed as automata, to sequence model decoding.
We present algorithms for both hard constraints and binary soft constraints.
In this paper we introduce two methods for deriving the intentional structure of complex questions.
This paper describes the structure and evaluation of the syntactico-semantic lexicon (SSL) of the German NaturalLanguage Understanding System VIE-LANG[3].
The SSL contains the rules according to which the mapping between net-structures and surface structures of a sentence is carried out.
The paper develops a constraint-based the-ory of prosodic phrasing and prominence, based on an HPSG framework, with an implementation in ALE.
The general aim is to define prosodic structures recursively, in parallel with the definition of syntactic structures.
We devise a linear-time algorithm for factoring syntactic re-orderings by binarizing synchronous rules when possible and show that the resulting rule set significantly improves the speed and accuracy of a state-of-the-art syntax-based machine translation system.
We provide a unified account of sentence-level and text-level anaphora within the framework of a dependency-based grammar model.
The paper describes an analogy-based measure of word-sense proximity grounded on distributional evidence in typical contexts, and illustrates a computational system which makes use of this measure for purposes of lexical disambiguation.
This work describes the implementation of an email summarisation system for use in a voice-based Virtual Personal Assistant developed for the EU FASiL Project.
We discuss existing approaches to train LR parsers, which have been used for statistical resolution of structural ambiguity.
This demo abstract describes the SmartWeb Ontology-based Annotation system (SOBA).
Question Classification is an important task in Question Answering Systems.
We have made experiments using lexical, syntactic and semantic features to test which ones made a better performance.
Our conclusion about the features is that a lexical, syntactic and semantic features combination obtains the best result.
This paper describes how NOMLEX, a dictionary of nominalizations, can be used in Information Extraction (IE).
The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill抯 rule-based part-of-speech tagger.
The main novelty in the system is the method for generating the semantic features, based on word co-occurrence probabilities.
This paper presents Trace & Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (uG) and ideas of Government & Binding Theory (GB) in an undogmatic way.
We find that tree-entropy can significantly reduce the amount of training annotation for both a history-based parser and an EM-based parser.
We explore the major functionalities andarchitectural implications of natural language generation for three key classes of interactive 3D worlds: self- explaining 3D environments, habitable 3D /earning environments, and interactive 3D narrative worlds.
This paper proposes a corpus-based language model for topic identification.
We analyze the association of noun-noun and noun-verb pairs in LOB Corpus.
The word association norms are based on three factors: 1) word importance, 2) pair co-occurrence, and 3) distance.
This paper reports our application of three naive models of centering theory for dialog.
One corpus is now available, and current plans call for corpora of increasing size and complexity over the next few years.Large vocabulary speech recognition requires transcribed speech, pronouncing dictionaries, and language models.
This paper presents a method for inducing transla-tion lexicons based on transduction models of cog-nate pairs via bridge languages.
Bilingual lexicons within languages families are induced using proba-bilistic string edit distance models.
The first method is used to detect any type of word segments.
superordinate substitution, and definite noun phrase reiteration.
The paper is about the issue of addressing in multi-party dialogues.
A method for the automatic prediction of the addressee of speech acts is discussed.
We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.
We identify two types of opinion-related entities ?expressions of opinions and sources of opinions ?along with the linking relation that exists between them.
Inspired by Roth and Yih (2004), we employ an integer linear programming approach to solve the joint opinion recognition task, and show that global, constraint-based inference can significantly boost the performance of both relation extraction and the extraction of opinion-related entities.
In this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks.
This paper presents a corpus-based approach to word sense disambiguation where a decision tree as-signs a sense to an ambiguous word based on the bigrams that occur nearby.
This approach is evalu-ated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.
In this paper we present a supervised Word Sense Disambiguation methodology, that exploits kernel methods to model sense distinctions.
A good decoding algorithm is critical to the success of any statistical machine translation system.
This paper presents a method of Japanese dependency structure analysis based on Sup-port Vector Machines (SVMs).
We apply SVMs to Japanese dependency structure iden-tification problem.
This paper presents a probabilistic framework, QARLA, for the evaluation of text summarisation systems.
This paper describes a standardised XML tagset (DTD) for annotated biomedical corpora and other resources, which is based on the Text Encoding Initiative Guidelines P4, a general and parameterisable standard for encoding language resources.
It is common practice to use tables of probabilities of single words, pairs of words, and triples of words (n-grams) as a prior model.
Our modeling framework treats content selection as a collective classification problem, thus allowing us to capture contextual dependencies between input items.
This paper presents a method for parsing associative Lambek grammars based on graph- theoretic properties.
The method amounts to find alternating spanning trees in graphs.
This paper addressees the problem of eliminating unsatisfactory outputs from machine translation (MT) systems.
Confidence measures for MT outputs include the rank-sum-based confidence measure (RSCM) for statistical machine translation (SMT) systems.
Most of the previous Korean noun extraction systems use a morphological analyzer or a Partof-Speech (POS) tagger.
morphosyntactic rules and morphological rules).This paper proposes a new noun extraction method that uses the syllable based word recognition model.
It finds the most probable syllable-tag sequence of the input sentence by using automatically acquired statistical information from the POS tagged corpus and extracts nouns by detecting word boundaries.
(TSR trees).
In this paper, we present ParaEval, an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations.
Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching.
ParaEval addresses three important issues: support for paraphrase/synonym matching, recall measurement, and correlation with human judgments.
The work includes (1) the development of an integer "break index" representation of prosodic phrase boundary information, (2) the automatic detection of prosodic phrase breaks using a hidden Markov model on relative duration of phonetic segments, and (3) the integration of the prosodic phrase break information in SRI's Spoken Language System to rule out alternative parses in otherwise syntactically ambiguous sentences.
In this paper, we propose a new probabilistic model for text catego-rization, that is based on a Single random Variable with Multiple Values (SVMV).
Here, we propose a scoring algoritlun to rank candidate parses based on an analysis-by-synthesis method which compares the observed prosodic phrase structure with the predicted structure for each candidate parse.
This paper describes a set of computer programs for Chinese corpus analysis.
This paper describes SemNet the in-ternal Knowledge Representation for LOIATAl.
We describe and evaluate the application of a spectral clustering technique (Ng et al., 2002) to the unsupervised clustering of German verbs.
Our previous work has shown that standard clustering techniques succeed in inducing Levin- style semantic classes from verb subcategorisation information.
We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.
In contrast to previous work, we particularly focus on clustering polysemic verbs.
It addresses the problem of learning from examples rules for word-forms analysis and synthesis.
This paper proposes two new meth-ods to identify the correct meaning of Japanese homonyms in text based on the noun-verb co-occurrence in a sentence which can be obtained easily from cor-pora.
The SmartNotes system consists of a laptop based note taking application, and a web based note retrieval system.
We deal with not only conjunctive noun phrases, but also conjunctive predicative clauses created by "Renyoh chuuslii-ho".
We present a progress report on our research on nominal compounds (NC's).
We examine a number of constraints on the semantic interpretation rules for NC's.
Results on this data strongly suggest that images can help with word sense disambiguation.
Automatic word segmentation is a basic requirement for unsupervised learning in morphological analysis.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is defined and basic algorithms for SLTAG are designed.
The basic idea of the method is to ap-ply bootstrapping to an existing corpus-based cross-language information retrieval (CLIR) approach.
This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other抯 output.
Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets.
We demonstrate an original and successful approach for both resolving and generating definite anaphora.
We propose and evaluate unsupervised models for extracting hypernym relations by mining co- occurrence data of definite NPs and potential antecedents in an unlabeled corpus.
It also substantially outperforms recent related work using pattern-based extraction of such hypernym relations for coreference resolution.
This paper presents an analysis of temporal anaphora in sentences which contain quantification over events, within the framework of Discourse Representation Theory.
We show some applications of this solution to additional temporal anaphora phenomena in quantified sentences.
A method is presented here for calculating such finite-state approximations from context-free grammars.
This paper describes corpus analysis and a psycholinguis-tic experiment regarding the acceptability of using non-restrictive NP modifiers to express semantic re-lations that might normally be signalled by `because' and 'then'.
This paper describes a new approach to the generation of referring expressions.
We propose to formalize a scene as a labeled directed graph and describe content selection as a subgraph construction problem.
Three features are unusual in PLUM's architecture: a domain- independent deterministic parser, processing of (the resulting) fragments at the semantic and discourse level, and probabilistic models.
unscripted) speech.
The chief advantage of a discriminative framework is the ability to score alignments based on arbitrary features of the matching word tokens, including orthographic form, predictions of other models, lexical context and so on.
In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT).
We use a maximum likelihood criterion to train a log-linear block bigram model which uses real- valued features (e.g.
block bigram features.
This mechanism encompasses both accommodation by discourse attachment and accommodation by temporal addition.
We present a hybrid text understanding methodology for the resolution of textual ellipsis.
It integrates language-independent conceptual criteria and language-dependent functional constraints.
SEM encodes logical semantics.
[PUS951 posits the lexical entry to be constituted by four structures: Event, Argument, Lexical-Inheritance and QUALIA.
Here we present work on using spatial knowledge in conjunction with information extraction (IE).
In this paper, we exploit these automata- theoretic results to obtain a characterization of the tree-adjoining languages by definability in the monadic second-order theory of these three-dimensional tree manifolds.
In this paper we describe a prosody- dependent duration model as a first step toward incorporating a prosodic consistency constraint into a speech recognizer.
As part of this model, we describe a text- based prosody prediction scheme, novel in its use of a preliminary integrated commaprediction/POS tagging step.
This final result suggests a benefit to integrating a prosodic consistency constraint into a speech recognition system.
We present a model and an experimental platform of a bootstrapping approach to statistical induction of natural language properties that is constraint based with voting components.
The procedure has been implemented for large-scale systemic grammars and builds on the formal equivalence between systemic grammars and typed unification based grammars.
The system consists of a new words recognizer, a base segmentation algorithm, and procedures for combining single characters, suffixes, and checking segmentation consistencies.
There is a real annotated corpus of Czech ?Prague Dependency Treebank (PDT).
We have adapted the letter-to-phoneme component of a text-to-speech synthesizer to a web-based software system that can teach word decoding to non-native speakers of English, English-speaking children, and adult learners.
We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.
This paper describes an algorithm for unifying disjunctive feature structures.
We present various linguistic applications of default unification.
The PANGLOSS system addresses this dilemma by integrating MT with machine-aided translation (MAT).
This article describes the implementation of I2R word sense disambiguation system (I2R ^ W5D) that participated in one senseval3 task: Chinese lexical sample task.
Our core algorithm is a supervised Naive Bayes classifier.
The optimal feature set includes partof-speech with position information in local context, and bag of words in topical context.
The senses are ranked using two sources of information: (1) the Internet for gathering statistics for word-word co- occurrences and (2) WordNet for measuring the semantic density for a pair of words.
This paper describes the NECA MNLG; a fully implemented Multimodal Natural Language Generation module.
This paper presents a semantically oriented, rule based method for single sentence text generation and discusses its implementation in the Kafka generator.
This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.
We describe a set of supervised machine learning experiments centering on the construction of statistical models of WH-questions.
The framework uses customized morpho-syntactic and syntactic analysis where the lexicons and their alphabets correspond to the symbol set acquired from the recognition process.
The paper outlines the methods of morpho-syntactic and syntactic post-processing currently in use.
This paper analyzes principles of human conversation based on the conversational goals of the participants.
We propose a distribution-based pruning of n-gram backoff language models.
Our method is based on then -gram distribution i.e.
In this paper, we look at comparing high- accuracy context-free parsers with high- accuracy finite-state (shallow) parsers on several shallow parsing tasks.
We also demonstrate that context- free parsers can train effectively on relatively little training data, and are more robust to domain shift for shallow parsing tasks than has been previously reported.
We introduce a generalization of categorial grammar extending its descriptive power, and a simple model of categorial grammar parser.
We describe an implemented system which uses the deterministic parser Fidditch to create the input for a set of prosody rules.
The prosody rules generate a prosody tree that specifies the location and relative strength of prosodic phrase boundaries.
This paper addresses recent results on Mandarin spoken dialogues and introduces the collection of a large Mandarin conversational dialogue corpus.
We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora.
In this paper, we mainly present building a semantically tagged bilingual corpus to word sense disambiguation (WSD) in English texts.
To assign semantic tags, we have taken advantage of bilingual texts via word alignments with semantic class names of LLOCE (Longman Lexicon of Contemporary English).
This semantically annotated corpus will be used to extract disambiguation rules automatically by TBL (Transformation-based Learning) method.
This paper describes a method for learning the countability preferences of English nouns from raw text corpora.
The method maps the corpus-attested lexico-syntactic properties of each noun onto a feature vector, and uses a suite of memory-based classifiers to predict membership in 4 countability classes.
The model is based on a EM clustering model with word classes and selectional restrictions as hidden features.
In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense disambiguation.
Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models.
We compute frequencies of unigrams, bigrams, and trigrams of word classes in order to further refine the disambiguation.
This new approach gives a more efficient representation of the data in order to disambiguate word part-of-speech.
The MPM part of PolyphraZ has 3 main web interfaces.
This paper presents a reestimation algorithm and a best-first parsing (BFP) algorithm for probabilistic dependency grammars (PDC).
The inside-outside algorithm is a probabilistic parameter reestimation algorithm for phrase structure grammars in Chomslcy Normal Form (CNF).
The reestiotation and BFP algorithms utilize CYK-style chart and the non- constituent objects as chart entries.
Its basic data structures are typed feature structures.
We describe a method and its implementation for self-monitoring during natural language generation.
This paper describes the structural an-notation of a spoken dialogue corpus.
In this paper, we present a solution to the prob-lem of generating Japanese numeral classifiers us-ing semantic classes from an ontology.
This paper shows how these problems may be circumvented using a rule-based, wait-and-see parsing strategy.
In this paper we present URES (Unsupervised Relation Extraction System), which extracts relations from the Web in a totally unsupervised way.
This paper presents a word-pair (WP) identifier that can be used to resolve homonym/segmentation ambiguities and perform syllable-to-word (STW) conversion effectively for improving Chinese input systems.
This paper presents an approach to automatically build a semantic perceptron net (SPN) for topic spotting.
It uses context at the lower layer to select the exact meaning of key words, and employs a combination of context, co-occurrence statistics and thesaurus to group the distributed but semantically related words within a topic to form basic semantic nodes.
It detects mistypes, Kana-to-Kanji misconversions, and stylistic errors.
This paper presents an empirically motivated theory of the discourse focusing nature of accent in spontaneous speech.
This paper addresses two issues concerning lexical access in connected speech recognition: 1) the nature of the Fe-lexical representation used to initiate lexical lookup 2) the points at which lexical look-up is triggered off this representation.
The few explicit and well- developed models of lexical access and word recognition in continuous speech (e.g.
Degraded text recognition is a difficult task.
In this paper, we propose that visual inter-word constraints can be used to facilitate candidate selection.
Our system improves other proposals presented so far due to the fact that we are able to solve and generate intersentential anaphora, to detect coreference chains and to generate Spanish zero-pronouns into English, issues that are hardly considered by other systems.
Based on the integration of the domain-specific named entity knowledge and syntactic as well as statistical information, this work mainly focuses on how to evaluate a proper multiword verb candidate.
This paper presents a unified approach to parsing, in which top-down, bottom- up and left-corner parsers are related to preorder, postorder and inorder tree traversals.
In the nominal compound analysis area, some corpus-based approaches have reported successful results by using statistal co- occurrences of nouns.
This paper presents a new model for Korean nominal compound analysis on the basis of linguistic and statistical knowledge.
The structure of a nominal compound is analyzed based on the linguistic lexical information extracted.
We describe a framework for the evaluation of summaries in English and Chinese using similarity measures.
This paper describes the representation of Basque Multiword Lexical Units and the automatic processing of Multiword Expressions.
The schema method is a framework for correcting grammatically ill-formed input.
We investigate the logical structure of concepts generated by conjunction and disjunction over a monotonic multiple inheritance network where concept nodes represent linguistic categories and links indicate basic inclusion (isA) and disjointness (IsNoTA) relations.
We apply our logical analysis to the sort inheritance and unification system of HPSG and also to classification in systemic choice systems.
This paper present a method based on the behavior of nonnative speaker for reduction sentence in foreign language.
This paper presents BIAS (Bahasa Indonesia Analyzer System), an analysis systemfor Indonesian language suitablefor multilingual machine translation system.
In this paper we present a logical treatment of semi- free word order and bounded discontinuous constituency.
We extend standard feature value logics to treat word order in a single formalism with a rigorous semantics without phrase structure rules.
This permits a natural interpretation of implicational universals in terms of theories, subtheories and implicational axioms.
In this paper we describe the extraction of thesaurus information from parsed dictionary definition sentences.
The dictionary is parsed using a head-driven phrase structure grammar of Japanese.
Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG).
In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text.
We also show how the parser can be used to parse questions for Question Answering.
In this paper we present a grammar formalism that combines the insights from Combinatory Categorial Grammar with feature structure unification.
We focus on the way theme, rheme, and focus are integrated in the compositional semantics, using Discourse Representation Theory as first-order semantic theory.
NLP systems for tasks such as question answering and information extraction typically rely on statistical parsers.
We present several statistical models of syntactic constituent order for sentence realization.
We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models.
This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a natural language interface to database query ap-plications.
Multimedia answers include videodisc im-ages and heuristically-produced complete sentences in text or text-to-speech form.
We present an input understanding method for a tutoring system teaching mathematical theorem proving.
In interpreting multilingual queries to databases whose domain information is described in a particular language, we must address the problem of word sense disambiguation.
Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.
In this paper, we analyze two particular multi- action constructions, utterances with means clauses and utterances with rationale clauses.
In this paper, we report on our experiment to extract Chinese multiword expressions from corpus resources as part of a larger research effort to improve a machine translation (MT) system.
We define noun phrase translation as a subtask of machine translation.
This enables us to build a dedicated noun phrase translation subsystem that improves over the currently best general statistical machine translation methods by incorporating special modeling and special features.
Hebrew includes a very productive noun-compounding construction called smixut.
Because smixut is marked morphologically and is restricted by many syntactic constraints, it has been the focus of many descriptive studies in Hebrew grammar.We present the treatment of smixut in HUGG, a FUF-based syntactic realization system capable of producing complex noun phrases in Hebrew.
We contrast the treatment of smixut with noun-compounding in English and illustrate the potential for paraphrasing it introduces.We specifically address the issue of determining when a smixut construction can be generated as opposed to other semantically equivalent constructs.
We describe an in-depth study of using a dictionary (WordNet) and web search engines (Altavista, MSN, and Google) to boost the performance of an automated question answering system, Webclopedia, in answering definition questions.
It consists of a word-based statistical language model, an initial estimation procedure, and a re-estimation procedure.
This paper is a first step towards a computational account of Binding Theory (BT).
Listen-Communicate-Show (LCS) is a new paradigm for human interaction with data sources.
We integrate a spoken language understanding system with intelligent mobile agents that mediate between users and information sources.
The	notion	of	a	commonsensealgorithm is presented as a basic ' data structure for modeling humancognition.
The natural language database query system incorporated in the KNOBS interactive planning system comprises a dictionary driven parser, APE-II, and script interpreter which yield a conceptual dependency conceptualization as a representation of the meaning of user input.
Conditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks.
We present the results of an analysis of a corpus of 331 TFRs, with particular attention to discourse segmentation and focusing.
In this paper we demonstrate that it is possible to parse dependency structures deterministically in linear time using syntactic heuristic choices.
We first prove theoretically that deterministic, linear parsing of dependency structures is possible under certain conditions.
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.
Graph unification remains the most expensive part of unification-based grammar parsing.
This paper proposes a method to resolve the reference of deictic Japanese zero pronouns which can be implemented in a practical machine translation system.
This method focuses on semantic and pragmatic constraints such as semantic constraints on cases, modal expressions, verbal semantic attributes and conjunc-tions to determine the deictic reference of Japanese zero pronouns.
In this paper, we describe the rationale and operation of the Computerized Oral Proficiency Instrument (COPI), a multimedia, computer- administered oral proficiency test.
?acctqacy of :text.
-a .framework fdr fr?
each sentence in the text astext information.
Thus, our context Model consists of parsed trees that are obtained by using an exist-ug syntactic parser.
the processing objectYenee: to	sentences .
:reSOlving-pronoun refrence-oii ie'fOe*?.
This paper presents the results of converting a standard Graharn/Hanison/Ruzzo (GHR) parser for a unification grammar into an agenda-driven parsing system.
This paper presents an algorithm for text summarization using the the-matic hierarchy of a text.
The algorithm first detects the thematic hierarchy of a source text with lexical cohe-sion measured by term repetitions.
This paper discusses an approach to incremental learning in natural language processing.
We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.
We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.
In this paper, we apply its idea to the sense classifica-tion of Japanese verbal polysemy in case frame acquisition from Japanese-English parallel corpora.
Measures of bilin-gual class/class association and bilingual class/frame association are introduced and used for discovering sense clusters in the sense distribution of English pred-icates and Japanese case element nouns.
In this paper we introduce a computer- assisted writing tool for deaf users of American Sign Language (ASL).
Our approach is based on the novel application of the longest common substring and string edit distance metrics.
In this paper a method to incorporate linguistic information regarding single-word and compound verbs is proposed, as a first step towards an SMT model based on linguistically-classified phrases.
We present the implementation of a system which extracts not only lexicalized grammars but also feature-based lexicalized grammars from Korean Sejong Treebank.
We report on some practical experiments where we extract TAG grammars and tree schemata.
In addition, we modify Treebank for extracting lexicalized grammars and convert lexicalized grammars into tree schemata to resolve limited lexical coverage problem of extracted lexicalized grammars.
OLACMS (stands for Open Language Archives Community Metadata Set) is a standard for describe language resources.
This pa-per addresses the reduction of parsing com-plexity by intra-sentence segmentation, and presents maximum entropy model for deter-mining segmentation positions.
This paper presents a message passing algorithm for principle-based parsing that avoids the overgeneration problem.
The grammar checking component, which is the focus of this paper, uses island processing (or chunking) rather than a full parse.
In this work we study the influence of corpus homogeneity on corpus-based NLP system performance.
We de-scribe a method to represent corpus homogeneity as a distribution of sim-ilarity coefficients based on a cross-entropic measure investigated in previ-ous works.
This paper presents a novel nonlocal language model which utilizes contextual information.
A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors.
The sum of word co-occurrence vectors represents the context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the long-distance lexical de-pendencies.
We describe a demonstration system built upon Topic Detection and Tracking (TDT) technology.
In this paper, we exploit non-local features as an estimate of long-distance dependencies to improve performance on the statistical spoken language understanding (SLU) problem.
An analysis that defines predicates for Wordnet verb classes and links them to semantic interpretation is presented.
In order to make use of lemmatisation and morphological and syntactic tagging of Czech texts, author proposes a method for construction of dependency word microcontexts fully automatically extracted from texts, and several ways how to exploit the microcontexts for the sake of increasing retrieval performance.
This paper describes the key characteristics of the parser and the distributed grammar.
In this paper, we present a general purpose model for both Chinese word segmentation and named entity recognition.
This model was built on the word sequence classification with probability model, i.e., conditional random fields (CRF).
The method of organization of word meanings is a crucial issue with lexical databases.
In order to find adjective hyperonyms, we utilize abstract nouns.
We constructed linguistic data by extracting semantic relations between abstract nouns and adjectives from corpus data and classifying abstract nouns based on adjective similarity using a self-organizing semantic map, which is a neural network model (Kohonen 1995).
In this paper we describe how to hierarchically organize abstract nouns (adjective hyperonyms) in a semantic map mainly using CSM.
We compare three hierarchical organizations of abstract nouns, according to CSM, frequency (Tf.CSM) and an alternative similarity measure based on coefficient overlap, to estimate hyperonym relations between words.
This paper deals with search algorithms for real-time speech recognition.
Word graphs have various applications in the field of machine translation.
We will describe the generation of word graphs for state of the art phrase-based statistical machine translation.
We will evaluate the quality of the word graphs using the well-known graph word error rate.
Additionally, we introduce the two novel graph-to-string criteria: the position-independent graph word error rate and the graph BLEU score.
First, we introduce the 揹ependency tree path?(DTP).
Next, we calculate the similarity between DTPs based on the ESK (Extended String Subsequence Kernel), which considers sequential patterns.
To resolve the problem, we propose "term distillation", a framework for query term selection in cross-database retrieval.
The experiments using the NTCIR-3 patent retrieval test collection demonstrate that term distillation is effective for cross-database retrieval.
We propose a lexicon smoothing method that takes the word base forms explicitly into account.
This study addresses pronominal anaphora resolution, including zero pronouns, in Chinese.
A syntactic, rule-based pronoun resolution algo-rithm, the 揌obbs algorithm?was run on 揼old standard?hand parses from the Penn Chinese Treebank.
We develop an algorithm for clarification dialogue recognition through the analysis of collected data on clarification dialogues and examine the importance of clarification dialogue recognition for question answering.
This design is used in the TRANSLATOR machine translation project.
The interlingua.
Any approach that attempts to relate directly various syntactic structure trees between SI.
We address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems.
We describe an approach to tagging a monolingual dictionary with linguistic features.
In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus.
We present an algorithm for automatically disambiguating noun-noun compounds by deducing the correct semantic relation between their constituent words.
Keywords: question answering, information retrieval, user modelling, readability.
One method is based on majority vote, while the other is a memory-based approach that integrates maximum entropy and conditional random field classifiers.
A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.
The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a log- linear disambiguation component and provides much richer representations theory.
We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings.
statistical lexical head- outward transducers.
The stack decoder is an attractive algorithm for controlling the acoustic and language model matching in a continuous speech recognizer.
We present a corpus-based study of the sequential ordering among premodifiers in noun phrases.
We propose and evaluate three approaches to identify sequential order among pre- modifiers: direct evidence, transitive closure, and clustering.
In this paper, I discuss issues pertinent to the design of a task-based evaluation methodology for a spoken machine translation (MT) system processing human to human communication rather than human to machine communication.
In this paper we present a Two-Phase LMR-RC Tagging scheme to perform Chinese word segmentation.
The purpose of this paper is to identify effective factors for selecting discourse organization cue phrases in instruction dialogue that signal changes in discourse structure such as topic shifts and attentional state changes.
We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
This paper describes the experiment of the English generation from interlingua by the example-based method.
The generator is implemented by using English Word Dictionary and Concept Dictionary developed in EDR.
Sumo is a formalism for universal segmentation of text.
This framework relies on a layered struc-ture representing the possible segmentations of the document.
JANUS is a multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous con-versation in a limited domain.
This interactive presentation describes LexNet, a graphical environment for graph-based NLP developed at the University of Michigan.
LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification).
We will demonstrate these tools on a variety of NLP tasks including summarization, question answering, and prepositional phrase attachment.
This paper presents an overview of the TIPSTER/SHOGUN project, the major results, and the SHOGUN data extraction system.
The original SHOGUN design integrated several different approaches by combining different knowledge sources, such as syntax, se-mantics, phrasal rules, and domain knowledge, at run-time.
In this paper seven algorithms are compared using a word alignment approach based on association clues and an English-Swedish bitext together with a handcrafted reference alignment used for evaluation.
We describe in this paper the MU- MIS Project (Multimedia Indexing and Searching Environment)1, which is concerned with the development and integration of base technologies, demonstrated within a laboratory prototype, to support automated multimedia indexing and to facilitate search and retrieval from multimedia databases.
This paper describes a project to construct a terminological knowledge base, called COGNITERM.
In particular we will introduce the Extensible MultiModal Annotation (EMMA) language specification.
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications.
The task is the on-demand generation of dialogue scripts and result summaries of dialogues.
The COR.UDIS system (COreference R.Ules with DIsambiguation Statistics) combines syntactico-semantic rules with statistics derived from an annotated corpus.
Then, the coreference resolution algorithm and the involved statistics are explained.
The schema argues for linearizing nonlinear representations before applying phonological and morphological rules.
This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.
The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes.
This paper presents a hybrid approach for location normalization which combines (i) lexical grammar driven by local context constraints, (ii) graph search for maximum spanning tree and (iii) integration of semi-automatically derived default senses.
The Korean Combinatory Categorial Grammar (KCCG) formalism can uniformly handle word order variation among arguments and adjuncts within.
In this paper, incremental pars-ing technique of a morpheme graph is devel-oped using the KCCG.
We present techniques for choosing the most plausible parse tree us-ing lexical information such as category merge probability, head-head co-occurrence heuristic, and the heuristic based on the coverage of sub-trees.
Probabilistic Recursive Transition Network(PRTN) is an elevated version of RTN to model and process languages in stochastic parameters.
We describe an extension of the WYSiwYm technology for knowledge editing through natural language feedback.
The extension will be included in a Java-based library package for producing WYSiwYm applications.
The method we propose then uses these trained parameters to define a kernel for reranking parse trees.
In experiments, we use a neural network based statistical parser as the probabilistic model, and use the resulting kernel with the Voted Perceptron algorithm to rerank the top 20 parses from the probabilistic model.
This resource can be used in machine translation and cross-lingual IR systems.
We coerce the English FrameNet into Chinese using a bilingual lexicon, frame context in FrameNet and taxonomy structure in HowNet.
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.
104-106) described a new methodology for testing lexical transfer in machine translation.
We describe a method for con-verting a document image into character shape codes and word shape tokens.
We present a new word-alignment approach that learns errors made by existing word alignment systems and corrects them.
By adapting transformation- based learning to the problem of word alignment, we project new alignment links from already existing links, using features such as POS tags.
This paper proposes a new type of transfer system, called a Similarity-driven 7iyinsfee System (SiniTran), for use in such case-based MT (CBMT).
This paper introduces new specificity determining methods for terms using compositional and contextual information.
We present an HMM-based approach and a maximum entropy model for speaker role labeling using Mandarin broadcast news speech.
During decoding, we use a block unigram model and a word-based trigram language model.
We show experimental results on block selection criteria based on unigram counts and phrase length.
We present a novel approach for generating the ideographic representations of a CJK name written in a Latin script.
We illustrate the approach with English-to-Japanese back-transliteration.
We extended this algorithm to recover extragrammatical sentence into grammatical one in running text.
This paper describes a hybrid model and the corresponding algorithm combining support vector machines (SVMs) with statistical methods to improve the performance of SVMs for the task of Chinese Named Entity Recognition (NER).
This paper explores the extent to which phoneme sequence constraintz can be used to identify word boundaries in continuous speech recognition.
One of these criteria is the anaphoric conditions of verbs described as a lexicon-grammar of anaphoric verbs.The present paper investigates atransformational analysis of verbs related to their anaphoric behaviour, and the adequacy of extension of the lexicon-grammar of M.GROSS to anaphoric conditions on verbs.
We propose a parser for constraint- logic grammars implementing HPSG that combines the advantages of dynamic bottom-up and advanced top- down control.
Recognizing idioms in a sentence is important to sentence understanding.
This paper discusses the lexical knowledge of idioms for idiom recognition.
We propose a set of lexical knowledge for idiom recognition.
In this paper, we represent singular definite noun phrases as functions in logical form.
This paper presents a first experience with evaluating sytems that address the issue of Logic Form Identification (LFi).
We have developed a formal description of Arabic syntax in Definite Clause Grammar.
This paper addresses issues in automated treebank construction.
We show how such a domain model can be used for topic identification of unseen calls.
Some Terminology In the discussion below, we shall use the term grammar to mean a system consisting of a lexicon, a syntax, a meaning representation lan-guage, and a semantic mapping.
We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data.
Through this reformulation we gain full reversibility for the SFG description and access for unification-based grammar descriptions to the rich semantic levels of description that SFG supports.
This paper reports the development of log- linear models for the disambiguation in wide-coverage HPSG parsing.
Hence we investigate the automatic categorization of text segments of scientific articles with XML markup into 16 topic types from a text type structure schema.
In the framework of statistical machine transla-tion (SMT), correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so-called alignment models.
We found the argumentative interpretation of utterances on a semantics defined at the linguistic level.
We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation.
We show the effectiveness of a WIDL-based NLG system in two sentence realization tasks: automatic translation and headline generation.
There are two major approaches to zero pronoun resolution: the heuristic approach and the machine learning approach.
We describe our analysis and modeling of the summarization process of Japa-nese broadcast news.
We then developed a summarization model on which we intend to build a summa-rization system.
The paper presents an unlexicalized probabilistic parsing model for German trained on the Negra treebank.
I present a modified version of Centering, Dynamic Centering, in which this assumption is removed.
This paper describes the tree-structured maximum mutual information (MMI) encoders used in SSI's Phonetic Engine?to perform large-vocabulary, continuous speech recognition.
We evaluated these MMI encoders by comparing them against a standard minimum distortion (MD) vector quantizer (encoder).
Both encoders produced code streams, which were used to train speaker-independent discrete hidden Markov models in a simplified version of the Sphinx system [3].
A small application called Whole Word Morpholo-gizer which does just this is outlined and discussed.
This paper describes the initial development of a natural language text processor, as the first step in an INRS dialogue-by-voice system.
This paper reports results in processing the textual version of ATIS (Air Travel Information System) queries.
We investigate the relative impact of pre- and post- translation document expansion for cross-language spoken document retrieval in Mandarin Chinese.
A method for automatic lexical acquisition is outlined.
These templates represent grammatically correct sentence patterns.
We propose a generalization of the supervised DOP model to unsupervised learning.
This new model, which we call U-DOP, initially assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from (a large subset of) these binary trees to compute the most probable parse trees.
We consider the problem of training logistic regression models for binary classification in information extraction and information retrieval tasks.
We describe thus a microsemantic parser which is uniquely based on an associative network of semantic priming.
This paper describes a new, large scale discourse-level annotation project ?the Penn Discourse TreeBank (PDTB).
This paper explores the usefulness of a technique from software engineering, namely code instrumen-tation, for the development of large-scale natural language grammars.
In 1989, our group first reported on the development of SUMMIT, a segment-based speaker-independent continuous-speech recognition system [13] .
In this paper we compare two grammar-based generation algorithms: the Semantic-Head-Driven Generation Algorithm (SHDGA), and the Essential Arguments Algorithm (EAA).
We propose a treatment of coordination based on the concepts of functor, argument and sub categorization.
This paper presents a proposal for iCLEF 2006, the interactive track of the CLEF cross-language evaluation campaign.
We are trying to find paraphrases from Japanese news articles which can be used for Information Extraction.
In this paper, we propose adding long-term grammatical information in a Whole Sentence Maximun Entropy Language Model (WSME) in order to improve the performance of the model.
One is the Viterbi algorithm for linear-chain models, such as HMMs or CRFs.
The other is the CKY algorithm for probabilistic context free grammars.
We conducted experiments with an EBMT system.
We introduce a new method of feature selection for text categorization.
This paper studies issues related to the compilation of a bilingual lexicon for technical terms.
As a method of translation estimation for technical terms, we employ a compositional translation estimation technique.
We present the concepts of weighted language, transduction and automaton from algebraic automata theory as a general framework for describing and implementing decoding cascades in speech and language processing.
Applications to speech recognition and to Chinese text segmentation will be discussed.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
The major features of MM-DCG include capability to handle an arbitrary number of !nodes and temporal information in grammar rules.
Further, we have developed MM-DCG translator to transfer rules in MM-DCG into Prolog predicates.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence.
We describe a similarity calculation model called IFSM (Inherited Feature Similarity Measure) between objects (words/concepts) based on their com-mon and distinctive features.
We pro-pose an implementation method for ob-taining features based on abstracted triples extracted from a large text corpus utilizing taxonomical knowledge.
relation based sim-ilarity measure and distribution based similarity measure.
We show that the concatenation cost can be accurately estimated from this corpus using a statistical n-gram language model over units.
A LOGICAL SEMANTICSFOR NONMONOTONIC SORTSMark A.
In this paper, we present a logical basis for NSs and non- monotonically sorted feature structures (NSFSs).
Then we could use default theories to describe feature structures.
We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic finite automaton.
the cross-entropy.
We discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata.
The paper describes several possibilities of using finite-state automata as means for speeding up the performance of a grammar-and-parsing-based (as opposed to pattern-matching-based) grammar-checker able to detect errors from a predefined set.
We demonstrate the feasibility of using unary primes in speech-driven language processing.
Translation memories are promising devices for automatic translation.
In this paper, the use of a hierarchical transla-tion memory, consisting of a cascade of finite state transducers, is proposed.
Natural language parsing requires ex-tensive lexicons containing subcategori-sation information for specific sublan-guages.
This paper describes an unsuper-vised method for acquiring both syntac-tic and semantic subcategorisation restric-tions from corpora.
For more generalized applications, in this paper we extend our previous approach by adding a phase of transitive (indirect) translation via an intermediate (third) language, and propose a transitive model to further exploit anchor-text mining in term translation extraction applications.
We present a system for identifying and tracking named, nominal, and pronominal mentions of entities within a text document.
Our maximum entropy model for mention detection combines two pre-existing named entity taggers (built to extract different entity categories) and other syntactic and morphological feature streams to achieve competitive performance.
We developed a novel maximum entropy model for tracking all mentions of an entity within a document.
We participated in the Automatic Content Extraction (ACE) evaluation and performed well.
A method is described by which a rhetorical-structure tree can be realized by a text structure made up of sections, paragraphs, sentences, verti-cal lists, and other textual patterns, with discourse connectives added (in the correct positions) to mark rhetorical relations.
We explore the use of Support Vector Ma-chines (SVMs) for biomedical named en-tity recognition.
In ad-dition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning.
We compare our SVM-based recognition system with a system using Maximum Entropy tagging method.
We provide D-ITG grammars to search these spaces completely and without redundancy.
We describe results based on the stan-dard logfile metrics as well as results based on additional qualitative metrics derived using the DATE dialogue act tagging scheme.
This paper describes a natural language query engine that enables users to search for entities, relationships, and events that are extracted from biological literature.
We focus on the usability of the natural language interface to users who are used to keyword- based information retrieval.
This paper describes an indexing substrate for typed feature structures (ISTFS), which is an efficient retrieval engine for typed feature structures.
This paper extends earlier work on the relation between syntax and intonation in language understanding in Combinatory Categorial Grammar (CCG).
Hdrug is an environment to develop grammars, parsers and generators for natural languages.
The system provides a graphical user interface with a command interpreter, and a number of visualisation tools, including visualisation of feature structures, syntax trees, type hierarchies, lexical hierarchies, feature structure trees, definite clause definitions, grammar rules, lexical entries, and graphs of statistical information of various kinds.Hdrug is designed to be as flexible and extendible as possible.
Grammatical formalisms that have been used range from context-free grammars to concatenative feature-based grammars (such as the grammars written for ALE) and nonconcatenative grammars such as Tree Adjoining Grammars.
The most prominent of these include decomposition trees, linear representations such as the Predicate Calculus, and semantic networks.
This paper presents a study on optimizing sentence pair alignment scores of a bilingual sentence alignment module.
This paper concentrates on the class of action verbs of movement, and builds on earlier work on lexical correspondences between languages and specific to this verb class.
We first examine the way prototypical verbs of movement are translated in the Collins- Robert (Collins 1978, henceforth CR) bilingual dictionary.
We take advantage of the results of linguistic research on verb types (e.g.
Motion Verbs.
In this paper, we report on data for movement verbs (or motion verbs).
Bilingual Corpus-based Analysis.
We propose a bottom-up variant of Earley deduction.
In this paper, we present the HybridTrim system which uses a machine learning technique to combine linguistic, statistical and positional information to identify topic labels for headlines in a text.
This paper discusses the partitive-genitive case alternation of Finnish adpositions.
Interactive spoken dialog provides many new challenges for spoken language systems.
The repair pattern is built by finding word matches and word replacements, and identifying fragments and editing terms.
Natural language image sense annotation involves increased semantic complexities compared to disambiguating word senses when annotating text.
This paper identifies the types of sentence fragments found In the text of two domains: medical records and Navy equipment status messages.
We present a new recall-based evaluation methodology for determining the effectiveness of different similarity measures on drug names.
In this paper we study the effectiveness of applying sentence compression on an extraction based multi-document summarization system.
This paper proposes an efficient example selection method for example-based word sense disambiguation systems.
We present a simple approach for Asian language text classification without word segmentation, based on statistical -gram language modeling.
This paper describes a prototype system to visualize and animate 3D scenes from car accident reports, written in French.
This paper presents a representation language in the notation of FOPC whose form facilitates the design of a semantic-network-like retriever.
sidesg.
oppositesg.
The paper presents a tabular interpretation for a kind of 2-Stack Automata.
This machine can extract morphemes from 10,000 character Japanese text by searching an 80,000 morpheme dictionary in 1 second.
Kenji characters are Chinese ideographs.
This morpheme extraction machine is designed as the first step toward achieving the natural language parsing accelerators.2 MACHINE DESIGNSTRATEGY2.1 MORPHEME EXTRACTIONMorphological analysis methods are generally composed of two processes: (1) a morpheme extraction process and (2) a morpheme determination process.
This paper describes our research on both the detection and subsequent resolution of recognition errors in spoken dialogue systems.
The algorithm is compared with three other pronoun resolution methods: Hobbs' syntax-based algorithm, Strube's S-list approach, and the BFP Centering algorithm.
The first method uses a standard HMM part-of-speech tagger with variable context length.
In this paper we show that an unsupervised method for ranking word senses automatically can be used to identify infrequently occurring senses.
We demonstrate this using a ranking of noun senses derived from the BNC and evaluating on the sense-tagged text available in both SemCor and the SENSEVAL-2 English all-words task.
We address the representation of nouns having complex argument structures like deverbal nominalisations.
The first recog- nition pass extracts letter hypotheses from the spelled part of the waveform and maps them to phonemic hypotheses via a hierarchical sub- lexical model capable of generating grapheme- phoneme mappings.
In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classifiers.
We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar.
The model parameters were learned from unlabelled training data by a probabilistic context-free parser.
A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce spatially complex ASL phenomena called 揷lassifier predicates.
As the first step in an automated text summarization algorithm, this work presents a new method for automatically identifying the central ideas in a text based on a knowledge-based concept counting paradigm.
This paper describes a system that provides customer service by allowing users to retrieve identification numbers of parts for medical systems using spoken natural language dialogue.
This result is achieved by combining a modified ca,seframe approach to linguistic knowledge representation with a parsing strategy able to integrate expectations from the language model and predictions from words.
We present a set of algorithms that en-able us to translate natural language sentences by exploiting both a trans-lation memory and a statistical-based translation model.
We describe a simple approach for integrating shallow and deep parsing.
We use phrase structure bracketing obtained from the Collins parser as filters to guide deep parsing.
In this paper, the usage and function of Chinese punctuations are studied in syntactic parsing and a new hierarchical approach is proposed for parsing long Chinese sentences.
In this paper we describe an architecture that utilizes this mistake-driven algorithm for multi-class prediction — selecting the part of speech of a word.
This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon.
We show how elements of the Qualia structure can be incorporated into semantic composition rules to make explicit the semantics of the combination adjective + noun.
We then discuss our method, which applies SVM induction over lexical and morphological features.
We present a FrameNet-based semantic role labeling system for Swedish text.
Values of lexical correlates lead to other word senses.
Thematic knowledge is a basis of semantic interpretation.
Using this way, a syntactic processor may become a thematic recognizer by simply deriving its thematic knowledge from its own syntactic knowledge.Keywords: Thematic Knowledge Acquisition, Syntactic Clues, Heuristics-guided Ambiguity Resolution, Corpus-based Acquisition, Interactive AcquisitionI.
syntactic processing systems and syntactically processed corpora).
machine translation), thematic role recognition is a major step.
This paper presents a system which automatically generates shallow semantic frame structures for conversational speech in unrestricted domains.We argue that such shallow semantic representations can indeed be generated with a minimum amount of linguistic knowledge engineering and without having to explicitly construct a semantic knowledge base.
To this end, we propose an aesthetically 揷lean?Bayesian inference scheme for exploiting lexical relations for passage-scoring for QA .
Current NER approaches include: dictionary-based, rule-based, or machine learning.
We represent shallow linguistic information as linguistic features in our ME model.
In this paper, we study aparsing technique whose purpose is to improve the practical efficiency of RCL parsers.
In this paper we propose a novel approach for ontology alignment and domain ontology extraction from the existing knowledge bases, WordNet and HowNet.
This paper evaluates a direct speech trans-lation Method with waveforms using the Inductive Learning method for short con-versation.
We define the notion of relative singularity of world objects as an abstraction class of the layer- membership relation
In this paper, I will discuss the effect of using variables in lexical category assignments in CCGs.
It will be shown that using variables in lexical categories can increase the weak generative capacity of CCGs beyond the class of grammars listed above.A Formal Definition for CCGsIn categorial grammars, grammatical entities are of two types: basic categories and functions.
This paper describes a system for analysing	naturallanguage based on the concept of case.
This paper presents a query tool for syntacti-cally annotated corpora.
The tool uses a query language that allows to search for tokens, syntactic cat-egories, grammatical functions and binary re-lations of (immediate) dominance and linear precedence between nodes.
Then we propose the rules to extract subjects and predicates from those sentences.
This paper introduces GLARF, a framework for predicate argument structure.
We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities.
as a hybrid system.
The standard design for a computer-assisted translation system consists of data entry of source text, machine translation,and revision of raw machine translation.
Knowledge-Based Report Generation is a technique for automatically generating natural language reports from computer databases.
The first application of the technique, a system for generating natural language stock reports from a daily stock quotes database, is partially implemented.
Three fundamental principles of the technique are its use of domain-specific semantic and linguistic knowledge, its use of macro-level semantic and linguistic constructs (such as whole messages, a phrasal lexicon, and a sentence-combining grammar), and its production system approach to knowledge representation.I.
WHAT IS KNOWLEDGE-BASEDREPORT GENERATIONA knowledge-based report generator is a computer program whose function • is to generate natural language summaries from computer databases.
Finally, it holds that macro-level knowledge units, such as whole seman tic messages, a phrasal lexicon, clausal grammatical categories, and a clause-combining grammar, provide an appropriate level of knowledge representation for generating that type of text which may be categorized as periodic summary reports.
These three tenets guide the design and implementation of a knowledge-based report generation system.II.
SAMPLE OUTPUT FROM AKNOWLEDGE-BASED REPORT GENERATORThe first application of the technique ofknowledge-based report generation is a partially implemented stock report generator called Ana.
We review studies of reference resolution, word recognition, and pragmatic effects on syntactic ambiguity resolution.
We present discriminative reordering models for phrase-based statistical machine translation.
The models are trained using the maximum entropy principle.
This paper presents the implementation of the Vietnamese generation module in ITS3, a multilingual machine translation (MT) system based on the Government & Binding (GB) theory.
Self-Organizing Maps (SOMs) are a good method to cluster and visualize large collections of documents, but they are computationally expensive.
In this paper, we investigate linguistically motivated reductions on the usual bagof-words representation, to improve efficiency.
Disambiguation is carried out with the latest version (April 1996) of the Constraint Grammar Parser (CGP).
The method we proposed is based on the machine learning and the hypothesis driven combining method for identifying names and corresponding faces.
SCNE is very common in written Chinese text.
This paper formulates the SCNE recognition within the source- channel model framework.
We construct two classifiers based on maximum entropy model (ME) and vector space model (VSM), respectively.
We used two conditional probabilistic models for this task, including conditional random fields (CRFs) and maximum entropy models.
In particular, we trained two conditional random field recognizers and one maximum entropy recognizer for identifying names of people, places, and organizations in unsegmented Chinese texts.
This paper presents a method for inducing the parts of speech of a language and partof-speech labels for individual words from a large text corpus.
Vector representations for the part-of-speech of a word are formed from entries of its near lexical neighbors.
A neural net trained on these spatial representations classifies individual contexts of occurrence of ambiguous words.
In this paper we describe EFLUF - an implementation of FLUF.
In this paper, we propose a method for ex-tracting key paragraphs in articles based on the degree of context dependency.
We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts.
We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model.
We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses.
We solve the problem of hierarchical structure in our tagging model by modeling underspecified tags, which are fully determined only at decoding time.
In creating an English grammar checking software product, we implemented a large-coverage grammar based on the dependency grammar formalism.
In this paper we sketch a decidable inference-based procedure for lexical dis-ambiguation which operates on semantic representations of discourse and concep-tual knowledge.
Content selection is a key factor of any successful document generation system.
This paper shows how a content selection algorithm has been implemented using an efficient combination of XML/XSL technology and the framework of RST for discourse modeling.
In this paper we present an approach to dialogue management that supports the generation of multifunctional utterances.
It is based on the multidimensional dialogue act taxonomy and associated context model as developed in Dynamic Interpretation Theory (DIT).
This paper proposes a method for detecting errors in article usage and singular plural usage based on the mass count distinction.
In this paper we describe our experience of using post-edit data to evaluate SUMTIME-MOUSAM, an NLG system that produces marine weather forecasts.
This paper describes using RDF/RDFS/XML to create and navigate a metadata model of relationships among entities in text.
In this paper we consider extended similarity metrics for documents and other objects embedded in graphs, facilitated via a lazy graph walk.
We provide a detailed instantiation of this framework for email data, where content, social networks and a timeline are integrated in a structural graph.
The suggested framework is evaluated for the task of disambiguating names in email documents.
This paper presents a data-driven language independent word segmentation system that has been trained for Chinese corpus at the second Chinese word segmentation bakeoff.
At discourse level, anaphoric and coreference expressions are annotated.
ConTroll is a grammar development system which supports the implementation of current constraint-based theories.
It uses strongly typed feature structures as its principal data structure and offers definite relations, universal constraints, and lexical rules to express grammar constraints.
This paper proposes a principled approach for analysis of semantic relations between constituents in compound nouns based on lexical semantic structure.
All results presented were generated by using the Ngram-based statistical machine translation system which has been enhanced from the last year抯 evaluation with a tagged target language model (using Part-Of-Speech tags).
This paper proposes a two-phase shift-reduce dependency parser based on SVM learning.
We demonstrate a proof-of-concept system that uses a shallow chunking-based technique for knowledge extraction from natural language text, in particular looking at the task of story understanding.
This technique is extended with a reasoning engine that borrows techniques from dynamic ontology refinement to discover the semantic similarity of stories and to merge them together.
To support context-based multimodal interpretation in conversational systems, we have developed a semantics-based representation to capture salient information from user inputs and the overall conversation.
We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.
The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.
This is a proposal for a an XML mark-up of argumentation.
The knowledge extraction process utilizes the structure property, statistical property as well as partial linguistic knowledge of the organization names to extract new organizations from domain texts.
We discuss impli-cations of our empirical findings for the task of text planning in the context of implementing multilingual natural language generation systems.
Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.
Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.
We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.
The paper presents a measure, based on the x2 statistic, for measuring both corpus similarity and corpus homogeneity.
We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora.
This paper presents the story understanding mechanism for creating computer animation scenarios.
Spoken Language Understanding (SLU) addresses the problem of extracting semantic meaning conveyed in an utterance.
A generative HMM/CFG composite model, which integrates easy-toobtain domain knowledge into a data-driven statistical learning framework, has previously been introduced to reduce data requirement.
The major contribution of this paper is the investigation of integrating prior knowledge and statistical learning in a conditional model framework.
We also study and compare conditional random fields (CRFs) with perceptron learning for SLU.
The first is the grammar of stress, or metrical phonology, which has received much attention in the learning model literature.
In this paper an architecture and an implementation for a linguistically based prosodic analyser is presented.
We report on the role of the Urdu grammar in the Parallel Grammar (ParGram) project (Butt et al., 1999; Butt et al., 2002).1 The ParGram project was designed to use a single grammar development plat-form and a unified methodology of grammar writ-ing to develop large-scale grammars for typologi-cally different languages.
domain-independent, semantic information for question interpretation.
We describe the new release of the RASP (robust accurate statistical parsing) system, designed for syntactic annotation of free text.
The new version includes a revised and more semantically-motivated output representation, an enhanced grammar and part-of-speech tagger lexicon, and a more flexible and semi-supervised training method for the structural parse ranking model.
In this paper I outline Type-inheritance Combinatory Categorial Grammar (TCCG), an implemented feature structure based CCG fragment of English.
TCCG combines the fully lexical nature of CCG with the type-inheritance hierarchies and complex feature structures of Head- driven Phrase Structure Grammars (HPSG).
Structural matching with pre-computed semantic annotations establishes the accurate and efficient retrieval of relational concepts.
This framework was applied to a text retrieval system for MEDLINE.
We present a method for compiling grammars into efficient code for head-driven generation in ALE.
Memory-based learning is a form of supervised learning based on similarity-based reasoning.
Memory-based tagging shares this advantage with other statistical or machine learning approaches.
Given a spoken query, we generate a transcription and detect OOV words through speech recognition.
In this paper, we explore how the addition of information to the text, in particular part of speech and dysfluency annotations, can be used to .
build more complex language models.
To answer these questions, we present a variety of kinds of analysis, from vocabulary distributions to perplexities on language models.
This additional assumption allows us to identify sentiment-bearing terms very reliably.
We then use these newly identified terms in various scenarios for the sentiment classification of sentences.
In contrast, this paper describes a new approach toward using contextual, dialog-based knowledge for speech recognition.
The grammars used for speech recognition dictate legal word sequences.
The mapping between syntactic structure and prosodic structure is a widely discussed topic in linguistics.
We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text.
This paper aims to analyze word dependency structure in compound nouns appearing in Japanese newspaper articles.
This paper presents a corpus-based approach which scans a corpus with a set of pattern matchers and gathers co-occurrence examples to analyze compound nouns.
Can we do text analysis without phrase structure?
In this paper, we propose a supervised learning-based approach which does coreference resolution by exploring the relationships between NPs and coreferential clusters.
Personal MT (PMT) is a new concept in dialogue- based MT (DBMT) , which we are currently studying and prototyping in the LIDIA project Ideally, a PMT system should run on PCs and be usable by everybody.
The paper briefly presents some of them (HyperText, distributed architecture, guided language, hybrid transfer/interlingua, the goes on to study in more detail the structure of the dialogue with the writer and the place of speech synthesis [1].KeywordsPersonal Machine Translation, dialogue-based Machine Translation, Man-Machine Dialogue, Ambiguity Resolution, Speech Synthesis.
In this paper we address the question of assigning semantic roles to sentences in Chinese.
Finally, we compare English and Chinese semantic-parsing performance.
We describe and evaluate an implemented system for general-knowledge question answering.
We present an automatic approach to learning criteria for classifying the parts-of-speech used in lexical mappings.
Associations among these and the parts-of-speech are learned using the lexical mappings contained in the Cyc knowledge base as training data.
The paper describes work on applying a general purpose natural language processing system to transfer-based interactive translation.
This paper proposes a method of finding correspondences of arbitrary length word sequences in aligned parallel corpora of Japanese and English.
Transforming syntactic representations in order to improve parsing accuracy has been exploited successfully in statistical parsing systems using constituency-based representations.
This paper deals with the use of computational linguistic analysis techniques for information access and ontology learning within the legal domain.
This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.
We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text.
The topic of the paper is the introduction of a formalism that permits a homogeneous representation of definite temporal adverbials, temporal quantifications (as frequency and duration), temporal conjunctions and tenses, andof their combinations with propositions.
The formal representation is based on the notions "phase-set" and "phase-operator", and it involves an interval logic.
The parser of QPATR uses a left-corner algorithm for context-free grammars and includes a facility for identifying new lexical items in input on the basis of contextual information.
In statistical machine translation, the generation of a translation hypothesis is computationally expensive.
Therefore, we present an extension to the ITG constraints.
We introduced a new linguistic representation, the Dynamic Hierarchical Phrasal Lexicon (DHPL) [Zemik88], to facilitate language acquisition.
We present a transliteration algorithm based on sound and spelling mappings us-ing finite state machines.
We apply our translit-eration algorithm to the transliteration of names from Arabic into English.
We trained two parsers with the training corpus in which the semantic argument information is attached to the constituent labels, we then used the resulting parse trees as the input of the pipelined SRL system.
We present our results of combining the output of various SRL systems using different parsers.
This paper will address the emerging standards for evaluation of spoken language systems.
By representing discourse graphically, we also show that interruptions are part of the local and global coherence that is brought about through the systematic phrase-to-phrase prosodic patterns of discourse.
This paper introduces a standard setting of binary features, inspired by the literature on named-entity recognition and text chunking, and derives corresponding real- valued features based on smoothed log- probabilities.
This paper proposes a discriminative HMM (DHMM) with long state dependence (LSDDHMM) to segment and label sequential data.
The LSD-DHMM overcomes the strong context independent assumption in traditional generative HMMs (GHMMs) and models the sequential data in a discriminative way, by assuming a novel mutual information independence.
In this paper, a variable-length mutual information- based modeling approach and an ensemble of kNN probability estimators are proposed to capture the long state dependence and the observation dependence respectively.
This suggests that the LSD-DHMM can effectively capture the long context dependence to segment and label sequential data.
We have been developing our own modified TBL learner initially to tackle the Part-of-Speech tagging problem, for integration in a hybrid NLL and rule- based system for information extraction (Ciravegna et al., 1999).
A variety of statistical methods for noun compound analysis are implemented and compared.
We illustrate tins point with the presentation of ALLiS, a learning system winch generates a regular expression grammar of non-recursive phrases from bracketed corpora.
The parser uses application-restricted grammars and lexicons obtained from ontologies representing the application specific knowledge.
This paper describes how recent linguistic results in explaining Japanese short and long distance scrarnbling can be directly incorporated into an existing principles-and-parameters-based parser with only trivial modifications.
Pronouns are the most common NPs in the speech that children hear.
Taken together, these results suggest that children might use regularities in pronoun/verb co-occurrences to help learn verbs, though whether this is actually so remains a topic for further research.
In this paper, we explore a number of alternatives for how these parse tree paths are encoded, focusing on the difference between automatically generated constituency parses and dependency parses.
This paper presents a genetic algorithm based approach to the automatic discovery of finite- state automata (FSAs) from positive data.
We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging.
Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.
We deal with the question as to whether there exists a polynomial time algorithm for computing the most probable parse tree of a sentence generated by a data-oriented parsing (DOP) model.
Therefore we describe DOP as a stochastic tree-substitution grammar (STSG).
This paper presents a grammar formalism designed for use in data-oriented approaches to language processing.
Both full-text information retrieval and large scale parsing require text preprocessing to identify strong lexical associations in textual databases.
In order to associate linguistic felicity with computational efficiency, we have conceived FASTR a unification-based parser supporting large textual and grammatical databases.
The grammar is composed of term rules obtained by tagging and lemmatizing term lists with an on-line dictionary.
This paper describes a new approach for estimating term weights in a text classification task.
The approach uses term co- occurrence as a measure of dependency between word features.
We present a uniform computational architecture for developing reversible grammars for parsing and generation, and for bidirectional transfer in MT.
The system chosen was the Caption Generation System.
Categorial Dependency Grammars (CDG) introduced in this paper make clear-cut distinction between local and distant word driven dependencies.
Besides this, CDGs represent a convenient frame for relating dependency grammar with linguistic semantics.
An outline of our approach to storing situation-to-language associations will then be provided.
In this paper, we presented a new technique for the semi梐utomatic tagging of Chinese text.
We use dependency grammar and employ a stack based shift/ reduce context梔ependent parser as the tagging mechanism.
Further more, if you want to obtain the co梠ccurrence frequency of each two adjacent part of speeches, which is helpful to the study of part of speech (POS) tagging, you must annotate the corpora with POS information.
This paper presents a maximum entropy method for the disambiguation of word senses as defined in HowNet.
The maximum entropy model treats semantic tags like parts-of-speech tags and achieves an overall accuracy of 89.39%, outperforming a baseline system, which picks the most frequent sense.
The system employs an interval-based temporal network for storing histor-ical information.
Lexical chains capture the semantic relations between words that occur throughout a text.
We focus on Example based machine translation and the automatization of the translation examples extraction by means of RDFrepositories.
In this paper, we show that Higher-Order Coloured Unification - a form of unification developed for automated theorem proving - provides a general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information.
We introduce a new method for disambiguating word senses that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique to achieve accuracy superior to the best published individual models.
We also contrast against another type of kernel method, the support vector machine (SVM) model, and show that our KPCA-based model outperforms the SVM-based model.
One of the currently most used statistics is the mutual information ratio.
This paper compares the mutual information ratio and a measure that takes temporal ordering into account.
Non-compositional expressions present a special challenge to NLP applications.
In this paper we describe a novel approach to lexical chain based segmentation of broadcast news stories.
The key proposal is to incorporate into lexical heads the WOC (Word Order Constraints) feature, which is used to constrain the word order of its projection.
In order to deal with ambiguity, the MORphological PArser MORPA is provided with a probabilistic context-free grammar (PCFG), i.e.
it combines a "conventional" context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse.
MORPA is a fully implemented parser developed for use in a text-to-speech conversion system.
Information Extraction (1E) systems are com-monly based on pattern matching.
The "Noisy Channel" 's are the promoters of statistically based approaches to language learning.
This paper describes a machine learning approach to classifying n-best speech recognition hypotheses as either correctly or incorrectly recognised.
Decanter illustrates a heuristic approach to extraction for information retrieval and question answering.
A parse of discourse is defined as a set of semantic dependencies among sentences that make up the discourse.
We also study effects of features such as clue words, distance and similarity on the performance of the discourse parser.
Identifying sentiments (the affective parts of opinions) is a challenging problem.
The system contains a module for determining word sentiment and another for combining sentiments within a sentence.
Currently, our goal is to provide a language model using transition statistics to disambiguate alternative parses for a speech recognition device.
The first description is treated by extending type symbol lattices to include complement type symbols.
Algorithms for augmented-WS unification have been developed using graph unification, and programs using these algorithms have been written in Common Lisp.
We have studied three aspects of robustness in such a system: accent differences, mixed language input, and the use of common feature sets for HMM-based speech recognizers for English and Cantonese.
In this paper, we explore how the taxo-nomic inheritance hierarchy in a seman-tic net can contribute to the resolution of associative anaphoric expressions.
In this paper we analyze story link detection and new event detection in a retrieval framework and examine the effect of a number of techniques, including part of speech tagging, new similarity measures, and an expanded stop list, on the performance of the two detection tasks.
We present an approach using syntactosemantic rules for the extraction of relational information from biomedical abstracts.
This list of names was included in the lexicon of our retrained part-of-speech tagger for use on molecular biology abstracts.
Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
We employ both unsupervised clustering and semi-supervised learning to recognize pitch accent in English and tones in Mandarin Chinese.
We introduce a new multi-threaded parsing algorithm on unification grammars designed specifically for multimodal interaction and noisy environments.
We have developed a summarization method that creates a summary suitable for the process of sifting information retrieval results.
The model is a mixture of the regeneration-based model and the rewriting-based model.
This paper constitutes an investigation into the generative capabilities of two-level phonology with respect to unilevel generative phonological rules.
We explore features of hand gesture that are correlated with coreference.
In part of speech tagging by Hidden Markov Model, a statistical model is used to assign grammatical categories to words in a text.
The second experiment reveals that there are three distinct patterns of Baum-Welch re-estimation.
We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.
Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.
The logic behind parsers for categorial grammars can be formalized in several different ways.
We present an empirical corpus study of the meaning and usage of time phrases in weather forecasts; this is based on a novel corpus analysis technique where we align phrases from the forecast text with data extracted from a numerical weather simulation.
In this paper discourse segments are defined and a method for discourse segmentation primarily based on abduction of temporal relations between segments is proposed.
This method is precise and computationally feasible and is supported by previous work in the area of temporal anaphora resolution.
Finally, an experimental evaluation of these variations is reported.Subject Areas: corpus-based language modeling, computational lexicography
This paper describes our word segmentation system and named entity recognition (NER) system for participating in the third SIGHAN Bakeoff.
This paper describes our effort on the task of edited region identification for parsing disfluent sentences in the Switchboard corpus.
We explore new feature spaces of a partof-speech (POS) hierarchy and relaxed for rough copy in the experiments.
To overcome the problem of not having enough manually labeled relation instances for supervised relation extraction methods, in this paper we propose a label propagation (LP) based semi-supervised learning algorithm for relation extraction task to learn from both labeled and unlabeled data.
That system employs constraint satisfaction, branch-and-bound and solution synthesis techniques to produce near linear-time processing for knowledge-based semantic analysis.
This is accomplished using a computational tool known as solution synthesis.
Mechanisms of compressidn in medical records for a collaborative study of breast cancer are described.
This paper describes an empirical study on the optimal granularity of the phrase structure rules and the optimal strategy for interleaving CFG parsing with unification in order to Unplement an efficient unification-based parsing system.
We claim that using "medium-grained" CEG phrase structure rules, which balance the computational cost of CPC; parsing and unification, are a cost-effective solution for making unification-based grammar both efficient and easy to maintain.
The grammar and the parser described in this paper are fully implemented and used as the Japanese analysis module in SL-TRANS, the speech-to-speech translation system of ATR.
A hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structures for semantic role labeling (SRL).
The hybrid kernel consists of two individual convolution kernels: a Path kernel, which captures predicate- argument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments.
We also combine our new hybrid tree kernel based method with the standard rich flat feature based method.
In this paper we deal with learning and forgetting of speech commands in speech dialogue systems.
We discuss two mathematical models for learning and four models for forgetting.
This paper introduces a new method for identifying candidate phrasal terms (also known as multiword units) which applies a nonparametric, rank-based heuristic measure.
We propose an approximation algorithm, based on canonical attributes and coarse- and fine-grained matching, that reduces the time complexity and ex-ecution time of thesaurus extraction with only a marginal performance penalty.
This part-ofspeech predictor will be used in a part-of-speech tagger to handle out-of-lexicon words.
We have developed an automated Japanese essay scoring system called Jess.
This paper presents SemFrame, a system that automatically induces the names and internal structures of semantic frames.
After SemFrame identifies sets of frame- evoking verb synsets, the conceptual density of nodes in the WordNet network for corresponding nouns and noun synsets is computed and analyzed.
We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages.
In this paper, we discuss the generation of paraphrases from predicate/argument structure using a simple, uniform generation methodology.
Central to our approach are lexico-grammatical resources which pair elementary semantic structures with their syntactic realization and a simple but powerful mechanism for combining resources.
We can also tokenize, lemmatise and part-of-speech tag the corpus, and load the data into a corpus query tool which supports sophisticated linguistic queries.
We have been developing ro-bust natural language tools for the au-tomated extraction of structured infor-mation from biomedical texts as part of a project we call MEDSTRACT.1 Here we will describe an architecture for developing databases for domain spe-cific information servers for research and support in the biomedical commu-nity.
In this paper, we want to describe a tag-ger/lemmatiser for Dutch medical voca-bulary, which consists of a full-form dic-tionary and a morphological recogniser for unknown vocabulary coupled to an expert system-like disambiguation mo-dule.
The tag-ger/lemmatiser currently functions as a lexical front-end for a syntactic parser.
This paper offers a provisional mathematical typology of metrical representations.
We present a cost-based (or energy-based) model of disambiguation.
This method of ambiguity resolution is implemented in DMTRANS PLUS, which is a second generation bi-directional English/Japanese machine translation system based on a massively parallel spreading activation paradigm developed at the Center for Machine Translation at Carnegie Mellon University.
This paper describes a system for the un-supervised learning of morphological suf-fixes and stems from word lists.
The sys-tem is composed of a generative probabil-ity model and a novel search algorithm.
This paper describes discriminative language modeling for a large vocabulary speech recognition task.
The models are encoded as deterministic weighted finite state automata, and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer.
This paper proposes a new discriminative training method, called minimum sample risk (MSR), of estimating parameters of language models for text input.
We have developed a parser generator for natural language processing.
This paper describes a speech interface to the Google search engine.
Rohrer.
One of the necessary tasks of a machine translation system is lexical transfer.
This paper contributes to the theory of substructural logics that are of interest to categorial grammarians.
In this paper, we describe a method of extracting information from an on-line resource for the construction of lexical entries for a multi-lingual, interlingual MT system (ULTRA).
We have been able to automatically generate lexical entries for interlingual concepts corresponding to nouns, verbs, adjectives and adverbs.
This paper proposes a description of German word order including phe-nomena considered as complex, such as scrambling, (partial) VP fronting and verbal pied piping.
This paper describes our work-in-progress in au-tomatic English-to-Korean text translation.
We tackle the ambiguity problem by incorporating syntactic and semantic categories in the anal-ysis grammar.
For system robustness, integration of two subsystems is under way: (i) a rule-based part-of-speech tagger to handle un-known words/constructions, and (ii) a word-for-word translator to handle other system failures.
plex constraints, that is, constraint disjunction (Crowhurst and Hewitt, 1995) and local constraint conjunction (Smolensky, 1995).
This paper presents a complete integrated NLG system which uses a Description logic for the content determination module, Segmented Discourse Representation Theory for the document structuring module and a lexicalized formalism for the tactical component.
We describe a bidirectional framework for natural language parsing and genera-tion, using a typed feature formalism arid an HPSG-based grammar with a parser arid generator derived from parallel pro-cessing algorithms.
We present an ap-proach to delayed lexical choice in gener-ation, based on subsumption within the sort hierarchy, using a lexicon of under-instantiated signs which are derived from the normal lexicon by lexical rules.
Schwa deletion is an important issue in grapheme-to-phoneme conversion for Indo- Aryan languages (IAL).
This paper describes a novel instance- based sentence boundary determination method for natural language generation that optimizes a set of criteria based on examples in a corpus.
its aggregation and referring expression generation capability).
This paper describes the referring expression generation mechanisms used in EPICURE, a computer program which produces natural language descriptions of cookery recipes.
We here explore a "fully" lexicalized Tree-Adjoining Grammar for discourse that takes the basic elements of a (monologic) discourse to be not simply clauses, but larger structures that are anchored on variously realized discourse cues.
1.n experiments comparing the four previously available machine-learning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best.
In this paper we discuss the notions of hypertext, blog, wiki and cognitive mapping in order to find a solution to the main problems of processing text data stored in these forms.
The paper also discusses problems of representing the valency information in case-frames arising in a spoken language environment.0.
This paper presents a transfer framework called LFT (Lexical-functional Transfer) for a machine translation system based on LFG (Lexical-functional Grammar).
Since LFG is a grammatical framework for sentence structure analysis of one language, for the purpose, we propose a new framework for specifying transfer rules with LFG schemata, which incorporates corresponding lexical functions of two different languages into an equational representation.
Multiple-class annotation is more challenging than single- class annotation.
In this paper, we took a single word classification approach to dealing with the multiple-class annotation problem using Support Vector Machines (SVMs).
This paper presents a multidimensional Dependency Grammar (DG), which decouples the dependency tree from word order, such that surface ordering is not determined by traversing the dependency tree.
We develop the notion of a word order domain structure, which is linked but structurally dissimilar to the syntactic dependency tree.
We then discuss the implementation of such a DG using constructs from a unification-based phrase-structure approach, namely Lexical-Functional Grammar (LFG).
We briefly present the Multi- Engine Machine Translation (MEMT) architecture, describing how it is well- suited for such an application.
We then describe our incorporation of interactive error correction throughout the system design.
A corpus-based analysis shows the existence of surface regularities related to metaphors.
These clues can be characterized by syntactic structures and lexical markers.
We present an object oriented model for representing the textual clues that were found.
We present a Korean question answering framework for restricted domains, called K-QARD.
This papers extends previous statistical models to class-to-class preferences, and presents a model that learns selectional preferences for classes of verbs.
The model is tested on a word sense disambiguation task which uses subject-verb and object-verb relationships extracted from a small sense-disambiguated corpus.
We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity.
Based on this formulation, different models for case identification and word-sense disambiguation are derived.
These results clearly demonstrate the superiority of the proposed models for deep-structure disambiguation.
This paper describes the use of a system of semantic rules to generate noun compounds, vague or polysemous words, and cases of metonymy.
This paper describes a hybrid approach to spontaneous speech parsing.
We have implemented an interactive, Web-based, chat-style machine translation system, supporting speech recognition and synthesis, local- or third-party correction of speech recognition and machine translation output, and online learning.
In this paper we describe the Senseval 3 Basque lexical sample task.
In this paper we describe two independent methods of improving speech recognizers: a machine translation (MT) method and a topic-based one.
morphological derivation and synonymy expansion) in web search strategies.
We first model standard language text using standard Chinese corpora and apply these models to detect anomalous chat text.
We study distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.
The REQUEST System is an experimental natural language query system based on large transformational grammar of English.
This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG).
In this paper we outline a lexical organization for Turkish that makes use of lexical rules for inflections, derivations, and lexical category changes to control the proliferation of lexical entries.
A lexical inheritance hierarchy facilitates the enforcement of type constraints.
A class of constraint-based categorial grammars is proposed in which the construction of both logical forms and strings is specified completely lexically.
In particular, we outline the structure of the geometrical scene description, the representation of events in a logic-oriented semantic representation language, the case-frame lexicon and the representation of the referential semantics based on the Flavor system.
We present two distinct methods for transliteration, one approach using phonetic transliteration, and the second using the temporal distribution of candidate pairs.
We then propose a novel score propagation method that utilizes the co-occurrence of transliteration pairs within document pairs.
We present a system for unsupervised tagging of words into classes produced by a distributional clustering technique called co-clustering.
We also show how state-level term emission models canbe augmented to account for morphological patterns using features automatically derived from the output of co-clustering.
Two classifiers -- Support Vector Machine (SVM) and Conditional Random Fields (CRFs) are applied here for the recognition of biomedical named entities.
This paper presents an original method and its implementation to extract terminology from corpora by combining linguistic filters and statistical methods.
MedSLT is a unidirectional medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different language pairs and subdomains.
This paper describes an all level approach on statistical natural language translation (SNLT).
Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a generalized form of a word alignment (WA).
The implementation of all the approaches combines generation algorithms in Prolog and HPSG grammars in ProFIT.
It is natural to combine a head-driven HPSG grammar with a head- driven generation algorithm.
We then switch to non-head-driven approaches.
for use with categorial grammar and indexed QLF, can be used with HPSG and Minimal Recursion Semantics.
The informing principles are those of modern 'lexicalist' unification-based linguistic theories: the English analysis grammar is based on Lexical-Functional Grammar (Bresnan, ed.
1982) and Generalized Phrase Structure Grammar (Gazdar et al 1985), the Japanese generation grammar on Categorial Grammar (Ades & Steedman 1982, Steedman 1985, Whitelock 1986).
We	propose	abootstrapping framework in which soft and hard matching pattern rules are combined in a cascading manner to realize a weakly supervised rule induction scheme.
Applying the noisy channel model to search query spelling correction requires an error model and a language model.
Typically, the error model relies on a weighted string edit distance measure.
Furthermore, we outline an exemplar-based approach in which se se-views are developed gradually and incrementally.
This paper argues for a novel data structure for the representation of discourse referents.
A so-called hashing list is employed to store discourse referents according to their grammatical features.
We introduce two general techniques to address the search problem, expectation-driven search and dy-namic grammar rule selection, and present the archi-tecture of an implemented generation system called IGEN.
Our approach uses a domain-specific genera-tion grammar that is automatically derived from a semantically tagged treebank.
In this paper we introduce an integrated approach for named entity translation deploying phrase-based translation, word-based translation, and transliteration modules into a single framework.
OF COLING-92, NANTES, Atm.
information technology test reports and medical finding reports.
Besides centering-based discourse analysis mechanisms for pronominal, nominal and bridging anaphora, SYNDIKATE is supplied with a learning module for automatically bootstrapping its domain knowledge as text analysis proceeds.
Semantic entropy is a measure of semantic ambiguity and uninformativeness.
This paper presents a method for measuring semantic entropy using translational distributions of words in parallel text corpora.
We identify problems with the Penn Tree- bank that render it imperfect for syntax- based machine translation and propose methods of relabeling the syntax trees to improve translation quality.
Prosodic patterns of discourse markers occurring in the recorded corpus have been analyzed.
This paper suggests a method to align Korean-English parallel corpus.
The proposed alignment al-gorithm is based on dynamic program-ming.
In this paper we present a model for statistical English-to-Korean transliteration that generates transliteration candidates with probability.
We investigate independent and relevant event-based extractive mutli-document summarization approaches.
replaced by case relations, using a a catalogue of patterns/interpretation pairs, a concept type hierarchy, and a set of selectional restriction rules on semantic interpretation types.
Two main problems in natural language generation are lexical selection and syntactic structure determination.
In this paper, a knowledge-based computational model which handles these two problems in interlingua approach is presented.
The developed system takes interlingua representations of individual sentences, performs lexical selection, and produces frame-based syntactic structures.
In this paper we show how two standard outputs from information extraction (IE) systems ?named entity annotations and scenario templates ?can be used to enhance access to text collections via a standard text browser.
We propose a method of organizing reading materials for vocabulary learning.
In this paper I will show how to use bidirectional unification grammars to define rever.sible relations between language dependent meaning representations.
As an alternative, we have developed an ef-ficient, trainable algorithm that uses a lex-icon with part-of-speech probabilities and a feed-forward neural network.
This work demonstrates the feasibility of using prior probabilities of part-of-speech assignments, as opposed to words or definite part-of-speech assignments, as contextual infor-mation.
This paper presents a trainable sentence planner for the MATCH dialog system.
A new type of thesaurus for word process-ing is proposed.
This paper focuses on IE tasks designed to support information discovery applications.
This paper describes the construction of language choice models for the microplanning of discourse relations in a Natural Language Generation system that attempts to generate appropriate texts for users with varying levels of literacy.
We describe how the design of microplanner is evolving.
This paper presents an implemented multi-tape two- level model capable of describing Semitic non-linear morphology.
This paper describes recent improvements in the weight estimation technique for sentence hypothesis rescoring using the N-Best formalism.
Automatically acquiring synonymous words (synonyms) from corpora is a challenging task.
Contextual information and the mapping from WordNet synsets to Cilin sense tags deal with word sense disambiguation.
Each generalized metaphor contains a recognition network, a basic mapping, additional transfer mappings, and an implicit intention component.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
Many probabilistic models for natural language are now written in terms of hierarchical tree structure.
We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
Relaxation labelling is an optimization technique used in many fields to solve constraint satisfaction problems.
In this paper we present our recent work on harvesting English-Chinese bitexts of the laws of Hong Kong from the Web and aligning them to the subpara-graph level via utilizing the number-ing system in the legal text hierarchy.
It is designed to provide telephone speech suitable for the development of automatic voice-interactive telephone services.
We describe a method for acquiring semantic cooccurrence restrictions for tuples of syntactically related words (e.g.
verb-object pairs) from text corpora automatically.
WordNet) to ambiguous words occurring in a syntactic dependency.
This paper proposes a generic mathematical formalism for the combination of various structures: strings, trees, dags, graphs and products of them.
After a short recall of our view of dependency grammars, we present two dependency parsers.
The second uses typed feature structures to add some semantic knowledge on dependency trees and parses in a more robust left to right manner.
A novel formalism is presented for Earley-like parsers.
It accommodates the simulation of non-deterministic pushdown automata.
In particular, the theory is applied to non-deterministic LR-parsers for RTN grammars.
A hybrid system is presented that combines statistical methods and multilingual parsing for detecting accurate collocational information from English, French, Spanish and Italian corpora.
We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors' of noun phrases.
By using these referential properties, our system determined the referents of noun phrases in Japanese sentences.
Furthermore we used the modifiers and possessors of noun phrases in determining the referents of noun phrases.
In this paper, a novel machine learning approach for the identification of named entity relations (NERs) called positive and negative case-based learning (PNCBL) is proposed.
This approach has been applied to the identification of domain-specific and cross-sentence NERs for Chinese texts.
In this paper, we propose and implement a model for bootstrapping parallel wordnets based on one monolingual wordnet and a set of cross-lingual lexical semantic relations.
In particular, we propose a set of inference rules to predict Chinese wordnet structure based on English wordnet and English-Chinese translation relations.
We automatically annotated documents with document structure and semantic tags by using taggers, and retrieve information by specifying structure represented by tags and words using ranked region algebra.
In this paper we discuss recent results from our efforts to make SPHINX, the CMU continuous-speech speaker- independent recognition system, robust to changes in the environment.
(Way and Gough, 2005) provide an in- depth comparison of their Example-Based Machine Translation (EBMT) system with a Statistical Machine Translation (SMT) system constructed from freely available tools.
This paper deals with the reference choices involved in the generation of argumentative text.
JANUS is a multi-lingual speech-to-speech translation system, which has been designed to translate spontaneous spoken language in a limited domain.
We end the paperby mentioning some projects for long-term research.
The second step interprets logical expression to generate network structure.
We have implemented set of programs which performs the stepwise translation.Experiments are in progress	for machinetranslation and question answering.
We argue that flexibility is an important property for unification-based formalisms.
Collocation translation is important for machine translation and many other NLP tasks.
First, dependency triples are extracted from Chinese and English corpora with dependency parsers.
Then, a dependency triple translation model is estimated using the EM algorithm based on a dependency correspondence assumption.
The generated triple translation model is used to extract collocation translations from two monolingual corpora.
We distinguish between context-free repre-sentability and context, free processing.
We examine the benefits of using multi-ple agents to produce explanations.
In this paper, we describe a corpus study of CRs in task-oriented dialogue and compare our findings to those reported in two prior studies.
Finally we identify form- function correlations which can inform the generation of CRs.
An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described.
We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies.
Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component.
The method has been applied in a speech translation project with large HPSG grammars.
For each language, the system detects the major news stories of the day using a group-average unsupervised agglomerative clustering process.
This paper describes named entity (NE) extraction based on a max-imum entropy (M.E.)
model and transformation rules.
This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature.
This paper introduces PhraseNet, a context- sensitive lexical semantic knowledge base system.
PhraseNet makes use of WordNet as an important knowledge source.
Immediate Dominance rules, metarules, and lexical rules are clearly distinguished in their form but all serve to capture the phenomenon of subcategorization.This paper proposes the extension of cooccurrence restrictions in GPSG to express constraints on the cooccurrence of categories within local trees.
We describe a simple sentence length approach to sentence alignment and a hybrid, multi-feature approach to perform word alignment.
We use a multi-feature approach with dictionary lookup as a primary technique and other methods such as local word grouping, transliteration similarity (edit-distance) and a nearest aligned neighbours approach to deal with many-to-many word alignment.
In order to incorporate long-distance information into the ME frame-work in a language model, a Whole Sentence Maximum Entropy Language Model (WSME) could be used.
In this paper, we propose the applica-tion of another sampling technique: the Perfect Sampling (PS).
Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
All the current rule-based and machine learning- based approaches for this task operate at the document level.
Polysemy is one of the major causes of difficulties in semantic clustering of words in a corpus.
The Hidden Markov Model (HMM) for part-of-speech (POS) tagging is typically based on tag trigrams.
This paper describes an ongoing effort to parse the Hebrew Bible.
We first constructed a cantillation treebank which encodes the prosodic structures of the text.
First, a very simple, randomized sentence-plan-generator (SPG) generates a potentially large list of pos-sible sentence plans for a given text-plan input.
Second, the sentence-plan-ranker (SPR) ranks the list of output sentence plans, and then selects the top-ranked plan.
The SPR uses ranking rules automatically learned from train-ing data.
In this paper, we investigate the use of multivariate Poisson model and feature weighting to learn naive Bayes text classifier.
This paper describes several key experiments in large vocabulary speech recognition.
We compare the performance of two feature preprocessing algorithms for microphone independence and we describe a new microphone adaptation algorithm based on selection among several codebook transformations.
Fixed weight smoothing of the mixture weights allowed the use of word-boundary-context-dependent triphone models for both speaker-dependent (SD) and speaker- independent (SI) recognition.
A new form of phonetic context model, the semiphone, is also introduced.
Word processors or computers used in Japan employ Japanese input method through keyboard stroke combined with Kana (phonetic) character to Kanji (ideographic, Chinese) character conversion technology.
The key factor of Kana-to-Kanji conversion technology is how to raise the accuracy of the conversion through the homophone processing, since we have so many homophonic Kanjis.
In this paper, we report the results of our Kana-to-Kanji conversion experiments which embody the homophone processing based on large scale collocation data.
We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.
The system divides query processing into the following phases:?Lexical lookup?Syntactic parsing?Semantic interpretation and selectional filtering?Quantifier scoping?Database query generation?Query optimization?Database retrievalThe syntactic and semantic rules used in the parsing and interpretation phases are expressed in a unification-based formalism.
Grammatical unification is then implemented simply as term unification in Prolog, which is the implementation language used in the system.In the semantic interpretation phase, logical form expressions are computed bottom-up by applying semantic interpretation rules keyed to the syntax rules.
We also demonstrate that an implementation of this algorithm is capable of learning auxiliary fronting in polar interrogatives (AFIPI) in English.
This document proposes a new taxon-omy for describing the quality of services which are based on spoken dialogue sys-tems (SDSs), and operated via a telephone interface.
We investigate the performance of the Structured Language Model (SLM) in terms of perplexity (PPL) when its components are modeled by connectionist models.
The EM training procedure can improve the connectionist models further, by using hidden events obtained by the SLM parser.
We developed a prototype information retrieval system which uses advanced natural language processing techniques to enhance the effectiveness of traditional key-word based document retrieval.
It utilizes the domain independent semantic form of The Rochester Interactive Planning System (TRIPS) with a domain independent stochastic surface generation module.
We show that a written text language model can be used to predict dialogue utterances from an over- generated word forest.
We present the concept of a "Segmental Neural Net" (SNN) for phonetic modeling in continuous speech recognition.
We segment Chinese text into words based on a word-based Chinese language model.
The Grapheme-to-Phoneme (G2P) conversion model achieves 98 % accuracy.
We show that large increases in speed can be obtained by tightly integrating the supertagger with the CCG grammar and parser.
We also further reduce the derivation space using constraints on category combination.
This paper proposes a probabilistic chunker to help the development of a partially bracketed corpus.
The chunker partitions the part-of-speech sequence into segments called chunks.
This paper focuses on mining tables from large-scale HTML texts.
Heuristic rules and cell similarities are employed to identify tables.
We also propose an algorithm to capture attribute-value relationships among table cells.
An analysis of English tense and aspect is presented that specifies temporal precedence relations within a sentence.
In this paper, we describe the research using machine learning techniques to build a comma checker to be integrated in a grammar checker for Basque.
This method learns empirical associations between free-form texts and canonical terms from human-assigned matches and determines a Linear Least Squares Fit (LLSF) mapping function which represents weighted connections between words in the texts and the canonical terms.
We introduce a learner capable of automatically extending large, manually written natural language Definite Clause Grammars with missing syntactic rules.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
This paper reports results on learning to automatically identify and predict problematic human-computer dialogues in a corpus of 4774 dialogues collected with the How May I Help You spoken dialogue system.
We train a problematic dialogue classifier using automatically-obtainable features that can identify problematic dialogues significantly better (23%) than the base-line.
This paper introduces a class of statistical mechanisms, called hidden understanding models, for natural language processing.
We address this problem by introducing hypergraphs for speech processing.
By converting ordinary word graphs to hyper- graphs one can reduce the number of edges considerably.
In this paper, we present a chunk based partial parsing system for spontaneous, conversational speech in unrestricted domains.
We show that the chunk parses produced by this parsing system can be usefully applied to the task of reranking Nbest lists from a speech recognizer, using a combination of chunk-based n-gram model scores and chunk coverage scores.The input for the system is Nbest lists generated from speech recognizer lattices.
The hypotheses from the Nbest lists are tagged for part of speech, "cleaned up" by a preprocessing pipe, parsed by a part of speech based chunk parser, and rescored using a backpropagation neural net trained on the chunk based scores.
We present a new framework for classifying common nouns that extends named- entity classification.
We propose a supervised, two-phase framework to address the problem of paraphrase recognition (PR).
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.
BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.
BABAR applies a Dempster-Shafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.
We present a novel method to identify effective surface text patterns using an inter- net search engine.
The paper deals with a new approach to importance evaluation of descriptive texts developed in the framework of SUSY, an experimental system in the domain of text summarization.
We present a comparative evaluation of two data-driven models used in translation selection of English-Korean machine translation.
Latent semantic analysis(LSA) and probabilistic latent semantic analysis (PLSA) are applied for the purpose of implementation of data-driven models in particular.
We have used k- nearest neighbor (k-NN) learning to select an appropriate translation of the unseen instances in the dictionary.
Thus, the indexation of such data requires indexing weighted automata.We present a general algorithm for the indexation of weighted automata.
We have used a recursively structured finite state machine for handling the changes in constituent order, implemented as a right-linear grammar backbone.
This paper presents an effective method of Korean compound noun segmen-tation based on lexical data extracted from corpus.
We perform a linguistic analysis of documents during indexing for information retrieval.
We describe the design and implementation of a question answering (QA) system called QARAB.
This paper describes an algorithm for open text shallow semantic parsing.
The algorithm relies on a frame dataset (FrameNet) and a semantic network (WordNet), to identify semantic relations between words in open text, as well as shallow semantic features associated with concepts in the text.
In this study wesuggest a practical dynamic semantic network available for NLP, which has the structure from associative concept dictionaries and the dynamics from a pulsed neural network.
We built the semantic network by means of constructing the platform called 'Brain Memory Model" based on a pulsed neural network first, then encoding data of associative concept dictionaries into it.
This paper describes a rule-based machine learning approach to morphological processing in the system called XMAS.
A Korean version of XMAS is effectively working in the English-Korean machine translation system KSHALT.
Parallel sentences are analyzed based on unification grammars, and structural matching is performed by making use of a similarity measure of word pairs in the two languages.
It promises to bridge the gap be-tween practical dialogue management and (pattern-based) dialogue model through integrating interaction patterns with the underling tasks and modeling interaction patterns via utterance groups using a high level construct different from dialogue act.
We present TISC, a language-independent and context-sensitive spelling checking and correction system designed to facilitate the automatic removal of non-word spelling errors in large corpora.
Its lexicon is derived from a very large corpus of raw text, without supervision, and contains word unigrams and word bigrams.
We introduce our system, FADA, which relies on question parsing, web page classification/clustering, and content extraction to find reliable distinct answers with high recall.
This paper explores the possibilities and limits of a discourse grammar applied to spontaneous speech.
Most discourse grammars (e.g.
Subjects describe the pixel-per-pixel development of sketch- maps on a computer screen.
Recently, we initiated a project to develop a phonetically-based spoken language understanding system called SUMMIT.
We describe a purely confidence-based geographic term disambiguation system that crucially relies on the notion of 損ositive?and 搉egative?context and methods for combining confidence-based disambiguation with measures of relevance to a user抯 query.
This paper presents a simple, general method for using the Mutual Information (MI) statistic trained on unannotated trivia questions to estimate question class/semantic tag correlation.
This MI method and a variety of question classifiers and semantic taggers are used to build short-answer extractors that show improvement over a hand-built match module using a similar question classifier and semantic tagger.
This paper describes machine learning based parsing and question classification for question answering.
This paper describes and compares a number of statistical and machine learning techniques for ordering se-quences of adjectives in the context of a natural language generation system.
The rule-based transfer and generation module takes the parsing tree as the input and operates on the information of POS tag, semantics or even the lexicon.
A named entity tagger and a dependency parser are used to analyze the question accurately.
This paper establishes a framework un-der which various aspects of prosodic morphology, such as templatic morphol-ogy and infixation, can be handled under two-level theory using an implemented multi-tape two-level model.
The paper provides a new computational analysis of root-and-pattern morphology based on prosody.
In the paper we describe a project to develop a system which has word-sense acquisition from information contained in computerized dictionaries and knowledge organization as its main objectives.
The first is support vectors which are extracted from the training samples by a machine learning technique, Support Vector Machines(SVM).
Graph grammars are a multidimensional generalization of linear string grammars.
In graph grammars string rewrite rules are generalized into graph rewrite rules.This paper presents a graph grammar formalism and parsing scheme for parsing languages with inherent configurationalflavor.
In this chapter a graph grammar formalism based on the notions of relational graph grammars (Rajlich 1975) and attributed programmed graph grammars (Bunke 1982) is developed for parsing languages with configurational structure.Definition 1.1 (relational graph, r-graph)Let ARCS, NODES, and PROPS be finite setsof symbols.
Morphological	r-graphrepresentation of word "ihmisten"	(thehumans).Definition 1.2 (r-production) An r-production RP is a pair: RP = (LS, RS)LS (left side) and RS	(right	side)	arer-graphs.
ethnologue.com).
Some familiarity with Montague's PTO and the basic DCG mechanism is assumed.Ke words Compositional Semantics, Definite Clause Grammar, Friedman Warren Algorithm, Intensional Logic, Montague Grammmar, Natural Language Processing.
PROLOG.
which are obtained mainly from word alignment.
We use blocks to infer better word alignment and improved word alignment which, in turn, leads to better inference of blocks.
We propose two new probabilistic models based on the inner- outer segmentations and use EM algorithms for estimating the models?parameters.
Explanation-based generalization is used to extract a specialized grammar from the original one using a training corpus of parse trees.
Figure 1 shows five examples of implicit parse trees.
One is a transfer model with monolingual head automata for analysis and generation; the ocher is a direct transduction model based on bilingual head transducers.
This paper presents a project report on NP, a working Natural language Plan inference system that uses feature structures and is based on assumptions.
The plan inference component is implemented using a feature-structure-based inference engine and models of plan recognition, prediction, and inference.
The inference engine is implemented using a rewriting system for pattern-matching, and an Assumption-based Truth Maintenance System (ATMS) for conjunctions.
The NP system is used to infer dialog- and domain-level plans, among other types.Original contributions include: a plan inference system that works directly from feature structures; a plan inference System that uses an ATMS and plan schema actions with preconditions and effects to infer hierarchical and chained plans; and, an inference engine that works with multiple feature- structure assertions and rules.Project Goal.
We present a method that integrates term variation in a hybrid ATR approach, in which term candidates are recognised by a set of linguistic filters and termhood assignment is based on joint frequency of occurrence of all term variants.
The quantitative language and translation models are based on relations between lexical heads of phrases.
This paper introduces a semi-supervised learning framework for creating training material, namely active annotation.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
We address the role of two phenomena with respect to the recovery of these relations: (1) the referential properties of tense, and (2) the role of temporal constraints imposed by coherence relations.
We propose a fast and reliable Question-answering (QA) system in Korean, which uses a predictive answer indexer based on 2-pass scoring method.
JAVOX provides a mechanism for the development of spoken-language systems from existing desktop applications.
In Combinatory Categorial Granunar (CCG) [Ste90, Ste911, semantic function-argument structures are corn- positionally produced through the course of a derivation.
In this paper we focus on how to improve pronoun resolution using the statistics- based semantic compatibility information.
In this paper, we present a system which can extract syntactic feature structures from a Korean Treebank (Sejong Tree- bank) to develop a Feature-based Lexicalized Tree Adjoining Grammars.
III we present results of our studies on voice modifications and transformations using the basic system.
IV results from our studies to determine the factors responsible for unnatural quality of synthetic speech from our system.
We propose a signal dependent analysis-synthesis scheme in Sec.
The lexical acquisition system presented in this paper incrementally updates linguistic properties of unknown words inferred from their surrounding context by parsing sentences with an HPSG grammar for German.
This paper presents on-going research on the building of an electronic dictionary of frozen sentences of European Portuguese.
We incorporate multiple anaphora resolution factors into a statistical framework — specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.
The second experiment investigates a method for unsupervised learning of gender/number/animaticity information.
Next, zero pronouns within a Japanese sentence are identified by using the syntactic and semantic stricture of the Japanese sentence and their antecedents within the English sentence are identified by using the characteristics of anaphoric and deictic expressions in English.
In this paper, I revise generalized phrase structure grammar (GPSG) linguistic theory so that it is more tractable and linguistically constrained.
Conventional algorithms are based on word-by-word models which require bilingual data with hundreds of thousand sentences for training.
The algorithm also poses the advantage of producing a tagged corpus for word sense disambiguation.
The model takes into account the effects of linguistic features, such as tense/aspect, temporal connectives, and discourse structures, and makes use of the fact that events are represented in different temporal structures.
A machine learning approach, Weighted Bayesian Classifier, is developed to map their combined effects to the corresponding relations.
An empirical study is conducted to investigate different combination methods, including lexical- based, grammatical-based, and role-based methods.
We investigate a series of graph-theoretic constraints on non-projective dependency parsing and their effect on expressivity, i.e.
In particular, we define a new measure for the degree of non-projectivity in an acyclic dependency graph obeying the single-head constraint.
Dialect groupings can be discovered objectively and automatically by cluster analysis of phonetic transcriptions such as those found in a. linguistic atlas.
This article presents a compression-based adaptive algorithm for Chinese Pinyin input.
Compression by Partial Match (PPM) is an adaptive statistical modelling technique that is widely used in the field of text compression.
In this paper, we describe a novel approach to spoken language analysis for translation, which uses a combination of grammar-based phrase-level parsing and automatic classification.
In this paper, we present a hybrid Chi-nese language model based on a com-bination of ontology with statistical method.
To evaluate the performance of this language model, we completed two groups of experiments on texts re-ordering for Chinese information re-trieval and texts similarity computing.
In this paper, we propose a method to extract descriptions of techni-cal terms from Web pages in order to utilize the World Wide Web as an encyclopedia.
We use linguistic patterns and HTML text structures to extract text fragments contain-ing term descriptions.
This paper presents our implemented computational model for interpreting and generating indirect answers to Yes-No questions.
This paper describes a method for detecting the boundaries of quotations and inserted clauses and that for improving the dependency accuracy by applying the detected boundaries to dependency structure analysis.
The quotations and inserted clauses are determined by using an SVM-based text chunking method that considers information on morphemes, pauses, fillers, etc.
We present translation results on the shared task 擡xploiting Parallel Texts for Statistical Machine Translation?generated by a chart parsing decoder operating on phrase tables augmented and generalized with target language syntactic categories.
We use a target language parser to generate parse trees for each sentence on the target side of the bilingual training corpus, matching them with phrase table lattices built for the corresponding source sentence.
We present BAYESUM (for 揃ayesian summarization?, a model for sentence extraction in query-focused summarization.
In this paper, we evaluate a two-pass parsing strategy proposed for the so-called `lexicalized' grammar.
Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization.
A number of machine translation systems based on the learning algorithms are presented.
To overcome this problem, we propose a method of machine translation using a Recursive Chain-linktype Learning.
From the results of evaluation experiments, we confirmed the effectiveness of Recursive Chain-link-type Learning.
This paper presents a computational model of incremental utterance produc-tion in task-oriented dialogues.
We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors.
Word co-occurrences form a graph, regarding words as nodes and co-occurrence relations as branches.
Thus, a co-occurrence graph can be constructed by co-occurrence relations in a corpus.
This paper discusses a clustering method of the co-occurrence graph, the decomposition of the graph, from a graph-theoretical viewpoint.
This paper describes our method based on Support Vector Machines for automatically assigning semantic roles to constituents of English sentences.
Automatic thesaurus construction is developed to solve the problem.
Conventional way to automatically construct thesaurus is by finding similar words based on context vector models and then organizing similar words into thesaurus structure.
Latent Semantic Index (LSI) was commonly used to overcome the problems.
We introduce an algorithm for scope resolution in underspecified semantic representations.
Scope preferences are suggested on the basis of semantic argument structure.
We develop a factored-model statistical parser for the Penn Chinese M-eebank, showing the implications of gross statistical differences between WSJ and Chinese M-eebanks for the most general methods of parser adaptation.
We combined a Japanese-Japanese dictionary, an English-Japanese dictionary, an acronym dictionary, an information science dictionary, and our office telephone directory.This paper is constructed as follows.
From an annotated corpus 126 def-inite clause grammar rules were constructed.
Tagging compound verb groups in an annotated corpus exploiting the verb rules is described.Keywords: compound verb groups, chunking, morphosyntactic tagging, inductive logic pro-gramming
The DCG (Definite Clause Grammar) formalism (Pereira & Warren 80) is adopted.
Section 2 gives a grouping of sentences containing coordinate conjunctions.
We present a prototype system called PROFILE which uses a client-server architecture to ex-tract noun-phrase descriptions of enti-ties such as people, places, and organiza-tions.
We present an evaluation of the approach and its applications to natural language generation and summarization.
This work investigates the use of text analysis in predicting the location of intonational phrase boundaries in natural speech, through analyzing 298 utterances from the DARPA Air Travel Information Service database.
For statistical modeling, we employ Classification and Regression Tree (CART) techniques.
to integrate media indexing with computer visualization to achieve effective content-based access to video information.
Text metadata.
This approach shows how to make multimedia information accessible to a text-based visualization system.
The IDAS natural-language generation system uses a KL-ONE type classifier to perform content determination, surface realisation, and part of text planning.
The first is a sentence extraction- based approach while the second is a language generation-based approach.
We present a new search algorithm for very large vocabulary continuous speech recognition.
TBLbased error correction is used to further improve chunking performance.
We also find that the hardest point of Chinese chunking is to identify the chunking boundary inside noun-noun sequences1.
In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence.
We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text.
We explore the descriptive power, in terms of syntactic phenomena, of a formalism that extends Tree- Adjoining Grammar (TAG) by adding a fourth level of hierarchical decomposition to the three levels TAG already employs.
Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation.
We extend Combinatory Categorial Grammar (CCG) with a generalized notion of multidimensional sign, inspired by the types of representations found in constraint-based frameworks like HPSG or LFG.
Specifically, the score is the probability of acoustic features of a hypothesized word sequence given an associated syntactic parse, based on acoustic and "language" (prosody/syntax) models that represent probabilities in terms of abstract prosodic labels.
This allows the extension of Translation Memory systems towards Example-based Machine Translation.
We describe the use of XML tokenisa-tion, tagging and mark-up tools to pre-pare a corpus for parsing.
This paper reports progress in development of evaluation methodologies for natural language systems.
This paper presents an algorithm for the compilation of regular formalisms with rule features into finite-state automata.
Tree Unification Grammar is a declarative unification-basid linguistic framework.
This paper presents a pilot study of the use of phrasal Statistical Machine Translation (SMT) techniques to identify and correct writing errors made by learners of English as a Second Language (ESL).
This paper is concerned with learning categorial grammars in Gold抯 model.
In tins paper, we describe a search procedure for sta-tistical machine translation (MT) based on dynamic programming (DP).
We present CarmelTC, a novel hybrid text classification approach for automatic essay grading.
We explore a novel computational approach to identifying 揷onstructions?or 搈ulti-word expressions?
(MWEs) in an annotated corpus.
The statistical grammar formalism used is that of stochastic tree substitution grammars (STSGs), such as used in Data-Oriented Parsing.
We present an algorithm for calculating the expected frequencies of arbitrary subtrees given the parameters of an STSG, and a method for estimating the parameters of an STSG given observed frequencies in a tree bank.
Our system disambiguates senses of a target word in a context by selecting a substituent among WordNet relatives of the target word, such as synonyms, hypernyms, meronyms and so on.
This proposal is considered with respect to two types' of discourse phenomena; anaphoric reference to event entities, and temporal binding.
Our method is based on a theoretically clear statistical model that integrates linguistic, acoustic and situational information.
We report tagging experiments on Japanese and English dialogue corpora manually labeled with SA tags.
We also report on some translation experiments on positive response expressions using SA tags.
This paper proposes a statistical method for learning dependency preference of Japanese subordinate clauses, in which scope embedding preference of subordinate clauses is exploited as a useful information source for disambiguat-ing dependencies between subordinate clauses.
Word forming units are thus relevant cues for the identification of terms in domain- specific texts.
This article describes a method for the automatic extraction of terms relying on the detection of classical prefixes and word-initial combining forms.
Word-forming units are identified using a regular expression.
This paper will present an enhanced probabilistic model for Chinese word segmentation and part-of-speech (POS) tagging.
We analyze humorous spoken conversations from a classic comedy television show, FRIENDS, by examining acoustic- prosodic and linguistic features and their utility in automatic humor recognition.
We propose a kana-kanji conversion system with input support based on prediction.
Experiments are presented which measure the perplexity reduction derived from incorporating into the predictive model utilised in a standard tag-n-gram part-of-speech tagger, contextual information from previous sentences of a document.
The baseline model utilized is a maximum entropy-based tag-ngram tagging model, embodying a standard tag-n-gram approach to tagging: i.e.
constraints for tag trigrams, bigrams, and and the word-tag occurrence frequency of the specific word being tagged, form the basis of prediction.
But it is another thing to demonstrate that extrasentential context supports an improvement in perplexity vis-a-vis a part-of-speech tagging model which employs state-of-the-art techniques: such as, for instance, the tagging model of a maximum entropy tag-n-grambased tagger.The present paper undertakes just such a demonstration.
In this paper, two approaches are presented which generalize the verification of coindexing constraints to deficient descriptions.
Variants of tree-distance are considered, including whole-vs-sub tree, node weighting, wild cards and lexical emphasis.
We derive string-distance as a special case of tree-distance and show that a particular parameterisation of tree-distance outperforms the string-distance measure.
We propose a new lexicalized grammar formalism called Lexicalized Tree Automata-based Grammar, which lexicalizes tree acceptors instead of trees themselves.
We discuss the properties of the grammar and present a chart parsing algorithm.
We have implemented a translation module for conversational texts using this formalism, and applied it to an experimental automaticinterpretation system (speech	translationsystem).
This paper presents a method to resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks.
Most previous word sense disambiguation approaches based on neural networks have limitations due to their huge feature set size.
In addition, a pattern construction method is described through which paraphrasing patterns can be efficiently learned from a paraphrase corpus and human experience.
Using the implemented paraphraser and the obtained patterns, a paraphrasing experiment was conducted and the results were evaluated.
Three types of syntactic variations were studied : permutation, expansion and substitution.
The graph obtained reveals the global organisation of research topics in the corpus.
The syntactic preferences such coded can be easily used to compute with semantic preferences.
Assuming that the goal of a person name query is to find references to a particular person, we argue that one can derive better relevance scores using probabilities derived from a language model of personal names than one can using corpus based occurrence frequencies such as inverse document frequency (idf).
We describe our experience with automatic alignment of sentences in parallel English-Chinese texts.
This paper discusses an information extraction (IE) system, Textract, in natural language (NL) question answering (QA) and examines the role of IE in QA application.
We have explored statistical models that use different representational units for parsing.
The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem.
We describe a method of text summarization that produces indicative-informative abstracts for technical papers.
We have carried out an evaluation to assess indicative-ness and text acceptability relying on human judgment.
Sentence ranking is a crucial part of generating text summaries.
We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking: A simple paragraph-based approach intended as a baseline, two word-based approaches, and two coherence-based approaches.
Our results suggest poor performance for the simple paragraph-based approach, whereas word- based approaches perform remarkably well.
We present a novel automatic approach to constructing a bilingual semantic network梩he BiFrameNet, to enhance statistical and transfer-based machine translation systems.
We automatically induce Chinese example sentences and their semantic roles, based on semantic structure alignment from the first stage of our work, as well as shallow syntactic structure.
We describe a method for retrieving counts for adjective-noun, noun-noun, and verb- object bigrams from the web by querying a search engine.
We deal with the automated acquisition of a Spanish medical subword lexicon from an already existing Portuguese seed lexicon.
Using two non-parallel monolingual corpora we determined Spanish lexeme candidates from Portuguese seed lexicon entries by heuristic cognate mapping.
We validated the emergent lexical translation hypotheses by determining the similarity of fixed-window context vectors on the basis of Portuguese and Spanish text corpora.
We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution).
In information extraction systems, pattern matchers are widely used to identify information of interest in a sentence.
In this paper, pattern matching in the TEXTRA CT information extraction system is described.
It comprises a concept search which identifies key words representing a concept, and a template pattern search which identifies patterns of words and phrases.
This paper proposes a new unsupervised learning method for obtaining English part-ofspeech(POS) disambiguation rules which would improve the accuracy of a PUS tagger.
This method has been implemented in the experimental system APRAS (Automatic POS Rule Acquisition System), which extracts PUS disambiguation rules from plain text corpora by utilizing different types of coded linguistic knowledge, i.e., PUS tagging rules and syntactic parsing rules, which are already stored in a fully implemented MT system.In our experiment, the obtained rules were applied to 1.7% of the sentences in a non-training corpus.
This paper presents a report processing method with object-centered semantics.
Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering.
A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking.
To cope with the unit problem, we propose a character-based chunking method.
Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs.
This paper presents a methodology for discovering domain-specific concepts and relationships in an at-tempt to extend WordNet.
This paper describes a system (RAREAS) which synthesizes marine weather forecasts directly from formatted weather data.
Synthesis of Arctic Marine Weather ForecastsThe RAREAS system was developed during a five-month effort to explore the feasibility of synthesizing marine weather bulletins from formatted weather forecast data.
We describe Talk'n'Travel, a spoken dialogue language system for making air travel plans over the telephone.
Literal movement grammars (LMGs) provide a general account of extraposition phenomena through an attribute mechanism allowing top-down displacement of syntactical information.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
Compared with standard HMM-based tagger, this tagger incorporates more contextual information into a lexical entry.
One improvement incorporates syntactic knowledge.
We predict discourse segment boundaries from linguistic features of utterances, using a corpus of spoken narratives as data.
We present two methods for developing segmentation algorithms from training data: hand tuning and machine learning.
We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics.
In our interface architecture the machine learning module replaces an elaborate semantic analysis component.
We use an existing interface to a production planning and control system as evaluation and compare the results achieved by different instance-based and model-based learning algorithms.
This paper describes DialogueView, a tool for annotating dialogues with utter-ance boundaries, speech repairs, speech act tags, and discourse segments.
The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
The main purpose of this paper is the exploitation and application of an audio and video bimodal corpus of the Chinese language in broadcasting.
This paper presents a method to construct Japanese KATAKANA variant list from large corpus.
At step 2, our system collects candidate pairs of KATAKANA variants from the collected KATAKANA words using a spelling similarity which is based on the edit distance.
This technique adopts an efficient LR parsing method and uses a reverse LR table constructed besides a standard LR table.
Triphone analysis is a new correction strategy which combines phonemic transcription with trigram analysis.
In this paper a new version of Montague Grammar (MG) is developed, which is suitable for application in question-answering systems.
It generates NL sentences and their logicalforms 'in parallel'.
This paper describes our attempt at NomBank-based automatic Semantic Role Labeling (SRL).
The framework uses a linear, pipeline based, bottom-up parsing algorithm, with a look ahead local search that serves to make the local predictions more robust.
In this paper, we describe a Dynamic Programming (DP) based search algorithm for statistical translation and present experimental results.
The statistical translation uses two sources of information: a translation model and a language model.
All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.
We present an upcoming web-based centre for information and documentation on language technology (LT) in Sweden and/or in Swedish.
Some ways to collect and represent data using data harvesting and XML are suggested.
In this work we present a method for Named Entity Recognition (NER).
One such example, explored in this article, is the mention detection and recognition task in the Automatic Content Extraction project, with the goal of identifying named, nominal or pronominal references to real-world entities梞entions?and labeling them with three types of information: entity type, entity subtype and mention type.
In this paper, a generic system that generates parsers from parsing schemata is applied to the particular case of the XTAG English grammar.
In order to be able to generate XTAG parsers, some transformations are made to the grammar, and TAG parsing schemata are extended with feature structure unification support and a simple tree filtering mechanism.
We propose a new method to improve the accuracy of Text Categorization using two- dimensional clustering.
Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing.
We report on an investigation of the pragmatic category of topic in Danish dialog and its correlation to surface features of NPs.
We discovered that NPs in epistemic matrix clauses (e.g.
We apply machine learning techniques to classify automatically a set of verbs into lexical semantic classes, based on distributional approximations of diatheses, extracted from a very large annotated corpus.
IntroductionOne useful way to look at computational morphology and phonology is in terms of transductions, that is, n-ary word relations definable by the element-wise concatenation of n-tuple labels along paths in a finite directed labeled graph.
coreferential) relations.
We present the design and development of a Hidden Markov Model for the division of news broadcasts into story segments.
In this paper, we propose keyword extraction method for dictation of radio news which consists of several domains.
This paper presents the use of probabilistic class-based lexica for disambiguation in target-word selection.
That is, only information provided by the target-verb, enriched by the condensed information of a probabilistic class-based lexi-con, is used.
'This paper describes a hybrid parsing method for Japanese which uses both a hand-crafted gram-mar and a statistical technique.
This utilization of alter-native trees enables us to construct a new statis-tical model called nipleilQuadruplet Model.
In this work, we present a new semantic language modeling approach to model news stories in the Topic Detection and Tracking (TDT) task.
We also cast the link detection sub- task of TDT as a two-class classification problem in which the features of each sample consist of the generative log-likelihood ratios from each semantic class.
This paper reports on the development of two spoken language collaborative interface agents built with the Collagen system.
This paper describes an exploration of the implicit synonymy relationship expressed by synonym lists in an on-line thesaurus.
This paper reports on the development of a head-lexicalized PCFG for the disambiguation of German morphological analyses.
The grammar is trained on unlabeled data using the Inside-Outside algorithm.
In this paper we investigate the use of linguistic knowledge in passage retrieval as part of an open-domain question answering system.
We use annotation produced by a deep syntactic dependency parser for Dutch, Alpino, to extract various kinds of linguistic features and syntactic units to be included in a multi-layer index.
We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers.
A simple method is described for the detection of semantically self-contained word phrase segments in title-like texts.
The method is based on a predetermined list of acceptable types of nominative syntactic patterns which can be recognized using a small domain-indepen-dent dictionary.
The records are used for the compilation of Key Word Phrase subject indexes (KWPSI).
We proposed a novel summarization system architecture which employs Gene Expression Programming technique as its learning mechanism.
A major architectural decision in designing a disambiguation model for segmentation and Part-of-Speech (POS) tagging in Semitic languages concerns the choice of the input-output terminal symbols over which the probability distributions are defined.
In this paper we develop a segmenter and a tagger for Hebrew based on Hidden Markov Models (HMMs).
We describe a probabilistic approach to content selection for meeting summarization.
We use skip- chain Conditional Random Fields (CRF) to model non-local pragmatic dependencies between paired utterances such as QUESTION-ANSWER that typically appear together in summaries, and show that these models outperform linear-chain CRFs and Bayesian models in the task.
We also discuss different approaches for ranking all utterances in a sequence using CRFs.
verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction.
We have also devised an automatic acquisition process for Web-derived answer patterns (AP) which utilizes question-answer pairs from TREC QA, the Google search engine and the Web.
This paper describes recent work on the Unisys ATIS Spoken Language System, and reports benchmark results on natural language, spoken language, and speech recognition.
After discussing various approaches to pho-netic alignment, I present a new algorithm that com-bines a number of techniques developed for se-quence comparison with a scoring scheme for com-puting phonetic similarity on the basis of multival-ued features.
In particular, we motivate a semantic-rich discourse representation and discuss the impact of refined discourse structure on question answering.
In this paper we explore the idea of using natural language generation systems for corpus annotation.
We focus here on exploring the use of the KPML (Komet-Penman MultiLingual) generation system for corpus annotation.
We describe the kinds of linguistic information covered in KPML and show the steps involved in creating a standard XML corpus representation from KPML抯 generation output.
Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity.
We present results on the relation discovery task, which addresses some of the shortcomings of supervised relation extraction by applying minimally supervised methods.
Previous work on relation discovery used a semantic space based on a term-bydocument matrix.
We find that representations based on term co-occurrence perform significantly better.
This paper describes a set of comparative exper-iments, including cross朿orpus evaluation, be-tween five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW, Decision Lists, and Boosting.
This paper proposes a two-phase example-based machine translation methodology which develops translation templates from examples and then translates using template matching.
This method improves translation quality and facilitates customization of machine translation systems.
This paper focuses on the automatic learning of translation templates.
Two-Phase Example-based Machine TranslationFigure 1 outlines our two-phase example-based machine
A system under construction is outlined which incorporates morphological checks (using new two-level error rules) over a directed letter graph, tag positional trigrams and partial parking.
Hence 'co-operative' error processing.
This paper presents methods for a qual-itative, unbiased comparison of lexical association measures and the results we have obtained for adjective-noun pairs and preposition-noun-verb triples ex-tracted from German corpora.
In this document, we present a language which associates type construction principles to constraint logic programming.
Finally, we give the procedural semantics of our language, combining type construction with SLDresolution.
In the context of the Papillon project, which aims at creating a multilingual lexical database (MLDB), we have developed Jeminie, an adaptable system that helps automatically building interlingual lexical databases from existing lexical resources.
In this paper, we will present a rather ,simplified description of an algorithm for transformational analysis (decomposition) of English sentences.
This paper describes GENIE, an object-oriented architecture that generates text with the intent of extending user expertise in interactive environments.
This paper presents a method for extracting subcorpora documenting different subcategorization frames for verbs, nouns, and adjectives in the 100 mio.
word British National Corpus.
Usual plan-based approaches to speech act interpretation require that performing a speech act implies its success.
For the task of recognizing dialogue acts, we are applying the Transformation-Based Learning (TBL) machine learning algorithm.
We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense, aspect, temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure.
The method combines a constraint-based approach with an approach based on preferences: we exploit the HPSG type hierarchy and unification to arrive at a temporal structure using constraints placed on that structure by tense, aspect, rhetorical structure and temporal expressions, and we use the temporal centering preferences described by (Kameyama et al., 1993; Poesio, 1994) to rate the possibilities for temporal structure and choose the best among them.The starting point for this work was Scha and Polanyi's discourse grammar (Scha & Polanyi 1988; Priist et a/ 1994).
This paper will focus on our temporal processing algorithm, and in particular on our analysis of narrative progression, rhetorical structure, perfects and temporal expressions.
There has been much interest in using phrasal movement to improve statistical machine translation.
Next, we consider the problem of detecting miscues during oral reading.
It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination.
This paper introduces new specificity determining methods for terms based on information theoretic measures.
We present our work on combining large- scale statistical approaches with local linguistic analysis and graph-based machine learning techniques to compute a combined measure of semantic similarity between terms and documents for application in information extraction, question answering, and summarisation.
We are using ITSPOKE to generate an empirically-based understanding of the ramifications of adding spoken language capabilities to text-based dialogue tutors.
We study unsupervised methods for learning refinements of the nonterminals in a treebank.
The e-rater system Tm I is an operational automated essay scoring system, developed at Educational Testing Service (ETS).
We fo-cus in this paper on the part of the se-mantic analyser which deals with seman-tic composition.
We explain how we use the domain model to handle metonymy dynamically, and more generally, to un-derlie semantic composition, using the knowledge descriptions attached to each concept of our ontology as a kind of concept-level, multiple-role qualia struc-ture.
We rely for this on a heuristic path search algorithm that exploits the graphic aspects of the conceptual graphs formalism.
This suggests a potentially broader role for disfluency modeling in adapting text-based tools for processing conversational speech.
Coupled- Context--Grammars are a generalization of context-free grammars obtained by combining limiter- Initials to parentheses which can only be an simultaneously.
Pos811D.
Second, the Coupled-Context-Free Grammars consider elements rewritten simultaneously as components of a parenthesis.
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.
Dependency StructureThe structuring principle of constituency trees is concatenation and the part-whole .relationship.
by a x-bar notation.
We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
This paper describes a prototype English-Japanese machine translation (MT) system developed at the Science Institute of IBM Japan, Ltd.
We present a model for sentence compression that uses a discriminative large- margin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.
To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames.
In this paper we present an empirical study of the potential and limitation of sentence extraction in text summarization.
Term recognition and clustering are key topics in automatic knowledge acquisition and text mining.
Beside features that represent contextual patterns, we use lexical and functional similarities between terms to define a combined similarity measure.
We introduce a multi-language named-entity recognition system based on HMM.
In this paper, we describe the architecture and accuracy of the named-entity system, and report preliminary experiments on automatic bilingual named-entity dictionary construction using the Japanese and English named-entity recognizer.
In this paper we present ONTOSCORE, a system for scoring sets of concepts on the basis of an ontology.
We apply our system to the task of scoring alternative speech recognition hypotheses (SRH) in terms of their semantic coherence.
We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system.
We present a machine-learning approach to modeling the distribution of noun phrases (NPs) within clauses with respect to a fine-grained taxonomy of grammatical relations.
Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy.
representation methods, hypermedia maps.
hypermedia writing.
?a									Address of author									Adjective 										Agent	.
1			Back梤eferencing in text		?.
11			Belief structure	.
.
46 Chaining, inference method .
In the paper the use of the notion "obligatory complement" in syntactic analysis is discussed.
We present a system which retrieves answers to queries based on coreference relationships between entities and events in the query and documents.
Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task.
We argue that the detection of entailment and contradiction relations between texts is a minimal metric for the evaluation of text understanding systems.
This paper describes a system to create animated 3D scenes of car accidents from reports written in Swedish.
The text-to-scene conversion process consists of two stages.
handwriting).
We present the final MIAMM system, a multimodal dialogue system that employs speech, haptic interaction and novel techniques of information visualization to allow a natural and fast access to large multimedia databases on small handheld devices.
This paper reports preliminary experiments on part-whole extraction from a corpus of anatomy definitions, using a fully automatic iterative algorithm to learn simple lexico-syntactic patterns from multiword terms.
The experiments show that meronyms can be extracted using these patterns.
In this paper we describe a method of classifying Japanese text documents using domain specific kanji characters.
We extracted these domain specific kanji characters by x2 method.
This paper proposes an LR parsing algorithm modified for grammars with feature-based categories.
The description of lexical predicates within the framework of frame semantics provides a natural method for selecting and structuring appropriate tagsets.
It represents two classes of concepts: objects of discourse and action schemata, the former resulting from nominal syntagms and the latter from the 'processes'.
The current paper introduces an intonational analysis of mono- and di-syllabic words based upon such a framework and compares results in progress with previous work on intonation.
The paper describes the construction of a lexical transducer for Korean that can be used for stemming and generation.
We will show by detailed examples the working of synchronous TAGs and some of its applications, for example in generation and in machine translation.The second development is the design of LR-style parsers for TAGs.
In this paper a method for controlling the dialog in a natural language (NL) system is presented.
It provides a deep modeling of information processing based on time dependent propositional attitudes of the interacting agents.
First, we describe the CU Communicator system that integrates speech recognition, synthesis and natural language understanding technologies using the DARPA Hub Architecture.
Next, we describe our more recent work on the CU Move dialog system for in-vehicle route planning and guidance.
We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics.
We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities.
In this paper we propose the trigger language model based IR system to resolve the problem.
We introduce the relative parameters into the document language model to form the trigger language model based IR system.
This paper presents a pipelined ar-chitecture for the generation of German monologues with contextually appropriate word order and accent placements for the realization of focus/background structures.
Word order is realized by grammatical competition based on linear prece-dence (LP) rules which are based on the discourse-relational features.
This paper presents a formal account of the temporal interpretation of text.
The distinct natural interpretations of texts with similar syntax are explained in terms of defeasible rules characterising causal laws and Gricean-style pragmatic maxims.
In syntax, the trend nowadays is towards lexicalized grammar formalisms.
This paper will present an algorithm for learning a link granunar of German.
a link grammar parse): Figure 1 shows a linkage for an English sentence.
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 11-18.
The inadequacy of some common strategies to answer this question in Machine Translation (MT) systems is shown.
In this paper, a flexible annotation schema called (SSTC) is introduced.
In order to describe the correspondence between different languages, we propose a variant of SSTC called synchronous SSTC (S-SSTC).
in Machine Translation).
The S-SSTC is very well suited for the construction of a Bilingual Knowledge Bank (BKB), where the examples are kept in form of S-SSTCs.KEYWORDS: parallel text, Structured String-Tree Correspondence (SSTC), Synchronous SSTC, Bilingual Knowledge Bank (BKB), Tree Bank Annotation Schema.
We explain dialogue management tech-niques for collaborative activities with hu-mans, involving multiple concurrent tasks.
Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document.
In this model, a sentence connectivity matrix is constructed based on cosine similarity.
Existing speech recognizers work best for "high-quality close-talking speech."
We consider in depth the semantic analysis in learning systems as well as some information retrieval techniques applied for measuring the document similarity in eLearning.
This paper describes and evaluates a detector of presuppositions (DP) for survey questions.
DP performs well using local characteristics of presuppositions.
We have used our pervious TREC QA system as baseline and augmented it with new answer clustering and co-occurrence maximization components to build the batch QA system.
These include context-dependent phonetic modelling, the use of a bigram language model in conjunction with a probabilistic LR parser, and refinements made to the lexicon.
This paper provides a model theoretic semantics to feature terms augmented with set descriptions.
We provide constraints to specify HPSG style set descriptions, fixed cardinality set descriptions, set-membership constraints, restricted universal role quantifications, set union, intersection, subset and disjointness.
It is shown that determining consistency of terms is a NP-complete problem.Subject Areas: feature logic, constraint-based grammars, HPSG
This paper presents some techniques that provide a standard parsing system for the analysis of ill-formed utterances.
The proposed framework involves two processes, partial grammar acquisition and grammar refinement.
We present the results of an experiment on extending the automatic method of Machine Translation evaluation BLUE with statistical weights for lexical items, such as tf.idf scores.
Island parsing is a powerful technique for parsing with Augmented Transition Networks (ATNs) which was developed and successfully applied in the HWIM speech understanding project.
Tins paper explores two directions for the next step beyond the state of the art of statistical parsing: probabilistic partial parsing and committee-based decision making.
We propose a new method for reformatting web documents by extracting semantic structures from web pages.
Our approach is to extract trees that describe hierarchical relations in documents.
We present a novel system for automatically marking up text documents into XML and discuss the benefits of XML markup for intelligent information retrieval.
Statistical part-of-speech(POS) taggers achieve high accuracy and robustness when based on large scale manually tagged corpora.
Currently we use a fine-grained POS tag set with about 500 tags.
Moreover, to cope with the datasparseness problem caused by exceptional phenom-ena, we introduce several other techniques such as word-level statistics, smoothing of word-level and POS-level statistics and a selective tri-grain model.
This paper presents a pilot study on how Population Test Method (PTM) may be used as an effective, empirical tool to define near-synonyms in a quantifiable manner.
This paper presents the results of an empirical investigation of temporal reference resolution in scheduling dialogs.
A pattern in the translation of locative prepositional phrases between English and Spanish is presented.
We briefly describe our cross document co-reference resolution algorithm and discuss applications these resolved references enable.
We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al., 2004) automatically extracted from treebanks, maximising the probability of a tree given an f-structure.
We describe work in progress on using quantitative methods to classify writing systems according to Sproat抯 (2000) classification grid using unannotated data.
An overview of the DLT (Distributed Language Translation) project is given.
The sys-tem we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences.
Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity.
We propose an approach that uses semantically motivated preposition selection and frequency information to determine if a locative PP is an argument or an adjunct.
This paper proposes an algorithm for causality inference based on a set of lexical knowledge bases that contain information about such items as event role, is-a hierarchy, relevant relation, antonymy, and other features.
These lexical knowledge bases have mainly made use of lexical features and symbols in HowNet.
Our partial parser for Chinese uses a learned classifier to guide a bottom-up parsing process.
We describe improvements in performance obtained by expanding the information available to the classifier, from POS sequences only, to include measures of word	association	derived	fromco-occurrence statistics.
We present here a new monolingual sentence alignment algorithm, combining a sentence-based TF*IDF score, turned into a probability distribution using logistic regression, with a global alignment dynamic programming algorithm.
We show how to do this using , -reduction constraints in the constraint language for A-structures (CLLS).
we diseuss a method for using automated corpus analysis to acquire word sense information l'or' text interpretatiou.
Our approach focuses on tying together word senses, us- lag a combination of world knowledge (ontology) with word knowledge (corpus data).
We will present results in the case of a transducer D representing a dictionary and R representing phonological rules.Keywords: ambiguity, deterministic, dictionary, transducer.
As mentioned in Mitkov (1996), solving the anaphora and extracting the antecedent are key issues in a correct translation.In this paper, we propose an Interlingual mechanism that we have called Interlingual Slot Structure (ISS) based on Slot Structure (SS) presented in FerrAndez et al.
The tool makes use of part-of-speech tagging and word-alignment programs to extract candidate terms and their translations.
V磂ronis (2004) has recently proposed an innovative unsupervised algorithm for word sense disambiguation based on small-world graphs called HyperLex.
Our method uses features for the translation model which use the translation dictionary, the number of words, part-of-speech, constituent words and neighbor words.
A semi-continuous hidden Markov model based on the multiple vector quantization codebooks is used here for large-vocabulary speaker-independent continuous speech recognition In the techniques employed here, the semi-continuous output probability density function for each codebook is represented by a comhinotion of the corresponding discrete output probabilities of the hidden Markov model and the continuous Gaussian density functions of each individual codebook.
continuous output probability by the combination of multiple codewords and multiple codebooks For a 1000-word speaker- Independent continuous speech recognition using a word-pair grammar.
The combination of a rule-based system and a statistical one is not parallel but serial: the rule-based system performing partial disambigua-tion with recall close to 100% is applied first, and a trigram HMM tagger runs on its results.
This paper describes adaptations of unsupervised word sense discrimination techniques to the problem of name discrimination.
We evaluated the proposed method with a task of recognizing named entities (genes) in biology text involving three species.
The structures covered include lists, narratives, subordinating and coordinating rhetorical relations, topic chains and interruptions.
The paper discusses the problem of parsing discourse, and compares different grammatical formalisms which could be used for describing discourse structure.
We demonstrate that in a realistically difficult authorship attribution scenario, deep linguistic analysis features such as context free production frequencies and semantic relationship frequencies achieve significant error reduction over more commonly used 搒hallow?features such as function word frequencies and part of speech trigrams.
We present an efficient, broad-coverage, principle-based parser for English.
Principles are constraints over X-bar structures.
Sentence analysis is divided into three steps.
The lexical analyser first converts the input sentence into a set of lexical items.
Then, a message passing algorithm for GB-parsing is used to construct a shared parse forest.
The revised grammar of this model combines the phrase structure and transformational rules of the underlying grammar into a single efficient component.
The underlying grammar in this system is a recognition grammar based on tne generative approach in linguistic theory.
Among the transformational grammarsi.
We present a constraint-based syntax-semantics interface for the construction of RMRS (Robust Minimal Recursion Semantics) representations from shallow grammars.
incomplete syntactic analysis.
We present RMRS semantics construction principles that can be applied to flat syntactic structures with various degrees of partiality.2 RMRS ?For Partial Semantic RepresentationCopestake (2003) presents a formalism for partial semantic representation that is derived from MRS semantics (Copestake et al., 2003).
(1990) Modelling the perception of concurrent vowels: vowels with different fundamental frequencies.
We present three systems for surface natural lan-guage generation that are trainable from annotated corpora.
We present experiments in which we generate phrases to describe flights in the air travel domain.
Multiset-CCG is a combinatory categorial formalism that can capture the syntax and interpretation of "free" word order in languages such as Turkish.
obtained from supervised training) by iterative ascent.
This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations.
We construct an objective function based on Minimum Description Length.
We present a comparative study of corpus- based methods for the automatic synthesis of email responses to help-desk requests.
We also describe the multiplicative combination of this dependency model with a model of linear constituency.
The product model outperforms both components on their respective evaluation metrics, giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing.
This paper describes the basic philosophy and implementation of MPLUS (M+), a robust medical text analysis tool that uses a semantic model based on Bayesian Networks (BNs).
In this paper, we propose two algorithms for automatically ontologizing (attaching) semantic relations into WordNet.
In this work, we present a learning approach that correctly determines the hierarchical structure of information filtering expressions 78.30% of the time.
In addition, we investigate the use of monolingual morpho-syntactic knowledge i.e.
base forms and POS tags.
The important part of semantics of complex.
(Lytinen 86) distinguishes two approaches to NI, processing.
This paper proposes machine learning techniques, which help disambiguate word meaning.
Context information is produced from rule-based translation such as part-of-speech tags, semantic concept, case relations and so on.
In this paper, we test on ParSit, which is an interlingual-based machine translation for English to Thai.
This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination.
The model utilized rich linguistic features that capture predicate- argument structure information of the target verbs.
We further enhanced the model with certain fine-grained semantic categories called lexical sets.
We describe a method for discriminative training of a language model that makes use of syntactic features.
The reranking model makes use of syntactic features together with a parameter estimation method that is based on the perceptron algorithm.
We propose a new inference system which oper-ates on underspecified semantic representations of scope and anaphora.
This system exploits anaphoric accessibility conditions from dynamic semantics to disambiguate scope ambiguities if possible.
without enumerating readings.
Most information extraction (IE) systems treat separate potential extractions as independent.
This allows for "collective information extraction" that exploits the mutual influence between possible extractions.
In this paper, we propose a method of dynamic programming matching for information retrieval.
It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging.
We will demonstrate the GIST system, which generates social security forms in English, Italian and German.
Named entities form the major components in a document.
This paper employs different types of information from different levels of text to extract named entities, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache and n-gram model.
In this paper, we show how to induce head-driven probabilistic parsers with latent heads from a tree-bank.
This paper describes a novel statistical named- entity (i.e.
"proper name") recognition system built around a maximum entity framework.
These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e.
A key task in an extraction system for query-oriented multi-document summarisation, necessary for computing relevance and redundancy, is modelling text semantics.
In the Embra system, we use a representation derived from the singular value decomposition of a term co-occurrence matrix.
We present evidence that head-driven parsing strategies lead to efficiency gains over standard parsing strategies, for lexicalist, concatenative and unification-based grammars.
A head-driven parser applies a rule only after a phrase matching the head has been derived.
We have used two different head-driven parsers and a number of standard parsers to parse with lexicalist grammars for English and for Dutch.
This paper describes a machine translation architecture that integrates the use of examples for flexible, idiomatic translations with the use of linguistic rules for broad coverage and grammatical accuracy.
Collocational word similarity is considered a source of text cohesion that is hard to measure and quantify.
An implementation, the VecTile system, produces similarity curves over texts using pre-compiled vector representations of the contextual behavior of words.
We propose a two-layered model for computing semantic and conceptual interpretations from dependency structures.
We model the distortion in the spelling of a name due to the speech recognizer as the effect of a noisy channel.
We also test a string edit distance based method.
The effectiveness of these models is evaluated on a name query retrieval task.
We are presenting a new, hybrid alignment architecture for aligning bilingual, linguistically annotated parallel corpora.
It is able to align simultaneously at paragraph, sentence, phrase and word level, using statistical and heuristic cues, along with linguistics-based rules.
We describe a system which automatically generates multimedia briefings from high-level outlines.
Example-based machine translation (EBMT) is based on a bilingual corpus.
In this paper, we de-scribe a method to acquire synonymous expres-sions from a bilingual corpus and utilize them to expand retrieval of similar sentences.
We offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification.
We offer an analy-sis couched in a version of Head-Driven Phrase Structure Grammar combined with a theory of information states (IS) in dialogue.
We discuss which recent word sense disambiguation algorithms are appropriate for sense tagging.
We describe an algorithm for recovering non-local dependencies in syntactic dependency structures.
The pattern- matching approach proposed by Johnson (2002) for a similar task for phrase structure trees is extended with machine learning techniques.
For English- Japanese machine translation, the syntax directed approach is effective where the Heuristic Parsing Model (HPM) and the Syntactic Role System play important roles.
In this paper we show how interaction of lexical and derivational semantics at the lexico-syntactic interface can be precomputed as a process of off-line lexical compilation comprising Cut elimination in partial proof-nets.
We propose more efficient algorithms approximating Tree Kernel: Tree Overlapping and Subpath Set.
In this paper, we discuss a method to screen inconsistencies in ontologies by applying a nat-ural language processing (NLP) technique, es-pecially, those used for word sense disambigua-tion (WSD).
The phrasal approach to language processing emphasizes the role of the lexicon as a knowledge source.
In this paper, we propose a novel paradigm for the Chinese-to-English speech-to-speech (S2S) translation, which is interactive under the guidance of dialogue management.
In this paper1 we introduce eXtensible MetaGrammar, a system that facilitates the development of tree based grammars.
To segment Japanese text, systems typically use knowledge-based methods and large lexicons.
The algorithm utilizes a hidden Markov model, a stochastic process, to determine word boundaries.
The extraction is performed in two major steps: incremental finite-state parsing and extraction of subject/verb and object/verb relations.
We also provide some error analysis; in particular, we evaluate the impact of POS tagging errors on subject/object dependency extraction.
Previous research on temporal anchoring and ordering has focused on the annotation and learning of temporal relations between events.
This paper describes the first steps in acquiring metric temporal constraints for events.
This paper describes techniques for unsupervised word sense disambiguation of English and German medical documents using UMLS.
In RST, the linguistic discourse structure is modeled recursively as a tree of related seg-ments.
In this paper we propose a cluster-specific NE transliteration framework.
We group name origins into a smaller number of clusters, then train transliteration and language models for each cluster under a statistical machine translation framework.
We also propose a phrase- based name transliteration model, which effectively combines context information for transliteration.
This paper briefly describes our rule-based heuristic analyzer for Finnish nominal and verb forms.
This paper describes an extension of the DAR-algorithm (Navarretta, 2004) for resolving intersentential pronominal anaphors referring to individual and abstract entities in texts and dialogues.
It concerns more particularly the lexical item representation using phonograins (i.e.
In this paper, a Markov chain Monte-Carlo method is proposed for learning Stochastic OT Grammars.
In this paper, we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus.
This paper presents a Named Entities (NE) recognition system for the English written lan-guage, which combines the wealth of the WORD-NET taxonomy and the effectiveness of tradi-tional rule-based approaches.
The paper describes two parsing schemes: a shallow approach based on machine learning and a cascaded finite-state parser with a hand-crafted grammar.
What is the relationship between syntax, prosody and phonetics?
In this paper, we address the issue of combining data-driven and grammar-based models for rapid prototyping of robust speech recognition and understanding models for a multimodal conversational system.
This paper addresses two major problems in closed task of Chinese word segmentation (CWS): tagging sentences interspersed with non-Chinese words, and long named entity (NE) identification.
For the latter problem, we apply postprocessing to our CWS output using automatically generated templates.
Three approaches to measuring text similarity are considered: n- gram overlap, Greedy String Tiling, and sentence alignment.
This paper presents a speech understanding component for enabling robust situated human-robot communication.
We discuss how deep interpretation and generation can be integrated with a knowledge representation designed for question answering to build a tutorial dialogue system.
We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging.
The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods.
We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments.
In addition, we proposed a confidence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmentation.
We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
We show that a baseline statistical machine translation system is significantly improved using this approach.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
We evaluate the model on entity extraction and noun-phrase chunking and show that it is more accurate for overlapping and non-contiguous segments, but it still performs well on simpler data sets for which sequential tagging has been the best method.
We introduce here a new technique which employs M-NLG during the phase of knowledge editing.
It is based on a general- purpose ngram model for word segmentation and a case-based learning approach to disambiguation.
We present the architecture and data model for TEXTRACT, a document analysis framework for text analysis components.
This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism.In parsers for unification-based grammar formalisms, complex phrase types are derived by incremental refinement of the phrase types defined in grammar rules and lexical entries.
This paper explores the effect of improved morphological analysis, particularly context sensitive morphology, on monolingual Arabic Information Retrieval (IR).
It also compares the effect of context sensitive morphology to non- context sensitive morphology.
We present a new appr.oach, illustrated by two algorithms, for parsing not only Finite State Grammars but also Context Free Grammars and their extension, by means of finite state mach'ines.
a finite-state transducer.
There has recently been considerable interest in the use of lexically-based statistical techniques to resolve prepositional phrase attachments.
In this paper we describe a method for performing word sense disambiguation (WSD).
The method relies on unsupervised learning and exploits functional relations among words as produced by a shallow parser.
We automatically classify verbs into lexical se-mantic classes, based on distributions of indica-tors of verb alternations, extracted from a very large annotated corpus.
We conclude that corpus-driven extraction of grammatical features is a promising methodology for fine-grained verb classification.
This paper describes the development of a rule-based computational model that describes how a feature-based representation of shared visual information combines with linguistic cues to enable effective reference resolution.
This work explores a language-only model, a visual- only model, and an integrated model of reference resolution and applies them to a corpus of transcribed task-oriented spoken dialogues.
This demo presents LeXFlow, a workflow management system for cross- fertilization of computational lexicons.
LeXFlow is a web-based application that enables the cooperative and distributed management of computational lexicons.
This paper proposes a method for detecting and correcting Japanese homophone errors in compound nouns.
A left-associative operation is used to build a lexicon of extended elementary trees.
In this paper, we define an event as one or more event terms along with the named entities associated, and present a novel approach to derive intra- and inter- event relevance using the information of internal association, semantic relatedness, distributional similarity and named entity clustering.
This paper describes a method for obtaining the semantic representation for a syntax tree in Systemic Grammar (SG).
We define a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights.
The quantity is computed in a parse for-est representation of the set of tree anal-yses for a given sentence, using vector sums and scaling by inside probability and flow.
In this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) tagging framework.
We also describe an exten-sion of the traditional POS tagging by combining it with the document-centered approach to proper name identification and abbreviation handling.
Due to the phenomenon of zero anaphora occurring in Chinese texts frequently, in addition to the centering model, we further employ the constraint rules to identify the antecedents of zero anaphors.
Unlike most traditional approaches to parsing sentences based on the integration of complex linguistic information and domain knowledge, we work on the output of a part-of-speech tagger and use shallow parsing instead of complex parsing to identify the topics from sentences.
We describe a number of experiments that demonstrate the usefulness of prosodic information for a processing module which parses spoken utterances with a feature-based grammar employing empty categories.
We introduce adaptivity in QA and IR by creating a hybrid system based on a dialogue interface and a user model.
Keywords: question answering, information retrieval, user modelling, dialogue interfaces.
We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machine- produced translation and human-produced reference translations.
A maximum entropy classifier can be used to extract sentences from documents.
This paper discusses general heuristics to control computation on symbolic constraints represented in terms of first-order logic programs.
Efficient computation for sentence parsing and generation naturally emerge from these heuristics, capturing the essence of standard.
parsing procedures and semantic head-driven generation.
This paper describes and evaluates the Metalinguistic Operation Processor (MOP) system for automatic compilation of metalinguistic information from technical and scientific documents.
Dialogue system for 3D virtual environ-ments).
2 Types of AlignmentWe evaluate the English桭rench word alignment data of the shared tasks from a phrase alignment perspective.
We show that phrase-based evaluation is closely related to word-based evaluation.
Example 1 shows the word-to-word alignment data of sample 91 for submission 12 and a plot of the data.
The analysis is based on a statisti-cal method and employs a beam search strategy.
This paper describes the application of discriminative reranking techniques to the problem of machine translation.
We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.
This paper presents a fully automated linguistic approach to measuring distance between phonemes across languages.
Two phonological distances are statistically derived from lexical frequency measurements.
The phonetic distance is combined with the phonological distances to produce a single metric that quantifies cross-language phoneme distance.The performances of target-language phoneme HMMs constructed solely with source language HMMs, first selected by the combined phonetic and phonological metric and then by a data-driven, acoustics distance-based method, are compared in context-independent automatic speech recognition (ASR) experiments.
This paper presents an evaluation of an ensemble朾ased system that participated in the English and Spanish lexical sample tasks of SENSEVAL-2.
The system com-bines decision trees of unigrams, bigrams, and co杘ccurrences into a single classi-fier.
This paper presents a specific part of HUGG, a generation grammar for Hebrew.
This part deals with determiners and quantifiers.
This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.
This paper discusses the possibility of building an ontology-based question answering system in the context of the Semantic Web presenting a proof-of-concept system.
This is done by using k-means with EM.
For meaning representations in NLP, we focus our attention on thematic aspects and conceptual vectors.
Then, we define antonymy functions which allows the construction of an antonymous vector and the enumeration of its potentially antinomic lexical items.
This paper presents a new application of the recently proposed machine learning method Alternating Structure Optimization (ASO), to word sense disambiguation (WSD).
Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems.We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models.
Here, a two-step talker-location algorithm is introduced.
This paper develops a method for recognizing relations and entities in sentences, while taking mutual dependencies among them into account.
Using probability distributions over verb subcategorisation frames, we obtained an intuitively plausible clustering of 57 verbs into 14 classes.
The automatic clustering was evaluated against independently motivated, hand- constructed semantic verb classes.
General structure of PORTUGA: A - analysis, G - generation.
Transfer: L - lexical, S - structural, T - tense, Sty - style.
This work investigates techniques for predicting the location of intonational phrase boundaries in natural speech, through analyzing a utterances from the DARPA Air Travel Information Service database.
For statistical modeling, we employ Classification and Regression Tree (CART) techniques.
We have been investigating an interactiveapproach for Open-domain QA (ODQA)and have constructed a spoken interactiveODQA system, SPIQA.
The combination is then used for an-swer extraction.
This paper describes a pilot version of a commercial application of natural language processing techniques to the problem of categorizing news stories into broad topic categories.
Its categorizations are dependent on fragmentary recognition using pattern-matching techniques.
This paper describes a unified framework for bilingual text matching by combining existing hand-written bilingual dictionaries and statistical techniques.
The process of bilingual text matching consists of two major steps: sentence alignment and structural matching of bilingual sentences.
Statistical techniques are applied to estimate word correspondences not included in bilingual dictionaries.
An ATN-Parser is presented with emphasis on the treatment of those phenomena which in the framework of transformational grammar are subsumed under the concept of WH-movement.
These gaps are modeled using a mixture of exponential distributions.
In this paper we present and evaluate a novel unsupervised approach, SALAAM, which exploits translational correspondences between words in a parallel Arabic English corpus to annotate Arabic text using an English WordNet taxonomy.
Using XML as a reference format, we approach the schema generation problem by application of inductive inference theory.
We evaluate the result of an inference process using the concept of Minimum Message Length.
In this paper, we used syntactic sub- trees that span potential argument structures of the target predicate in tree kernel functions.
We describe a branch of dictionary science, and recommend the term lexicomptry for it, that deals with the mathematical and statistical aspects of dictionaries.
In this paper, we compare two approaches to modeling subtask structure in dialog: a chunk-based model of subdialog sequences, and a parse-based, or hierarchical, model.
We evaluate these models using customer agent dialogs from a catalog service domain.
We present preliminary results of experiments with two types of recurrent neural networks for a natural language learning task.
The neural networks, Elman networks and Recurrent Cascade Correlation (RCC), were trained on the text of a first-year primary school reader.
In this paper, we present preliminary results of experiments with recurrent neural networks for a natural language learning task.
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 3-10.
The three-tiered discourse representation defined in (Luperfoy, 1991) is applied to multimodal human- computer interface (HCI) dialogues.
The first tier holds a linguistic analysis of surface forms.
We call this the G-view of constraints.
This paper describes a prototype disambiguation module, KANKEI, which was tested on two corpora of the TRAINS project.
Unlike previous statistical disambiguation systems, this technique thus combines evidence from bigrams, trigrams, and the 4-gram around an ambiguous attachment.
We apply the C4.5 decision tree learner in interpreting Japanese relative clause constructions, based around shallow syntactic and semantic processing.
In this paper, we describe our approach to intermediate semantic representations in the interpretation of temporal expressions.
In this paper, we describe TANGO as a collocational concordancer for looking up collocations.
In this paper, we study how to generate features from various data representations, such as surface texts and parse trees, for answer extraction.
We propose and compare three methods, including feature vector, string kernel and tree kernel, to represent the syntactic features in Support Vector Machines.
This paper explores the large-scale acquisition of sense-tagged examples for Word Sense Disambiguation (WSD).
This paper introduces an efficient analyser for the Chinese language, which efficiently and effectively integrates word segmentation, part-of-speech tagging, partial parsing and full parsing.
The Chinese efficient analyser is based on a Hidden Markov Model (HMM) and an HMM-based tagger.
In this paper, we study the effect of different word-level preprocessing schemes for Arabic on the quality of phrase-based statistical machine translation.
The Head-driven Phrase Structure Grammar project (HPSG) is an English language database query system under development at Hewlett-Packard Laboratories.
1982I), in four significant respects: syntax, lexical representation, parsing, and semantics.
We propose to score phrase translation pairs for statistical machine translation using term weight based models.
These models employ tf.idf to encode the weights of content and non-content words in phrase translation pairs.
The translation probability is then modeled by similarity functions defined in a vector space.
Using these models in a statistical machine translation task shows significant improvements.
Experiments on movie review data from Epinions.com demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data.
We describe a method for identifying systematic patterns in translation data using part-ofspeech tag sequences.
This paper provides an evaluation of the OntoLearn ontology learning system.
We propose a draft scheme of the model formalizing the structure of communicative context in dialogue interaction.
To achieve a more pragmatic DRT model, this paper extends standard DRT framework to incorporate more pragmatic elements such as representing agents?cognitive states and the complex process through which agents recognize utterances employing the linguistic content in forming mental representations of other agent抯 cognitive states.
We describe how movement is handled in a class of computational devices called active production networks (APNs).
This paper introduces a Chinese word tokenization system through HMM-based chunking.
Word alignment using recency-vector based approach has recently become popular.
Through two experiments, three methods for constructing word vectors, i.e., LSA-based, cooccurrence-based and dictionary-based methods, were compared in terms of the ability to represent two kinds of similarity, i.e., taxonomic similarity and associative similarity.
The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity, while the LSAbased and the cooccurrence-based word vectors better reflect associative similarity.
The patterns we derived are rules which map surface syntactic structures to semantic case frames, which serve as the canonical representation of questions.
This paper presents a method, based on Turney (2003), for inferring the SO of a word from its statistical association with strongly-polarized words and morphemes in Chinese.
In this paper, a pragmatics-first approach to specifying the meaning of utterances in terms of plans is presented.
The Dependency syntactic model is applied to establish the dependency-based grammatical relations between the components within the clause.
Here, we investigate Chinese word segmentation for statistical machine translation.
In the paper, we extend CBS for decision tree classifiers.
This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system.
The word sense disambiguation is applied to verbs and nouns.
We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis.
We integrated this global word sense disambiguation into our zero pronoun resolution system, and conducted experiments of zero pronoun resolution on two different domain corpora.
In this paper we present a method for transforming the WordNet glosses into logic forms and further into axioms.
The paper demonstrates the utility of the WordNet axioms in a question answering system to rank and extract answers.
This paper reports the first part of a project that aims to develop a knowledge extrac-tion and knowledge discovery system that extracts causal knowledge from textual da-tabases.
We classified compound nouns into three classes: countable, uncountable, plural only.
We present a semi-automatic method for extracting fine-grained semantic relations between verbs.
We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.
Verbal idioms can be divided into two main groups: non-compositional id-ioms as kick the bucket and composi-tional/decomposable idioms as spill the beans.
Taking these facts into account we propose an adequate way to represent the idiomatic meaning by Kamp's Discourse Represen-tation Theory (DRT).
Furthermore, we show how to parse idiomatic sentences and how to process the proposed seman-tic representation.
Lexicalized Tree Adjoining Grammars have proved useful for NLP.
We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms.
This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.
We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.
The (ii-system is a tree-to-tree transducer developed for teaching purposes in madhine.translation.
We refer to intentions derived from structural differences as objective intentions and intentions derived from contents differences as subjective intentions.
In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora.
The applied techniques include building deep semantic representations, application of categories of patterns underlying a formal reconstruction, and using pragmatically- motivated and empirically justified preferences.
This paper describes new default unification, lenient default unification.
We extract grammar rules from the results of robust parsing using lenient default unification.
The nearest related technologies are information retrieval (search engines), document categorization, information extraction and named entity detection.
This paper reports on the LEARNING COMPUTATIONAL GRAMMARS (LCG)project, a postdoc network devoted to studying the application of machine learning techniques to grammars suitable for computational use.
We focused on syntax, esp.
noun phrase (NP) syntax.
Based on the concepts of bidirectional conversion and automatic evaluation, we propose two user- adaptation mechanisms, character-preference learning and pseudo-word /earning, for resolving Chinese homophone ambiguities in syllable-to-character conversion.
This paper presents a trainable approach to making these attachments through transformation sequences and error-driven learning.
Our goal is to develop fast speech recognition algorithms, and supporting hardware capable of recognizing continuous speech from a bigram- or trigram-based 20,000-word vocabulary or a 1,000- to 5,000- word SLS.
We describe an MDL based grammar of a language that contains morphology and lexical categories.
We present an algorithm for collapsing morphological classes (signatures) by using syntactic context.
This paper presents an ontology-based semanticframework to question answering.
Answer retrieval is done using subsumption and unification, and queries are expanded incrementally using ontological rules.
This paperl decribes a computational treatment of the semantics of relational nouns.
Transfer-based Machine Translation systems re-quire a procedure for choosing the set of transfer rules for generating a target language transla-tion from a given source language sentence.
We propose a solution to this problem based on current best-first chart parsing algorithms.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
A system for the automatic production of controlled index terms is presented using linguistically-motivated techniques.
This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unification- based shallow-level parser using transformational rules over syntactic patterns.
This paper presents a method for searching the web for sentences expressing opinions.
We collected declaratively subjective clues in opinion- expressing sentences from Japanese web pages retrieved with opinion search queries.
The presentation focuses on the major methodological principles underlying the design of TOPIC: a frame representation model that incorporates various integrity constraints, text parsing with focus on text cohesion and text coherence properties of expository texts, a lexically distributed semantic text grammar in the format of word experts, a model of partial text parsing, and text graphs as appropriate representation structures for text condensates.
This paper presents a technique to deal with multiword nominal terminology in a computational Lexical Functional Grammar.
This method treats multiword terms as single tokens by modifying the preprocessing stage of the grammar (tokenization and morphological analysis), which consists of a cascade of two-level finite-state automata (transducers).
Explanation-based Learning (EBL) is a technique to speed-up parsing.
Abductive EBL allows extending the deductive closure of the parser.
We describe a statistical technique for assigning senses to words.
This paper compares two approaches to computational semantics, namely semantic unification in Lexicalized Tree Adjoining Grammars (LTAG) and Lexical Resource Semantics (LRS) in HPSG.
This paper describes the SyntaLex entries in the English Lexical Sample Task of SENSEVAL-3.
There are four entries in all, where each of the different entries corresponds to use of word bigrams or Part of Speech tags as features.
The systems rely on bagged decision trees, and focus on using pairs of lexical and syntactic features individually and in combination.
The first is for acquiring conjunctive relationships from corpora, as measures of word similarity that can be used in addition to thesauruses.
We describe a machine learning system for the recognition of names in biomedical texts.
In this paper we present some novel applications of Explanation-Based Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars.
The paper describes a localized connectionist model of language generation, focusing on the representation and use of sequencing information.
We assume a two-stage language analysis system.
This paper presents PROVERB a text planner for argumentative texts.
We describe a statistical technique for assigning senses to words.
We adapt van Noord's Prolog generator for use with an HPSG grammar in ProFIT.
We must adopt recent theoretical proposals for lexicalized scoping and context.
We briefly describe a word alignment system that combines two different methods in bitext correspondences identification.
This paper discusses the treatment of fixed word expressions developed for our ITS-2 French- English translation system.
Inter-sentence similarity is estimated by latent semantic analysis (LSA).
Boundary locations are discovered by divisive clustering.
We have implemented a restricted domain parser calledPlume.
These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects.
We describe the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words, so-called text chunking.
In order to incorporate this kind of long distance context dependency in the ngram model of our Mandarin speech recognition system, this paper proposes a novel MI-Ngram modeling approach.
This new MI-Ngram model consists of two components: a normal ngram model and a novel MI model.
That is, the MI-Ngram model incorporates the word occurrences beyond the scope of the normal ngram model.
We present an approach to bounded constraint- relaxation for entropy maximization that corresponds to using a double-exponential prior or E1 regularizer in likelihood maximization for log-linear models.
This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA).
Our approach thus represents a combination of grammar-based and empirical natural language processing.
This paper examines several key issues in computing and using anaphoricity information to improve learning-based coreference systems.
In particular, we present a new corpus-based approach to anaphoricity determination.
We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear models.
These results have implications for automating discourse annotation based on syntactic annotation.
We extend default unification to non-parallel structures, which is important for speech and multimodal dialog systems.
These features include lexical, lexico grammatical and semantic phenomena.
Perhaps the most popularly known are psychodynamic, Cognitive Behavioural Therapy(CBT), and interpersonal therapies such as family therapy, narrative therapy, emotion-focussed therapy, solution-focussed therapy.
This paper investigates the usefulness of sentence-internal prosodic cues in syntactic parsing of transcribed speech.
We compared the accuracy of a statistical parser on the LDC Switchboard treebank corpus of transcribed sentence-segmented speech using various combinations of punctuation and sentence-internal prosodic information (duration, pausing, and f0 cues).
This paper proposes an unsupervised learning algorithm for Optimality Theoretic grammars, which learns a complete constraint ranking and a lexicon given only unstructured surface forms and morphological relations.
The learning algorithm, which is based on the Expectation- Maximization algorithm, gradually maximizes the likelihood of the observed forms by adjusting the parameters of a probabilistic constraint grammar and a probabilistic lexicon.
The paper presents the algorithm抯 results on three constructed language systems with different types of hidden structure: voicing neutralization, stress, and abstract vowels.
A novel bootstrapping approach to Named Entity ^NE^ tagging using concept-based seeds and successive learners is presented.
First, decision list is used to learn the parsing-based NE rules.
The normal method for representing anaphoric dependencies in Unification Based gram-mar formalisms is that of re-entrance.
A new approach to structure-driven generation is presented that is based on a separate semantics as input structure.
These two steps are implemented as SVM classifiers using LIBSVM.
Features take into account the static context as well as relations dynamically built during parsing.We experimented two main additions to our implementation of Nivre抯 parser: N- best search and bidirectional parsing.
To construct a single-head, rooted, and cycle-free tree, we applied the ChuLiu/Edmonds optimization algorithm.
By default, these classes run a generalization of agenda- based parsing, prioritizing the partial parses by some figure of merit.
Intra-syllabic constraints are encoded as acyclic finite automata with input alphabets of phonemic symbols.
These automata in turn form the transitions in cyclic finite automata that encode the inter-syllabic constraints of word-level phonology.
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs).
Using these models in a phrase-based SMT system, we show significant improvements in translation quality for certain language pairs.
This paper proposes an effective parsing method for example-based machine translation.
Transfer-Driven Machine Translation (TDMT) achieves efficient and robust translation within the example-based framework by adopting this parsing method.
Thus, pattern-matching achieves efficient parsing.
It is also useful in treating spoken language, which sometimes deviates from conventional grammar, while grammar-based parsing has difficulty treating unrestricted spoken language.This paper proposes a constituent boundary parsing method based on pattern-matching, and shows its effectiveness for spoken language translation within the example-based framework.
constituent boundary description by a part-of-speech bigram, and classification of patterns according to linguistic levels such as simple sentence and notIll phrase,Transfer-Driven Machine Translation (TDMT) (ruruse, 1992, 1994) uses the constituent boundary parsing method presented in this paper, as an alternative to gram in analysis, and makes the best use of the example-based framework.
Section 4 explains structural disambiguation using distance calculations in the ex air p1 framework.
This paper presents evidence from several natural languages that unification—variable-matching combined with variable substitution—is the wrong mechanism for effecting agreement.
variable-matching without variable substitution.
We study the interplay of the discourse structure of a scientific argument with formal citations.
This paper presents an effective approach for resume information extraction to support automatic resume management and routing.
A cascaded information extraction (IE) framework is designed.
We propose an expressive pattern language with extended semantics of the sequence pattern, supporting negation, permutation and regular patterns that is especially appropriate for querying XML annotated documents with multi-dimensional markup.
We introduce an algorithm for designing a predictive left to right shift-reduce non-deterministic push-down machine corresponding to an arbitrary unrestricted context-free grammar and an algorithm for efficiently driving this machine in pseudo-parallel.
This paper presents a method that measures the similarity between compound nouns in different languages to locate translation equivalents from corpora.
We present in this paper the method of English-to-Korean(E-K) transliteration and back-transliteration.
The E-K transliteration method has three steps.
This paper summarizes the formalism of Category Cooccurrence Restrictions (CCRs) and describes two parsing algorithms that interpret it.
in morphological analysis).
between source program and generated lexica.
These ratios can be considered as one overall language-specific criterion for the analysis, evaluation and validation of lexical dB-s in Arabic.Keywords: Arabic lexical databases ?Arabic script ?word-formatives grammar ?lemma- entries ?morphosyntactic specifiers.
hi this paper we present an integrated system for tagging and chunking texts from a certain language.
Tins includes bigram models or finite-state automata learnt using grammatical inference techniques.
We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures.
The system was implemented in the framework of finite-state transducer technology, using linguistic criteria as well as frequency distributions derived from a database.
Further we examine the use of sentence level syntactic pattern features to increase performance.
Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.
Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based).
In this paper, we adapt the TF-IDF model to the Japanese grapheme-phoneme alignment task, by way of a simple statistical model and an incremental learning method.
In the incremental learning method, grapheme-phoneme alignment paradigms are disambiguated one at a time according to the relative plausibility of the highest scoring alignment schema, and the statistical model is re-trained accordingly.
Emdros is a text database engine for linguistic analysis or annotation of text.
Emdros implements the EMdF text database model and the MQL query language.
Junction Grammar, a model of language structure developed by Eldon Lytle, is being used to define the interlingua for a machine- assisted translation project.
Junction Grammar representations (called junction trees) consist of word sense information interrelated by junctions, which contribute syntactic and semantic information.
This paper presents a comparison of a rule- based and a statistical semantic information modeling technique.
For the rule?based method we employ Embedded Grammar (EG) tagging and for the statistical method we use a previously proposed Semantic Structured Language Modeling (SSLM) technique.
Combining EG and SSLM using linear interpolation results in further improvement.
We also use the features obtained from EG and SSLM for confidence measurement.
We define a single class hierarchy for a metagrammar which allows us to automatically generate grammars for different languages from a single compact metagrammar hierarchy.
We show how 'delayed evaluation' techniques from constraint-logic programming can be used to process such lexical rules.
In this paper, we review SPHINX-II and summarize our recent efforts on improved speech recognition.
This paper describes a Chinese word segmentor (CWS) for the third International Chinese Language Processing Bakeoff (SIGHAN Bakeoff 2006).
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.
This paper presents an extended GLR parsing algorithm with grammar PCFG* that is based on Tomita抯 GLR parsing algorithm and extends it further.
So our syntactic parsing system is implemented based on rule-based approach and statistics approach.
Furthermore our experiments are executed in two fields: Chinese base noun phrase identification and full syntactic parsing.
The experiments prove that the extended GLR parsing algorithm with PCFG* is an efficient	parsing	method	and	astraightforward way to combine statistical property with rules.
This method is implemented in the Terminology Extraction Sotware LEXTER.
In this paper, we propose methods for linking relevant segments in hypertext authoring of a set of related manuals.
idf based method.
The paper reports on progress in building computational models of a constructivist approach to language development.
Augmented phrase structure	grammarsconsist of phrase structure rules with embedded conditions and structure-building actions written in a specially developedlanguage.
INTRODUCTIONAn augmented phrase structure grammar (APSG) consists of a collection of phrase structure, rules which are augmented by arbitrary conditions and structure building actions.
decoding and encoding.
In this paper we address the issue of useradaptivity for annotation guidelines.
We present results of machine learning experiments designed to identify user corrections of speech recog-nition errors in a corpus collected from a train in-formation spoken dialogue system.
Ensuring consistency of Part-of-Speech (POS) tagging plays an important role in constructing high-quality Chinese corpora.
Our method builds a vector model of the context of multi-category words, and uses the k-NN al-gorithm to classify context vectors con-structed from POS tagging sequences and judge their consistency.
We introduce a typed feature logic system providing both universal implicational principles as well as definite clauses over feature terms.
In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar.
By including a chunker, a supertagger, a PP attacher, and a fast probabilistic parser we were able to improve upon the baseline by 3.2%, bringing the overall labelled accuracy to 9 1.
In this paper, we describe the extension of an existing monolingual QA system for English-to-Chinese and English-toJapanese cross-lingual question answering (CLQA).
There are two main methodologies for constructing the knowledge base of a natural language analyser: the linguistic and the data-driven.
Recent state-of- the-art part-of-speech taggers are based on the data-driven approach.
Because of the known feasibility of the linguistic rule-based approach at related levels of description, the success of the data- driven approach in part-of-speech analysis may appear surprising.
A new tagger of English that uses only linguistic distributional rules is outlined and empirically evaluated.
Compared to the 95-97% accuracy of its best competitors, this result suggests the feasibility of the linguistic approach also in part-of-speech analysis.
In this paper we describe an approach to reducing the complexity of Arabic morphology generation using discrimination trees and transformational rules.
This paper proposes a framework of language inde-pendent morphological analysis and mainly concen-trate on tokenization, the first process of morpholog-ical analysis.
We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar.
A conviction arose that lexicalized PCFGs (where head words annotate phrasal nodes) were the key tool for high performance PCFG parsing.
This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model.
Here we describe a method for measuring inter-annotator agreement for these event duration distributions.
In this paper, subclasses of monadic context- free tree grammars (CFTGs) are compared.
The InfoXtract location normalization module consists of local pattern matching and discourse co-occurrence analysis as well as default senses.
Multiple knowledge sources are used in a number of ways: (i) pattern matching driven by local context, (ii) maximum spanning tree search for discourse analysis, and (iii) applying default sense heuristics and extracting default senses from the web.
We investigate Global Index Grammars (GIGs), a grammar formalism that uses a stack of indices associated with productions and has restricted context-sensitive power.
We show also how GIGs can represent structural descriptions corresponding to HPSGs (Pollard and Sag, 1994) schemas.
In this paper we propose a machine- learning approach to paragraph boundary identification which utilizes linguistically motivated features.
We investigate the relation between paragraph boundaries and discourse cues, pronominalization and information structure.
In this paper we present a semantic study of motion complexes (ie.
of a motion verb followed by a spatial preposition).
We focus on the spatial and the temporal intrinsic semantic properties of the motion verbs, on the one hand, and of the spatial prepositions, on the other hand.
We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text.
In this paper we present an evaluation of Carmel-Tools, a novel behavior oriented approach to authoring and maintaining domain specific knowledge sources for robust sentence-level language understanding.
The availability of robust and deep syntactic parsing can improve the performance of Question Answering systems.
We define a measure for the observable amount of paradigmatic modifiability of terms and, subsequently, test it on bigram, trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus.
Based upon a statistically trained speech translation system, in this study, we try to combine distinctive features derived from the two modules: speech recognition and statistical machine translation, in a log- linear model.
This paper deals with the way temporal connectives affect Temporal Structure as well as Discourse Structure in Narratives.
We focus on the sequence labelling problem, particularly POS tagging and NER tasks.
This paper proposes a parser based fully upon the connectionist model(called "CM parser" hereafter).
In order to realize the CM parser, we use Sigma-PiUnits to implement a constraint of grammatical category order or word order, and a copy mechanism of sub-parse trees.
We have explored the usefulness of incorporating speech and discourse features in an automatic speech summarization system applied to meeting recordings from the ICSI Meetings corpus.
The authors collect lexical data for a module of English syntactic analysis in the context of a bilingual research project.
This method is based on mutual information and context dependency.
We describe an algorithm that recursively applies Latent Dirichlet Allocation with an orthogonality constraint to discover morphological paradigms as the latent classes within a suffix-stem matrix.
This paper compares the consistency- based account of agreement phenomena in 'unification-based' grammars with an implication-based account based on a simple feature extension to Lambek Categorial Grammar (LCG).
We describe a linguistically expressive and easy to implement parallel semantics for quasi-deterministic finite state transducers (FSTS) used as acceptors.
This paper describes strategies for automatic recognition of unknown variants of known words in a natural language processing system.
The types of lexical variants which are detectable include in-flexional aberrations, ad hoc abbreviations and spelling/typographical errors.
My nearest neighbor approach to lexical acquisition computes the distance between an unknown word and examples from the CiLin thesaurus based upon its morphological structure.
We apply a decision tree based approach to pronoun resolution in spoken dialogue.
Our system deals with pronouns with NP- and non-NP-antecedents.
We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features.
We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.
The models use syntactic and lexical features.
We discuss some sets of grammars whose generative power lies between that of the set of context-free grammars and that of the set of context- sensitive grammars.
This paper describes a Verb Phrase Ellipsis (VPE) detection system, built for robustness, accuracy and domain independence.
This method retrieve collocations in the following stages: 1) extracting strings of characters as units of collocations 2) extracting recurrent combinations of strings in accordance with their word order in a corpus as collocations.
Prepositional phrase attachment is a ma-jor cause of structural ambiguity in nat-ural language.
To cope with this problem, we introduce a hybrid method of integrating corpus-based approach with knowledge-based techniques, using a wide-variety of information that comes from annotated corpora and a machine-readable dictionary.
The grammar constructs the structural tree capturing the dialogical functions of the discourse using functional subcategorization.
We integrate PropBank semantic role labels to an existing statistical parsing model producing richer output.
We show conclusive results on joint learning and inference of syntactic and semantic representations.
We call this model a. cache trigram language model (CTLM) since we are caching the recent history of words.
Tense, temporal adverbs, and temporal connectives provide information about when events described in English sentences occur.
To extract this temporal information from a sentence, it must be parsed into a semantic representation which captures the meaning of tense, temporal adverbs, and temporal connectives.
(2001) using English combinations.
We have constructed the IPA Lexicon of Basic Japanese Nouns (IPAL-BN), which has a hierarchical structure based on the syntactic and semantic proper-ties of nouns.
Thus we have developed a method for specifying the kind of relationship between subentries, using special cognitive devices such as metaphor, metonymy, and synecdoche.
We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.
We show that we can automatically classify semantically related phrases into 10 classes.
This scheme improves the robustness for statistical machine translation models.
Two HMMbased translation models are tested to use these bilingual clusters.
This paper describes a media-independent knowledge representation scheme, or content language, for describing the content of communicative goals and actions.
In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction.
The correlations are further incorporated into a Maximum Entropy-based ranking model which estimates path weights from training.
In this paper, we will investigate a cross-linguistic phenomenon referred to as complex prepositions (CPs), which is a frequent type of multiword expressions (MWEs) in many languages.
using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective.
Such kernels use a term similarity measure based on the WordNet hierarchy.
We have developed a word sense disambiguation algorithm, following Cheng and Wilensky (1997), to disambiguate among WordNet synsets.
Our goal is to improve retrieval precision through word sense disambiguation.
We report grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars.
We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both systems.
This paper presents a framework for clustering in text-based information retrieval systems.
Co-occurrence relations can serve two main purposes in language processing.
This paper discusses a method for collecting co-occurrence data, acquiring lexical relations from the data, and applying these relations to semantic analysis.
These operators include a subset of traditional deductive rules of inference, argumentation theoretic rules of refutation, and inductive reasoning patterns.
We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that trans-forms speech signals through successive representa-tions of linguistic, dialogue, and domain knowledge.
This	paper	is	concerned with thespecifications and the implementation of a particular concept of word-based lexicon to be used for large natural language processing systems such as machine translation systems, and compares it with the morpheme-based conception of the lexicon traditionally assumed in computational linguistics.It will	be argued that, although lessconcise, a relational word-based	lexicon issuperior to-a morpheme-based	lexicon	from atheoretical, computational and also	practicalviewpoint.
Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data.
High-quality lexical resources are needed to both train and evaluate Word Sense Disambiguation (WSD) systems.
This paper presents techniques for multimedia annotation and their application to video summarization and translation.
This paper discusses the advantages for practical .hi- directional grammars Of combining a lexical focus with the GPSG-originated principle of immediate-?dominanceilinear-precedence (11)/I ,P) rule partitioning.
Finally the characteristics of the algorithm are analysed.Keywords: segmentation, connection, character-net, ambiguity, unknown words.
This paper introduces a method for computational analysis of move structures in abstracts of research articles.
We also present a prototype concordancer, CARE, which exploits the move-tagged abstracts for digital learning.
The paper describes a parser for Categorial Grammar which provides fully word by word incremental interpretation.
The paper includes a brief discussion of the relationship between basic Categorial Grammar and other formalisms such as HPSG, Dependency Grammar and the Lambek Calculus.
We discuss a Japanese text input method for mobile phones.
In this paper, we propose a method to group brackets in a bracketed corpus (with lexical tags), according to their local contextual information, as a first step towards the automatic acquisition of a context-free grammar.
Two techniques, distributional analysis and hierarchical Bayesian clustering, are applied to exploit local contextual information for computing similarity between two brackets.
This paper describes an approach to adapt an existing multilingual Open-Domain Question Answering (ODQA) system for factoid questions to a Restricted Domain, the Geographical Domain.
The new system uses external resources like GNS Gazetteer for Named Entity (NE) Classification and Wikipedia or Google in order to obtain relevant documents for this domain.
This paper describes a system that produces extractive summaries of short works of literary fiction.
This paper proposes a unified Transformation Based Learning (TBL, Brill, 1995) framework for Chinese Entity Detection and Tracking (EDT).
It consists of two sub models: a mention detection model and an entity tracking/coreference model.
This paper provides a theory of performatives as a test case for our rationally based theory of illocutionary acts.
The paper introduces a dependency-based grammar and the associated parser and focusses on the problem of determinism in parsing and recovery from errors.
Relatedness between word senses is measured using the WordNet:: Similarity Perl modules.
SCsyn is the unweighted syntax score.
SCsem is the semantic weighting.
Annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data.
We describe three analyses on the effects of spontaneous speech on continuous speech recognition performance.
Johnston 1998a proposes a modular approach to multimodal language processing in which spoken language parsing is completed before multimodal parsing.
This architecture greatly simplifies the spoken language parsing grammar and enables predictive information from spoken language parsing to drive the application of multimodal parsing and gesture combination rules.
We present the design of a practical context-sensitive glosser, incorporating current techniques for lightweight linguistic analysis based on large-scale lexical resources.
We discuss extensions to our plan-based discourse processor in order to make this possi-ble.
We evaluate those extensions and demonstrate the advantage of exploiting context-based predictions over a purely non context-based approach.
Human evaluations of machine translation are extensive but expensive.
The QCS information retrieval (IR) system is presented as a tool for querying, clustering, and summarizing document sets.
We present the dialogue component of the speech-to-speech translation system VERBMOBIL.
A dialogue memory is constructed incrementally.
This paper describes the LT NSL sys-tem (McKelvie et al, 1996), an architec-ture for writing corpus processing tools.
This paper proposes a grammar-based approach to semantic annotation which combines the notions of robust parsing and fuzzy grammars.
This paper shows that two uncertainty- based active learning methods, combined with a maximum entropy model, work well on learning English verb senses.
The linguistic framework of Generalized Phrase Structure Grammar offers tools for dealing with -word order variation.
We propose a NLP methodology for analyzing patent claims that combines symbolic grammar formalisms with data- intensive methods while enhancing analysis robustness.
The classification task is an integral part of named entity extraction.
In this paper, a general method of maximizing top-down constraints is proposed.
To accomplish this, the model introduces Viterbi parsing under two-dimensional stochastic CFGs.
We propose a novel context heterogeneity similarity measure between words and their translations in helping to compile bilingual lexicon entries from a non-parallel English-Chinese corpus.
Current algorithms for bilingual lexicon compilation rely on occurrence frequencies, length or positional statistics derived from parallel texts.
Based on this information, we derive statistics of bilingual word pairs from a non-parallel corpus.
These statistics can be used to bootstrap a bilingual dictionary compilation algorithm.
In this paper, we describe how quantifiers can be generated in a text generation system.
The use of two-way finite automata for Arabic noun stem and verb root inflection leads to abstractions based on finite-state transition network topology as well as the form and content of network arcs.
We present a 1000-word continuous speech recognition (CSR) system that operates in real time on a personal computer (PC).
(Unification Categorial Grammar).
UCG is a feature grammar incorporating some basic insights from GPSG [GAZDAR 85] and HPSG [POLLARD 84].
signsemantics ?
indexsemantics ?
An important problem that is related to phrase-based statistical translation models is the obtaining of word phrases from an aligned bilingual training corpus.
In this work, we propose obtaining word phrases by means of a Stochastic Inversion Translation Grammar.
It is a key technology of Information Extraction and Open-Domain Question Answering.
First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems.
We also present an SVM-based feature selection method and an efficient training method.
Thus the basic framework of weighted constraint dependency parsing is extended by the notion of dynamic dependency parsing.
We present a robust approach for linking already existing lexi-cal/semantic hierarchies.
Previous work has argued that memory-based learning is better than abstraction-based learning for a set of language learning tasks.
In this paper, we present an E-K transliteration model using pronunciation and contextual rules.
A PSG can be expressed by a tree graph where every node has a correspondent label.
We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.
Here, we motivate and illustrate our approach using discourse-level annotations of text and speech data drawn from the CALLHOME, COCONUT, MUC-7, DAMSL and TRAINS annotation schemes.
We show how annotation graphs can represent hybrid multi-level structures which derive from a diverse set of file formats.
This paper presents a brief overview of the bidirectional (Japanese and English) Transfer- Driven Machine Translation system, currently being developed at ATR.
The aim of this development is to achieve bidirectional spoken dialogue translation using a new translation technique, TDMT, in which an example-based framework is fully utilized to translate the whole sentence.
INTRODUCTIONTransfer-Driven Machine Translation[D,121(91, (TDMT) is a translation technique which utilizes empirical transfer knowledge compiled from actual translation examples.
With this transfer-centered translation mechanism together with the example- based framework[3],[4].
TRANSFER-DRIVENARCHITECTUREThe bidirectional TDMT system, shown in Figure 1, translates English into Japanese and Japanese into English.
1 Configuration of Bidirectional TDMTSystem64
This paper presents a method for diacritics restora-tion based on learning mechanisms that act at let-ter level.
Industrial applications of a reversible, string-based, unification approach called Humor (High-speed Unification Morphol-ogy) is introduced in the paper.
The seven categories of the scheme are based on rhetorical moves of argumentation.
We describe a framework for large-scale distributed corpus annotation using peerto-peer (P2P) technology to meet this need.
Assisting in foreign language learning is one of the major areas in which natural language processing technology can contribute.
Based on this approach, we conducted experiments on estimating the Test of English for International Communication (TOEIC) score.
documents with a major geographic component.
expressions denoting geographical localisations).
We present a linguistic analyser which recognises them, performing a semantic analysis and computing symbolic representations of their "content".
This report summarizes the system and its performance on the MUG-3 task.
This paper presents a study of the effects of syntax and melodic configuration on turn- taking in Southern British English.
The results suggest that syntactic completion or non-completion is the main factor in predicting turn-taking behaviour.
?Speech preprocessors are important.
We give a qualitative analysis of the zone identification (ZI) process in biology articles.
This paper proposes the use of situation theory as a basic semantic formalism for defining general semantic theories.
After a general description of Discourse Representation Theory an encoding of DRT in ASTL is given.
Advantages and disadvantages of this method are then discussed.Topic: computational formalisms in semantics and discourse
This work provides the essential foundations for modular construction of (typed) unification grammars for natural languages.
We find they can capture effectively allophonic variation, alternative pronunciation, word co-articulation and segmental durations.
We present an algorithm for the generation of sentences from the semantic representations of Unification Categorial Grammar.
We discuss a variant of Shieber's semantic mono tonicity requirement and its utility in our algorithm.
In this paper we first briefly describe our representation language Objtalk, and then illustrate how it is used for building an understanding system for processing German newspaper texts about the jobmarket situation.keywords: newspaper processing, ATN, frames semantic grammar, object-oriented programming, Obj TalkConcepts as active schemataWe have developped an object-orientedrepresentation language - called Obj Talk - in which objects are frame-like data structures which have behavioral traits and communicate through message passing.
An aspect of developing language interaction models is an investigation of dialogue structure.
In the paper a notion of elementary communicative triad (SR-triad) is introduced to model the "stimulus-reaction" relation between utterances in the dialogue.
In this paper a system for categorisation and automatic authoring of news streams in different languages is presented.
Authoring across documents in different languages is triggered by Named Entities and event recognition.
This kind of multilingual analysis relies on a lexical knowledge base of nouns(i.e.
the EuroWordnet Base Concepts) shared among English, Spanish and Italian lexicons.
Both components are needed for word sense resolution.
A contextual representation of a word sense consists of topical context and local context.
Our goal is to construct contextual representations by automatically extracting topical and local information from textual corpora.
We review an experiment evaluating three statistical classifiers that automatically extract topical context.
In this paper, we introduce an information- theoretic approach to automatic evaluation of summaries based on the Jensen-Shannon divergence of distributions between an automatic summary and a set of reference summaries.
The project is devoted to the evaluation of parallel text alignment techniques.
In its first period ARCADE ran a competition between six systems on a sentence-to-sentence alignment task which yielded two main types of results.
Dutch subordinate clauses.
Dutch subordinate clauses for non-transformational theories of grammar.
GPSG also includes the idea of rule schemata - rules with variables over categories.
We then examine the effect of integrating pausal and spontaneous speech phe-nomena into syntactic rules for speech recognition, using118 sentences.
This paper presents a format for representing the linguistic form of utterances, called situation schemata, which is rooted in the situation semantics of Barwise and Perry.
I also show that WordNet categories improve a system that performs aspectual classification with linguistically-based numerical indicators.
We present the newest implementation of the LINGSTAT machine-aided translation system.
INTRODUCTIONLINGSTAT is an interactive machine-aided translation system designed to increase the productivity of a translator.
Similarity of a word to a context is estimated using a proximity measure in corpus- derived "semantic space".
Polarized dependency (PD-) grammars are proposed as a means of efficient treatment of discontinuous construc-tions.
The discussion is motivated with a case of study in multimodal reference resolution.
In Section 3, an incremental model for multimodal reference resolution is illustrated.
Finally, in the conclusion of the paper, a reflexion on the relation between spacial deixis and anaphora is advanced.
The previous probabilistic part-of-speech tagging models for agglutinative languages have considered only lexical forms of morphemes, not surface forms of words.
By experiments, we show that the proposed model outperforms the bigram Hidden Markov model (HMM)-based tagging model.based tagging model.2 Korean POS tagging modelIn this section, we first describe the standard morpheme-unit tagging model and point out a mistake of this model.
Then, we describe the proposed model.2.1 Standard morpheme-unit modelThis section describes the HMM-based morpheme- unit model.
The morpheme-unit POS tagging model is to find the most likely sequence of morphemes M and corresponding POS tags T for a given sentence W, as follows (Kim et al., 1998; Lee et al., 2000):
Within the machine translation system Verb-mobil, translation is performed simultaneously by four independent translation modules.
This paper describes a method for converting a task-dependent grammar into a word predictor of a speech understanding system.
We have solved this problem by applying an algorithm for bottom-up parsing.
We study how two graph algorithms apply to topic-driven summarization in the scope of Document Understanding Conferences.
Our algorithms select sentences for extraction.
A bottom-up generation algorithm for principle-based grammars is proposed.
We describe a new technique for con-structing finite-state transducers that in-volves reapplying the regular-expression compiler to its own output.
Speaker independent phonetic transcription of fluent speech is performed using an ergodic continuously variable duration hidden Markov model (CVDHMM) to represent the acoustic, phonetic and phonotac tic structure of speech.
The transcription algorithm was then combined with lexical access and parsing routines to form a complete recognition system.
We consider the problem of extracting specified types of information from natural language text.
We therefore use preference semantics: selecting the analysis which maximizes the number of semantic patterns matched.
This paper describes an extremely lexicalized probabilistic model for fast and accurate HPSG parsing.
In the spoken language machine trans-lation project Verbmobil, the seman-tic formalism Language for Underspec-ified Discourse representation structures (LUD) is used.
We present a comparison of two formalisms for representing natural language utterances, namely deep syntactical Tectogrammatical Layer of Functional Generative Description (FGD) and a semantic formalism, MultiNet.
We define a semantic chunk as the sequence of words that fills a semantic role defined in a semantic frame.
We explore two semantic chunking tasks.
In the first task we simultaneously detect the target word and segments of semantic roles.
We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural- language processing.
Comparing the performance of PLTIGs with non-hierarchical N-gram models and PCFGs, we show that PLTIG combines the best aspects of both, with language modeling capability comparable to N-grams, and improved parsing performance over its nonlexicalized counterpart.
Furthermore, training of PLTIGs displays faster convergence than PCFGs.
This paper' presents an example-basedrescoring method that validates SMT translation candidates and judges whether the selected decoder output is good or not.
This paper describes the user expertise model in AthosMail, a mobile, speech-based e-mail system.
This paper proposes a new indicator of text structure, called the lexical cohesion profile (LCP), which locates segment boundaries in a text.
LCP records mutual similarity of words in a sequence of text.
LCP may provide valuable information for resolving anaphora and ellipsis.
This paper presents an approach which exploits general-purpose algorithms and resources for domain-specific semantic class disambiguation, thus facilitating the generalization of semantic patterns from word-based to class-based representations.
Through the mapping of the domain- specific semantic hierarchy onto Word- Net and the application of general-purpose word sense disambiguation and semantic distance metrics, the approach proposes a portable, wide-coverage method for disambiguating semantic classes.
For topic identification, we will outline techniques based on stereotypical text structure, cue words, high-frequency indicator phrases, intratext connectivity, and discourse structure centrality.
For topic fusion, we will outline some ideas that have been proposed, including concept generalization and semantic association.
Semantic language model is a technique that utilizes the semantic structure of an utterance to better rank the likelihood of words composing the sentence.
Natural-Language Generation from flat semantics is an NP-complete problem.
We show how to convert TAG generation problems into dependency parsing problems, which is useful because optimizations in recent dependency parsers based on constraint programming tackle exactly the combinatorics that make generation hard.
State of the art spelling correction systems, e.g.
Traditionally, word sense disambiguation (WSD) involves a different context classification model for each individual word.
In this research, maximum entropy modeling is used to train the word independent context pair classification model.
Then based on the context pair classification results, clustering is performed on word mentions extracted from a large raw corpus.
trigger words and parsing structures.
(HLT-NAACL Workshop on Parallel Texts 2006).
We have studied different techniques to improve the standard Phrase-Based translation system.
Probabilistic Latent Semantic Analysis (PLSA) is an information retrieval technique proposed to improve the problems found in Latent Semantic Analysis (LSA).
We have applied both LSA and PLSA in our system for grading essays written in Finnish, called Automatic Essay Assessor (AEA).
We report the results comparing PLSA and LSA with three essay sets from various subjects.
The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution.
We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm.
While most work on parsing with PCFGs has focused on local correlations between tree configurations, we attempt to model non-local correlations using a finite mixture of PCFGs.
This paper presents results on experiments using this approach, in which statisti-cal models of the term selection and term ordering are jointly applied to pro-duce summaries in a style learned from a training corpus.
We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks.
We present an operational framework allowing to express a large scale Tree Adjoining Grammar (TAG) by using higher level operational constraints on tree descriptions.
This paper describes the results of a preliminary study of a Knowledge Engineering approach to Natural Language Understanding.
Classifier combination is a promising way to improve performance of word sense disambiguation.
As an option to the parsing, this paper discusses a way to identify the subject and the predicate, known as the pola-grammar technique.
Parsing spoken input introduces serious problems not present in parsing typed natural language.
This paper describes an extension of semantic caseframe parsing to restricted-domain spoken input.
The semantic caseframe grammar representation is the same as that used for earlier work on robust parsing of typed input.
We ran both Brill抯 rule-based tagger and TNT, a statistical tagger, with a default German newspaper-language model on a medical text corpus.
Concept grammar rules are built by mapping concepts from the lexicon onto the concept-structure patterns present in a set of training responses.
This paper analyses the syntax and semantics of English comparatives, and some types of ellipsis.
a range of current grammatical theories.
We investigate linguistic-knowledge-based word similarity measures while other previous works heavily rely on statistical information, and their limits will be discussed.
The first enhancement considers topic representations in terms of relevant topic relations instead of relevant terms.
The second enhancement is based on ranking the topic themes.
Topic representations are integrated in two NLP applications: Information Extraction and Multi- Document Summarization.
The method relays in the use of a wide-coverage noun taxonomy and a statistical measure of the co-occurrence of linguistic items.
Integrating puns into normal text may involve complex search.
We present a language-independent and unsupervised algorithm for the segmentation of words into morphs.
Our bottom-up deterministic parser adopts Nivre抯 algorithm (Nivre, 2004) with a preprocessor.
Support Vector Machines (SVMs) are utilized to determine the word dependency attachments.
In this paper, we propose a framework in which information about the multi-word expressions can be used in the word-alignment task.
The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information (constituent bracketing) in a partially parsed corpus.
In this paper, we present a formal definition of reversible NLG systems and develop a classification of existing natural language dialog systems in this framework.
The three sources of information are appositives, compound nouns, and ISA clauses.
We apply heuristics to these syntactic structures, embed them in a bootstrapping architecture, and combine them with co-training.
Previous works on question classification are based on complex natural language processing techniques: named entity extractors, parsers, chunkers, etc.
This paper reviews four approaches to the annotation of reference in corpora.
Motivated by a systematic analysis of Chinese	semantic	relationships,	weconstructed a Chinese semantic framework based on surface syntactic relationships, deep semantic relationships and feature structure to express dependencies between lexical meanings and conceptual structures, and relations that underlie those lexical meanings.
This paper investigates model merging, a technique for deriving Markov models from text or speech corpora.
This paper proposes an alignment adaptation	approach	to	improvedomain-specific (in-domain) word alignment.
We compare representatives of two principal approaches to computing phonetic similarity: manually-designed metrics, and learning algorithms.
We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
In this paper we present a preliminary study aimed at automatically identifying 搃rrelevance?in the domain of telephone conversations.
We first created an HPSG treebank from the EDR corpus by using heuristic conversion rules, and then extracted lexical entries from the tree- bank.
We also trained a statistical parser for the grammar on the tree- bank, and evaluated the parser in terms of the accuracy of semantic-role identification and dependency analysis.
One task is the extraction of cognates from bilingual text.
This paper presents an algorithm for the unsupervised learning of a simple morphology of a natural language from raw text.
A generative probabilistic model is applied to segment word forms into morphs.
We present a simple approximation method for turn-ing a Head-Driven Phrase Structure Grammar into a context-free grammar.
GAMBL is a word expert approach to WSD in which each word expert is trained using memory- based learning.
A further innovation on earlier versions of memory- based WSD is the use of grammatical relation and chunk features.
This paper describes a computational model of concept acquisition for natural language.
Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words.
The acoustic models make use of dictionary phonetic spellings together with models for phonemes in context.
We present a computational model for generation and recognition of Georgian verb conjugations, relying on the analysis of Georgian verb structure as a word-level template.
The model combines a set of finite-state transducers with a default inheritance mechanism.1
In a rule based machine translation system, the grammar consists of a lot of rewriting rules.
We propose new approach to the problem that relies on term specificity and similarity measures.
DAAL03-89-00031.
In this paper, we introduce a large-scale test collection for multiple document summarization, the Text Summarization Challenge 3 (TSC3) corpus.
This paper addresses the issue of designing embodied conversational agents that exhibit appropriate posture shifts during dialogues with human users.
We present an analysis of human monologues and dialogues that suggests that postural shifts can be predicted as a function of discourse state in monologues, and discourse and conversation state in dialogues.
On the basis of these findings, we have implemented an embodied conversational agent that uses Collagen in such a way as to generate postural shifts.
A grammar formalism, Field and Category Grammar (FCG), is described, which beside constituent structure and functional structure recognizes a level of field structure.
We present a lexicon-free post-processing method for optical character recognition (OCR), implemented using weighted finite state machines.
We present an LG-based approach to recognizing of Proper Names in Korean texts.
IntroductionIn this paper, we present a description of the typology of nominal phrases containing Proper Names (PN) and the local grammars [Gro87],[Moh94] constructed on the basis of this description.
Current lexical semantic representations for natural language applications view verbs as simple predicates over their arguments.
The purpose of this study is to construct a semantic analysis method for disambiguating Japanese compound verbs.
We construct a method employing 110 disambiguation rules based on the semantic features of the first verb of a compound and syntactic patterns consisting of co-occurrence between verbs and nouns.
We describe iNeATS ?an interactive multi-document summarization system that integrates a state-of-the-art summarization engine with an advanced user interface.
In this paper, we describe our experiments on statistical word sense disambiguation (WSD) using two systems based on different approaches: Naive Bayes on word tokens and Maximum Entropy on local syntactic and semantic features.
In the second system, sense resolution is done using an approximate syntactic structure as well as semantics of neighboring nouns as features to a Maximum Entropy learner.
The hierarchical structure is discovered wing lexical cohesion methods combined with hierarchical agglomerative clustering.
The list of concepts are discovered by n-gram analysis filtered by part-of-speech patterns.
This work reports on three human tense annotation experiments for Chinese verbs in Chinese-to-English translation scenarios.
The analyses also find the verb telicity feature, aspect marker presence and syntactic embedding structure to be strongly associated with tense, suggesting their utility in the automatic tense classification task.
This approach is successful for entailments.
Hence, we view the study of these functional units as lexical pragmatics rather than lexical semantics.
Default Logic is one formal method for performing default reasoning in the area of Artificial Intelligence called Knowledge Representation.
We suggest that the use of non-classical inferencing techniques such as default reasoning will prove fruitful in the realm of lexical reasoning.
We present here a methodology for obtaining semantic information on verb aspect by parsing a corpus and automatically applying linguistic tests with a set of structural analysis tools.
We present a computationally tractable account of the interactions between sentence markers and focus marking in Somali.
We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse.
We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG.
We describe an implementation of data- driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.
Practically applied in the DIRECT-INFO EC R&D project we show how such linguistic annotation contributes to semantic annotation of multi- modal analysis systems, demonstrating also the use of the XML schema of MPEG-7 for supporting cross-media semantic content annotation.
In this paper, we describe an empirical evaluation of an adaptive mixed initiative spoken dialogue system.
Finally, the recognizer has been modified to use bigram back-off language models.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
The former models certain uses of multisetvalued feature structures in unification-based formalisms, while the latter is motivated by word order variation and by "quasi-trees", a generalization of trees.
Speech Recognition and Machine Translation are also changing the world.
Managing Gigabytes: Compressing and Indexing Documents and Images, Academic Press/Morgan Kaufmann.
A CYK-table-driven interactive relaxation parsing method oi spoken Korean, integrated with the CYK-based morphological analysis is introduced.
We argue that grammatical processing is a viable alternative to concept spotting for processing spoken input in a practical dialogue system.
We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input.
WORD- NET DOMAINS).
Statistical machine translation systems are based on one or more translation models and a language model of the target language.
While many different translation models and phrase extraction algorithms have been proposed, a standard word n-gram back-off language model is used in most systems.In this work, we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary.
We consider the translation of European Parliament Speeches.
This paper reports on a new statistical approach to machine aided translation of terminology bank.
We introduce a new categorial formal-ism based on intuitionistic linear logic.
Incorporating these restrictions into Head-Driven Phrase Structure Grammar (HPSG) has caused us to examine the treatment of nominal modification in HPSG.
We evaluate empirically a scheme for combining classifiers, known as stacked generalization, in the context of anti-spam filtering, a novel cost-sensitive application of text categorization.
We developed a novel classification of concept attributes and two supervised classifiers using this classification to identify concept attributes from candidate attributes extracted from the Web.
We demonstrate a novel application of EM-based clustering to multivariate data, exemplified by the induction of 3- and 5-dimensional probabilis-tic syllable classes.
We then propose a novel approach to grapheme-to-pho-neme conversion and show that syl-lable structure represents valuable information for pronunciation sys-tems.
In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) sub- compounds from complex noun phrases using both corpus statistics and linguistic heuristics.
Our system for semantic role labeling is multi-stage in nature, being based on tree pruning techniques, statistical methods for lexicalised feature encoding, and a C4.5 decision tree classifier.
We use both shallow and deep syntactic information from automatically generated chunks and parse trees, and develop a model for learning the semantic arguments of predicates as a multi-class decision problem.
To translate SMS texts, traditional approaches model such irregularities directly in Machine Translation (MT).
This paper describes eight telephone-speech corpora at various stages of development at the Center for Spoken Language Understanding.
It combines word-based frequency and position method to get categorization knowledge from the title field only.
The system contains a multivalued part-of-speech disambiguator and a novel parser employing bottom-up recognition to find the constituent phrases of larger structures that might be too difficult to analyze.
We investigate the problem of learning a part-of-speech (POS) lexicon for a resource-poor language, dialectal Arabic.
We frame the lexicon acquisition problem as a transductive learning problem, and perform comparisons on three transductive algorithms: Transductive SVMs, Spectral Graph Transducers, and a novel Transductive Clustering method.
In this paper, we will present first the syntactic analysis, based on a chart parser that uses a LFG grammar for French, and the semantic analysis, based on conceptual graphs.
Then we will show how these two analyses collaborate to produce semantic representations and sentences.
Before concluding, we will show how these modules are used through a distributed architecture based on CORBA (distributed Smalltalk) implementing the CARAMEL multi-agent architecture.
We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees.
We report here on our experiments with POST (Part of Speech Tagger) to address problems of ambiguity and of understanding unknown words.
This paper presents an adaptive learning framework for Phonetic Similarity Modeling (PSM) that supports the automatic construction of transliteration lexicons.
This paper presents experiments with the evaluation of automatically produced summaries of literary short stories.
Since parsing results given by the lexicalized grammar cannot be decomposed into independent sub-events, we apply a maximum entropy model for feature forests, which allows probabilistic modeling without the independence assumption.
STS is a small experimental sentence translation system developed to demonstrate the efficiency of our lexicalist model of translation.
Based on a GB-inspired parser, lexical transfer and lexical projection, STS provides real-time accurate English.
This paper proposes a new approach to segmentation of utterances into sentences using a new linguistic model based upon Maximum-entropy-weighted Bidirectional N-grams.
Language Understanding (NLU) systems,particularly for data-driven ones.
knowledge and language-specific heuristics.
This paper examines the properties of feature- based partial descriptions built On top of Halliday's systemic networks.
This paper describes the general architecture of generation in the ACORD project.
SemEval is an attempt to define a task-independent technology-based evaluation for language- understanding systems consisting of three parts: word-sense identification, predicate-argument structure determination, and identification of coreference relations.
This paper describes a Chinese word segmentor (CWS) based on backward maximum matching (BMM) technique for the 2nd Chinese Word Segmenta-tion Bakeoff in the Microsoft Research (MSR) closed testing track.
Our CWS comprises of a context-based Chinese unknown word identifier (UWI).
In this paper we discuss an algorithm for the assignment of pitch accent positions in text-to-speech conversion.
When a base-line speech recognition system generates al-ternative hypotheses for a sentence, we will utilize the word preferences based on topic coherence to select the best hy-pothesis.
This paper revisits the one sense per collocation hypothesis using fine-grained sense distinctions and two different corpora.
This explains the low results when performing word sense disambiguation across corpora.
In this paper, we report the adaptation of a named entity recognition (NER) system to the biomedical domain in order to participate in the 擲hared Task Bio-Entity Recognition?
As a measure of similarity, the concept of longest collocation couple is introduced, which is the basis of clustering similar words.
We proposed a method of machine transla-tion using inductive learning with genetic algorithms, and confirmed the effectiveness of applying genetic algorithms.
This paper discusses research on distinguishing word meanings in the context of information retrieval systems.
We conducted experiments with three sources of evidence for making these distinctions: morphology, part-of-speech, and phrases.
We have focused on the distinction between homonymy and polysemy (unrelated vs. related meanings).
Our results support the need to distinguish homonymy and polysemy.
This paper presents an algorithm for tagging words whose part-of-speech properties are unknown.
The algorithm is based on predicates defined for WordNet verb classes.
Figure 1: Dimensions of Language Engineering Complexity.
These generic systems include: (1) integrated multi-rate voice/data communications terminal; (2) interactive speech enhancement system; (3) voice-controlled pilot's associate system; (4) advanced air traffic control training systems; (5) battle management command and control support system with spoken natural language interface; and (6) spoken language translation system.
We demonstrate TextRank ?a system for unsupervised extractive summarization that relies on the application of iterative graph- based ranking algorithms to graphs encoding the cohesive structure of a text.
This paper presents a novel method of generating and applying hierarchical, dynamic topic-based language models.
It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques.
These combined models help capture long-distance lexical dependencies.
We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.
This paper describes a method to find phrase-level translation patterns from parallel corpora by applying dependency structure analysis.
We use statistical dependency parsers to determine dependency relations between base phrases in a sentence.
Common algorithms for sentence and word-alignment allow the automatic identification of word translations from parallel texts.
We describe I/J-terms and how they have been incorporated into the Logic Grammar formalism.
Its taxonomic reasoning facilitates semantic type-class reasoning during grammatical analysis.
A semantic-representation-to--speech system communicates orally the information given in a semantic representation.
Such a system must Integrate a text generation module, a phonetic conversion module, a prosodic module and a speech synthesizer.
It is loosely based on a new psycholinguistic theory of coordinative ellipsis proposed by Kempen.
To investigate the feasibility of automated summary evaluation based on the recent BLEU method from machine translation, we use accumulative n-gram overlap scores between system and human summaries.
In this paper, I argue for the use of a probabilistic form of tree-adjoining grammar (TAG) in statistical natural language processing.
One approach for achieving this is to represent the classification model as a weighted finite-state transducer (WFST).
In this paper, we present a compilation procedure to convert the rules resulting from an AdaBoost classifier into an WFST.
This paper investigates the linguistic application of pragmatic-based constraints to the 憇emantic?notion of presupposition in DRT.
In this paper, we report on an effort to provide a general-purpose spoken language generation tool for Concept-to-Speech (CTS) applications by extending a widely used text generation package, FUF/SURGE, with an intonation generation component.
As a first step, we applied machine learning and statistical models to learn intonation rules based on the semantic and syntactic information typically represented in FUF/SURGE at the sentence level.
In this paper, we introduce a generic approach to elliptic coordination modeling through the parsing of Ltag grammars.
This paper briefly outlines the WYSIWYM (What You See is What You Meant) approach to knowledge editing and focuses on the role of coreferring Noun Phrases in the feedback texts that are generated by a WYSIWYM system and which play a key role in this approach.
A Bayesian classifier trained on explicitly structured MEDLINE abstracts generates these argumentative categories.
By combining finite-state and unification-based formalisms, the grammar formalism used in SProUT offers both processing efficiency and a high degree of decalrativeness.
We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs.
Next section describes the prototype which is based on a Transformational System as well as on a rewriting system of c-graphs which constitutes the nodes of the Transformational System.
In this paper, DARPA Resource Management task is used as the domain to investigate the performance of speaker-adaptive speech recognition.
We describe an algorithm for Word Sense Disambiguation (WSD) that relies on a lazy learner improved with automatic feature selection.
This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE.
We study the effect of the Cartesian product operator on memory-based language learning, and demonstrate its effect on generalization accuracy and data compression for a number of linguistic classification tasks, using k-nearest neighbor learning algorithms.
We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment between a source parse tree and a target string.
In this paper, a representation for syntactic dependency trees (D-trees) is defined through a finite set of axioms.
Bracketed D-tree representations (cf.
We address the problem of transliterating English names using Chinese orthography in support of cross-lingual speech and text processing applications.
We also present an evaluation of this module in retrieval of Mandarin spoken documents from the TDT corpus using English text queries.
In this paper, we propose a classification of gram-mar development strategies according to two crite-ria: hand-written versus automatically acquired grammars, and grammars based on a low versus high level of syntactic abstraction.
In this paper, we describe the acquisition and organization of knowledge sources for machine translation (MT) systems.
Automatic summarization and information extraction are two important Internet services.
This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two tasks initiated by SUMMAC-1.
We present an algorithm for generating appropriate anaphoric expressions which takes the temporal structure of texts and knowledge about ambiguous contexts into account.
We selected three typical word alignment models ranging over statistical-based ones and heuristic-based ones, to test whether cross language similarities can improve the performance of word alignment models.
In this paper interpretation principles for simple and complex frame-adverbial expressions are presented.
We also explore a potential application in document clustering that is based upon different types of lexical changes.
Our method is a statistical approach based on what we call textual entailment patterns, prototypical sentences hiding entailment relations among two activities.
This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues.
A generative probability model for unification- based grammars is presented in which rule probabilities depend on the feature structure of the expanded constituent.
We describe in this paper a boolean Information Retrieval system that adds word semantics to the classic word based indexing.
We consider lexical operations and their representation in a unification based lexicon and the role of lexical semantic information.
We introduce a framework for semantic interpreta-tion in which dependency structures are mapped to conceptual representations based on a parsimonious set of interpretation schemata.
information technology test reports and medical finding reports.
This paper presents a corpus-based account of structural priming in human sentence processing, focusing on the role that syntactic representations play in such an account.
We estimate the strength of structural priming effects from a corpus of spontaneous spoken dialogue, annotated syntactically with Combinatory Categorial Grammar (CCG) derivations.
The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model.
In (Hindle and Rooth, 1993) hereafter H&R, lexicalized rules are derived according to the probability of noun-preposition or verb-preposition bigrams for am-biguous structures like verb-noun-preposition-noun se-quences.
In (Resnik and Hearst, 1993) class-based trigrams are obtained by generalizing the PP head, using WordNet synonymy sets.
(Franz, 1995) uses a loglinear model to estimate preferred attachments according to the linguistic features of co-occurring words (e.g.
bigrams, the accompanying noun determiner, etc.).
(Brill and Resnik, 1994) use transformation- based error-driven learning (Brill, 1992) to derive disambiguation rules based on simple context information (e.g.
In this paper we report on our natural language informa-tion retrieval (NLIR) project as related to the recently concluded 5th Text Retrieval Conference (TREC-5).
In this paper we discuss both manual and automatic procedures for query expansion within a new stream-based information retrieval model.
We present an adaptive technique that enables users to produce a high quality dictionary parsed into its lexicographic components (headwords, pronunciations, parts of speech, translations, etc.)
In this paper, baseline speech recognition performance is determined both for a single remote microphone and for a signal derived from a delay-and-sum beamformer using an eight-microphone linear array.
We describe a novel technique and imple-mented system for constructing a subcate-gorization dictionary from textual corpora.
We compare two stories by finding three cosine similarities based on names, topics and the full text.
This paper presents a semantic class prediction model of Chinese two-character compound words based on a character ontology, which is set to be a feasible conceptual knowledge re-source grounded in Chinese characters.
Linear precedence (LP) rules are widely used for stating word order principles.
We show a type-based encoding in an HPSG-style formalism that supports processing.
We propose four new methods to accurately determine word sense dominance using raw text and a published thesaurus.
In the process, we create a word朿ategory co- occurrence matrix, which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses, as well.
We apply Support Vector Machines (SVMs) to identify English base phrases (chunks).
We apply weighted voting of 8 SVMs-based systems trained with distinct chunk repre-sentations.
This paper describes a method for analyzing Japanese double-subject construction having an adjective predicate based on the valency structure.
We describe a word alignment platform which ensures text pre-processing (tokenization, POS-tagging, lemmatization, chunking, sentence alignment) as required by an accurate word alignment.
This paper reports on efforts to automatically identify and classify discourse markers in Chinese texts using heuristic-based and corpus-based data-mining methods, as an integral part of automatic text summarization via rhetorical structure and Discourse Markers.
A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model.
In this paper, we propose a recursive graph based scheme for semantic annotation of Chinese phrases.
This paper presents a practical method for a global structure analyzing algorithm of Japanese long sen-tences with lexical information, a method which we call Lexical Discourse Grammar (LDG).
Finally, we evaluate our system by real corpus and present the experiment results.Key Words: Word Alignment, Chunk Alignment, Bilingual Corpus, Lexicon Extraction
This paper describes a data source and methodology for producing customized test suites for molecular biology entity identification systems.
We first present a version of definite clauses (positive Horn clauses) that is based on this logic.
We also describe a higher-order logic programming language, called AProlog, which represents programs as higher-order definite clauses and interprets them using a depth-first interpreter.
This is to be contrasted with the more common practice of using two entirely different computation paradigms, such as DCGs or ATNs for parsing and frames or semantic nets for semantic processing.
We describe a formal framework for interpretation of words and compounds in a discourse context which integrates a symbolic lexicon/grammar.
word-sense probabilities, and a pragmatic component.
We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues.
We then automatically extract features representing acoustic-prosodic and other linguistic information from the speech signal and associated transcriptions.
We compare the results of machine learning experiments using different feature sets to predict the annotated emotions.
The supervised learning system is based on lexical features and bagged decision trees.
They are:?Vector-quantized energy-normalized Mel-cepstra?Vector-quantized smoothed 40-ms晅ime derivatives of the Mel-cepstra?Energy?Smoothed 40-ms energy differencesWe use 256-word speaker-independent code- books to vector-quantize the Mel-cepstra and the Melcepstral differences.
The resulting four-feature-perframe vector is used as input to the DECIPHER HMMbased speech recognition system.Pronunciation ModelsDECIPHER uses pronunciation models generated by applying a phonological rule set to word base- forms.
Speaker- independent pronunciation probabilities are then estimated using these bushy word networks and the forward- backward algorithm in DECIPHER.
We have shown in Cohen90 that this modeling improves system performance.Acoustic ModelingDECIPHER builds and trains word models by using context-based phone models arranged according to the pronunciation networks for the word being modeled.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
The components are implemented as SVM classifiers using libSVM.
Experiments were conducted to find kernel parameters for the Radial Basis Function (RBF) kernel.
This paper proposes a generation method for feature-structure-based unification grammars.
The method enables feature structure retrieval via multiple indices.
This paper discusses Korean morpho-logical analysis and presents three probabilistic models for morphological analysis.
part-of-speech tagged corpora).
Default inheritance is a useful tool for encoding linguistic generalisations that have exceptions.
In this paper we show how the use of an order independent typed default unification operation can provide non-redundant highly structured and concise representation to specify a network of lexical types, that encodes linguistic information about verbal sub- categorisation.
The method involves using a bilingual term list to learn source- target surface patterns.
We present a prototype called TermMine that applies the method to translate terms.
The S-graph is an acyclic directed graph with exactly one start node and one end node.
It includesa) a morphosyntactic type (MS), i.e.
), the C/O-concept is based on context-free rules.
Where GPSG employs meta-rules, derived categories, and the ID/LP-formalism, LFG uses different structural concepts (C- and F-structures) and - above all - lexical knowledge.
LFG and GPSG are augmented PS- grammars.
The algorithm works by first using a large corpus to find semantic neighbors of the unknown word, which we accomplish by combining latent semantic analysis with part-of-speech information.
We evaluate the contribution made by part-of-speech tagging and show that automatic filtering using the class-labelling algorithm gives a fourfold improvement in accuracy.
The method we are currently working on uses an expectation-maximization (EM) based word-clustering algorithm, and we have evaluated the effectiveness of this method using Japanese verb phrases.
This paper investigates the application of co- training and self-training to word sense disambiguation.
sociated with competing hypotheses.
This article introduces a bidirectionalgrammar generation system called feature structure-directed generation, developed for a dialogue translation system.
We have been developing a spoken language system to recognize and understand spontaneous speech.
2 Previous workThe MT system described in this paper combines hand-built analysis and generation components with automatically learned example-based transfer patterns.
finite^state and probabilistic methods).
The approach combines a statistical model of named entity states with a lattice representation of hypothesized words and errors annotated with recognition confidence scores.
This paper proposes methods for extractingloanwords from Cyrillic Mongolian corporaand producing a Japanese朚ongolianbilingual dictionary.
We propose astemming method for Mongolian to extractloanwords correctly.
This paper describes PEGASUS, a spoken language interface for on-line air travel planning that we have recently developed.
This paper describes our preliminary research on "attention-sharing" in infants' language acquisition.
This paper outlines a formal computational semantics and pragmatics of the major speech act types.
The purpose of this paper is to auto-matically generate Chinese chunk bracketing by a bottom-to-top map-ping (BTM) model with a BTM data-set.
The BTM model is designed as a supporting model with parsers.
We de-fine a word-layer matrix to generate the BTM dataset from Chinese Tree-bank.
Our model matches auto-learned patterns and templates against seg-mented and POS-tagged Chinese sen-tences.
This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame.
Unknown words are recognized with reliability in role-based HMM.
An HHMM-based system ICTCLAS was accomplished.
We present a tool developed for annota-ting corpora with argument structure re-presentations.
Among others, we show how the as-signment of grammatical functions can be automatised using standard part-of-speech tagging methods.
We report that a proper employment of MWEs concerned enables us to put forth a tractable framework, which is based on a multiple nesting of semantic operations, for the processing of non-inferential, Non- propositional Contents (NPCs) of natural Japanese sentences.
We present a global sequence-processing method that repairs inconsistent local decisions.
In this document, we analyze several as-pects related to the semantics of preposi-tions.
In this paper, we present a stochastic language modeling tool which aims at retrieving variable-length phrases (multigrams), assuming bigram dependencies between them.
We present the first algorithm that computes optimal orderings of sentences into a locally coherent discourse.
In a learning phase, linguistic knowledge such as conceptual co-occurrence patterns and syntactic role distribution of antecedents is extracted from a large-scale corpus.
Unlike previous research based on co-occurrence patterns at the lexical level, we represent co-occurrence patterns with concept types in a thesaurus.
We describe a method for incorporating syntactic information in statistical machine translation systems.
In this paper we present a multiclassifier approach for multilabel document classification problems, where a set of k-NN classifiers is used to predict the category of text documents based on different training subsampling databases.
We investigate the change in performance of automatic subcategorization acquisition when a word sense disambiguation (WSD) system is employed to guide the acquisition process.
This paper discusses the design of the EuroWordNet database, in which semantic databases like WordNetl .5 for several languages are combined via a so-called inter-lingual-index.
This involves deriving abstract relationships among conceptual units.
This paper describes unsupervised learning algorithm for disambiguating verbal word senses using term weight learning.
In our method, collocations which characterise every sense are extracted using similarity-based estimation.
For the results, term weight learning is performed.
Processing discourse connectives is important for tasks such as discourse parsing and generation.
This paper presents experiments into modelling the substitutability of discourse connectives.
It shows that substitutability effects distributional similarity.
A novel variance- based function for comparing probability distributions is found to assist in predicting substitutability.
We introduce a new interactive corpus exploration tool called InfoMagnets.
In particular, this paper presents an in-depth investigation of the entity detection and recognition (EDR) task for Arabic.
We start by highlighting why segmentation is a necessary prerequisite for EDR, continue by presenting a finite-state statistical segmenter, and then examine how the resulting segments can be better included into a mention detection system and an entity recognition system; both systems are statistical, build around the maximum entropy principle.
We apply our method to two tasks ?semantic role labeling and recognizing textual entailment ?and achieve useful performance gains from the superior pipeline architecture.
Examples show how word dependency across phrases can be modeled.
The verbgraphs may be used to support NL update.
This paper builds on recent research investigating sentence ordering in text production by evaluating the Centering-based metrics of coherence employed by Karamanis et al.
The Prague Czech-English Dependency Tree- bank (PCEDT) is a new syntactically annotated Czech-English parallel resource.
Using the formalism of generalized phrase structure grammar (GPSG) in an NI, system (e.g.
In this paper we present the RWTH FSA toolkit ?an efficient implementation of algorithms for creating and manipulating weighted finite-state automata.
The claim is substantiated through experimental data and an illustration of a word sense disambiguation system (SENSE) capable of using contextually-relevant semantic similarity.
OF COLING-92, NAN-rEs, Auc.
We address Question Answering (QA) for biographical questions, i.e., questions asking for biographical facts about persons.
We present BioGrapher, a biographical QA system that addresses this problem by machine learning algorithms for biography classification.
We present two novel paraphrase tests for automatically predicting the inherent semantic relation of a given compound nominalisation as one of subject, direct object, or prepositional object.
We compare these to the usual verb朼rgument paraphrase test using corpus statistics, and frequencies obtained by scraping the Google search engine interface.
This paper describes a system that performs hierarchical error repair for ill- formed sentences, with heterarchical control of chart items produced at the lexical, syntactic, and semantic levels.
The system uses an augmented context-free grammar and employs a bidirectional chart parsing algorithm.
This paper focuses on the heterarchical processing of integrated- agenda items (i.e.
Many different metrics exist for evaluating parsing results, including Viterbi, Crossing Brackets Rate, Zero Crossing Brackets Rate, and several others.
We apply a complementary similarity measure to find a hierarchical word structure.
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language.
Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections.Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91 % F-measure.
The second method learns less restrictive patterns that include bags of words and relation-specific named entity tags.
In this paper, we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models.
We propose a method which explicitly takes into account such interdependencies during the EM training of the statistical alignment models.
We investigate the usefulness of evolutionary al-gorithms in three incarnations of the problem of feature relevance assignment in memory-based language processing (MBLP): feature weight-ing, feature ordering and feature selection.
Both types of models are trained by Expectation- Maximization (EM) algorithms for maximum likelihood estimation.
Case dependencies and noun class generalization are represented as features in the maximum entropy approach.
In this paper we discuss and evaluate CTXMATCH, an approach to interoperability that discovers mappings among CHs considering the semantic interpretation of their nodes.
CTXMATCH performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.
We describe the baseline phrase-based translation system and various refinements.
We present translation results for three tasks: Verb- mobil, Xerox and the Canadian Hansards.
In this work, we report on supervised learning experiments to learn this distinction for the difficult case of prepositional phrases attached to the verb.
We develop statistical indicators of linguistic diagnostics for argumenthood, and we approximate them with counts extracted from an annotated corpus.
The WordNet lexical ontology, which is primarily composed of common nouns, has been widely used in retrieval tasks.
To support this claim, we build a fine-grained proper noun ontol-ogy from unrestricted news text and use this ontology to improve performance on a question answering task.
This paper proposes an empirical approach to the development of a computational model for assessing texts according to cohesiveness.
In this paper, we discuss inter-dialect MT in general and Cantonese-Mandarin MT in particular.
Synchronous Context-Free Grammars (SCFGs) have been successfully exploited as translation models in machine translation applications.
The paper describes an interface between generator and synthesizer of the German language concept-to-speech system VieCtoS.
It discusses phenomena in German intonation that depend on the interaction between grammatical dependencies (projection of information structure into syntax) and prosodic context (performance- related modifications to intonation patterns).Phonological processing in our system corn- prises segmental as well as suprasegmental dimensions such as syllabification, modification of word stress positions, and a symbolic encoding of intonation.
We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function.
Semantic relatedness is a special form of linguistic distance between words.
As a result of the corpus-based approach, test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts.
This paper presents Trace dr Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (UG) and ideas of Government dr Binding Theory (on).
in GPSO and HPSG.
This process is called transliteration.
Specifically focusing on sequential segmentation tasks, i.e.
text chunking and named entity recognition, we introduce a loss function that closely reflects the target evaluation measure for these tasks, namely, segmentation F-score.
The Valency Lexicon of Czech Verbs, Version 1.0 (VALLEX 1.0) is a collection of linguistically annotated data and documentation, resulting from an attempt at formal description of valency frames of Czech verbs.
The query translation approach is employed using the LDC bilingual wordlist.
In this paper we present a two-stage statistical word segmentation system for Chinese based on word bigram and word- formation models.
We present a general architecture for incremental interaction between modules in a speech-tointention continuous understanding dialogue system.
This paper presents a novel language-independent question/answering (Q/A) system based on natural language processing techniques, shallow query understanding, dynamic sliding window techniques, and statistical proximity distribution matching techniques.
Supervised learning methods for WSD yield better performance than unsupervised methods.
We develop compositionality and acceptability measures that draw on linguistic properties specific to LVCs, and demonstrate that these statistical, corpus-based measures correlate well with human judgments of each property.
This document shows how the factorized syntactic descriptions provided by Meta- Grammars coupled with factorization operators may be used to derive compact large coverage tree adjoining grammars.
Word Manager supports the definition, access and maintenance of lexical databases.
In this paper, we present three semantic grouping methods: similarity- based, verb-based and category-based grouping, and their implementation in the SLUI toolkit.
Computer games is an interesting application for spoken and multimodal dialogue systems.
the four axioms of the Dependency Grammar (DG).
hi this paper we describe how in NMG (Non-Monotonic Grammar), by monitoring a logic parser, a truth maintenance system can significantly.
Bayesian, decision tree and neural network classifiers) to discover language model errors.
We have examined 2 general types of features: model-based and language-specific features.
very large corpora of strings.
In this paper, we use a machine learning framework for semantic argument parsing, and apply it to the task of parsing arguments of eventive nominalizations in the FrameNet database.
We present a practically unsupervised learning method to produce single-snippet answers to definition questions in question answering systems that supplement Web search engines.
Unlike usual automatic text categorization systems, which rely on data- intensive models induced from large training data, our automatic text categorization tool applies data-independent classifiers: a vector-space engine and a pattern matcher are combined to improve ranking of Medical Subject Headings (MeSH).
The first part of the study focuses on the text to MeSH categorization task.
We investigate the impact of parse quality on a syntactically-informed statistical machine translation system applied to technical text.
Treating shallow parsing as part-of-speech tag-ging yields results comparable with other, more elaborate approaches.
A learning model is introduced based on the statistical analysis of the distribution of genitives?semantic relations on a large corpus.
This paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaus-tively parsing the Penn Treebank with the Treebank抯 own CFG grammar.
In this paper, we explore the effects of data fusion on First Story Detection [1] in a broadcast news domain.
Using the TDT1 evaluation methodology we evaluate a number of document representation strategies and propose reasons why our data fusion experiment shows performance improvements in the TDT domain.KeywordsLexical Chaining, Data Fusion, First Story Detection.
We examine three different types of sense clustering criteria with an Information Retrieval application in mind: methods based on the wordnet structure (such as generalization, cousins, sisters...); co-occurrence of senses obtained from Sem-cor; and equivalent translations of senses in other languages via the EuroWordNet InterLingual Index (ILI).
b) co-occurrence of senses in Semcor provide strong evidence for Information Retrieval clusters, un-like methods based on wordnet structure and systematic polysemy.
This pa-per describes our approach to detecting and re-cording adjectival meaning, compares it with the body of knowledge on adjectives in literature and presents a detailed, practically tested methodolo-gy for the acquisition of lexical entries for adjec-tives.
We describe our ongoing efforts at adaptive statistical language modeling.
We describe a succession of ME models, culminating in our current Maximum Likelihood / Maximum Entropy (ML/ME) model.
We report here empirical results of a series of studies aimed at automatically predicting information quality in news documents.
We present a cut and paste based text summa-rizer, which uses operations derived from an anal-ysis of human written abstracts.
'This paper deals with the training phase of a Markov-type linguistic model that is based on transition probabilities between pairs and triplets of syntactic categories.
Word category prediction is used to implement an accurate word recognition system.
To solve this problem, NETgram, which is the neural network for word category prediction, is proposed.
We investigate the problem of summarizing text documents that contain errors as a result of optical character recognition.
We conclude by proposing possible ways of improving the performance of noisy document summarization.
This paper deals with query translation issue in cross-language information retrieval, proper names in particular.
Models for name identification, name translation and name searching are presented.
IntroductionThis paper suggests a formulation of events and actions that seems powerful enough to define a wide range of event and action verbs in English.
He provides a classification of event verbs that includes verbs of change cgo verbs) and verbs that assert a state remaining constant over an interval of time (STAY verbs), and defines a representation of action verbs of both types•hy introducing the notion of agentive causality and permission.
We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host.
We present two techniques for combining evidence from dictionary-based and corpus-based translation lexicons, and show that backoff translation outperforms a technique based on merging lexicons.
We present a novel method for segmenting the input sentence into words and assigning parts of speech to the words.
It consists of a statistical language model and an efficient two-pass N-best search algorithm.
'this paper describes techniques to corn-pile lexical entries in HPSG (Pollard and Sag, 1987; Pollard and Sag, 1993)-style grammar into a set of finite state au-tomata.
In this paper we present an ambiguity preserving translation approach which transfers ambiguous LFG f-structure representations.
They contain information about linguistic and lexicographic properties of words, and word combinations.
The paper studies question classification through machine learning approaches, namely, different classifiers and multiple classifier combination method.
By using compositive statistic and rule classifiers, and by introducing dependency structure from Minipar and linguistic knowledge from Wordnet into question representation, the research shows high accuracy in question classification.
Multi-word terms are traditionally identified using statistical techniques or, more recently, using hybrid techniques combining statistics with shallow linguis-tic information.
GM 1)/I PSI [hate.
In order to find a unified approach for Chinese word segmentation, the author develop a Chinese lexical analyzer PCWS using direct maximum entropy model.
We propose a unified solution to detect unknown words in Chinese texts.
We present a thorough evaluation comprising supervised and unsupervised modes, and both lexical-sample and all-words tasks.
Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation.
First, we apply self-organized document maps for mod-eling the broader subject of dis-course based on the occurrence of content words in the dialogue con-text.
The approach is based on the idea of matching certain lexicosyntactic patterns conveying a certain semantic relation on the World Wide Web using standard search engines.
Probabilistic Latent Semantic Analysis (PLSA) models have been shown to provide a better model for capturing polysemy and synonymy than Latent Semantic Analysis (LSA).
In this paper we present a method for using LSA analysis to initialize a PLSA model.
This paper describes the NLMenu System, a menu-based natural language understanding system.
In this paper a new language processor is proposed, in which unification grammar and Markov language model are integrated in a word lattice parsing algorithm based on an augmented chart, and the island-driven parsing concept is combined with various preference-first parsing strategies defined by different construction principles and decision rules.
In this paper, we study the impact of a group of features extracted automatically from machine-generated parse trees on coreference resolution.
We report initial results on the relatively novel task of automatic classification of author personality.
We explore both binary and multiple classification, using differing sets of n-gram features.
Spelling recognition is an approach to enhance a speech recognizer抯 ability to cope with incorrectly recognized words and out-of-vocabulary words.
In order to implement Thai spelling recognition, Thai alphabets and their spelling methods are analyzed.
We describe a technique for automatically constructing a taxonomy of word senses from a machine readable dictionary.
Previous taxonomies developed from dictionaries have two properties in common.
We show that hierarchies of this type can be automatically constructed, by using the semantic category codes and the subject codes of the Longman Dictionary of Contemporary English (LDOCE) to disambiguate the genus terms in noun definitions.
This paper proposes a unified evaluation method for multiple reading support systems such as a sentence translation system and a word translation system.
This paper describes our preliminary attempt to automatically recognize zero adnominals, a subgroup of zero pronouns, in Japanese discourse.
The paper presents new word-sense disambiguation and POS tagging experiments, and integrates apparently conflicting reports from other recent work.
This paper presents an unsupervised method for choosing the correct translation of a word in context.
We describe an advanced text processing system for information retrieval from natural language document collections.
Two statistical measures are computed: the measure of informational contribution of words in phrases, and the similarity measure between words.APPROXIMATE PARSING WITH TTPTTP (Tagged Text Parser) is a top down English parser specifically designed for fast, reliable processing of large amounts of text.
We examine the relationship between the two grammatical formalisms: Tree Adjoining Grammars and Head Grammars.
A speech recogniser, based on Hidden Markov Models in forced segmenta-tion mode is used to outline phone boundaries within spoken logatoms.
coreference in corpora for such systems.
We present Evita, an application for recognizing events in natural language texts.
憆ealization?
In this paper we present an efficient context-free (CF) bottom-up, non deterministic parser.
This work attempts to provide a robust Thai morphological analyzer which can automatically assign the correct part-of-speech tag to the correct word with time and space efficiency.
It consists of preference based pruning, syntactic based pruning and semantic based pruning.
We present two methods for unsupervised segmentation of words into morpheme-like units.
In the second method, Max-imum Likelihood (ML) optimization is used.
In this paper we explore the power of surface text patterns for open-domain question answering systems.
EVALING is an 'Item Banking'2 system: exercise database allowing dynamic design of questionnaires.
A modal temporal logic is developed for this purpose.
This paper presents a named entity classification system that utilises both orthographic and contextual information.
Supervised and unsupervised learning techniques used in the recombination of models to produce the final results.
A compositional account of the semantics of German prefix verbs in HPSG is outlined.
We consider only those verbs that are formed by productive synchronic rules.
We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser.
Statistical machine translation systems use a combination of one or more translation models and a language model.
Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system.
We present a word alignment procedure based on a syntactic dependency analysis of French/English parallel corpora called 揳lignment by syntactic propagation?
This paper regards Question Answering (QA) as Question-Biased Term Extraction (QBTE).
We have discovered a simple and effective filter, the Relevancy Signatures Algorithm, and demonstrated its performance in the domain of terrorist event descriptions.
The Relevancy Signatures Algorithm is based on the natural language processing technique of selective concept extraction, and relies on text representations that reflect predictable patterns of linguistic context.This paper describes text classification experiments conducted in the domain of terrorism using the MUC-3 text corpus.
Our algorithm automatically derives relevancy signatures from a training corpus using selective concept extraction techniques.
In this paper, we propose a method of gen-erating bilingual keyword clusters or thesauri from parallel or comparable bilingual corpora.
The method combines morphological and lex-ical processing, bilingual word alignment, and graph-theoretic cluster generation.
We study a number of natural language decipherment problems using unsupervised learning.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
The first is the derivation of precise probability scores for partial hypotheses containing islands, in the context of a Stochastic-Context-Free-Grammar (SCFG) for Language Modeling (LM).
We are interested in the computation of Pr(th) when th is a partial interpretation of a spoken sentence generated by a Stochastic Context-Free Grammar (SCFG) G,.
Interesting island-driven parsers have been proposed by [13], [12], [6] who have also discussed the motivations for considering these parsers for ASU.
In this paper we address the issue of automatically assigning information status to discourse entities.
Using an annotated corpus of conversational English and exploiting morpho-syntactic and lexical features, we train a decision tree to classify entities introduced by noun phrases as old, mediated, or new.
The present selection primarily focuses on speech and natural language systems for speech recognition and synthesis.
We describe the head transducer model used in an experimental English-toMandarin speech translation system.
Head transduction is a translation method in which weighted finite state transducers are associated with source- target word pairs.
In clas-sification tasks, we contrast the use of text and prosodic features.
Word fragments pose serious problems for speech recognizers.
building classifiers using acoustic-prosodic features.
Chinese Pinyin in our case.
To perform the task of dynamic chat language term normalization, we extend the source channel model by incorporating the phonetic mapping models.
ProAlign combines several different approaches in order to produce high quality word word alignments.
Like EM-based methods, a probability model is used to rank possible alignments.
We describe HAMSAH (HAifa Morphological System for Analyzing Hebrew), a morphological processor for Modern Hebrew, based on finite-state linguistically motivated rules and a broad coverage lexicon.
This paper presents a corpus-based approach for deriving heuristics to locate the antecedents of relative pronouns.
This paper presents an analysis of semantic association norms for German nouns.
We subsequently used the clustering to predict noun ambiguity and to discriminate senses in our target nouns.
This article presents a new semantic-based transfer approach developed and applied within the Verbmobil Machine Translation project.
of natural language texts (Bod, 1993c).
This paper deals with automatic classification of Arabic web documents.
plus petits.
ex: eireuter.2- le verbe syntaxique entre dans tine construction syntaxique.
Section 1 describes the creation of the WFDH Web-based Frequency Dictionary of Hungarian from the raw corpus.
A set of statistical measures are used to identify significant word units in both samples.
Identification of single word terms is based on the notion of word intervals.
Two-word terms are identified through the computation of mutual information, and the extension of mutual information assists in capturing multi-word terms.
One way to address the problems of the grammar-based ap-proach is to compile recognition gram-mars from grammars written in a more expressive formalism.
We combine the semantic information provided by different resources and extract new semantic knowledge to improve the performance of a recognizing textual entailment system.
Associated or related terms.
and complex content identifiers derived from thesauruses and knowledge bases, or constructed by automatic word grouping techniques.
have therefore been proposed for text identification purposes.The area of associative content analysis and information retrieval is reviewed in this study.
MLR, an extended LA parser, is introduced, and its application to natural language parsing is discussed.
An LA parser is a shift-reduce parser which is deterministically guided by a parsing table.
A parsing table can be obtained automatically from a context- free phrase structure grammar.
Our method also provides an elegant solution to the problem of multi-part-of -speech words such as "that".
This position paper looks critically at a number of aspects of current research into spoken language translation (SLT) in the medical domain.
We focus briefly on the issue of feedback in SLT systems, pointing out the difficulties of relying on text-based paraphrases.
Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.
This paper presents an answer selection method based on Support Vector Machines (SVM) for Open-Domain Question Answering (QA).
We evaluate the performance measured by mean reciprocal rank (MRR) and the correct ratio of answer ranked first.
Using word shape tokens composed of these charactershape codes, a properly trained text tagger can extract part-of-speech information from scanned document images.
This paper introduces a bilingual MRD (English-Chinese LDOCE) to help with the construction of a Chinese WordNet.
The module for mutations is based on syntax patterns and the one for protein-protein interactions on NLP.
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar.
Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.
Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer.
Chinese word segmentation and Part-ofSpeech (POS) tagging have been commonly considered as two separated tasks.
In this paper, we present a system that performs Chinese word segmentation and POS tagging simultaneously.
We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features.
We propose an approximated joint decoding method by reranking the N-best segmenter output, based POS tagging information.
The system is based on lexicons and rules from an earlier KIMMO-style two-level morphological system, reworked exten-sively using Xerox Finite-State Morphol-ogy tools.
We implement, a num-ber of both bag-of-words and word order-sensitive similarity metrics, and test each (wet' character-based and word-based indexing.
Our results indicate that char aster-based indexing is consistently.
A methodology is presented for component-based machine translation (MT) evaluation through causal error analysis to complement existing global evaluation methods.
We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners.
This paper proposes a statistical model for finding domain specific words (DSW秙) in particular domains, and thus building the association among them.
This paper proposes a mistake-driven mixture method for learning a tag model.
To well reflect the data distribution, we represent each tag model as a hierarchical tag (i.e.,NTT1 < proper noun < noun) context tree.
We evaluate several tag models by implementing Japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models.
We present a method for improving dependency structure analysis of Chi-nese.
Support Vector Ma-chines (SVMs) are utilized to deter-mine the word dependency relations.
We utilize the global features to solve this.
We supply the top-down information by constructing SVMs based root node finder to solve this problem.
This paper shows how DATR, a widely used formal language for lexical knowledge representation, can be used to define an LTAG lexicon as an inheritance hierarchy with internal lexical rules.
A bottom-up featural encoding is used for LTAG trees and this allows lexical rules to be implemented as covariation constraints within feature structures.
We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.
In this paper, we argue that TAG naturally supports the integration of three main ways of reducing complexity: polarity filtering, delayed adjunction and empty semantic items elimination.
We describe an architecture for spoken dialogue interfaces to semi-autonomous systems that trans-forms speech signals through successive representa-tions of linguistic, dialogue, and domain knowledge.
This paper describes a chunk-based parser/semantic analyzer used by a language learning model.
We present the first "sizable grammar written for TAG.
This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input.
This paper describes a multilingual data extraction system under development for the Department of Defense (Dot)).
This paper presents a method for identifying token instances of verb particle constructions (VPCs) automatically, based on the output of the RASP parser.
The proposed method pools together instances of VPCs and verb-PPs from the parser output and uses the sentential context of each such instance to differentiate VPCs from verb-PPs.
We also discuss the expansion of group common nouns and group proper nouns to enhance retrieval recall.
Successful proper noun boundary identification within the part of speech tagger is essential for successful categorization.
The proper noun classification module is designed to assign a category code to each proper noun entity, using 30 categories generated from corpus analysis.
Standardization of variant proper nouns occurs at three levels of processing.
The motivation behind the approach is to expand existing methods for content based information retrieval.
in content based information retrieval.
In this paper we discuss the formal relationship between the classes of languages generated by Tree Adjoining Grammars and Head Grammars.
NeATS is a multi-document summarization system that attempts to extract relevant or interesting portions from a set of documents about some topic and present them in coherent order.
We propose models for semantic orientations of phrases as well as classification methods based on the models.
This paper describes a first attempt at a sta-tistical model for simultaneous syntactic pars-ing and generalized word-sense disambigua-tion.
Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.
We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models.
To exploit this contextual feature, we propose the technique of temporalfeature modification, which takes various sources of lexical change into account, including changes in term frequency, associative strength between terms and categories, and dynamic categorization systems.
This paper presents an unsupervised approach to lexical acquisition with the goodness measure description length gain (DLG) formulated following classic information theory within the minimum description length (MDL) paradigm.
This paper describes an approach to automatic text processing which is entirely basedon syntactic form.
We present some lessons we have learned from using software infrastructure to support coursework in natural language dialogue and embodied conversational agents.
In the project "Procedural Dialogue Models" being carried on at the University of Bielefeld we have developed an incremental multilevel parsing formalism to reconstruct task-oriented dialogues.
A new technique to locate content-represent-ing words for a given document image using abstract representation of character shapes is described.
Since most previous works for 11MM-based tagging consider only part-of-speech infOrmation in contexts, their models cannot utilize lexical information which is crucial for resolving some morphological ambiguity.
The lexicalized models use a simplified back-off smoothing technique to overcome data sparseness.
We also introduce a simple trick to transfer information about distant entities by embedding label information into non-entity labels.
This paper shows that default-based phonologies have the potential to capture morphophonological generalisations which cannot be captured by non-default theories.
We define two main features — possessor and ref-set and discuss how the grammar handles complex syntactic co-occurrence phenomena based on this input.
-We will discuss the-semi-recursive algorithm for text generation, as defined for the GTAG formalism, and its implementation in the CLEF project.
We will show how to use lexical choice constraints and properties of the LTAG grammar to minimize the backtracking of the semi-recursive algorithm.
We describe a new sentence realization framework for text-to-text applications.
This framework uses IDL-expressions as a representation formalism, and a generation mechanism based on algorithms for intersecting IDL-expressions with probabilistic language models.
We describe a multimodal application architecture which combines finite-state multimodal language processing, a speech-act based multimodal dialogue manager, dynamic multimodal output generation, and user-tailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output.
This paper describes an approach to translate rarely occurring named entities (NE) by combining phonetic and semantic similarities.
The phonetic similarity is estimated from a surface string transliteration model, and the semantic similarity is calculated from a context vector semantic model.
This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT).
The DP-based decoding algorithms are analyzed in terms of shortest path-finding algorithms, where the similarity to DP-based decoding algorithms in speech recognition is demonstrated.
Prepositional Phrase is the key issue in structural ambiguity.
Four different attach-ments are told out according to their functionality: noun attachment, verb attachment, sentence-level attachment, and predicate-level attachment.
Although single-document summarization is a well-studied task, the nature of multi- document summarization is only beginning to be studied in detail.
Are multi-document summaries less extractive than single- document summaries?
Ad-ditional consistency knowledge is discovered by a meta-bootstrapping algorithm applied to unlabeled texts.
This paper presents a decision-tree approach to the problems of part-ofspeech disambiguation and unknown word guessing as they appear in Modem Greek, a highly inflectional language.
We incorpo-rated information on participants' social roles and genders into transfer rules and dictionary entries.
In this paper, we propose a sentence ordering algorithm using a semi-supervised sentence classification and historical ordering strategy.
This paper describes a method for com-piling a constraint-based grammar into a potentially more efficient form for pro-cessing.
Various informations can be used to align parallel texts at word level: co-occurrence frequencies, position difference, part-of-speech, graphic resemblance, etc.
This paper introduces a method that generates simulated multimodal input to be used in testing multimodal system implementations, as well as to build statistically motivated multi- modal integration modules.
We present a statistical phrase-based translation model that uses hierarchical phrases?phrases that contain subphrases.
In this paper I present ongoing work on the data-oriented parsing (DOP) model.
We give a novel method for parsing these words by estimating the probabilities of unknown subtrees.
This paper presents a Bayesian model for unsupervised lixtruhnz, of verb selectional preferences.
We propose a framework to derive the distance between concepts from distributional measures of word co-occurrences.
The on demand statistical machine translation uses the Direct Translation model along with a novel statistical Arabic Morphological Analyzer to yield state-of-the-art translation quality.
This paper discusses ensembles of simple but het-erogeneous classifiers for word-sense disambigua-tion, examining the Stanford-CS224N system en-tered in the SENSEVAL-2 English lexical sample task.
First-order classifiers are combined by a second-order classifier, which variously uses ma-jority voting, weighted voting, or a maximum en-tropy model.
This paper presents the construction of a manually annotated Chinese shallow Treebank, named PolyU Treebank.
Phrase-based Grammar, proposed by Peking University, is used to guide the design and implementation of the PolyU Treebank.
The design issues, including corpus material preparation, standard for word segmentation and POS tagging, and the guideline for phrase bracketing and annotation, are presented in this paper.
We present a novel classifier-based deterministic parser for Chinese constituency parsing.
Our parser computes parse trees from bottom up in one pass, and uses classifiers to make shift-reduce decisions.
Semantic relations between text concepts denote the core elements of lexical semantics.
Our approach first identifies the syntactic patterns that encode intentions, then we select syntactic and semantic features for a SVM learning classifier.
In conclusion, we discuss the application of INTENTION relations to Q&A.
This paper proposes a statistical approach to Pinyin-based Chinese input.
This approach uses a trigram-based language model and a statistically based segmentation.
Also, to deal with real input, it also includes a typing model which enables spelling correction in sentence-based Pinyin input, and a spelling model for English which enables modeless Pinyin input.
For example, topic-comment structures, the ba-constructions, the bei-constructions, relative clause constructions, appositive clause constructions, and serial verb constructions.
The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy.
This paper discusses a system of translation of locative prepositions between English and French.
This paper introduces knowledge representations of conceptualizations of objects, and a method for translating prepositions based on these conceptual representations.
In this paper, we extend an existing statistical parsing model to produce richer output parse trees, annotated with PropBank semantic role labels.
This paper presents a unified theory of verbal irony for developing a computa-tional model of irony.
We then describe and evaluate a combined alignment and classification algorithm, which achieves an F-score on alignment of .85 (using EuroWordNet) and an F-score of .80 on semantic relation classification.
database tuples).
Computational models in the form of finite state automata (FSA) also illustrate the describable regularity of German and Mandarin Chinese speech repairs in a formal way.
This paper proposes a word spacing model using a hidden Markov model (HMM) for refining Ko-rean raw text corpora.
We consider word spacing problem as a classification problem such as Part-of-Speech (POS) tagging and have experimented with var-ious models considering extended context.
An algorithm, the Forward-Backward Word-Life Algorithm, is described.
This paper presents Exills, a true e- learning solution which integrates natural language processing tools and virtual reality1.
The software module GRAPHON (GRAPHeme-PHONeme-conversion) has been developed to convert any given German text into it phonetic transcription (I.P.A.
CommandTalk combines a number of separate components integrated through the use of the Open Agent Ar-chitecture, including the Nuance speech recognition system, the Gemini natural-language parsing and interpretation sys-tem, a contextual-interpretation module, a "push-to-talk" agent, the ModSAF battle-field simulator, and "Start-It" (a graph-ical processing-spawning agent).
This paper presents a probabilistic formalization of analogical matching, and describes how this model is applied to speech translation in the framework of translation by analogy.Figure 1: Example-based Translation Architecture
In this study, the idea of ECOC is applied to memory-based language learning with local (k-nearest neighbor) classifiers.
This paper evaluates the accuracy of HPSG parsing in terms of the identification of predicate-argument relations.
We could directly compare the output of HPSG parsing with Prop- Bank annotations, by assuming a unique mapping from HPSG semantic representation into PropBank annotation.
We report on a series of experiments with probabilistic context-free grammars predicting English and German syllable structure.
The analysis of one of our phonotactic grammars shows that interesting phonotactic constraints are learned.
The primary technical challanges relating to spoken dialogue systems that arise in this project are speech recognition in noise, open-microphone, and recording voice annotations.
'Fins paper describes a data-driven method for hierarchical clustering of words in which a large vocabulary of Nii-glish words is clustered bottom--up, with respect to corpora ranging in size from 5 to 50 million words, using a greedy al-gorithm that tries to riiinimize average loss of mutual information of adjacent classes.
This paper compares a number of generative probability models for a wide- coverage Combinatory Categorial Grammar (CCG) parser.
They are probability, rank, and entropy.
This paper explores the problem of identifying sen-tence boundaries in the transcriptions produced by automatic speech recognition systems.
This paper describes two algorithms which construct two different types of generators for lexical functional grammars (LFGs).
The first type generates sentences from functional structures and the second from semantic structures.
The latter works on the basis of extended LFGs, which contain a mapping from f-structures into semantic structures.
In the paper we propose a black-box method for comparing the lexical coverage of MT systems.
We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.
We describe a quantitative analysis of the information gain and the information redundancy for various combinations of feature types inspired by both dependency structure and bigram structure, using a Chinese treebank and taking word prediction as the object.
From this representation, a new text is generated, using a text model and action rules.This process is done in six steps : word analysis, sentence analysis using a Functional Grammar, reference solving and inference, construction of the text pattern, sentence generation, and word generation.
It consists of lexical data, a Functional Grammar, a knowledge network, action rules for reference solving and sentence generation, models of text, rules of structuration, and sentence schema.Text representation, included in the semantic network, is composed of different kinds of objects (not necessarily distinct) : text organization, syntactical information, objects introduced by the discourse, affirmations on these objects, and links between these affirmations.
Human face-to-face conversation is an ideal model for human-computer dialogue.
We have developed an experimental system that integrates speech dialogue and facial animation, to investigate the effect of introducing communicative facial expressions as a new modality in human-computer conversation.
This paper deals with contextual aspects of locative preposition processing.
Semantic role labeling (SRL) methods typically use features from syntactic parse trees.
We propose a novel method that uses Lexicalized Tree-Adjoining Grammar (LTAG) based features for this task.
We convert parse trees into LTAG derivation trees where the semantic roles are treated as hidden information learned by supervised learning on annotated data derived from PropBank.
We extracted various features from the LTAG derivation trees and trained a discriminative decision list model to predict semantic roles.
In this paper we propose to define selectional preference and semantic similarity as information-theoretic relationships involving conceptual classes, and we demonstrate the applicability of these definitions to the resolution of syntactic ambiguity.
We model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees.
We present new results on the relation between context-free parsing strategies and their probabilistic counter-parts.
This paper presents a detailed account of prepositional mismatch between our handcrafted verb lexicon and a semantically annotated corpus.
In this paper we report about an implemented system for supporting authoring claims for patents describing apparatuses.
The basic conceptual units of our system are Phonemes-in-Context (PICs), which are represented as Hidden Matkov Models, each of which is expressed as a sequence of Phonetic Elements (PELs).
We propose a collaborative framework for collecting Thai unknown words found on Web pages over the Internet.
Our main goal is to design and construct a Web- based system which allows a group of interested users to participate in constructing a Thai unknown-word open dictionary.
Our framework includes word segmentation and morphological analysis modules for handling the non-segmenting characteristic of Thai written language.
To take advantage of large available text resource on the Web, our unknown-word boundary identification approach is based on the statistical string pattern-matching algorithm.Keywords: Unknown words, open dictionary, word segmentation, morphological analysis, word-boundary detection.
Bilingual word alignment forms the foundation of most approaches to statistical machine translation.
Current word alignment methods are predominantly based on generative models.
It then describes a hybrid extraction system based on a multilingual parser.
This paper investigates the usefulness of prosodic features in classifying rhetorical relations between utterances in meeting recordings.
We explore the use of speculative language in MEDLINE abstracts.
In this paper, we attempt to capture the regularity underlying these classes of paraphrases, focusing on the paraphrasing of Japanese light-verb constructions (LVCs).
We propose a paraphrasing model for LVCs that is based on transforming the Lexical Conceptual Structures (LCSs) of verbal elements.
We also propose a refinement of an existing LCS dictionary.
Experimental results show that our LCS-based paraphrasing model characterizes some of the semantic features of those verbs required for generating paraphrases, such as the direction of an action and the relationship between arguments and surface cases.
We are developing stylistic grammars to provide the basis for a French and English stylistic parser.
Our stylistic grammar is a branching stratificational model, built upon a foundation dealing with lexical, syntactic, and semantic stylistic realizations.
Overall, we are implementing a computational schema of stylistics in French-to-English translation.
Most statistical parsers have used the grammar induction approach, in which a stochastic grammar is induced from a treebank.
Such controllers may be stochastic; here, we focus on greedy controllers, which result in deterministic parsers.
We also describe Markov parsing models, a general framework for parser modeling and control, of which the parsers reported here are a special case.
In this pa-per we study the problem of term selection and the performance of various features for unsuper-vised text classification.
The features studied are: principal components, independent com-ponents, and non-negative components.
Lasso is a regularization method for parameter estimation in linear models.
This paper explores the use of lasso for statistical language modeling for text input.
Therefore, we investigate two approximation methods, the boosted lasso (BLasso) and the forward stagewise linear regression (FSLR).
It integrates syntactic, semantic and contextual processing serially.
The syntactic analyzer obtains rough syntactic structures from the text.
Then, the contextual analyzer obtains contextual information from the semantic structure extracted by the semantic analyzer.
Our system uses a context-free grammar parser named Extended-Lingol as a syntactic analyzer to analyze the Japanese sentences and produce parsing trees.
We propose a method for identifying diathesis alter-nations where a particular argument type is seen in slots which have different grammatical roles in the alternating forms.
The method uses selectional pref-erences acquired as probability distributions over WordNet.
Repair processing plays an important role in spoken language processing systems.
This paper proposes a method for correcting Chinese repetition repairs and demonstrates the effects of repair processing in Chinese homophone disambiguation.
We induce categorial type assignments from a dependency treebank (Torino University treebank, TUT) and use the obtained categories with annotated dependency relations to study the distributional behavior of Italian words and reach an empirically founded part-of-speech classification.
We developed a client-server speech translation system with mobile wireless clients.
We define state transition grammars (STG) as an intermediate formalism between grammars and parsing algorithms which is intended to separate the description of a parsing strategy from the grammar formalism.
Various grammar formalisms are characterized in terms of properties of STG's.
We define an Earley parsing schema for STG's and characterize the valid parse items.
We also discuss the usabil-ity of STG's for head-corner parsing and direct parsing of sets of tree constraints.
We propose projection models that exploit lexical and syntactic information.
This is a paper that describes computational linguistic activities on Philippines languages.
VOYAGER is a speech understanding system currently under development at MIT.
This paper describes the preliminary evaluation of VOYAGER, using a spontaneous speech database that was also recently collected.
We present a new approach to extracting keyphrases based on statistical language models.
We use typed feature structures for encoding linguistic knowledge.
We show the application of this representational device for the architecture of linguistic knowledge sources for multilingual generation.
As an example, we describe the use of interacting collocational and syntactic constraints in the generation of French and German sentences.
This paper presents a framework for unsupervised natural language morphology induction wherein candidate suffixes are grouped into candidate inflection classes, which are then arranged in a lattice structure.
In this paper, we propose a statistical dialogue analysis model based on speech acts for Korean-English dialogue machine translation.
The model uses syntactic patterns and N-grams reflecting the hierarchical discourse structures of dialogues.
The syntactic pattern includes the syntactic features that are related with the language dependent expressions of speech acts.
The N-gram of speech acts based on hierarchical recency approximates the context.
We present a document compression system that uses a hierarchical noisy-channel model of text production.
Our results support the claim that discourse knowledge plays an important role in document summarization.
In this paper we first propose a new statistical parsing model, which is a generative model of lexicalised context-free grammar.
We then extend the model to include a probabilistic treatment of both sub- categorisation and wh-movement.
We discuss some consequence relations in DRT useful to discourse semantics.
We incorporate some consequence relations into DRT using se- writ calculi.
One is how to ensure the quality of word segmentation and Part-of-Speech (POS) tagging, because its consequence has an adverse impact on the performance of NE recognition.
We discuss the potential of KPCA for leveraging unannotated data for partially-unsupervised training to address these issues, leading to a composite model that combines both the supervised and semi-supervised models.
We present Minimum Bayes-Risk word alignment for machine translation.
This paper represents one of the first steps towards an XDG-based integrated generation architecture by tackling what is arguably the most basic among generation tasks: lexicalization.
Herein we present a constraint-based account of disjunction in lexicalization, i.e.
We describe new algorithms for train-ing tagging models, as an alternative to maximum-entropy models or condi-tional random fields (CRFs).
We give exper-imental results on part-of-speech tag-ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
MONA is an automata toolkit providing a compiler for compiling formulae of monadic second order logic on strings or trees into string automata or tree automata.
The paper' presents a lightweight knowledge-based reasoning framework for the JAVELIN open-domain Question Answering (QA) system.
We propose a constrained representation of text meaning, along with a flexible unification strategy that matches questions with retrieved passages based on semantic similarities and weighted relations between words.
The semantics of Montague's The proper treatment of quan-tification in ordinary English (PTO) uses an intensional modelto evaluate formulas.
Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers.
Distributional similarity requires large volumes of data to accurately represent infrequent words.
In this paper, we explore statistical language modelling for a speech-enabled MP3 player application by generating a corpus from the interpretation grammar written for the application with the Grammatical Framework (GF) (Ranta, 2004).
We present an effective training algorithm for linearly-scored dependency parsers that implements online large- margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).
Our work addresses the integration of speech recognition and language processing for whole spoken dialogue systems.To filter ill-recognized words, we design an on-line computing of word confidence scores based on the recognizer output hypothesis.
To infer as much information as possible from the retained sequence of words, we propose a bottom-up syntacticosemantic robust parsing relying on a lexicalized tree grammar and on integrated repairing strategies.
We propose a new method to resolve ambiguity in translation and meaning interpretation using linguistic statistics extracted from dual corpora of source and target languages in addition to the logical restrictions described on dictionary and grammar rules for ambiguity resolution.
This paper presents an implemented, psychologically plausible parsing model for Government Binding theory grammars.
We treat nouns that behave adjectively, which we call adjectival nouns, extracted from large corpora.
We investigate how adjectival nouns are similar to adjectives and different from non-adjectival nouns by using self-organizing semantic maps.
We create five kinds of semantic maps, i.e., semantic maps of abstract nouns organized via (1) adjectives, (2) adjectival nouns, (3) non-adjectival nouns and (4) adjectival and adjectival nouns and a semantic map of adjectives, adjectival nouns and non-adjectival nouns organized via collocated abstract nouns, and compare them with each other to find similarities and differences.
In this paper, we propose a new stochastic language model that integrates local and global constraints effectively and describe a speech recognition system based on it.
We describe a highly interactive system for bidirectional, broad-coverage spoken language communication in the healthcare area.
Sentence simi-larity is determined using both surface-oriented measures and semantic groups obtained from merging the output templates of an information extraction subsystem.
We present the first stage of the discursive annotation of a corpus in Spanish.
In this paper, we describe a system to rank sus-pected answers to natural language questions.
This paper describes a media-independent, compositional, plan-based approach to representing attributive descriptions for use in integrated text and graphics generation.
This paper shows how to formally characterize language learning in a finite parameter space as a Markov structure.
We choose as a starting point the GW Triggering Learning Algorithm (TLA).
We investigate a statistical sentence generation method which recombines words to form new sentences.
The environment includes automatic generation of Grammatical Framework (GF) grammars for robust interpretation of spontaneous speech, and uses application databases to generate lexical entries and grammar rules.
The GF grammar is compiled to an ATK or Nuance language model for speech recognition.
Log-linear models provide a statistically sound framework for Stochastic "Unification-Based" Grammars (SUBGs) and stochastic versions of other kinds of grammars.
We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical- Functional Grammar.
We examine one coarse category of named entities, persons, and describe a method for automatically classifying person instances into eight finer- grained subcategories.
We present a supervised learning method that considers the local context surrounding the entity as well as more global semantic information derived from topic signatures and WordNet.
The system also provides a framework for integrating spoken language information, namely intonation and prosody, into the speech act interpretation process.
We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts.
The prevailing dual-route model of oral read-ing claims that a lexical route is used for the pronunciation of words and a non-lexical route processes nonwords.
A variation of Belief Net named as Collocation Map is used to compute the probabilities.
The Belief Net captures the conditional independences of words, which is obtained from the cooccurrence relations.
Our learning algorithms build a construction grammar language model, and generalize using form-based patterns and the learners conceptual ystem.
Model-theoretic semantics provides acomputationally attractive means of representing the semantics of natural language.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
This paper addresses the problem of large-scale acquisition of computational-semantic lexicons from machine-readable resources.
We report on our results of disambiguating the verbs in the semantic filters by adding WordNeti sense annotations.
We investigate the suitability of sub-categorization acquisition for evalu-ation of word sense disambiguation (WSD) systems.
We modify an exist-ing subcategorization acquisition sys-tem to enable it to benefit from WSD.
We present a small scale experiment with manually sense annotated data which shows that accurate WSD in-deed does improve the accuracy of the acquired subcategorization frames (SCFs).
We also applied the lexicon to the lexical choice and realization component of a practical generation application by using a multi-level feedback architecture.
The paper presents a new way of accounting for the meaning of verbs in natural languages, using a diagrammatic notation based on the Unified Modeling Language (UML).
We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.
We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering.
A new natural language system, TINA, has been developed for applications involving speech understanding tasks, which integrates key ideas from context free grammars, Augmented Transition Networks (ATN's) [1], and Lexical Functional Grammars (LFG's) [2].
This paper describes an automated system for assigning quality scores to recorded call center conversations.
The system combines speech recognition, pattern matching, and maximum entropy classification to rank calls according to their measured quality.
We also describe and compare the effectiveness of three complementary methods of signal processing for robust speech recognition: acoustical pre-processing, microphone array processing, and the use of physiologically- motivated models of peripheral signal processing.
We have built an MEMM-based tagger using this training data set and evaluated the performance of acronym generation.
9000 Japanese scientific papers.
We propose the use of Lexicalized Tree Adjoining Grammar (LTAG) as a source of features that are useful for reranking the output of a statistical parser.
This paper presents Delphi, the natural language component of the BBN Spoken Language System.
Analysis components include an agenda-based best-first parser and a fall-back component for partial understanding that works by fragment combination.
We present our approach to this back-transliteration problem based on processes such as bilingual geographic name lookup, name suggestion using place name character and pair frequencies, and confirmation via a collection of monolingual names or the WWW.
This paper describes the AT&T ATIS data collection system, with emphasis on the development of the speech-in, speech- out interaction paradigm.
This paper proposes a method for dealing with repairs in action control dialogue to resolve participants?misunderstanding.
We extend Traum抯 grounding act model by introducing degree of groundedness, and partial and mid-discourse unit grounding.
This paper has proposed a special method that produces text summary by detecting thematic areas in Chinese document.
In addition, a novel parameter, which is known as representation entropy, is used for summarization redundancy evaluation.
This paper provides a cognitive basis for anaphora resolution and focusing.
In this article we outline a basic approach to treating metonymy properly in a multilingual machine translation system.
(4) The analysis and generation components treat metonymy differently using the patterns.
Pattern-Based Machine Translation is one of the machine translation methods which performs syntactic analysis and structure transfer at the same time using bilingual pat-terns.
Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have non- compositional meanings.
In order to reduce the parameter space, we pre-processed the training corpus using a word lemmatizer and a bilingual term extraction algorithm.
This paper describes the Mandarin-English Information (MEI) project, where we investigated the problem of cross-language spoken document retrieval (CL-SDR), and developed one of the first English-Chinese CL-SDR systems.
The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase- based translation with word-by-word translation.
Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval.KeywordsCross-language, spoken document retrieval, English-Chinese
Combining a naive Bayes classifier with the EM algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction.
We present a new framework for rapid development of mixed-initiative dialog systems.
Esfinge is a general domain Portuguese question answering system.
Distinctions are established between grammatical aspect, aspectual class and the aspectual perspective of a sentence in discourse; it is shown that in English, grammatical aspect under-determines the aspectual perspective.NARRATIVESThis paper investigates the varieties of temporal knowledge and temporal reasoning that are at work in understanding extended narratives.
In this paper, we present a semantic role labeler (or chunker) that groups syntactic chunks (i.e.
This amounts to tagging syntactic chunks with semantic labels using the IOB representation.
The chunker is realized using support vector machines as oneversus-all classifiers.
In this paper, we describe MITRE抯 contribution to the logical form generation track of Senseval-3.
In this paper, we examine semantic differences in cases of paraphrase and subsumption, in an effort to understand what makes one sentence significantly more informative than another.
These properties of variant transduction arise from combining techniques for paraphrase generation, classification, and example- matching.
Another factor in selecting which approach to use in a particular situation is whether there is sufficient uncertainty to warrant the need to make educated guesses (statistical approach) rather than assertions (symbolic approach).In our work in gisting, word spotting, and topic classification, we have successfully integrated symbolic and statistical approaches in a range of tasks, including language modeling for speech recognition, information extraction from speech, and topic and event spotting.
In this paper we report on a set of compu-tational tools with (n)SGML pipeline data flow for uncovering internal structure in natural language texts.
We inves-tigate different tree-based stochas-tic models for lexical choice.
In light of these benchmarks, we then consider three possible transformations to dialogue protocols, formulated within an issue-based approach to dialogue management.
We propose a context-sensitive method to predict noun- phrases in the next utterance of a telephone inquiry dialogue.
This paper examines some of the processes creating sets of polite expressions, deictiet expressions, and compound ions phrases, which are common in telephone inquiry dialogue.
This paper presents a novel statistical model for automatic identification of English baseNP.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
Our model also integrates lexical information.
This paper discusses an approach to augmenting a lexicon for knowledge-based machine translation (KBMT) with information derived from WordNet.
Several heuristics are used to find the WordNet synonym sets corresponding to the concepts in the Mikrokosrubs language-independent ontology.
The result is a lexicon acquisition tool that produces plausible lexical mappings from English words into the Milcrokosmos ontology.
We are trying to extend the boundary of Information Extraction (IE) systems.
This paper describes efforts underway to construct a large- scale ontology to support semantic processing in the PAN- GLOSS knowledge-base machine translation system.
We therefore use phonetic recognition of utterances and search for salient phonetic sequences within the decodings.
This paper proposes a novel, corpus- based, method for producing mappings between lexical resources.
lir this paper, we propose an al-gorithm for a semi-automatic extraction of nested uninterrupted and interrupted collocations, paying particular attention to nested collocation.
Phonetic baseforms are the basic recognition units in most large vocabulary speech recognition systems.
This paper describes a series of experiments in which the phonetic baseform is deduced automatically for new words by utilizing actual utterances of the new word in conjunction with a set of automatically derived spelling-tosound rules.
We present experiments aiming at an automatic classification of Spanish verbs into lexical semantic classes.
We present a learning framework for structured support vector models in which boosting and bagging methods are used to construct ensemble models.
In this paper, we present empirical data from a corpus study on the linear order of subjects and objects in German main clauses.
Our findings suggest an influence of grammatical functions on the ordering of verb complements.
This paper describes a system in PROLOG for the automatic transformation of a grammar, written in LFG formalism, into a DCG-based parser.
This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
In this paper, we describe an efficient A* search algorithm for statistical machine translation.
This paper presents a comprehensive NLP sys-tem by Melingo that has been recently developed for Arabic, based on MorfixTm ?an operational formerly developed highly successful comprehen-sive Hebrew NLP system.The system discussed includes modules for morphological analysis, context sensitive lemmati-zation, vocalization, text-to-phoneme conversion, and syntactic-analysis-based prosody (intonation) model.
This paper shows that in the context of statistical weblog classification for splog filtering based on n-grams of tokens in the URL, further segmenting the URLs beyond the standard punctuation is helpful.
This paper examines the phenomenon of consonant spreading in Arabic sterns.
There is a mismatch between the distribution of information in text, and a variety of grammatical formalisms for describing it, including ngrams, context-free grammars, and dependency grammars Rather than adding probabilities to existing grammars, it is proposed to collect the distributions of flexibly sized partial trees.
In this paper we introduce MULTIVOC, a real-world text-to-speech product geared to the French language.Starting from a ordinary French text, MUL-TIVOC generates in real-time a high quality speech using a synthesis-by-diphone method.
Gene and protein named-entity recognition (NER) and normalization is often treated as a two-step process.
We have built a dictionary based gene and protein NER and normalization system that requires no supervised training and no human intervention to build the dictionaries from online genomics resources.
The paper describes the results of a compari-son of two annotation systems for intonation, the tone-based ToBI approach and the tune-based approach proposed by Systemic Func-tional Grammar (sFG).
The goal of this compar-ison is to define a mapping between the two sys-tems for the purpose of concept-to-speech gen-eration of English.
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.
Automatic acquisition of paraphrase knowledge for content words is proposed.
This interface is stated in terms of Extensible Dependency Grammar (XDG), a grammar formalism we newly specify.
This generalises the concept of under- specification.
This paper details a software architecture for discourse processing in spoken dialogue systems, where the three component tasks of discourse processing are (1) Dialogue Management, (2) Context Tracking, and (3) Pragmatic Adaptation.
A motivation of this work is reusable discourse processing software for integration with non-discourse modules in spoken dialogue systems.
As there are few annotated resources that can be used to develop a good P-Name extraction system, this paper presents a bootstrapping algorithm, called PN-Finder, to tackle this problem.
The algorithm uses a combination of P-Name and context word probabilities to identify new P-Names.
The system described here is a large-vocabulary continuous-speech recognition (CSR) system with results obtained using the Wall Street Journal-based database [15].
The recognizer uses a stack decoder-based search strategy[1, 7, 14] with a left-to-right stochastic language model.
In this paper, we introduce SEMTAG, a toolbox for TAG-based parsing and generation.
In this paper, a memory-based pars-ing method is extended for han-dling compositional structures.
An approximate word-matching algorithm for Chinese is presented.
Comparing the original sentence with the optimal string, spelling error detection and correction is realized simultaneously.
This paper describes the development of QuestionBank, a corpus of 4000 parse- annotated questions for (i) use in training parsers employed in QA, and (ii) evaluation of question parsing.
We introduce a new method for recovering empty nodes and their antecedents (capturing long distance dependencies) from parser output in CFG trees using LFG f-structure reentrancies.
In summary, QuestionBank provides a useful new resource in parser-based QA research.
This paper presents a novel training algorithm for a linearly-scored block sequence translation model.
We here investigate questions with implicatures related to biography facts in a web-based QA system, PowerBio.
We compare the performances of Decision Tree, Na飗e Bayes, SVM (Support Vector Machine), and ME (Maximum Entropy) classification methods.
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
passivisation.
subordinators and verbs).
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
GermaNet and FrameNet.
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
We name the algorithm minimummean-log-spectral-distance (MMLSD).
EBMT (Example-Based Machine Translation) is proposed.
Features include: morphological variants; WordNet synonyms and hypernyms; log-likelihood-based word pairings dy-namically obtained from baseline sen-tence alignments; and formal string features such as word-based edit dis-tance.
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
LFG f-structures or HPSG feature structures, to dependency triples simple.
at Grenoble.
the construction of domain- independent lexica.
possible structural and lexical attributes.
Processing stages include lexical lookup, syntactic parsing, semantic analysis, and pragmatic analysis.
groups of words).
LFG and PATR-II.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.
We describe experiments learning quasi-synchronous context-free grammars from bitext.
Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval.
CATEGORY assigns names in hierachies.
in a prepositional phrase.
Pragmatic.
We propose a lexical organisation for multilingual lexical databases (MLDB).
-- the problem of diacritics and digraphs.
problem oriented systems.
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
phonological rules or metrical systems).
distinctive features.
unscripted) speech.
block bigram features.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
Bilingual Corpus-based Analysis.
domain-independent, semantic information for question interpretation.
morphological derivation and synonymy expansion) in web search strategies.
information technology test reports and medical finding reports.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
WordNet) to ambiguous words occurring in a syntactic dependency.
We distinguish between context-free repre-sentability and context, free processing.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
PROLOG.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
word British National Corpus.
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.
1			Back梤eferencing in text		?.
handwriting).
base forms and POS tags.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
noun phrase (NP) syntax.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects.
These features include lexical, lexico grammatical and semantic phenomena.
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
idf based method.
Dutch subordinate clauses.
For topic identification, we will outline techniques based on stereotypical text structure, cue words, high-frequency indicator phrases, intratext connectivity, and discourse structure centrality.
trigger words and parsing structures.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
WORD- NET DOMAINS).
information technology test reports and medical finding reports.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
in GPSO and HPSG.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
Associated or related terms.
We present Minimum Bayes-Risk word alignment for machine translation.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
The strategy includes parsing and phrase prediction algorithms.
The paper presents the Constructive Dialogue Model as a new approach to formulate system goals in intelligent di-alogue systems.
This paper describes a. method for positioning un-known words in an existing thesaurus by using word-to-word relationships with relation (case) markers extracted from a large corpus.
We investigate prototype-driven learning for primarily unsupervised grammar induction.
This paper investigates the automatic identification of aspects of Information Structure (IS) in texts.
We automatically detect t(opic) and f(ocus), using node attributes from the treebank as basic features and derived features inspired by the annotation guidelines.
In this paper, we explain a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling.
Syllable-to-word (STW) conversion is important in Chinese phonetic input methods and speech recognition.
We define two concept-level CMs, which are on content-words and on semantic-attributes, using 10-best outputs of the speech recognizer and parsing with phrase-level grammars.
We present an approach to statistical machine translation that combines ideas from phrase-based SMT and traditional grammar-based MT.
This paper describes the Italian all-words sense disambiguation task for Senseval-3.
This paper describes a multi-lingual phrase-based Statistical Machine Translation system accessible by means of a Web page.
This paper describes SMES, an informa-tion extraction core system for real world German text processing.
This paper presents a method of resolv-ing ambiguity by using a variant of cir-cumscription, prioritized circumscrip-tion.
information retrieval, translation unit discovery, and corpus studies.
This paper describes how to construct a finite-state machine (FSM) approximating a 'unification-based' grammar using a left-corner grammar transform.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency.
This paper introduces the N-th order Ergodic Multigram TIMM for language modeling of such languages.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
In this paper we investigate probabilistic, contextual, and phonological factors that influence pitch accent placement in natural, conversational speech in a sequence labeling setting.
We present a learning-based method to identify single-snippet answers to definition questions in question answering systems for document collections.
Lastly, methods for bilingual terminology extraction and thesaurus enrichment are discussed.
We present a novel, data-driven method for integrated shallow and deep parsing.
This paper describes and evaluates a language-independent bootstrapping algorithm based on iterative learning and re-estimation of contextual and morphological patterns captured in hierarchically smoothed trie models.
This paper proposes an English adverb ordering method based on adverb gram-matical functions (subjuncts, adjuncts, dis-juncts and conjuncts) and meanings (pro-cess, space, time etc.
We present a named entity recognition and classification system that uses only probabilistic character-level features.
We use average mutual informaLion as global similarity metric to do classification.
The sys-tem is an application of maximum entropy clas-sification for question/answer type prediction and named entity marking.
This paper presents a word segmenta-tion system in France Telecom R&D Beijing, which uses a unified approach to word breaking and OOV identifica-tion.
We describe sign translation using example based machine translation technology.
This paper describes a comparative applica-tion of Grammar Learning by Partition Searchto four different learning tasks: deep parsing,NP identification, flat phrase chunking and NPchunking.
We compare the effect of lexical, temporal, syntactic, semantic, and prosodic features on system performance.
The paper describes a similarity-based model to present the morphological rules for Chinese com-pound nouns.
We present a discriminative, large- margin approach to feature-based matching for word alignment.
The CIAOSENSO WSD system is based on Conceptual Density, WordNet Domains and frequences of WordNet senses.
We analyzed eye gaze, head nods and attentional focus in the context of a direction-giving task.
It also facilitates dependency-based evaluation of phrase structure parsers.
This paper presents the user-centered interface of a summarization system for physicians in Bone Marrow Transplan-tation (BMT).
the complexity of language domain and concept inventory).
We introduce CST (cross-document structure theory), a paradigm for multi-document analysis.
A dynamicthreshold using time decay function and spanning window is proposed.
We present an approach for extracting relations from texts that exploits linguistic and empirical strategies, by means of a pipeline method involving a parser, partof-speech tagger, named entity recognition system, pattern-based classification and word sense disambiguation models, and resources such as ontology, knowledge base and lexical databases.
Systems to facilitate parsing and structural transfer.
In this paper we present Morphy, an integrated tool for German morphology, part-ofspeech tagging and context-sensitive lemmatization.
The paper describes an extensive experiment in inside-outside estimation of a lexicalized probabilistic context free grammar for German verb- final clauses.
Using k-NN, na飗e Bayes and centroidbased classifiers, the experimental results show that the multi-dimensional-based and hierarchical-based classification performs better than the flat-based classifications.
This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG pars-ing system.
First we map verb tokens in sentential contexts to a fixed set of seed verbs using WordNet :: Similarity and Moby抯 Thesaurus.
It uses n-gram mutual information, relative frequency count and parts of speech as the features for compound extraction.
which consist of rules and metarules.
relative clauses.
passivisation.
and questions.
The sentence-level discourse parser uses syntactic information and cue phrases to segment sentences into elementary discourse units and to generate discourse structures of sentences.
We present a method for corpus-based induction of an LFG syntax-semantics interface for frame semantic processing in a computational LFG parsing architecture.
We evaluate the results by applying the inducedframe assignment rules to LFG parser output.'
We use LFG抯 functional representations to distinguish local and non-local role assignments.
Each parser consists of a finite state control that drives the moves of a Bottom-up Embedded Pushdown Automaton.
The domain- specific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech (POS) tagging, named entity recognition, and named entity relation identitfication.
Our method of capturing ungrammaticalities involves using malrules (also called 'error productions/.
a tagset containing information about wordclasses.
The maximum entropy classifier is trained to identify and classify the predicates?semantic arguments together.
Lastly we show that patterns of information exchanges in speaker alternation and initiative-taking can be used to characterise three-party dialogues.
This paper describes a project tagging a spontaneous speech corpus with morphological information such as word segmentation and parts-ofspeech.
We propose a statistical dialogue analysis model to determine discourse structures as well as speech acts using maximum entropy model.
The design and implementation of an XML-based corpus environment for multilevel anno-tated multimodal (language) data is described.
subordinators and verbs).
We present an application of ambiguity packing and stochastic disambiguation techniques for Lexical-Functional Grammars (LFG) to the domain of sentence condensation.
Like most existing approaches it utilizes clustering of word co-occurrences.
In SPHINX-II, we incorporated additional dynamic and speaker-normalized features, replaced discrete models with sex-dependent semi-continuous hidden Markov models, augmented within-word triphones with between-word triphones, and extended generalized triphone models to shared- distribution models.
The configuration of SPHINX-II being used for this task includes sex-dependent, semi-continuous, shared- distribution hidden Markov models and left context dependent between-word triphones.
This paper studies the enrichment of Spanish WordNet with synset glosses automatically obtained from the English Word- Net glosses using a phrase-based Statistical Machine Translation system.
This paper presents a framework for the definition of monotonic repair rules on chart items and Lexicalized Tree Grammars.
We use a highly interactive bottom-up inductive logic programming (ILP) algorithm to learn `missing' grammar rules from an incomplete grammar.
The system uses core technologies such as speaker segmentation, automatic speech recognition, transcription alignment, keyword extraction and speech indexing and retrieval to make spoken communications easy to navigate.
A language-independent method of finite- state surface syntactic parsing and word-disambiguation is discussed.
The system learns lexico-structural transfer rules using syntactic pat-tern matching, statistical co-occurrence and error-driven filtering.
a noun in a noun phrase).
We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference.
mantics of Combinatory Categorial Grammar, including its handling of coordination constructs.
This paper proposes the automatic generation of Fill-in-the-Blank Questions (FBQs) together with testing based on Item Response Theory (IRT) to measure English proficiency.
GermaNet and FrameNet.
We discuss maximum a posteriori estimation of continuous density hidden Markov models (CDHMM).
We present an unsupervised system that exploits linguistic knowledge resources, namely English and German lexical databases and the World Wide Web, to identify English inclusions in German text.
We then apply our method to two complementary tasks: information ordering and extractive summarization.
We investigated the applicability of probabilistic context-free grammars to syllabification and grapheme-to-phoneme conversion.
the language used in official encyclopaedic articles.
We extracted meaningful bigrams using an evaluation function and extended the bigrams to n-gram collocations by generating equivalence sets, a-covers.
We view a modeling problem for n-gram collocations as that for clustering of cohesive words.
This paper reports results on automatically training a Problematic Dialogue Identifier to classify problematic human-computer dialogues using a corpus of 1242 DARPA Communicator dialogues in the travel planning domain.
directly stored relations.
A semantic net needs proper representation of lexical gaps.
In this paper we describe an approach to coherent sentence ordering for summarizing newspaper articles.
This article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding (SLU).
This paper describes improved HMM-based word level alignment models for statistical machine translation.
We investigate several voting- and arbiter- based combination strategies over a diverse pool of unsupervised WSD systems.
In this paper, we compare the rela-tive effects of segment order, segmen-tation and segment contiguity on the retrieval performance of a translation memory system.
This paper presents a method that as-sists in maintaining a rule-based named-entity recognition and classifi-cation system.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We introduce a character-based chunking for unknown word identification in Japanese text.
The method is built upon SVM-based chunking, by use of character n-gram and surrounding context of n-best word segmentation candidates from statistical morphological analysis as features.
The paper deals with generation of natural language text in a dialog system.
The model is based on the Factorial Hidden Markov Model (FHMM) with distributed hidden states representing partof-speech and noun phrase sequences.
We describe a method for interpreting abstract flat syntactic representations, LFG f- structures, as underspecified semantic representations, here Underspecified Discourse Representation Structures (UDRSs).
QACIAD (Question Answering Challenge for Information Access Dialogue) is an evaluation framework for measuring interactive question answering (QA) technologies.
We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs).
We present a machine learning approach to the novel task of identifying fragments and their antecedents in multiparty dialogue.
basic segmentation, named entity recognition, error-driven learner and new word detector.
We represent discourse structures as headed trees and model them with probabilistic head-driven parsing techniques.
In this paper, we investigate the acoustic prosodic mark-ing of demonstrative and personal pronouns in task-oriented dialog.
Generative-Transformational Grammars, General Phrase Structure Grammars, Lexical Functional Grammar.
We develop a Data-Oriented Parsing (DOP) model based on the syntactic representations of Lexical- Functional Grammar (LFG).
The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization, PoS, morphology, lexical, named entity, phrase chunk and (for German) topological sentence field analyses from shallow components.
MaxEnt includes external gazetteers in the system.
off-line) material in foreign languages.
Caseframe parsers employ both semantic and syntactic knowledge.
We describe a corpus-based investigation of proposals in dialogue.
The algorithm combines four original alignment models based on relative corpus frequency, con-textual similarity, weighted string simi-larity and incrementally retrained inflec-tional transduction probabilities.
audio recordings containing spoken text.
Collocational knowledge is necessary for language generation.
We developed automatic corpus-based K-TOBI labeling tools and prediction methods based on several lexicosyntactic linguistic features for decision-tree induction.
The type of local dependencies considered are sequences of part of speech categories for words.
We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank.
We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG.
We name the algorithm minimummean-log-spectral-distance (MMLSD).
We then describe a head-driven chart parser for lexicalized SFG.
These enhancements include function-phrase modeling, between-word coarticulation modeling, and corrective training.
The knowledge sources used were partof-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
EBMT (Example-Based Machine Translation) is proposed.
Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays.
Features include: morphological variants; WordNet synonyms and hypernyms; log-likelihood-based word pairings dy-namically obtained from baseline sen-tence alignments; and formal string features such as word-based edit dis-tance.
The model is an m-component mixture of Ingram models.
We describe SmartMail, a prototype system for automatically identifying action items (tasks) in email messages.
Spelling out means making explicit.
We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences.
We have proposed an incremental trans-lation method in Transfer-Driven Ma-chine Translation (TDMT).
This paper assesses two new approaches to deterministic parsing with respect to the analysis of unbounded dependencies (UDs).
Intersentential elliptical utterances occur frequently in information-seeking dialogues.
all bills only.
We introduce feature terms containing sorts, variables, negation and named disjunction for the specification of feature structures.
and lean declarative impIe.mentation.
LFG f-structures or HPSG feature structures, to dependency triples simple.
In Online Inquiry-Based Learning (OIBL) learners search for information to answer driving questions.
Three N-gram models, a baseline model (Bound-length N-gram) and two new models (Chunk-bound N- gram and Dependency-linked N-gram) are compared.
This paper presents our work on real-izing expressions of doubt appropriately in natural language dialogues.
Non-fiction	11.
Fiction	K. General Fiction	I.
(WSJ) speech corpus.
It takes into account the effects of linguistic features such as tense/aspect, temporal connectives, and discourse structures.
We apply the supervised decision list learn-ing method to Japanese named entity recogni-tion.
This paper describes the Lycos Retriever system, a deployed system for automatically generating coherent topical summaries of popular web query topics.
Core Roles versus Adjuncts).
We provide an XML serialization for intercomponent communication.
A number of operational models are introduced, with web interfaces for lexical databases, DFSA matrices, finite- state phonotactics development, and DATR lexica.
Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in- sequence n-grams automatically.
The second method relaxes strict n-gram matching to skipbigram matching.
Skip-bigram co- occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
We explore unsupervised language model adaptation techniques for Statistical Machine  Translation.
We propose an approach of combining lexical information, web statistics, and inverse search based on Google to backward translate a Chinese named entity (NE) into English.
The classifier presented uses part of speech tagging, cascaded finite state transducers, and simple disambiguation rules.
We apply default inheritance hierarchies to generating the morphology of Hebrew verbs.
The criteria involve integration of knowledge from several sources: pattern matching, syntactic and semantic analysis, and acoustics.
To disambiguate homograph, several efficient approaches have been proposed such as part-of-speech (POS) n-gram, Bayesian classifier, decision tree, and Bayesian-hybrid approaches.
Among the types of ambiguities handled are prepositional phrases, very compound nominals, adverbials, relative clauses, and preposed prepositional phrases.
Categorial unification grammars (CUGs) embody the essential properties of both unification and categorial grammar formalisms.
a local area network).
This paper presents an approach for processing incomplete and inconsistent knowledge.
The approach of KPSG provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system.
The context in which a target word occurs is represented by surface lexical features such as unigrams, bigrams, and second order co-occurrences.
valid prefixes.
We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.
Several grammars have been proposed for modeling RNA pseudoknotted structure.
We present ongoing work on prosody predic-tion for speech synthesis.
This paper introduces a grammar formalism specifically designed for syntax-based statistical machine translation.
This paper presents a Unicode based Chinese word segmentor.
at Grenoble.
Achieving this goal requires identification of not only the different topics in the documents but also of the particular flow of these topics.Our approach to content similarity evaluation employs n-grams of lexical chains and measures similarity using the cosine of vectors of n-grams of lexical chains, vectors of tf*idf-weighted keywords, and vectors of unweighted lexical chains (unigrams of lexical chains).
Generation procedure in SEMSYNThis section summarizes the SEMSYN genration procedure.
We learn models of answer type, query content, and answer extraction from clusters of similar questions.
We examine the utility of speech and lexical features for predicting student emotions in computer- human spoken tutoring dialogues.
generate non_grammatical sent_ ences.
This paper designs a novel lexical hub to disambiguate word sense, using both syntagmatic and paradigmatic relations of words.
We describe the use of energy function op-timisation in very shallow syntactic pars-ing.
In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.
the construction of domain- independent lexica.
The computational model, called Augmented Dependency Grammar (ADG), formulates not only the linguistic dependency structure of sentences but also the semantic dependency structure using the extended deep case grammar and field-oriented fact-knowledge based inferences.
This paper describes a grapheme-to-phoneme conversion method using phoneme connectivity and CCV conversion rules.
The phrase-break detector assigns phrase breaks using part-of-speech (POS) information.
We integrate various features, including simple deterministic features, morphological features, POS features and semantic trigger features, to capture various evidences especially for biomedical named entity and evaluate their contributions.
The NER system uses a hybrid algorithm based on Class-based language model and rule-based knowledge.
In this paper, we present a corpus-based supervised word sense disambiguation (WSD) system for Dutch which combines statistical classification (maximum entropy) with linguistic information.
This suite comprises several modules, namely: a sentence chunker, a tokenizer, a POS tagger, featurizers and lemmatizers.
We further show that topic translation with online machine translation resources yields effective CL-SR.
Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unification- based grammar (UBG).
1 IntroductionStochastic Unification-Based Grammars (SUBGs) use log-linear models (also known as exponential or MaxEnt models and Markov Random Fields) to define probability distributions over the parses of a unification grammar.
This position paper contrasts rhetorical structuring of propositions with intentional decomposition using communicative acts.
Modules include a syllabification program, a fast tnorphological parser, a lexical database, a phonological knowledge base, transliteration rules, and phonological rules.
We evaluate both syntactic structure and case structure.
We discuss a grammar development process used to generate the trees of the wide-coverage Lexicalized Tree Adjoining Grammar (LTAG) for English of the XTAG Project.
This paper examines the use of an unsupervised statistical model for determining the attachment of ambiguous coordinate phrases (CP) of the form n1 p n2 cc n3.
Collocation-based tagging and bracketing pro- grains have attained promising results.
We present a new chart parsing method for Lambek grammars, inspired by a method for D- Tree grammar parsing.
possible structural and lexical attributes.
some kind of character-stream.
First, the pre-processing modules for tokenisation, sentence splitting, paragraph segmentation, partof-speech tagging, clause chunking and noun phrase extraction are outlined.
Our algorithm generates lists of non- anaphoric noun phrases and noun phrase patterns from a training corpus and uses them to recognize non-anaphoric noun phrases in new texts.
We also investigated clustering of output documents from term level retrieval.
We evaluate Basilisk on six semantic categories.
We present an overview of TARSQI, a modular system for automatic temporal annotation that adds time expressions, events and temporal relations to news texts.
Theories of discourse structure hypothesize a hierarchical structure of discourse segments, typically tree-structured.
In this paper, we explore prosodic cues to discourse segmentation in human- computer dialogue.
This paper explores the segmentation of tutorial dialogue into cohesive topics.
DTG involve two composition operations called subsertion and sister-adjunction.
Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours.
In this paper, we consider a number of algo-rithms for estimating the parameters of ME mod-els, including iterative scaling, gradient ascent, con-jugate gradient, and variable metric methods.
This paper proposes a new error-driven HM3/1- based text chunk tagger with context-dependent lexicon.
Processing stages include lexical lookup, syntactic parsing, semantic analysis, and pragmatic analysis.
A deductive approach is used to predict vowel and consonant places of articulation.
The Smooth Injective Map Recognizer (SIMR) is a new bitext mapping algorithm.
SenseClusters is a freely available word sense discrimination system that takes a purely unsupervised clustering approach.
time, causality.
This paper describes the system MC-WSD presented for the English Lexical Sample task.
Onto these structures, phonological rules are applied such as the "letter梩osound" rules, automatic word stress rules,internal stress hierarchy rules indicating secondary stress,external sandhi rules, phonological focus assignment rules, logical focus assignment rules.
This paper describes an original hybrid system that extracts multiword unit candidates from part-of-speech tagged corpora.
We also char-acterize part-of-speech sequences that play a role in detecting non-native speech.
This paper presents a novel method for unsupervised word sense disam-biguation, which combines multiple in-formation sources, including seman-tic relations, large unlabeled corpora, and cross-lingual distributional statis-tics.
This paper presents a human梞achine dialogue model in the field of task梠riented dialogues.
This paper presents the results of an experiment using machine-readable dictionaries (MRDs) and corpora for building concatenative units for text to speech (ITS) systems.
groups of words).
The methods include Linear Discriminant Analysis, Supervised Vector Quantization, Shared Mixture VQ, Deleted Estimation of Context Weights, NMI Estimation Using "N-Best" Alternatives, Cross- Word Triphone Models.
shows suds a system for retrieving a Japanese dictionary.
We compare vectors containing counts of trigrams of part-of-speech (POS) tags in order to obtain an aggregate measure of syntax difference.
Central to this WSD framework is the sense division and semantic relations based on topical analysis of dictionary sense definitions.
LFG and PATR-II.
Most question answering (QA) systems rely on both keyword index and Named Entity (NE) tagging.
Data-Oriented Translation (DOT), based on Data- Oriented Parsing (DOP), is a language-independent MT engine which exploits parsed, aligned bitexts to produce very high quality translations.
This paper describes a Chinese word segmentation system based on unigram language model for resolving segmen-tation ambiguities.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
An empirical comparison of CFG filtering techniques for LTAG and HPSG is presented.
We explore the relationship between question answering and constraint relaxation in spoken dialog systems.
We present a method for compositionally translating noun-noun (NN) compounds, using a word-level bilingual dictionary and syntactic templates for candidate generation, and corpus and dictionary statistics for selection.
Collocation map is a sigmoid belief network that can be constructed from bigrams.
In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
OF COLING-92.
The present study deals with conflict resolution process in metaphorical interpretation for the noun phrase.
A co- occurrence pattern matrix with semantic categories is built based on these WCP.
1 Concept of collocational analysis770
ARGUMENTATION AND THE SEMANTIC PROGRAM.
For each sentence or utterance, these graphs include lexical information (words), syntactic information (such as the part of speech tags of the words and the syntactic parse of the utterance), and semantic information (such as the named entities and semantic role labels).
We introduce the bilingual dual-coding theory as a model for bilingual mental representation.
In this paper, a sememe co-occurrence frequency based WSD method was introduced.
In this paper, we compare and contrast IDF with inverse sentence frequency (ISF) and inverse term frequency (ITF).
We experimented with this approach to disambiguate 7 highly ambiguous verbs in English- Portuguese translation.
This paper presents novel approaches to reordering in phrase-based statistical machine translation.
The system is built around two separate neural network methodologies: context vectors and self organizing maps.
We have coupled the parser LCFLEX with a lexicon and ontology derived from two lexical resources, Verb- Net for verbs and CoreLex for nouns.
The other components are a selective unigram cache, a conditional bigram cache, and a conventional static trigram.
We investigate the effects of lexicon size and stopwords on Chinese information retrieval using our method of short-word segmentation based on simple language usage rules and statistics.
Association for Computational Linguistics.
Both models combine lexical, syntactic, and prosodic information.
This paper proposes the 揌ierarchical Directed Acyclic Graph (HDAG) Kernel?for structured natural language data.
Most statistical machine translation systems employ a word-based alignment model.
Linear algebraic technique called LSA/SVD is used to find co-relationships of sparse words.
We propose a method of using associative	strength	among	morphemes,morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech.
The model integrates information from a variety of sources, such as word co-occurrence, lexical chains, cue phrases, punctuation, and tense.
Among them are parallel multiple context-free grammars (pmcfg's) and lexical-functional grammars (lfg's).
Next, a method for natural language transfer is presented that integrates translation examples (represented as typed feature structures with source-target indices) with linguistic rules and constraints.
We present a novel disambiguation method for unification-based grammars (UBGs).
We extended language modeling approaches in information retrieval (IR) to combine collaborative filtering (CF) and content-based filtering (CBF).
We define POS blocks to be groups of parts of speech.
We focus on the production of efficient descriptions of objects, actions and events.
We take into account synonymy and hyperonymy.
The basic compo-nents include basic segmentation, factoid recognition, and named entity recognition.
This paper presents an unsupervised topic identification method integrating linguistic and visual information based on Hidden Markov Models (HMMs).
We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change.
We investigate the effect of corpus size in combining supervised and unsupervised learning for two types of attachment decisions: relative clause attachment and prepositional phrase attachment.
This paper shows that the dependency analyzer can be improved by introducing a Root-Node Finder and a Prepositional-Phrase Attachment Resolver.
We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG).
We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.
We connect VerbNet and FrameNet by mapping the FrameNet frames to the VerbNet Intersective Levin classes.
We look at self-triggerability across hyperlinks on the Web.
We adopt a probabilistic parsing strategy to provide a hierarchical lexical analysis of a word, including information such as morphology, stress, syllabification, phonemics and graphemics.
We measure phonetic (un)relatedness between dialects using Levenshtein distance, and classify by clustering distances but also by analysis through multidimensional scaling.
We propose a comprehensive theory of code- mixed discourse, encompassing equivalence- point and insertional code-switching, palindromic constructions and lexical borrowing.
We discuss shift- reduce parsing of stochastic context-free grammars and show how to construct a probabilistic score for ranking competing parse hypotheses.
keywords: Multilingual generation, lexical choice, controlled languages.
An efficient bit-vector-based CKY-style parser for context-free parsing is presented.
This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.
We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
We describe a biographical multi-document summarizer that summarizes information about people described in the news.
This paper proposes an automatic in-terpretation system that integrates free-style sentence translation and parallel text based translation.
This paper introduces an open computational framework for visual perception and grounded language acquisition called Experience-Based Language Acquisition (EBLA).
The algorithms included in this study are Hidden Markov Model, Maximum Entropy, Memory-Based Learning, and Transformation-Based Learning.
This paper investigates the potential for projectinglinguistic annotations including part-of-speech tagsand base noun phrase bracketings from one languageto another via automatically word-aligned parallelcorpora.
These features are derived from algorithms including automatic speech recognition, automatic speech indexing, speaker identification, prosodic and audio feature extraction.
We describe experiments learning quasi-synchronous context-free grammars from bitext.
We discuss a tagging schema and a tagging tool for labeling the rhetorical structure of texts.
This paper presents a semantic model for Chinese garden-path sentences.
This paper presents and analyzes an incremental	algorithm	for	theconstruction of Acyclic Non-deterministic Finite-state Automata (NFA).
This paper describes text meaning represen-tation for Chinese.
Our acquisition algorithm uses clues such as itemization or listing in HTML documents and statistical measures such as document frequencies and verb-noun co-occurrences.
We present a domain-independent topic segmentation algorithm for multi-party speech.
One of the most active and promising areas of statistical machine translation (SMT) research are tree-based SMT approaches.
In this approach we integrate a symbolic se-mantic segmentation parser with a learn-ing dialog act network.
These contain a range of temporal constructions, including time adverbials, progressive aspect and various aspectual classes.
Structural (attachment.)
The method predicts unknown words by recursively extracting common character strings.
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
We investigate generalizations of the allsubtrees "DOP" approach to unsupervised parsing.
theme/rheme and background/kontrast.
Our system integrates automatic term recognition, term variation management, context-based automatic term clustering, ontology-based inference, and intelligent tag information retrieval.
without inflections or conjugations.
Then a linear- unification parser for English syllables is introduced.
This constrained context-free model is specified by a stochastic context-free prior distribution with N-gram frequency constraints.
We show how idioms can be parsed in lexicalized TAGs.
The HMM-LR algorithm uses a generalized LR parser as a language model and hidden Markov models (HMMs) as phoneme models.
We propose a method for semi-automatic classi-fication of verbs to Levin classes via the seman-tic network of WordNet.
We describe the treatment of questions (Question-Answer Typology, question parsing, and results) in the Weblcopedia question answering system.
the Italian wordnet in EuroWordNet (ItalWordNet).
First, extending word similarity measures from direct co-occurrences to co- occurrences of co-occurred words, we compute the word similarities using not co-occurred words but co- occurred clusters.
We describe a novel method for detecting errors in task-based human-computer (HC) dialogues by automatically deriving them from semantic tags.
We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support user- directed multidocument summarization.
This paper presents a disambiguation approach for translating non-segmented-Kana into Kanji.
Unsupervised ontology discovery is a key component.
We present a treebank conversion method by which we construct an RMRS bank for HPSG parser evalu-ation from the TIGER Dependency Bank.
These rules are noun sequences with part-of-speech tags.
We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization.
The HKUST word sense disambiguation systems benefit from a new nonlinear Kernel Principal Component Analysis (KPCA) based disambiguation technique.
Transformation-based learning is chosen to predict recognition errors by integrating word confidence scores with linguistic features.
POS-tagging, lemmatisation and a thesaurus-like thematic dictionary.
We first classify prepositional attachments according to semantic equivalence of phrase heads and then apply inferential heuristics for understanding the validity of prepositional structures.
sequences and simplified part-of-speech tem-plates in identifying their syntactic categories.
The language analysis strategy used by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing.
the event profile.
Example-based machine translation (EBMT) is a promising translation method for speechto-speech translation because of its robustness.
In this paper, we explore facets of instructional texts: general prototypical structures, rhetorical structure and natural argumentation.
A rule-based approach uses caseframes and sense classes.
This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.
We present the results of experiments aimed at assigning domains to speech recognition hypotheses (SRH).
This project is focused on discourse connectives, which include explicit connectives such as subordinate and coordinate conjunctions, discourse adverbials, as well as implicit discourse connectives that are inferable from neighboring sentences.
Graph unification is the most expensive part of unification-based grammar parsing.
This paper describes an attribute grammar specification of the Government-binding theory.
Grammars for parsing have predominantly used generative rewrite rules.
CATEGORY assigns names in hierachies.
We present an integrated architecture for word-level and sentence-level processing in a unification-based paradigm.
In this paper a bidirectional parser for Lexicalized Tree Adjoining Grammars will be presented.
in a prepositional phrase.
This paper mainly describes a Chinese named entity recognition (NER) system NER@ISCAS, which integrates text, part-of-speech and a small-vocabularycharacter-lists feature for MSRA NER open track under the framework of Conditional Random Fields (CRFs) model.
Our system performs two procedures: Out-of-vocabulary extraction and word segmenta-tion.
We compose three out-of-vocabulary extraction modules: Character-based tag-ging with different classifiers ?maximum entropy, support vector machines, and con-ditional random fields.
We also com-pose three word segmentation modules ?character-based tagging by maximum en-tropy classifier, maximum entropy markov model, and conditional random fields.
It adopts dependency decision making and example-based approaches.
It labels semantic roles of parsed sentences.
We are concerned with dependency- oriented morphosyntactic parsing of running text.
In this paper we discuss cascaded Memory- Based grammatical relations assignment.
Pragmatic.
The paper proposes an information-theory-based method for feature types analysis in probabilistic evaluation modelling for statistical parsing.
of deep cases relations (or thematic relations).
In this paper, we present a method of estimating referents of demonstrative pronouns, personal pronouns, and zero pronouns in Japanese sentences using examples, surface expressions, topics and foci.
Language processing of the corpus texts so far included morpho-syntactic analysis, POS tagging and shallow syntactic parsing.
We address the problem of multi-way relation classification, applied to identification of the interactions between proteins in bioscience text.
Markov order-1 conditional random fields (CRFs) and semi-Markov CRFs are two popular models for sequence segmentation and labeling.
This paper describes a prototype for automatically scoring College Board Advanced Placement (AP) Biology essays.'.
This paper describes a new templaterepresentation and generalization method.
This paper examines feature selection for log linear models over rich constraint-based grammar (HPSG) representations by building decision trees over features in corresponding probabilistic context free grammars (PCFGs).
We propose a lexical organisation for multilingual lexical databases (MLDB).
This paper then applies the graph-structured stack to various natural language parsing methods, including ATN, LA parsing, categorial grammar and principle- based parsing.
-- the problem of diacritics and digraphs.
We present a scheme for efficiently representing a lexicalized tree-adjoining grammar (LTAG).
We describe the CoNLL-2002 shared task: language-independent named entity recogni-tion.
This paper presents an LTAG account for binding of reflexives and reciprocals in English.
In this paper, we propose ellipsis handling method for follow-up questions in Information Access Dialogue (IAD) task of NTCIR QAC3.
We report here the contribution of default sense selection, idiomatic usage, syntactic and semantic clues, subcategorization patterns, word forms, syntactic usage, context, selectional preferences, and topics or subject fields.
In the GPSG framework.
We show the retrieval examples with the following characteristic features: phrasal expression, long-distance dependency, idiom, synonym, and semantic ambiguity.
This paper presents Archivus, a multi- modal language-enabled meeting browsing and retrieval system.
Approaches are wide and include key wording, statistical analysis, pattern matching, and a method using lexical, syntactic, and semantic filters.
This paper discusses a system for grammat-ically describing and parsing entries from machine-readable dictionary tapes and a lexical data base representation for storing the dictionary information.
The paper also proposes a combinatorial and dynamic organization for lexical systems, which is based on the notion of virtual accumulation and the abstraction levels of lexical sets.Keywords: dictionary resources, lexicalacquisition,	lexical	production,	lexicalaccumulation, computational lexicography.
We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic.
This paper presents LexPhon, a computational framework for modelling segmental aspects of Lexical Phonology (LP).
It features automatic interface generation and self-organization.
The heart of the system is a rule-based top-down DCG-style parser, which uses an LFG oriented grammar organization.
We report results for training and test-ing an automatic classifier to label the in-formation provider抯 utterances in spoken human-computer and human-human dia-logues with DATE (Dialogue Act Tagging for Evaluation) dialogue act tags.
We introduce a first-order language for semantic underspecification that we call Constraint Language for Lambda-Structures (CLLS).
This paper proposes a method for reinforcing countability prediction by introducing a novel concept called one countability per discourse.
We present a new compositional tense-aspect deindexing mechanism that makes use of tense trees as components of discourse contexts.
This paper studies and evaluates disambiguation strategies for the translation of tense between German and English, using a bilingual corpus of appointment scheduling dialogues.
This paper investigates stacking and voting methods for combining strong classifiers like boosting, SVM, and TBL, on the named-entity recognition task.
We describe an automatic Word Sense Disambiguation (WSD) system that disambiguates verb senses using syntactic and semantic features that encode information about predicate arguments and semantic classes.
This paper describes the open-source Phrase-Based Statistical Machine Translation Decoder - Phramer.
problem oriented systems.
domain.
the grammatical morphemes  of the language.
RULES  normalized sequence of categoriesPARSERinput sentence [MORPHOLOGICAL ANALYZER.
We describe the generation of communicative ac-tions in an implemented embodied conversational agent.
This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars (TFSGs).
This paper extends the base noun phrase(BNP) identification into a research on Chinese base phrase identification.
The paper describes a new unification based grammar formalism called Regular Unification Grammar (RUG).
It con-sists of an approximate word match-ing method and an N-best word seg-mentation algorithm using a statistical language model.
We present a very efficient statistical incremental parser for LTAG-spinal, a variant of LTAG.
ASL natural language generation (NLG) is a special form of multimodal NLG that uses multiple linguistic output channels.
A Chinese word segmentation algorithm based on forward maximum matching and word binding force is proposed in this paper.
PARSE GRAPH STRUCTUREThe Parse Graph Structure (PGS) is built by the parser while applying grammar rules.
are described.Keywords: Computational Lexicography, Lexical Knowledge Base, Lexical Semantics.
Two IBL algorithms are utilized: k-Nearest Neighbor (kNN), and Priority Maximum Likelihood (PML) with a modified back-off combination method.
In this paper we present an overview of recent developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic theory of discourse structure.
They are morphological transformation and morpheme Identification.
This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling.
This paper describes an operational se-mantics for DATR theories.
In this paper we present a polynomial time parsing algorithm for Combinatory Categorial Grammar.
The method exploits patterns of non-transitivity in translations across multiple languages.
A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector.
3.3 GB ytes of text.
We consider here the problem of Chinese named entity (NE) identification using statistical language model(LM).
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex- dependent models.
This paper presents a generalized unknown morpheme handling method with POSTAG(POStech TAGger) which is a statistical/rule based hybrid POS tagging system.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined.
We present a memory-based learning (MBL) approach to shallow parsing in which POS tagging, chunking, and identification of syntactic relations are formulated as memory-based modules.
We discuss the relevance of our method for linguis-tics and language technology.
This paper describes a new statistical parser which is based on probabilities of dependencies between head-words in the parse tree.
phonological rules or metrical systems).
Our environment accepts grammars consisting of binary dependency relations andgrammatical functions.
We first show that non-reentrant unification grammars generate exactly the class of context- free languages.
distinctive features.
This paper presents a comparative study of probabilistic treebank parsing of German, using the Negra and T╱Ba-D/Z tree- banks.
Synchronous Tree Adjoining Grammars can be used for Machine Translation.
We present a corrective model for recovering non-projective dependency structures from trees generated by state-of-the- art constituency-based parsers.
He thus formulated aPreferred Argument Structure (PAS) for the preferential structural configurations of arguments.
Our approach yields phrasal and single word lexical paraphrases as well as syntactic para-phrases.
We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
Many linguistse.
This paper presents a new view of Explanation-Based Learning (EBL) of natural language parsing.
We present algorithms for both hard constraints and binary soft constraints.
This demo abstract describes the SmartWeb Ontology-based Annotation system (SOBA).
The NRC system approaches WSD as a classical supervised machine learning problem, using familiar tools such as the Weka machine learning software and Brill抯 rule-based part-of-speech tagger.
This paper presents Trace & Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (uG) and ideas of Government & Binding Theory (GB) in an undogmatic way.
We analyze the association of noun-noun and noun-verb pairs in LOB Corpus.
This paper presents a method for inducing transla-tion lexicons based on transduction models of cog-nate pairs via bridge languages.
We present an approach for the joint extraction of entities and relations in the context of opinion recognition and analysis.
This paper presents a method for parsing associative Lambek grammars based on graph- theoretic properties.
Most of the previous Korean noun extraction systems use a morphological analyzer or a Partof-Speech (POS) tagger.
(TSR trees).
We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods.
The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is defined and basic algorithms for SLTAG are designed.
unscripted) speech.
In this paper, we present a novel training method for a localized phrase-based prediction model for statistical machine translation (SMT).
block bigram features.
SEM encodes logical semantics.
This paper describes an algorithm for unifying disjunctive feature structures.
We propose a distribution-based pruning of n-gram backoff language models.
In this paper, we look at comparing high- accuracy context-free parsers with high- accuracy finite-state (shallow) parsers on several shallow parsing tasks.
This paper presents a reestimation algorithm and a best-first parsing (BFP) algorithm for probabilistic dependency grammars (PDC).
This paper presents a unified approach to parsing, in which top-down, bottom- up and left-corner parsers are related to preorder, postorder and inorder tree traversals.
In this paper we present a logical treatment of semi- free word order and bounded discontinuous constituency.
This permits a natural interpretation of implicational universals in terms of theories, subtheories and implicational axioms.
The dictionary is parsed using a head-driven phrase structure grammar of Japanese.
This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a natural language interface to database query ap-plications.
We define noun phrase translation as a subtask of machine translation.
A method of sense resolution is proposed that is based on WordNet, an on-line lexical database that incorporates semantic relations (synonymy, antonymy, hyponymy, meronymy, causal and troponymic entailment) as labeled pointers between word senses.
Graph unification remains the most expensive part of unification-based grammar parsing.
This method focuses on semantic and pragmatic constraints such as semantic constraints on cases, modal expressions, verbal semantic attributes and conjunc-tions to determine the deictic reference of Japanese zero pronouns.
the processing objectYenee: to	sentences .
This paper presents an algorithm for text summarization using the the-matic hierarchy of a text.
This paper discusses an approach to incremental learning in natural language processing.
A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors.
In this paper we describe how to hierarchically organize abstract nouns (adjective hyperonyms) in a semantic map mainly using CSM.
First, we introduce the 揹ependency tree path?(DTP).
To resolve the problem, we propose "term distillation", a framework for query term selection in cross-database retrieval.
This study addresses pronominal anaphora resolution, including zero pronouns, in Chinese.
We describe an approach to tagging a monolingual dictionary with linguistic features.
In this paper, we describe data collection, transcription, word segmentation, and part-of-speech annotation of this corpus.
statistical lexical head- outward transducers.
We present a corpus-based study of the sequential ordering among premodifiers in noun phrases.
LexNet includes LexRank (for text summarization), biased LexRank (for passage retrieval), and TUMBL (for binary classification).
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications.
We coerce the English FrameNet into Chinese using a bilingual lexicon, frame context in FrameNet and taxonomy structure in HowNet.
We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
This paper explores the extent to which phoneme sequence constraintz can be used to identify word boundaries in continuous speech recognition.
This paper describes the tree-structured maximum mutual information (MMI) encoders used in SSI's Phonetic Engine?to perform large-vocabulary, continuous speech recognition.
These templates represent grammatically correct sentence patterns.
The other is the CKY algorithm for probabilistic context free grammars.
We present the concepts of weighted language, transduction and automaton from algebraic automata theory as a general framework for describing and implementing decoding cascades in speech and language processing.
We have developed M ultiModal Definite Clause Grammar (MM-DCG), an extension of Definite Clause Grammar.
Further, we have developed MM-DCG translator to transfer rules in MM-DCG into Prolog predicates.
HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way.
relation based sim-ilarity measure and distribution based similarity measure.
A LOGICAL SEMANTICSFOR NONMONOTONIC SORTSMark A.
In this paper, we present a logical basis for NSs and non- monotonically sorted feature structures (NSFSs).
We consider the problem of computing the Kullback-Leibler distance, also called the relative entropy, between a probabilistic context-free grammar and a probabilistic finite automaton.
Natural language parsing requires ex-tensive lexicons containing subcategori-sation information for specific sublan-guages.
This paper extends earlier work on the relation between syntax and intonation in language understanding in Combinatory Categorial Grammar (CCG).
Motion Verbs.
In this paper, we report on data for movement verbs (or motion verbs).
Bilingual Corpus-based Analysis.
This paper discusses the partitive-genitive case alternation of Finnish adpositions.
This paper proposes an efficient example selection method for example-based word sense disambiguation systems.
The first method uses a standard HMM part-of-speech tagger with variable context length.
We address the representation of nouns having complex argument structures like deverbal nominalisations.
This paper presents a semantic interpretation of adjectival modification in terms of the Generative Lexicon.
Thematic knowledge is a basis of semantic interpretation.
Current NER approaches include: dictionary-based, rule-based, or machine learning.
We present discriminative reordering models for phrase-based statistical machine translation.
In this paper we describe EFLUF - an implementation of FLUF.
We perform Noun Phrase Bracketing by using a local, maximum entropy-based tagging model, which produces bracketing hypotheses.
This paper describes using RDF/RDFS/XML to create and navigate a metadata model of relationships among entities in text.
domain-independent, semantic information for question interpretation.
In this paper I outline Type-inheritance Combinatory Categorial Grammar (TCCG), an implemented feature structure based CCG fragment of English.
TCCG combines the fully lexical nature of CCG with the type-inheritance hierarchies and complex feature structures of Head- driven Phrase Structure Grammars (HPSG).
We present a method for compiling grammars into efficient code for head-driven generation in ALE.
Memory-based learning is a form of supervised learning based on similarity-based reasoning.
We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text.
We apply our translit-eration algorithm to the transliteration of names from Arabic into English.
This paper proposes a discriminative HMM (DHMM) with long state dependence (LSDDHMM) to segment and label sequential data.
This paper presents a genetic algorithm based approach to the automatic discovery of finite- state automata (FSAs) from positive data.
We use dependency grammar and employ a stack based shift/ reduce context梔ependent parser as the tagging mechanism.
In this paper, we explore how the taxo-nomic inheritance hierarchy in a seman-tic net can contribute to the resolution of associative anaphoric expressions.
This paper constitutes an investigation into the generative capabilities of two-level phonology with respect to unilevel generative phonological rules.
A hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structures for semantic role labeling (SRL).
This part-ofspeech predictor will be used in a part-of-speech tagger to handle out-of-lexicon words.
This paper offers a provisional mathematical typology of metrical representations.
This paper describes a system for the un-supervised learning of morphological suf-fixes and stems from word lists.
This paper describes discriminative language modeling for a large vocabulary speech recognition task.
Rohrer.
We describe a bidirectional framework for natural language parsing and genera-tion, using a typed feature formalism arid an HPSG-based grammar with a parser arid generator derived from parallel pro-cessing algorithms.
its aggregation and referring expression generation capability).
This paper describes the use of a system of semantic rules to generate noun compounds, vague or polysemous words, and cases of metonymy.
We have implemented an interactive, Web-based, chat-style machine translation system, supporting speech recognition and synthesis, local- or third-party correction of speech recognition and machine translation output, and online learning.
In this paper we describe the Senseval 3 Basque lexical sample task.
morphological derivation and synonymy expansion) in web search strategies.
This paper describes log-linear parsing models for Combinatory Categorial Grammar (CCG).
This paper describes an all level approach on statistical natural language translation (SNLT).
It is natural to combine a head-driven HPSG grammar with a head- driven generation algorithm.
OF COLING-92, NANTES, Atm.
information technology test reports and medical finding reports.
We investigate independent and relevant event-based extractive mutli-document summarization approaches.
Two main problems in natural language generation are lexical selection and syntactic structure determination.
We extract abstract syntactic function-based subcategorisation frames (LFG semantic forms), traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames, with or without preposition information for obliques and particle information for particle verbs.
We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
verb-object pairs) from text corpora automatically.
WordNet) to ambiguous words occurring in a syntactic dependency.
We present a method for determining the referents of noun phrases in Japanese sentences by using the referential properties, modifiers, and possessors' of noun phrases.
We distinguish between context-free repre-sentability and context, free processing.
model and transformation rules.
This paper introduces PhraseNet, a context- sensitive lexical semantic knowledge base system.
Existing methods learn statistics on trimming context-free grammar (CFG) rules.
We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.
We perform a linguistic analysis of documents during indexing for information retrieval.
This paper describes machine learning based parsing and question classification for question answering.
ethnologue.com).
PROLOG.
We present experiments training log-linear combinations of models for dependency parsing and for machine translation.
We propose a signal dependent analysis-synthesis scheme in Sec.
The second experiment investigates a method for unsupervised learning of gender/number/animaticity information.
We use linguistic patterns and HTML text structures to extract text fragments contain-ing term descriptions.
We present BAYESUM (for 揃ayesian summarization?, a model for sentence extraction in query-focused summarization.
Thus, a co-occurrence graph can be constructed by co-occurrence relations in a corpus.
Tagging compound verb groups in an annotated corpus exploiting the verb rules is described.Keywords: compound verb groups, chunking, morphosyntactic tagging, inductive logic pro-gramming
Text metadata.
Part-of-Speech patterns extracted from parallel corpora have been used to enhance a translation resource for statistical phrase-based machine translation.
We explore a novel computational approach to identifying 揷onstructions?or 搈ulti-word expressions?
We propose a kana-kanji conversion system with input support based on prediction.
We describe a method for retrieving counts for adjective-noun, noun-noun, and verb- object bigrams from the web by querying a search engine.
This paper proposes an error-driven HMM-based text chunk tagger with context-dependent lexicon.
We then use the subjects' segmentations to evaluate the correlation of discourse segmentation with three linguistic cues (referential noun phrases, cue words, and pauses), using information retrieval metrics.
This paper describes DialogueView, a tool for annotating dialogues with utter-ance boundaries, speech repairs, speech act tags, and discourse segments.
Triphone analysis is a new correction strategy which combines phonemic transcription with trigram analysis.
Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing.
In this work, we present a new semantic language modeling approach to model news stories in the Topic Detection and Tracking (TDT) task.
We discuss combining knowledge-based (or rule-based) and statistical part-of-speech taggers.
We describe a probabilistic approach to content selection for meeting summarization.
verbs, named entities and base noun phrases) in passage context utilization and answer sentence extraction.
A system under construction is outlined which incorporates morphological checks (using new two-level error rules) over a directed letter graph, tag positional trigrams and partial parking.
This paper presents a method for extracting subcorpora documenting different subcategorization frames for verbs, nouns, and adjectives in the 100 mio.
word British National Corpus.
This paper will focus on our temporal processing algorithm, and in particular on our analysis of narrative progression, rhetorical structure, perfects and temporal expressions.
It supports Chinese and English word segmentation, Chinese named entity recognition, Chinese part of speech tagging and phrase conglutination.
Pos811D.
Common parsers employ phrase structure descriptions, rule-based grammars, and derivation or transition oriented recognition.
We describe an implementation of a hybrid statistical/symbolic approach to repairing parser failures in a speech-to-speech translation system.
representation methods, hypermedia maps.
1			Back梤eferencing in text		?.
11			Belief structure	.
handwriting).
Next, we describe our more recent work on the CU Move dialog system for in-vehicle route planning and guidance.
Word order is realized by grammatical competition based on linear prece-dence (LP) rules which are based on the discourse-relational features.
We propose a new method for reformatting web documents by extracting semantic structures from web pages.
Recent work has considered corpus-based or statistical approaches to the problem of prepositional phrase attachment ambiguity.
We show how to do this using , -reduction constraints in the constraint language for A-structures (CLLS).
incomplete syntactic analysis.
This paper proposes a bidirctional hierarchical clustering algorithm for simultaneously clustering words of different parts of speech based on collocations.
base forms and POS tags.
Context information is produced from rule-based translation such as part-of-speech tags, semantic concept, case relations and so on.
This paper employs different types of information from different levels of text to extract named entities, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache and n-gram model.
"proper name") recognition system built around a maximum entity framework.
Example-based machine translation (EBMT) is based on a bilingual corpus.
We describe an algorithm for recovering non-local dependencies in syntactic dependency structures.
This paper briefly describes our rule-based heuristic analyzer for Finnish nominal and verb forms.
This paper addresses two major problems in closed task of Chinese word segmentation (CWS): tagging sentences interspersed with non-Chinese words, and long named entity (NE) identification.
Three approaches to measuring text similarity are considered: n- gram overlap, Greedy String Tiling, and sentence alignment.
This paper presents a speech understanding component for enabling robust situated human-robot communication.
We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging.
The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods.
Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking.
We present the architecture and data model for TEXTRACT, a document analysis framework for text analysis components.
This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism.In parsers for unification-based grammar formalisms, complex phrase types are derived by incremental refinement of the phrase types defined in grammar rules and lexical entries.
This paper explores the effect of improved morphological analysis, particularly context sensitive morphology, on monolingual Arabic Information Retrieval (IR).
a finite-state transducer.
There has recently been considerable interest in the use of lexically-based statistical techniques to resolve prepositional phrase attachments.
This paper describes a method for obtaining the semantic representation for a syntax tree in Systemic Grammar (SG).
In this paper we tackle sentence boundary disam-biguation through a part-of-speech (POS) tagging framework.
parsing procedures and semantic head-driven generation.
Dialogue system for 3D virtual environ-ments).
We apply the C4.5 decision tree learner in interpreting Japanese relative clause constructions, based around shallow syntactic and semantic processing.
We propose to score phrase translation pairs for statistical machine translation using term weight based models.
We describe a method for identifying systematic patterns in translation data using part-ofspeech tag sequences.
This paper provides an evaluation of the OntoLearn ontology learning system.
We propose a method for compiling bilingual terminologies of multi-word terms (MWTs) for given translation pairs of seed terms.
This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.
This paper describes new default unification, lenient default unification.
noun phrase (NP) syntax.
We describe an MDL based grammar of a language that contains morphology and lexical categories.
We present an algorithm for collapsing morphological classes (signatures) by using syntactic context.
The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unification- based shallow-level parser using transformational rules over syntactic patterns.
This paper presents a technique to deal with multiword nominal terminology in a computational Lexical Functional Grammar.
This paper compares two approaches to computational semantics, namely semantic unification in Lexicalized Tree Adjoining Grammars (LTAG) and Lexical Resource Semantics (LRS) in HPSG.
We describe a machine learning system for the recognition of names in biomedical texts.
In this paper we present some novel applications of Explanation-Based Learning (EBL) technique to parsing Lexicalized Tree-Adjoining grammars.
This paper presents PROVERB a text planner for argumentative texts.
Inter-sentence similarity is estimated by latent semantic analysis (LSA).
These include hierarchical relationships, gender differences, temporal ordering, and prototype-variant effects.
This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA).
These features include lexical, lexico grammatical and semantic phenomena.
We compared the accuracy of a statistical parser on the LDC Switchboard treebank corpus of transcribed sentence-segmented speech using various combinations of punctuation and sentence-internal prosodic information (duration, pausing, and f0 cues).
The CP types considered include adjective-verb (AV), noun-verb (NV), adverb-verb (Adv-V), and verb-verb (VV) composites.
This paper proposes an effective parsing method for example-based machine translation.
variable-matching without variable substitution.
We present a Czech-English statistical machine translation system which performs tree-to-tree translation of dependency structures.
Emdros is a text database engine for linguistic analysis or annotation of text.
Our CWS is based on backward maximum matching with word support model (WSM) and contextual-based Chinese unknown word identification.
idf based method.
In this paper, subclasses of monadic context- free tree grammars (CFTGs) are compared.
We investigate the relation between paragraph boundaries and discourse cues, pronominalization and information structure.
This paper deals with the way temporal connectives affect Temporal Structure as well as Discourse Structure in Narratives.
This paper compares the consistency- based account of agreement phenomena in 'unification-based' grammars with an implication-based account based on a simple feature extension to Lambek Categorial Grammar (LCG).
This paper describes strategies for automatic recognition of unknown variants of known words in a natural language processing system.
We show conclusive results on joint learning and inference of syntactic and semantic representations.
We have constructed the IPA Lexicon of Basic Japanese Nouns (IPAL-BN), which has a hierarchical structure based on the syntactic and semantic proper-ties of nouns.
In this paper, we explore correlation of dependency relation paths to rank candidate answers in answer extraction.
This paper presents a framework for clustering in text-based information retrieval systems.
SCsem is the semantic weighting.
We combine statistical knowledge with well known features in a Support Vector Machine pronoun resolution classifier.
Industrial applications of a reversible, string-based, unification approach called Humor (High-speed Unification Morphol-ogy) is introduced in the paper.
Dutch subordinate clauses.
Dutch subordinate clauses for non-transformational theories of grammar.
We then examine the effect of integrating pausal and spontaneous speech phe-nomena into syntactic rules for speech recognition, using118 sentences.
A bottom-up generation algorithm for principle-based grammars is proposed.
We present a comparison of two formalisms for representing natural language utterances, namely deep syntactical Tectogrammatical Layer of Functional Generative Description (FGD) and a semantic formalism, MultiNet.
We explore two semantic chunking tasks.
We present an empirical study of the applicability of Probabilistic Lexicalized Tree Insertion Grammars (PLTIG), a lexicalized counterpart to Probabilistic Context-Free Grammars (PCFG), to problems in stochastic natural- language processing.
LCP may provide valuable information for resolving anaphora and ellipsis.
For topic identification, we will outline techniques based on stereotypical text structure, cue words, high-frequency indicator phrases, intratext connectivity, and discourse structure centrality.
trigger words and parsing structures.
(HLT-NAACL Workshop on Parallel Texts 2006).
The paper proposes a Constrained Entity- Alignment F-Measure (CEAF) for evaluating coreference resolution.
Topic representations are integrated in two NLP applications: Information Extraction and Multi- Document Summarization.
We present a language-independent and unsupervised algorithm for the segmentation of words into morphs.
Previous works on question classification are based on complex natural language processing techniques: named entity extractors, parsers, chunkers, etc.
This paper proposes an alignment adaptation	approach	to	improvedomain-specific (in-domain) word alignment.
We present a simple approximation method for turn-ing a Head-Driven Phrase Structure Grammar into a context-free grammar.
The acoustic models make use of dictionary phonetic spellings together with models for phonemes in context.
We describe an implementation of data- driven selection of emphatic facial displays for an embodied conversational agent in a dialogue system.
This paper defines multiset-valued linear index grammar and unordered vector grammar with dominance links.
WORD- NET DOMAINS).
We consider the translation of European Parliament Speeches.
We introduce a new categorial formal-ism based on intuitionistic linear logic.
We apply these Simple Synchrony Networks to mapping sequences of word tags to parse trees.
knowledge and language-specific heuristics.
It proposes and evaluates new cluster generation, hierarchical smoothing and adaptive topic-probability estimation techniques.
This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE.
This paper presents some possibilities of a structural approach for handling communication failures in task-oriented oral dialogues.
information technology test reports and medical finding reports.
In the process, we create a word朿ategory co- occurrence matrix, which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses, as well.
We describe a word alignment platform which ensures text pre-processing (tokenization, POS-tagging, lemmatization, chunking, sentence alignment) as required by an accurate word alignment.
We examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues.
They are:?Vector-quantized energy-normalized Mel-cepstra?Vector-quantized smoothed 40-ms晅ime derivatives of the Mel-cepstra?Energy?Smoothed 40-ms energy differencesWe use 256-word speaker-independent code- books to vector-quantize the Mel-cepstra and the Melcepstral differences.
Models used include unique-phone-in-word, phone-in- word, triphone, biphone, and generalized-phone forms of biphones and triphones, as well as context-independent models.
part-of-speech tagged corpora).
The method involves using a bilingual term list to learn source- target surface patterns.
LFG and GPSG are augmented PS- grammars.
We propose astemming method for Mongolian to extractloanwords correctly.
This paper describes PEGASUS, a spoken language interface for on-line air travel planning that we have recently developed.
This paper describes our preliminary research on "attention-sharing" in infants' language acquisition.
This paper outlines a formal computational semantics and pragmatics of the major speech act types.
This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame.
An HHMM-based system ICTCLAS was accomplished.
We introduce a new interactive corpus exploration tool called InfoMagnets.
Using the formalism of generalized phrase structure grammar (GPSG) in an NI, system (e.g.
We present BioGrapher, a biographical QA system that addresses this problem by machine learning algorithms for biography classification.
This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language.
We investigate the usefulness of evolutionary al-gorithms in three incarnations of the problem of feature relevance assignment in memory-based language processing (MBLP): feature weight-ing, feature ordering and feature selection.
CTXMATCH performs a linguistic processing of the labels attached to the nodes, including tokenization, Part of Speech tagging, multiword recognition and word sense disambiguation.
This paper presents Trace dr Unification Grammar (TUG), a declarative and reversible grammar formalism that brings together Unification Grammar (UG) and ideas of Government dr Binding Theory (on).
in GPSO and HPSG.
This process is called transliteration.
This paper presents a novel language-independent question/answering (Q/A) system based on natural language processing techniques, shallow query understanding, dynamic sliding window techniques, and statistical proximity distribution matching techniques.
Bayesian, decision tree and neural network classifiers) to discover language model errors.
Word category prediction is used to implement an accurate word recognition system.
We present two techniques for combining evidence from dictionary-based and corpus-based translation lexicons, and show that backoff translation outperforms a technique based on merging lexicons.
Various shallow methods have been proposed to translate compound nouns, notable amongst which are memory-based machine translation and word-to-word compositional machine translation.
This paper describes the NLMenu System, a menu-based natural language understanding system.
We examine the relationship between the two grammatical formalisms: Tree Adjoining Grammars and Head Grammars.
We present Evita, an application for recognizing events in natural language texts.
It consists of preference based pruning, syntactic based pruning and semantic based pruning.
We present two methods for unsupervised segmentation of words into morpheme-like units.
The method combines morphological and lex-ical processing, bilingual word alignment, and graph-theoretic cluster generation.
These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation.
plus petits.
Associated or related terms.
This paper presents an answer selection method based on Support Vector Machines (SVM) for Open-Domain Question Answering (QA).
Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data.
Chinese word segmentation and Part-ofSpeech (POS) tagging have been commonly considered as two separated tasks.
We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features.
We implement, a num-ber of both bag-of-words and word order-sensitive similarity metrics, and test each (wet' character-based and word-based indexing.
We present a method for improving dependency structure analysis of Chi-nese.
This paper describes a chunk-based parser/semantic analyzer used by a language learning model.
Successful proper noun boundary identification within the part of speech tagger is essential for successful categorization.
in content based information retrieval.
This paper describes a first attempt at a sta-tistical model for simultaneous syntactic pars-ing and generalized word-sense disambigua-tion.
This paper describes an approach to translate rarely occurring named entities (NE) by combining phonetic and semantic similarities.
This paper presents a series of efficient dynamic-programming (DP) based algorithms for phrase-based decoding and alignment computation in statistical machine translation (SMT).
Prepositional Phrase is the key issue in structural ambiguity.
Four different attach-ments are told out according to their functionality: noun attachment, verb attachment, sentence-level attachment, and predicate-level attachment.
We present a statistical phrase-based translation model that uses hierarchical phrases?phrases that contain subphrases.
This paper presents a Bayesian model for unsupervised lixtruhnz, of verb selectional preferences.
database tuples).
In this study, the idea of ECOC is applied to memory-based language learning with local (k-nearest neighbor) classifiers.
We report on a series of experiments with probabilistic context-free grammars predicting English and German syllable structure.
This paper compares a number of generative probability models for a wide- coverage Combinatory Categorial Grammar (CCG) parser.
This paper explores the problem of identifying sen-tence boundaries in the transcriptions produced by automatic speech recognition systems.
The features studied are: principal components, independent com-ponents, and non-negative components.
This paper explores the use of lasso for statistical language modeling for text input.
We propose projection models that exploit lexical and syntactic information.
The model uses syntactic patterns and N-grams reflecting the hierarchical discourse structures of dialogues.
We present Minimum Bayes-Risk word alignment for machine translation.
We create five kinds of semantic maps, i.e., semantic maps of abstract nouns organized via (1) adjectives, (2) adjectival nouns, (3) non-adjectival nouns and (4) adjectival and adjectival nouns and a semantic map of adjectives, adjectival nouns and non-adjectival nouns organized via collocated abstract nouns, and compare them with each other to find similarities and differences.
Tree Adjoining Grammar (TAG) is a formalism for natural language grammars.
We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.
9000 Japanese scientific papers.
This paper presents Delphi, the natural language component of the BBN Spoken Language System.
It uses two steps: the N-best Part-Of-Speech (POS) tagging and baseNP identification given the N-best POS-sequences.
We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.
In this paper, a memory-based pars-ing method is extended for han-dling compositional structures.
This paper presents a novel training algorithm for a linearly-scored block sequence translation model.
We compare the performances of Decision Tree, Na飗e Bayes, SVM (Support Vector Machine), and ME (Maximum Entropy) classification methods.
