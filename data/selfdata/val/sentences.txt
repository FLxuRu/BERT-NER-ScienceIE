Recovery of Empty Nodes in Parse Structures.
In this paper, we describe a new algorithm for recovering WH-trace empty nodes.
Our approach combines a set of hand-written patterns together with a probabilistic model.
Because the patterns heavily utilize regular expressions, the pertinent tree structures are covered using a limited number of patterns.
The probabilistic model is essentially a probabilistic context-free grammar -LRB- PCFG -RRB- approach with the patterns acting as the terminals in production rules.
We evaluate the algorithm 's performance on gold trees and parser output using three different metrics.
Our method compares favorably with state-of-the-art algorithms that recover WH-traces.
Regularized Least-Squares Classification For Word Sense Disambiguation.
The paper describes RLSC-LIN and RLSCCOMB systems which participated in the Senseval-3 English lexical sample task.
These systems are based on Regularized Least-Squares Classification -LRB- RLSC -RRB- learning method.
We describe the reasons of choosing this method, how we applied it to word sense disambiguation, what results we obtained on Senseval1, Senseval-2 and Senseval-3 data and discuss some possible improvements.
Automatic Indexing of Specialized Documents : Using Generic vs. Domain-Specific Document Representations.
The shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized.
In the biomedical domain, continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as MEDLINE ®.
In this paper, we evaluate two statistical methods of producing MeSH ® indexing recommendations for the genetics literature, including recommendations involving subheadings, which is a novel application for the methods.
We show that a generic representation of the documents yields both better precision and recall.
We also find that a domainspecific representation of the documents can contribute to enhancing recall.
Unsupervised Semantic Role Labeling.
We present an unsupervised method for labeling the arguments of verbs with their semantic roles.
Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based.
A novel aspect of our approach is the use of verb , slot , and noun class information as the basis for backing off in our probability model.
We achieve 50 -- 65 % reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data.
Towards An Optimal Lexicalization In A Natural-Sounding Portable Natural Language Generator For Dialog Systems.
In contrast to the latest progress in speech recognition, the state-of-the-art in natural language generation for spoken language dialog systems is lagging behind.
The core dialog managers are now more sophisticated ; and natural-sounding and flexible output is expected, but not achieved with current simple techniques such as template-based systems.
Portability of systems across subject domains and languages is another increasingly important requirement in dialog systems.
This paper presents an outline of LEGEND, a system that is both portable and generates natural-sounding output.
This goal is achieved through the novel use of existing lexical resources such as FrameNet and WordNet.
Phrase-Based Backoff Models For Machine Translation Of Highly Inflected Languages.
We propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level.
The model is evaluated on the Europarl corpus for German-English and FinnishEnglish translation and shows improvements over state-of-the-art phrase-based models.
Growing Semantic Grammars.
A critical path in the development of natural language understanding -LRB- NLU -RRB- modules lies in the difficulty of defining a mapping from words to semantics : Usually it takes in the order of years of highly-skilled labor to develop a semantic mapping, e.g., in the form of a semantic grammar, that is comprehensive enough for a given domain.
Yet, due to the very nature of human language, such mappings invariably fail to achieve full coverage on unseen data.
Acknowledging the impossibility of stating a priori all the surface forms by which a concept can be expressed, we present GsG : an empathic computer system for the rapid deployment of NLU front-ends and their dynamic customization by non-expert end-users.
Given a new domain for which an NLU front-end is to be developed, two stages are involved.
In the authoring stage, GSQ aids the developer in the construction of a simple domain model and a kernel analysis grammar.
Then, in the run-time stage, GSG provides the enduser with an interactive environment in which the kernel grammar is dynamically extended.
Three learning methods are employed in the acquisition of semantic mappings from unseen data : -LRB- i -RRB- parser predictions, -LRB- ii -RRB- hidden understanding model, and -LRB- iii -RRB- end-user paraphrases.
A baseline version of GsG has been implemented and prellminary experiments show promising results.
COOPML : Towards Annotating Cooperative Discourse.
In this paper, we present a preliminary version of COOPML, a language designed for annotating cooperative discourse.
We investigate the different linguistic marks that identify and characterize the different forms of cooperativity found in written texts from FAQs, Forums and emails.
Statistical Phrase-Based Models For Interactive Computer-Assisted Translation.
Obtaining high-quality machine translations is still a long way off.
A postediting phase is required to improve the output of a machine translation system.
An alternative is the so called computerassisted translation.
In this framework, a human translator interacts with the system in order to obtain high-quality translations.
A statistical phrase-based approach to computer-assisted translation is described in this article.
A new decoder algorithm for interactive search is also presented, that combines monotone and nonmonotone search.
The system has been assessed in the TransType-2 project for the translation of several printer manuals, from -LRB- to -RRB- English to -LRB- from -RRB- Spanish, German and French.
Multi-View Co-Training of Transliteration Model.
This paper discusses a new approach to training of transliteration model from unlabeled data for transliteration extraction.
We start with an inquiry into the formulation of transliteration model by considering different transliteration strategies as a multi-view problem, where each view exploits a natural division of transliteration features, such as phonemebased, grapheme-based or hybrid features.
Then we introduce a multi-view Cotraining algorithm, which leverages compatible and partially uncorrelated information across different views to effectively boost the model from unlabeled data.
Applying this algorithm to transliteration extraction, the results show that it not only circumvents the need of data labeling, but also achieves performance close to that of supervised learning, where manual labeling is required for all training samples.
Relative Compositionality of Multi-word Expressions : A Study of Verb-Noun -LRB- V-N -RRB- Collocations.
Recognition of Multi-word Expressions -LRB- MWEs -RRB- and their relative compositionality are crucial to Natural Language Processing.
Various statistical techniques have been proposed to recognize MWEs.
In this paper, we integrate all the existing statistical features and investigate a range of classifiers for their suitability for recognizing the non-compositional Verb-Noun -LRB- V-N -RRB- collocations.
In the task of ranking the V-N collocations based on their relative compositionality, we show that the correlation between the ranks computed by the classifier and human ranking is significantly better than the correlation between ranking of individual features and human ranking.
We also show that the properties ` Distributed frequency of object ' -LRB- as defined in -LRB- 27 -RRB- -RRB- and ` Nearest Mutual Information ' -LRB- as adapted from -LRB- 18 -RRB- -RRB- contribute greatly to the recognition of the non-compositional MWEs of the V-N type and to the ranking of the V-N collocations based on their relative compositionality.
Using A Hybrid System Of Corpus - And Knowledge-Based Techniques To Automate The Induction Of A Lexical Sublanguage Grammar.
Porting a Natural Language Processing -LRB- NLP -RRB- system to a new donmin renmins one of the bottlenecks in syntactic parsing, because of the amount of effort required to fix gaps in the lexicon, and to attune the existing grammar to the idiosyncracics of the new sublanguage.
This paper shows how thc process of fitting a lexicalizcd grammar to a domain can be automated to a great extent by using a hybrid system that combines traditimml knowledgebased techniques with a corpus-based approach.
Bootstrapping Spoken Dialog Systems With Data Reuse.
Building natural language spoken dialog systems requires large amounts of human transcribed and labeled speech utterances to reach useful operational service performances.
Furthermore, the design of such complex systems consists of several manual steps.
The User Experience -LRB- UE -RRB- expert analyzes and de nes by hand the system core functionalities : the system semantic scope -LRB- call-types -RRB- and the dialog manager strategy which will drive the human-machine interaction.
This approach is extensive and error prone since it involves several non-trivial design decisions that can only be evaluated after the actual system deployment.
Moreover, scalability is compromised by time, costs and the high level of UE know-how needed to reach a consistent design.
We propose a novel approach for bootstrapping spoken dialog systems based on reuse of existing transcribed and labeled data, common reusable dialog templates and patterns, generic language and understanding models, and a consistent design process.
We demonstrate that our approach reduces design and development time while providing an effective system without any application speci c data.
Shared Preferences.
This paper attempts to develop a theory of heuristics or preferences that can be shared between understanding and generation systems.
We first develop a formal analysis of preferences and consider the relation between their uses in generation and understanding.
We then present a bidirectional algorithm for applying them and examine typical heuristics for lexical choice, scope and anaphora in :, more detail.
Exploiting Headword Dependency And Predictive Clustering For Language Modeling.
This paper presents several practical ways of incorporating linguistic structure into language models.
A headword detector is first applied to detect the headword of each phrase in a sentence.
A permuted headword trigram model -LRB- PHTM -RRB- is then generated from the annotated corpus.
Finally, PHTM is extended to a cluster PHTM -LRB- C-PHTM -RRB- by defining clusters for similar words in the corpus.
We evaluated the proposed models on the realistic application of Japanese Kana-Kanji conversion.
Experiments show that C-PHTM achieves 15 % error rate reduction over the word trigram model.
This demonstrates that the use of simple methods such as the headword trigram and predictive clustering can effectively capture long distance word dependency, and substantially outperform a word trigram model.
Automatic Slide Generation Based on Discourse Structure Analysis.
In this paper, we describe a method of automatically generating summary slides from a text.
The slides are generated by itemizing topic\/non-topic parts that are extracted from the text based on syntactic\/case analysis.
The indentations of the items are controlled according to the discourse structure, which is detected by cue phrases, identi cation of word chain and similarity between two sentences.
Our experiments demonstrates generated slides are far easier to read in comparison with original texts.
Spelling-Checking For Highly Inflective Languages.
Spelling-checkers have become an integral part of most text processing software.
From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words.
This approach is sufficient for languages with little inflection such as English, but fails for highly inflective languages such as Czech, Russian, Slovak or other Slavonic languages.
We have developed a special method for describing inflection for the purpose of building spelling-checkers for such languages.
The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy, whereas the number of recognized word forms exceeds 6 million -LRB- for Czech -RRB-.
Further, a special method has been developed for easy word classification.
Alternative Approaches For Generating Bodies Of Grammar Rules.
We compare two approaches for describing and generating bodies of rules used for natural language parsing.
In today 's parsers rule bodies do not exist a priori but are generated on the fly, usually with methods based on n-grams, which are one particular way of inducing probabilistic regular languages.
We compare two approaches for inducing such languages.
One is based on n-grams, the other on minimization of the Kullback-Leibler divergence.
The inferred regular languages are used for generating bodies of rules inside a parsing procedure.
We compare the two approaches along two dimensions : the quality of the probabilistic regular language they produce, and the performance of the parser they were used to build.
The second approach outperforms the first one along both dimensions.
Using Machine Translation Evaluation Techniques to Determine Sentence-level Semantic Equivalence.
The task of machine translation -LRB- MT -RRB- evaluation is closely related to the task of sentence-level semantic equivalence classification.
This paper investigates the utility of applying standard MT evaluation methods -LRB- BLEU, NIST, WER and PER -RRB- to building classifiers to predict semantic equivalence and entailment.
We also introduce a novel classification method based on PER which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence.
Our results show that MT evaluation techniques are able to produce useful features for paraphrase classification and to a lesser extent entailment.
Our technique gives a substantial improvement in paraphrase classification accuracy over all of the other models used in the experiments.
Parsing Word-Aligned Parallel Corpora In A Grammar Induction Context.
We present an Earley-style dynamic programming algorithm for parsing sentence pairs from a parallel corpus simultaneously, building up two phrase structure trees and a correspondence mapping between the nodes.
The intended use of the algorithm is in bootstrapping grammars for less studied languages by using implicit grammatical information in parallel corpora.
Therefore, we presuppose a given -LRB- statistical -RRB- word alignment underlying in the synchronous parsing task ; this leads to a significant reduction of the parsing complexity.
The theoretical complexity results are corroborated by a quantitative evaluation in which we ran an implementation of the algorithm on a suite of test sentences from the Europarl parallel corpus.
REFTEX - A Context-Based Translation Aid.
The system presented in this paper produces bilingual passages of text from an original -LRB- source -RRB- text and one -LRB- or more -RRB- of its translated versions.
The source text passage includes words or word compounds which a translator wants to retrieve for the current translating of another text.
The target text passage is the equivalent version of the source text passage.
On the basis of a comparison of the contexts of these words in the concorded passage and his own text, the translator has to decide on the utility of the translation proposed in the target text passage.
The program might become a component of translator 's work bench.
Supervised Grammar Induction Using Training Data With Limited Constituent Information.
Corpus-based grammar induction generally relies on hand-parsed training data to learn the structure of the language.
Unfortunately, the cost of building large annotated corpora is prohibitively expensive.
This work aims to improve the induction strategy when there are few labels in the training data.
We show that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses.
They account for only 20 % of all constituents.
For inducing grammars from sparsely labeled training data -LRB- e.g., only higher-level constituent labels -RRB-, we propose an adaptation strategy, which produces grammars that parse almost as well as grammars induced from fully labeled corpora.
Our results suggest that for a partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases.
Active Learning for Word Sense Disambiguation with Methods for Addressing the Class Imbalance Problem.
In this paper, we analyze the effect of resampling techniques, including undersampling and over-sampling used in active learning for word sense disambiguation -LRB- WSD -RRB-.
Experimental results show that under-sampling causes negative effects on active learning, but over-sampling is a relatively good choice.
To alleviate the withinclass imbalance problem of over-sampling, we propose a bootstrap-based oversampling -LRB- BootOS -RRB- method that works better than ordinary over-sampling in active learning for WSD.
Finally, we investigate when to stop active learning, and adopt two strategies, max-confidence and min-error, as stopping conditions for active learning.
According to experimental results, we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions.
Markov Random Field Based English Part-Of-Speech Tagging System.
Probabilistic models have been widely used for natural language processing.
Part-of-speech tagging, which assigns the most likely tag to each word in a given sentence, is one.
of tire problems which can be solved by statisticM approach.
Many researchers haw ~ tried to solve the problem by hidden Marker model -LRB- HMM -RRB-, which is well known as one of the statistical models.
But it has many difficulties : integrating heterogeneous information, coping with data sparseness prohlem, and adapting to new environments.
In this paper, we propose a Markov radom field -LRB- MRF -RRB- model based approach to the tagging problem.
The MRF provides the base frame to combine various statistical information with maximum entropy -LRB- ME -RRB- method.
As Gibbs distribution can be used to describe a posteriori probability of tagging, we use it in ma.
ximum a posteriori -LRB- MAP -RRB- estimation of optimizing process.
Besides, several tagging models are developed to show the effect of adding information.
Experimental results show that the performance of the tagger gets improved as we add more statistical information, and that Mt -LCB- F-based tagging model is better than ttMM based tagging model in data sparseness problem.
Positioning for Conceptual Development using Latent Semantic Analysis.
With increasing opportunities to learn online, the problem of positioning learners in an educational network of content offers new possibilities for the utilisation of geometry-based natural language processing techniques.
In this article, the adoption of latent semantic analysis -LRB- LSA -RRB- for guiding learners in their conceptual development is investigated.
We propose five new algorithmic derivations of LSA and test their validity for positioning in an experiment in order to draw back conclusions on the suitability of machine learning from previously accredited evidence.
Special attention is thereby directed towards the role of distractors and the calculation of thresholds when using similarities as a proxy for assessing conceptual closeness.
Results indicate that learning improves positioning.
Distractors are of low value and seem to be replaceable by generic noise to improve threshold calculation.
Furthermore, new ways to flexibly calculate thresholds could be identified.
Reversibility In A Constraint And Type Based Logic Grammar : Application To Secondary Predication.
In this document, we present a formalism for natural language processing which associates type construction principles to constraint logic programming.
We show that it provides more uniform, expressive and efficient tools for parsing and generating language.
Next, we present two abstract machines which enable us to design, in a symmetric way, a parser and a generator from that formalism.
This abstract machinery is then exemplified by a detailed study of secondary predication within the framework of a principledbased description of language : Government and Binding theory.
Inducing Search Keys for Name Filtering.
This paper describes ETK -LRB- Ensemble of Transformation-based Keys -RRB- a new algorithm for inducing search keys for name filtering.
ETK has the low computational cost and ability to filter by phonetic similarity characteristic of phonetic keys such as Soundex, but is adaptable to alternative similarity models.
The accuracy of ETK in a preliminary empirical evaluation suggests that it is well-suited for phonetic filtering applications such as recognizing alternative cross-lingual transliterations.
Machine Translation Based On NLG From XML-DB.
The purpose of this study is to propose a new method for machine translation.
Wehave proceeded through with two projects for report generation -LRB- Kittredge and Polguere, 2000 -RRB- : Weather Forecast and Monthly Economic Report to be produced in four languages : English, Japanese, French, and German.
Their input data is stored in XML-DB.
We applied a three-stage pipelined architecture -LRB- Reiter and Dale, 2000 -RRB-, and each stage was implemented as XML transformation processes.
Weregard XML stored data as language-neutral intermediate form and employ the so-called ` sublanguage approach ' -LRB- Somers, 2000 -RRB-.
The machine translation process is implemented via XMLDB as a kind of interlingua approach instead of the conventional structure transfer approach.
Capturing Out-Of-Vocabulary Words In Arabic Text.
The increasing flow of information between languages has led to a rise in the frequency of non-native or loan words, where terms of one language appear transliterated in another.
Dealing with such out of vocabulary words is essential for successful cross-lingual information retrieval.
For example, techniques such as stemming should not be applied indiscriminately to all words in a collection, and so before any stemming, foreign words need to be identified.
In this paper, we investigate three approaches for the identification of foreign words in Arabic text : lexicons , language patterns , and n-grams and present that results show that lexicon-based approaches outperform the other techniques.
Advanced Human-Computer Interface And Voice Processing Applications In Space.
Much interest already exists in the electronics research community for developing and integrating speech technology to a variety of applications, ranging from voice-activated systems to automatic telephone transactions.
This interest is particularly true in the field of aerospace where the training and operational demands on the crew have significantly increased with the proliferation of technology.
Indeed, with advances in vehicule and robot automation, the role of the human operator has evolved from that of pilot\/driver and manual controller to supervisor and decision maker.
Lately, some effort has been expended to implement alternative modes of system control, but automatic speech recognition -LRB- ASR -RRB- and human-computer interaction -LRB- HCI -RRB- research have only recently extended to civilian aviation and space applications.
The purpose of this paper is to present the particularities of operator-computer interaction in the unique conditions found in space.
The potential for voice control applications inside spacecraft is outlined and methods of integrating spoken-language interfaces onto operational space systems are suggested.
Word Sense Disambiguation Criteria : A Systematic Study.
This article describes the results of a systematic indepth study of the criteria used for word sense disambiguation.
Our study is based on 60 target words : 20 nouns, 20 adjectives and 20 verbs.
Our results are not always in line with some practices in the field.
For example, we show that omitting noncontent words decreases performance and that bigrams yield better results than unigrams.
Application Of Analogical Modelling To Example Based Machine Translation.
This paper describes a self-modelling , incremental algorithm for learning translation rules from existing bilingual corpora.
The notions of supracontext and subcontext are extended to encompass bilingual information through simultaneous analogy on both source and target sentences and juxtaposition of corresponding results.
Analogical modelling is performed during the learning phase and translation patterns are projected in a multi-dimensional analogical network.
The proposed fi'amework was evaluated on a small training corpus providing promising results.
Suggestions to improve system performance are
Outline Of The International Standard Linguistic Annotation Framework.
This paper describes the outline of a linguistic annotation framework under development by ISO TC37 SC WG1-1.
This international standard provides an architecture for the creation, annotation, and manipulation of linguistic resources and processing software.
The goal is to provide maximum flexibility for encoders and annotators, while at the same time enabling interchange and re-use of annotated linguistic resources.
We describe here the outline of the standard for the purposes of enabling annotators to begin to explore how their schemes may map into the framework.
Speech to speech machine translation : Biblical chatter from Finnish to English.
Speech-to-speech machine translation is in some ways the peak of natural language processing, in that it deals directly with our original, oral mode of communication -LRB- as opposed to derived written language -RRB-.
As such, it presents challenges that are not to be taken lightly.
Although existing technology covers each of the steps in the process, from speech recognition to synthesis, deriving a model of translation that is effective in the domain of spoken language is an interesting and challenging task.
If we could teach our algorithms to learn as children acquire language, the result would be useful both for language technology and cognitive science.
We propose several potential approaches, an implementation of a multi-path model that translates recognized morphemes alongside words,andaweb-interfacetotestourspeech translation tool as trained for Finnish to English.
We also discuss current approaches to machine translation and the problems they face in adapting simultaneously to morphologically rich languages and to the spoken modality.
A New Method for Sentiment Classification in Text Retrieval.
Traditional text categorization is usually a topic-based task, but a subtle demand on information retrieval is to distinguish between positive and negative view on text topic.
In this paper, a new method is explored to solve this problem.
Firstly, a batch of Concerned Concepts in the researched domain is predefined.
Secondly, the special knowledge representing the positive or negative context of these concepts within sentences is built up.
At last, an evaluating function based on the knowledge is defined for sentiment classification of free text.
We introduce some linguistic knowledge in these procedures to make our method effective.
As a result, the new method proves better compared with SVM when experimenting on Chinese texts about a certain topic.
Machine Learning Methods For Chinese Web Page Categorization.
This paper reports our evaluation of k Nearest Neighbor -LRB- kNN -RRB-, Support Vector Machines -LRB- SVM -RRB-, and Adaptive Resonance Associative Map -LRB- ARAM -RRB- on Chinese web page classification.
Benchmark experiments based on a Chinese web corpus showed that their predictive performance were roughly comparable although ARAM and kNN slightly outperformed SVM in small categories.
In addition, inserting rules into ARAM helped to improve performance, especially for small welldefined categories.
FLSA : Extending Latent Semantic Analysis With Features For Dialogue Act Classification.
We discuss Feature Latent Semantic Analysis -LRB- FLSA -RRB-, an extension to Latent Semantic Analysis -LRB- LSA -RRB-.
LSA is a statistical method that is ordinarily trained on words only ; FLSA adds to LSA the richness of the many other linguistic features that a corpus may be labeled with.
We applied FLSA to dialogue act classification with excellent results.
We report results on three corpora : CallHome Spanish, MapTask, and our own corpus of tutoring dialogues.
Data-Driven Classification Of Linguistic Styles In Spoken Dialogues.
Language users have individual linguistic styles.
A spoken dialogue system may benefit from adapting to the linguistic style of a user in input analysis and output generation.
To investigate the possibility to automatically classify speakers according to their linguistic style three corpora of spoken dialogues were analyzed.
Several numerical parameters were computed for every speaker.
These parameters were reduced to linguistically interpretable components by means of a principal component analysis.
Classes were established from these components by cluster analysis.
Unseen input was classified by trained neural networks with varying error rates depending on corpus type.
A first investigation in using special language models for speaker classes was carried out.
Cluster-Based Query Expansion for Statistical Question Answering.
Document retrieval is a critical component of question answering -LRB- QA -RRB-, yet little work has been done towards statistical modeling of queries and towards automatic generation of high quality query content for QA.
This paper introduces a new, cluster-based query expansion method that learns queries known to be successful when applied to similar questions.
We show that cluster-based expansion improves the retrieval performance of a statistical question answering system when used in addition to existing query expansion methods.
This paper presents experiments with several feature selection methods used individually and in combination.
We show that documents retrieved using the cluster-based approach are inherently different than documents retrieved using existing methods and provide a higher data diversity to answers extractors.
Translating Lexical Semantic Relations : The First Step Towards Multilingual Wordnets.
Establishing correspondences between wordnets of different languages is essential to both multilingual knowledge processing and for bootstrapping wordnets of low-density languages.
We claim that such correspondences must be based on lexical semantic relations, rather than top ontology or word translations.
In particular, we define a translation equivalence relation as a bilingual lexical semantic relation.
Such relations can then be part of a logical entailment predicting whether source language semantic relations will hold in a target language or not.
Our claim is tested with a study of 210 Chinese lexical lemmas and their possible semantic relations links bootstrapped from the Princeton WordNet.
The results show that lexical semantic relation translations are indeed highly precise when they are logically inferable.
Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction.
This paper explores the relationship between various measures of unsupervised part-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
Word Sense Disambiguation Using Sense Examples Automatically Acquired From A Second Language.
We present a novel almost-unsupervised approach to the task of Word Sense Disambiguation -LRB- WSD -RRB-.
We build sense examples automatically, using large quantities of Chinese text, and English-Chinese and Chinese-English bilingual dictionaries, taking advantage of the observation that mappings between words and meanings are often different in typologically distant languages.
We train a classifier on the sense examples and test it on a gold standard English WSD dataset.
The evaluation gives results that exceed previous state-of-the-art results for comparable systems.
We also demonstrate that a little manual effort can improve the quality of sense examples, as measured by WSD accuracy.
The performance of the classifier on WSD also improves as the number of training sense examples increases.
Lexical Discrimination With The Italian Version Of WordNet.
We present a prototype of the Italian version of WORDNET, a general computational lexical resource.
Some relevant extensions are discussed to make it usable for parsing : in particular we add verbal selectional restrictions to make lexical discrimination effective.
Italian WORDNET has been coupled with a parser and a number of experiments have been performed to individuate the methodology with the best trade-off between disambiguation rate and precision.
Results confirm intuitive hypothesis on the role of selectional restrictions and show evidences for a WORDNET-Iike organization of lexical senses.
A Japanese Predicate Argument Structure Analysis using Decision Lists.
This paper describes a new automatic method for Japanese predicate argument structure analysis.
The method learns relevant features to assign case roles to the argument of the target predicate using the features of the words located closest to the target predicate under various constraints such as dependency types , words , semantic categories , parts of speech , functional words and predicate voices.
We constructed decision lists in which these featuresweresortedbytheirlearnedweights.
Using our method, we integrated the tasks of semantic role labeling and zero-pronoun identification, and achieved a 17 % improvement compared with a baseline method in a sentence level performance analysis.
FastSum : Fast and Accurate Query-based Multi-document Summarization.
Wepresentafastquery-basedmulti-document summarizer called FastSum based solely on word-frequency features of clusters, documents and topics.
Summary sentences are ranked by a regression SVM.
The summarizer does not use any expensive NLP techniques such as parsing, tagging of names or even part of speech information.
Still, the achieved accuracy is comparable to the best systems presented in recent academic competitions -LRB- i.e., Document Understanding Conference -LRB- DUC -RRB- -RRB-.
Because of a detailed feature analysis using Least Angle Regression -LRB- LARS -RRB-, FastSum can rely on a minimal set of featuresleading tofastprocessingtimes : 1250 news documents in 60 seconds.
Lattice-Based Search For Spoken Utterance Retrieval.
Recent work on spoken document retrieval has suggested that it is adequate to take the singlebest output of ASR, and perform text retrieval on this output.
This is reasonable enough for the task of retrieving broadcast news stories, where word error rates are relatively low, and the stories are long enough to contain much redundancy.
But it is patently not reasonable if one 's task is to retrieve a short snippet of speech in a domain where WER 's can be as high as 50 % ; such would be the situation with teleconference speech, where one 's task is to find if and when a participant uttered a certain phrase.
In this paper we propose an indexing procedure for spoken utterance retrieval that works on lattices rather than just single-best text.
We demonstrate that this procedure can improve F scores by over five points compared to singlebest retrieval on tasks with poor WER and low redundancy.
The representation is flexible so that we can represent both word lattices, as well as phone lattices, the latter being important for improving performance when searching for phrases containing OOV words.
